{"page": 1, "text": "Dense Passage Retrieval for Open-Domain Question Answering\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "Vladimir Karpukhin, Barlas Oğuz, Sewon Min†, Patrick Lewis,\nLedell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih\nFacebook AI *University of Washington #Princeton University\n{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "Abstract\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "Open-domain question answering relies on ef-\nficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented us-\ning dense representations alone, where em-\nbeddings are learned from a small number\nof questions and passages by a simple dual-\nencoder framework. When evaluated on a\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong Lucene-\nBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks.¹\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "1 Introduction\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "Open-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions us-\ning a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), inter alia), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context retriever first\nselects a small subset of passages where some\nof them contain the answer to the question, and\nthen (2) a machine reader can thoroughly exam-\nine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\nopen-domain QA to machine reading is a very rea-\nsonable strategy, a huge performance degradation\nis often observed in practice², indicating the needs\nof improving retrieval.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "*Equal contribution\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "The code and trained models have been released at\nhttps://github.com/facebookresearch/DPR.\n²For instance, the exact match score on SQUAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "Retrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords effi-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense, latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n\"Who is the bad guy in lord of the rings?\", which can\nbe answered from the context \"Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy.\" A term-based system would\nhave difficulty retrieving such a context, while\na dense retrieval system would be able to better\nmatch \"bad guy” with “villain\" and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional flexibility to have a task-specific\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefficiently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)).\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "6769\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "However, it is generally believed that learn-\ning a good dense vector representation needs a\nlarge number of labeled pairs of question and con-\ntexts. Dense retrieval methods have thus never\nbe shown to outperform TF-IDF/BM25 for open-\ndomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\nobjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then fine-\ntuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6769-6781,\nNovember 16-20, 2020. 2020 Association for Computational Linguistics\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "QA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot fine-tuned using pairs of questions and answers,\nthe corresponding representations could be subop-\ntimal.\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), with-\nout additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\nof question and passage pairs. Through a series\nof careful ablation studies, our final solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective compar-\ning all pairs of questions and passages in a batch.\nOur Dense Passage Retriever (DPR) is exception-\nally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\nOur contributions are twofold. First, we demon-\nstrate that with the proper training setup, sim-\nply fine-tuning the question and passage encoders\non existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\nopen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve compara-\nble or better results on multiple QA datasets in the\nopen-retrieval setting, compared to several, much\ncomplicated systems.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "2 Background\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "The problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \"Who first voiced Meg on Family\nGuy?\" or \"Where was the 8th Dalai Lama born?\", a\nsystem is required to answer it using a large corpus\nof diversified topics. More specifically, we assume\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "6770\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "the extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more pas-\nsages in the corpus. Assume that our collection\ncontains D documents, d1, d2,...,dD. We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units³ and get M\ntotal passages in our corpus C {P1, P2, ..., PM},\nwhere each passage p; can be viewed as a sequence\n(2). Given a question q,\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "of tokens w(i), w(i),\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": ",\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "...\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "\"\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "We\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "|pi|\nthe task is to find a span ws\n(\ni) (i)\nWS+1\"\"\nfrom\none of the passages p¿ that can answer the question.\nNotice that to cover a wide variety of domains, the\ncorpus size can easily range from millions of docu-\nments (e.g., Wikipedia) to billions (e.g., the Web).\nAs a result, any open-domain QA system needs to\ninclude an efficient retriever component that can se-\nlect a small set of relevant texts, before applying the\nreader to extract the answer (Chen et al., 2017).4\nFormally speaking, a retriever R : (q,C) → CF\nis a function that takes as input a question q and a\ncorpus C and returns a much smaller filter set of\ntexts CFCC, where |Cƒ| = k < |C|. For a fixed\nk, a retriever can be evaluated in isolation on top-k\nretrieval accuracy, which is the fraction of ques-\ntions for which CF contains a span that answers the\nquestion.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "3 Dense Passage Retriever (DPR)\nWe focus our research in this work on improv-\ning the retrieval component in open-domain QA.\nGiven a collection of M text passages, the goal of\nour dense passage retriever (DPR) is to index all\nthe passages in a low-dimensional and continuous\nspace, such that it can retrieve efficiently the top\nk passages relevant to the input question for the\nreader at run-time. Note that M can be very large\n(e.g., 21 million passages in our experiments, de-\nscribed in Section 4.1) and k is usually small, such\nas 20-100.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "3.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "Overview\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 2, "text": "Our dense passage retriever (DPR) uses a dense\nencoder Ep() which maps any text passage to a d-\ndimensional real-valued vectors and builds an index\nfor all the M passages that we will use for retrieval.\n3 The ideal size and boundary of a text passage are func-\ntions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing fixed-length passages performs better in both retrieval\nand final QA accuracy, as observed by Wang et al. (2019).\n*Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which retrieves and generates the answers, respectively.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "At run-time, DPR applies a different encoder EQ (•)\nthat maps the input question to a d-dimensional\nvector, and retrieves k passages of which vectors\nare the closest to the question vector. We define\nthe similarity between the question and the passage\nusing the dot product of their vectors:\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "sim(q, p) Eq(q)'Ep(p).\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "(1)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "Although more expressive model forms for measur-\ning the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the represen-\ntations of the collection of passages can be pre-\ncomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis dis-\ntance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nfinds other similarity functions perform compara-\nbly (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders.\nEncoders Although in principle the question and\npassage encoders can be implemented by any neu-\nral networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, un-\ncased) and take the representation at the [CLS]\ntoken as the output, so d: == 768.\nInference During inference time, we apply the\npassage encoder Ep to all the passages and index\nthem using FAISS (Johnson et al., 2017) offline.\nFAISS is an extremely efficient, open-source li-\nbrary for similarity search and clustering of dense\nvectors, which can easily be applied to billions of\nvectors. Given a question q at run-time, we derive\nits embedding vq EQ(q) and retrieve the top k\npassages with embeddings closest to vq.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "3.2 Training\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "Training the encoders so that the dot-product sim-\nilarity (Eq. (1)) becomes a good ranking function\nfor retrieval is essentially a metric learning prob-\nlem (Kulis, 2013). The goal is to create a vector\nspace such that relevant pairs of questions and pas-\nsages will have smaller distance (i.e., higher simi-\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "6771\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "larity) than the irrelevant ones, by learning a better\nembedding function.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "+\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "m\ni=1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "Let D = {{qi, Pi¯‚ Pi¸₁‚· · · ‚ Pin)} be the\ntraining data that consists of m instances. Each\ninstance contains one question q; and one relevant\n(positive) passage pit, along with n irrelevant (neg-\native) passages P₁j. We optimize the loss function\nas the negative log likelihood of the positive pas-\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "sage:\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "L(qi, P†‚P₁¸¹¨··‚ Pi¸n)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "― log\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "esim (qi,P)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "esim(qi,p) +1 esim(qi‚Pi,j)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "(2)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "Positive and negative passages For retrieval\nproblems, it is often the case that positive examples\nare available explicitly, while negative examples\nneed to be selected from an extremely large pool.\nFor instance, passages relevant to a question may\nbe given in a QA dataset, or can be found using the\nanswer. All other passages in the collection, while\nnot specified explicitly, can be viewed as irrelevant\nby default. In practice, how to select negative ex-\namples is often overlooked but could be decisive\nfor learning a high-quality encoder. We consider\nthree different types of negatives: (1) Random: any\nrandom passage from the corpus; (2) BM25: top\npassages returned by BM25 which don't contain\nthe answer but match most question tokens; (3)\nGold: positive passages paired with other questions\nwhich appear in the training set. We will discuss the\nimpact of different types of negative passages and\ntraining schemes in Section 5.2. Our best model\nuses gold passages from the same mini-batch and\none BM25 negative passage. In particular, re-using\ngold passages from the same batch as negatives\ncan make the computation efficient while achiev-\ning great performance. We discuss this approach\nbelow.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 3, "text": "In-batch negatives Assume that we have B\nquestions in a mini-batch and each one is asso-\nciated with a relevant passage. Let Q and P be the\n(Bxd) matrix of question and passage embeddings\nin a batch of size B. S = QPT is a (B × B) ma-\ntrix of similarity scores, where each row of which\ncorresponds to a question, paired with B passages.\nIn this way, we reuse computation and effectively\ntrain on B² (qi, pj) question/passage pairs in each\nbatch. Any (qi, pj) pair is a positive example when\ni = j, and negative otherwise. This creates B train-\ning instances in each batch, where there are B 1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "negative passages for each question.\nThe trick of in-batch negatives has been used in\nthe full batch setting (Yih et al., 2011) and more\nrecently for mini-batch (Henderson et al., 2017;\nGillick et al., 2019). It has been shown to be an\neffective strategy for learning a dual-encoder model\nthat boosts the number of training examples.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "4 Experimental Setup\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "In this section, we describe the data we used for\nexperiments and the basic setup.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "4.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Wikipedia Data Pre-processing\nFollowing (Lee et al., 2019), we use the English\nWikipedia dump from Dec. 20, 2018 as the source\ndocuments for answering questions. We first apply\nthe pre-processing code released in DrQA (Chen\net al., 2017) to extract the clean, text-portion of\narticles from the Wikipedia dump. This step re-\nmoves semi-structured data, such as tables, info-\nboxes, lists, as well as the disambiguation pages.\nWe then split each article into multiple, disjoint text\nblocks of 100 words as passages, serving as our\nbasic retrieval units, following (Wang et al., 2019),\nwhich results in 21,015,324 passages in the end.5\nEach passage is also prepended with the title of the\nWikipedia article where the passage is from, along\nwith an [SEP] token.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "4.2\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Question Answering Datasets\nWe use the same five QA datasets and train-\ning/dev/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\nNatural Questions (NQ) (Kwiatkowski et al.,\n2019) was designed for end-to-end question an-\nswering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\nTriviaQA (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\nWebQuestions (WQ) (Berant et al., 2013) consists\nof questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\nCurated TREC (TREC) (Baudiš and Šedivỳ,\n2015) sources questions from TREC QA tracks\n\"However, Wang et al. (2019) also propose splitting docu-\nments into overlapping passages, which we do not find advan-\ntageous compared to the non-overlapping version.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "6772\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Dataset\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Natural Questions\nTriviaQA\nWebQuestions\nCuratedTREC\nSQUAD\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Train\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Dev\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Test\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "79,168 58,880\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "8,757\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "3,610\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "78,785 60,413 8,837\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "11,313\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "3,417 2,474\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "361\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "2,032\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "1,353 1,125\n78,713\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "133\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "694\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "70.096 8,886\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "10,570\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Table 1: Number of questions in each QA dataset. The\ntwo columns of Train denote the original training ex-\namples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "as well as various Web sources and is intended for\nopen-domain QA from unstructured corpora.\nSQUAD v1.1 (Rajpurkar et al., 2016) is a popu-\nlar benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia para-\ngraph, and asked to write questions that could be\nanswered from the given text. Although SQUAD\nhas been used previously for open-domain QA re-\nsearch, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "Selection of positive passages Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA6, we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the ques-\ntion will be discarded. For SQUAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the num-\nber of questions in training/dev/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "5 Experiments: Passage Retrieval\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 4, "text": "In this section, we evaluate the retrieval perfor-\nmance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Ap-\npendix A.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Training Retriever\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "NQ\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Top-20\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Top-100\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "TriviaQA WQ TREC SQUAD NQ TriviaQA WQ TREC SQUAD\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "None\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "BM25\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "59.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "66.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "55.0 70.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "68.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "73.7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "76.7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "71.1 84.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "80.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "78.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "79.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "73.2 79.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "63.2 85.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "85.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "81.4 89.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "77.2\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "BM25 + DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "76.6\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "79.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "71.0 85.2\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "71.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "83.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "84.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "80.5 92.7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "81.3\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "79.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "78.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "75.0 89.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "51.6\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "86.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "84.7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "82.9 93.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "67.6\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Multi\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "BM25 + DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "78.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "79.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "74.7 88.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "66.2\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "83.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "84.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "82.3 94.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "78.6\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved\npassages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQUAD). See text for more details.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "traditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Sec-\ntion 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQUAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10-5 using Adam, linear\nscheduling with warm-up and dropout rate 0.1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be de-\nsirable to obtain a single retriever that works well\nacross the board. To this end, we train a multi-\ndataset encoder by combining training data from\nall datasets excluding SQUAD. In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\nof top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25(q,p) + sim(q, p) as the ranking function.\nWe used \\ = 1.1 based on the retrieval accuracy in\nthe development set.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "5.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": ".\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Main Results\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Table 2 compares different passage retrieval sys-\ntems on five QA datasets, using the top-k accuracy\n(k = {20, 100}). With the exception of SQUAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when k is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\non Natural Questions). When training with mul-\n8SQUAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\nmore in Section 5.1.\n'Lucene implementation. BM25 parameters b = 0.4 (doc-\nument length normalization) and k₁ = 0.9 (term frequency\nscaling) are tuned using development sets.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Top-k accuracy (%)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "90\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "80\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "50\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "40\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "20\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "40\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "BM25\n# Train: 1k\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "# Train: 10k\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "# Train: 20k\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "# Train: 40k\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "# Train: all (59k)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "60\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "80\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "100\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "k: # of retrieved passages\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "Figure 1: Retriever top-k accuracy with different num-\nbers of training examples used in our dense passage re-\ntriever vs BM25. The results are measured on the de-\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "tiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\nWe conjecture that the lower performance on\nSQUAD is due to two reasons. First, the annota-\ntors wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\nonly 500+ Wikipedia articles and thus the distribu-\ntion of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "5.2 Ablation Study on Model Training\nTo understand further how different model training\noptions affect the results, we conduct several addi-\ntional experiments and discuss our findings below.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 5, "text": "6773\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Sample efficiency We explore how many train-\ning examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top-k\nretrieval accuracy with respect to different num-\nbers of training examples, measured on the devel-\nopment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 ex-\namples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question-passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "In-batch negative training We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of-N training set-\nting, where each question in the batch is paired\nwith a positive passage and its own set of n neg-\native passages (Eq. (2)). We find that the choice\nof negatives random, BM25 or gold passages\n(positive passages from other questions) — does\nnot impact the top-k accuracy much in this setting\nwhen k > 20.\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\nor from the whole training set. Effectively, in-batch\nnegative training is an easy and memory-efficient\nway to reuse the negative examples already in the\nbatch rather than creating new ones. It produces\nmore pairs and thus increases the number of train-\ning examples, which might contribute to the good\nmodel performance. As a result, accuracy consis-\ntently improves as the batch size grows.\nFinally, we explore in-batch negative training\nwith additional \"hard\" negative passages that have\nhigh BM25 scores given the question, but do not\ncontain the answer string (the bottom block). These\nadditional passages are used as negative passages\nfor all questions in the same batch. We find that\nadding a single BM25 negative passage improves\nthe result substantially while adding two does not\nhelp further.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Impact of gold passages We use passages that\nmatch the gold contexts in the original datasets\n(when available) as positive examples (Section 4.2).\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Туре\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Gold\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "#N\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "IB Top-5 Top-20 Top-100\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Random\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "X 47.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "64.3\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "77.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "BM25\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "50.0 63.3\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "74.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "✓ 42.6 63.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "78.3\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Gold\nGold\nGold\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "✓\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "51.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "69.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "80.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "31\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "✓\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "52.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "70.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "82.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "127\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "✓\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "55.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "73.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "83.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "65.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "77.3\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "84.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "31+64\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "✓\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "64.5 76.4\n65.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "84.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "78.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "84.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "G.+BM25(1) 31+32 ✓\nG.+BM25\nG.+BM25(1) 127+128 ✓\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Table 3: Comparison of different training schemes,\nmeasured as top-k retrieval accuracy on Natural Ques-\ntions (development set). #N: number of negative\nexamples, IB: in-batch training. G.+BM25 (¹) and\nG.+BM25 (2) denote in-batch training with 1 or 2 ad-\nditional BM25 negatives, which serve as negative pas-\nsages for all questions in the batch.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Our experiments on Natural Questions show that\nswitching to distantly-supervised passages (using\nthe highest-ranked BM25 passage that contains the\nanswer), has only a small impact: 1 point lower\ntop-k accuracy for retrieval. Appendix A contains\nmore details.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "6774\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 6, "text": "Similarity and loss Besides dot product, cosine\nand Euclidean L2 distance are also commonly used\nas decomposable similarity functions. We test these\nalternatives and find that L2 performs compara-\nble to dot product, and both of them are superior\nto cosine. Similarly, in addition to negative log-\nlikelihood, a popular option for ranking is triplet\nloss, which compares a positive passage and a nega-\ntive one directly with respect to a question (Burges\net al., 2005). Our experiments show that using\ntriplet loss does not affect the results much. More\ndetails can be found in Appendix B.\nCross-dataset generalization One interesting\nquestion regarding DPR's discriminative training\nis how much performance degradation it may suf-\nfer from a non-iid setting. In other words, can\nit still generalize well when directly applied to\na different dataset without additional fine-tuning?\nTo test the cross-dataset generalization, we train\nDPR on Natural Questions only and test it directly\non the smaller WebQuestions and CuratedTREC\ndatasets. We find that DPR generalizes well, with\n3-5 points loss from the best performing fine-tuned\nmodel in top-20 retrieval accuracy (69.9/86.3 vs.\n75.0/89.1 for WebQuestions and TREC, respec-\ntively), while still greatly outperforming the BM25\nbaseline (55.0/70.9).\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "5.3 Qualitative Analysis\nAlthough DPR performs better than BM25 in gen-\neral, passages retrieved by these two methods dif-\nfer qualitatively. Term-matching methods like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\nor semantic relationships better. See Appendix C\nfor examples and more discussion.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "5.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "Run-time Efficiency\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "The main reason that we require a retrieval compo-\nnent for open-domain QA is to reduce the number\nof candidate passages that the reader needs to con-\nsider, which is crucial for answering user's ques-\ntions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors 10,\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 pas-\nsages per question. In contrast, BM25/Lucene (im-\nplemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Com-\nputing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\non a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "6 Experiments: Question Answering\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "In this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "6.1 End-to-end QA System\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA sys-\ntem consists of a neural reader that outputs the\nanswer to the question. Given the top k retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth =200, search depth = 128.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "6775\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "score is chosen as the final answer. The passage\nselection model serves as a reranker through cross-\nattention between the question and the passage. Al-\nthough cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its non-\ndecomposable nature, it has more capacity than the\ndual-encoder model sim(q, p) as in Eq. (1). Apply-\ning it to selecting the passage from a small number\nof retrieved candidates has been shown to work\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let P¿ € R¹×h (1 ≤ i ≤ k) be\na BERT (base, uncased in our experiments) rep-\nresentation for the i-th passage, where L is the\nmaximum length of the passage and h the hidden\ndimension. The probabilities of a token being the\nstarting/ending positions of an answer span and a\npassage being selected are defined as:\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "Pstart,i\nstart,i \n(s)\nPend,i(t)\nPselected (i)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "where p\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "W\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "(3)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "softmax (Pi\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "(start) s'\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "softmax (P₂Wend) +\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "(4)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "softmax (PTW selected),, (5)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "[CLS]\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "k\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "[P 1,..., P[CLS)]] = Rhxk and\nW start, Wend, Wselected Є Rh are learnable vectors.\nWe compute a span score of the s-th to t-th words\nfrom the i-th passage as Pstart, i (s) × Pend,i(t), and\na passage selection score of the i-th passage as\nPselected (i).\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "During training, we sample one positive and\nm 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. m is a hyper-parameter and we\nuse m\n24 in all the experiments. The training ob-\njective is to maximize the marginal log-likelihood\nof all the correct answer spans in the positive pas-\nsage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\nof the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQUAD)\nand 4 for small (TREC, WQ) datasets, and tune k\non the development set. For experiments on small\ndatasets under the Multi setting, in which using\nother datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "6.2 Results\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 7, "text": "Table 4 summarizes our final end-to-end QA re-\nsults, measured by exact match with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Training Model\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "NQ TriviaQA WQ TREC SQUAD\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "BM25+BERT (Lee et al., 2019)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "26.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "47.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "17.7 21.3\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "33.2\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "ORQA (Lee et al., 2019)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "33.3\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "45.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "36.4 30.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "20.2\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "HardEM (Min et al., 2019a)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "28.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "50.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "GraphRetriever (Min et al., 2019b)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "34.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "56.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "36.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "PathRetriever (Asai et al., 2020)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "32.6\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "56.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "REALMWiki (Guu et al., 2020)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "39.2\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "40.2 46.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "REALMNews (Guu et al., 2020)\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "40.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "-\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "40.7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "42.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "BM25\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "32.6\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "52.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "29.9 24.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "38.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Single\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "41.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "56.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "34.6 25.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "29.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "BM25+DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "39.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "57.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "35.2 28.0\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "36.7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "41.5\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "56.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "42.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "49.4\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "24.1\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Multi\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "BM25+DPR\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "38.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "57.9\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "41.1 50.6\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "35.8\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Table 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers.\nREALMwiki and REALM News are the same model but pretrained on Wikipedia and CC-News, respectively. Single\nand Multi denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets\n(all except SQUAD). For WQ and TREC in the Multi setting, we fine-tune the reader trained on NQ.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "see that higher retriever accuracy typically leads to\nbetter final QA results: in all cases except SQUAD,\nanswers extracted from the passages retrieved by\nDPR are more likely to be correct, compared to\nthose from BM25. For large datasets like NQ and\nTriviaQA, models trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\non smaller datasets like WQ and TREC, the multi-\ndataset setting has a clear advantage. Overall, our\nDPR-based models outperform the previous state-\nof-the-art results on four out of the five datasets,\nwith 1% to 12% absolute differences in exact match\naccuracy. It is interesting to contrast our results to\nthose of ORQA (Lee et al., 2019) and also the\nconcurrently developed approach, REALM (Guu\net al., 2020). While both methods include addi-\ntional pretraining tasks and employ an expensive\nend-to-end training regime, DPR manages to out-\nperform them on both NQ and TriviaQA, simply\nby focusing on learning a strong passage retrieval\nmodel using pairs of questions and answers. The\nadditional pretraining tasks are likely more useful\nonly when the target training sets are small. Al-\nthough the results of DPR on WQ and TREC in the\nsingle-dataset setting are less competitive, adding\nmore question-answer pairs helps boost the perfor-\nmance, achieving the new state of the art.\nTo compare our pipeline training approach with\njoint learning, we run an ablation on Natural Ques-\ntions where the retriever and reader are jointly\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "6776\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "trained, following Lee et al. (2019). This approach\nobtains a score of 39.8 EM, which suggests that our\nstrategy of training a strong retriever and reader in\nisolation can leverage effectively available supervi-\nsion, while outperforming a comparable joint train-\ning approach with a simpler design (Appendix D).\nOne thing worth noticing is that our reader does\nconsider more passages compared to ORQA, al-\nthough it is not completely clear how much more\ntime it takes for inference. While DPR processes\nup to 100 passages for each question, the reader\nis able to fit all of them into one batch on a sin-\ngle 32GB GPU, thus the latency remains almost\nidentical to the single passage case (around 20ms).\nThe exact impact on throughput is harder to mea-\nsure: ORQA uses 2-3x longer passages compared\nto DPR (288 word pieces compared to our 100\ntokens) and the computational complexity is super-\nlinear in passage length. We also note that we\nfound k 50 to be optimal for NQ, and k\nleads to only marginal loss in exact match accu-\nracy (40.8 vs. 41.5 EM on NQ), which should be\nroughly comparable to ORQA's 5-passage setup.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "7\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Related Work\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "= 10\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 8, "text": "Passage retrieval has been an important compo-\nnent for open-domain QA (Voorhees, 1999). It\nnot only effectively reduces the search space for\nanswer extraction, but also identifies the support\ncontext for users to verify the answer. Strong sparse\nvector space models like TF-IDF or BM25 have\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 9, "text": "been used as the standard method applied broadly\nto various QA tasks (e.g., Chen et al., 2017; Yang\net al., 2019a,b; Nie et al., 2019; Min et al., 2019a;\nWolfson et al., 2020). Augmenting text-based re-\ntrieval with external structured information, such\nas knowledge graph and Wikipedia hyperlinks, has\nalso been explored recently (Min et al., 2019b; Asai\net al., 2020).\nThe use of dense vector representations for re-\ntrieval has a long history since Latent Semantic\nAnalysis (Deerwester et al., 1990). Using labeled\npairs of queries and documents, discriminatively\ntrained dense encoders have become popular re-\ncently (Yih et al., 2011; Huang et al., 2013; Gillick\net al., 2019), with applications to cross-lingual\ndocument retrieval, ad relevance prediction, Web\nsearch and entity retrieval. Such approaches com-\nplement the sparse vector methods as they can po-\ntentially give high similarity scores to semantically\nrelevant text pairs, even without exact token match-\ning. The dense representation alone, however, is\ntypically inferior to the sparse one. While not the\nfocus of this work, dense representations from pre-\ntrained models, along with cross-attention mecha-\nnisms, have also been shown effective in passage\nor dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\nof the BERT encoders.\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to re-\ntrieve relevant passages iteratively using reformu-\nlated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) pro-\npose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input ques-\ntions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\noutperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages dur-\ning training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 9, "text": "6777\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 9, "text": "effective solution that shows stronger empirical per-\nformance, without relying on additional pretraining\nor complex joint training schemes.\nDPR has also been used as an important mod-\nule in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the pre-\nvious iteration to discover new negatives and con-\nstruct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raf-\nfel et al., 2019), achieving good performance on\nopen-domain QA and other knowledge-intensive\ntasks.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 9, "text": "8 Conclusion\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 9, "text": "In this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the tradi-\ntional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\nour empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional val-\nues. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\nopen-domain question answering benchmarks.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 9, "text": "Acknowledgments\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 9, "text": "We thank the anonymous reviewers for their helpful\ncomments and suggestions.\n", "metadata": {"pdf_path": "data/raw/papers/2020.emnlp-main.550.pdf", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "date": "11/16/2020", "type": "pdf", "author": "Karpukhin et al", "reference_page": 9}}
{"page": 1, "text": "Searching for Best Practices in Retrieval-Augmented Generation\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu,\nZhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li,\nQi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng*, Xuanjing Huang\nSchool of Computer Science, Fudan University, Shanghai, China\nShanghai Key Laboratory of Intelligent Information Processing\n{xiaohuawang22}@m. fudan.edu.cn\n{zhengxq, xjhuang}@fudan.edu.cn\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "Abstract\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "Retrieval-augmented generation (RAG) tech-\nniques have proven to be effective in integrat-\ning up-to-date information, mitigating halluci-\nnations, and enhancing response quality, par-\nticularly in specialized domains. While many\nRAG approaches have been proposed to en-\nhance large language models through query-\ndependent retrievals, these approaches still suf-\nfer from their complex implementation and pro-\nlonged response times. Typically, a RAG work-\nflow involves multiple processing steps, each of\nwhich can be executed in various ways. Here,\nwe investigate existing RAG approaches and\ntheir potential combinations to identify opti-\nmal RAG practices. Through extensive experi-\nments, we suggest several strategies for deploy-\ning RAG that balance both performance and ef-\nficiency. Moreover, we demonstrate that multi-\nmodal retrieval techniques can significantly en-\nhance question-answering capabilities about vi-\nsual inputs and accelerate the generation of mul-\ntimodal content using a \"retrieval as generation\"\nstrategy. Code and resources are available at\nhttps://github.com/FudanDNN-NLP/RAG.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "1 Introduction\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "Generative large language models are prone to pro-\nducing outdated information or fabricating facts,\nalthough they were aligned with human preferences\nby reinforcement learning (Ouyang et al., 2022) or\nlightweight alternatives (Liu et al., 2023; Rafailov\net al., 2023; Yuan et al., 2023; Zhao et al., 2023b).\nRetrieval-augmented generation (RAG) techniques\naddress these issues by combining the strengths\nof pretraining and retrieval-based models, thereby\nproviding a robust framework for enhancing model\nperformance (Gao et al., 2023). Furthermore, RAG\nenables rapid deployment of applications for spe-\ncific organizations and domains without necessi-\ntating updates to the model parameters, as long as\nquery-related documents are provided.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "*Corresponding Author.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "Many RAG approaches have been proposed to\nenhance large language models (LLMs) through\nquery-dependent retrievals (Cai et al., 2022; Gao\net al., 2023; Li et al., 2022). A typical RAG\nworkflow usually contains multiple intervening pro-\ncessing steps: query classification (determining\nwhether retrieval is necessary for a given input\nquery), retrieval (efficiently obtaining relevant doc-\numents for the query), reranking (refining the order\nof retrieved documents based on their relevance to\nthe query), repacking (organizing the retrieved doc-\numents into a structured one for better generation),\nsummarization (extracting key information for re-\nsponse generation from the repacked document and\neliminating redundancies) modules. Implementing\nRAG also requires decisions on the ways to prop-\nerly split documents into chunks, the types of em-\nbeddings to use for semantically representing these\nchunks, the choice of vector databases to efficiently\nstore feature representations, and the methods for\neffectively fine-tuning LLMs (see Figure 1).\nWhat adds complexity and challenge is the vari-\nability in implementing each processing step. For\nexample, in retrieving relevant documents for an in-\nput query, various methods can be employed. One\napproach involves rewriting the query first and us-\ning the rewritten queries for retrieval (Ma et al.,\n2023a). Alternatively, pseudo-responses to the\nquery can be generated first, and the similarity be-\ntween these pseudo-responses and the backend doc-\numents can be compared for retrieval (Gao et al.,\n2022). Another option is to directly employ em-\nbedding models, typically trained in a contrastive\nmanner using positive and negative query-response\npairs (Wang et al., 2022; Xiao et al., 2023). The\ntechniques chosen for each step and their combi-\nnations significantly impact both the effectiveness\nand efficiency of RAG systems. To the best of our\nknowledge, there has been no systematic effort to\npursue the optimal implementation of RAG, partic-\nularly for the entire RAG workflow.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "17716\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 1, "text": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 17716–17736\nNovember 12-16, 2024 2024 Association for Computational Linguistics\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 2, "text": "In this study, we aim to identify the best practices\nfor RAG through extensive experimentation. Given\nthe infeasibility of testing all possible combinations\nof these methods, we adopt a three-step approach to\nidentify optimal RAG practices. First, we compare\nrepresentative methods for each RAG step (or mod-\nule) and select up to three of the best-performing\nmethods. Next, we evaluate the impact of each\nmethod on the overall RAG performance by testing\none method at a time for an individual step, while\nkeeping the other RAG modules unchanged. This\nallows us to determine the most effective method\nfor each step based on its contribution and interac-\ntion with other modules during response generation.\nOnce the best method is chosen for a module, it\nis used in subsequent experiments. Finally, we\nempirically explore a few promising combinations\nsuitable for different application scenarios where\nefficiency might be prioritized over performance,\nor vice versa. Based on these findings, we suggest\nseveral strategies for deploying RAG that balance\nboth performance and efficiency.\nThe contributions of this study are three-fold:\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 2, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 2, "text": "Through extensive experimentation, we thor-\noughly investigated existing RAG approaches\nand their combinations to identify and recom-\nmend optimal RAG practices.\n• We introduce a comprehensive framework of\nevaluation metrics and corresponding datasets\nto comprehensively assess the performance of\nretrieval-augmented generation models, cover-\ning general, specialized (or domain-specific),\nand RAG-related capabilities.\n• We demonstrate that the integration of multi-\nmodal retrieval techniques can substantially\nimprove question-answering capabilities on\nvisual inputs and speed up the generation of\nmultimodal content through a strategy of \"re-\ntrieval as generation\".\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 2, "text": "2 Related Work\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 2, "text": "Ensuring the accuracy of responses generated by\nLarge Language Models (LLMs) such as Chat-\nGPT (OpenAI, 2023) and LLaMA (Touvron et al.,\n2023a) is essential. However, simply enlarg-\ning model size does not fundamentally address\nthe issue of hallucinations (Wang et al., 2023b;\nZhang et al., 2023c), especially in knowledge-\nintensive tasks and specialized domains. Retrieval-\naugmented generation (RAG) addresses these chal-\nlenges by retrieving relevant documents from exter-\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 2, "text": "nal knowledge bases, providing accurate, real-time,\ndomain-specific context to LLMs (Gao et al., 2023).\nPrevious works have optimized the RAG pipeline\nthrough query and retrieval transformations, en-\nhancing retriever performance, and fine-tuning both\nthe retriever and generator. These optimizations\nimprove the interaction between input queries, re-\ntrieval mechanisms, and generation processes, en-\nsuring the accuracy and relevance of responses.\n2.1 Query and Retrieval Transformation\nEffective retrieval requires queries accurate, clear,\nand detailed. Even when converted into em-\nbeddings, semantic differences between queries\nand relevant documents can persist. Previous\nworks have explored methods to enhance query\ninformation through query transformation, thereby\nimproving retrieval performance. For instance,\nQuery2Doc (Wang et al., 2023a) and HyDE (Gao\net al., 2022) generate pseudo-documents from orig-\ninal queries to enhance retrieval, while TOC (Kim\net al., 2023) decomposes queries into subqueries,\naggregating the retrieved content for final results.\nOther studies have focused on transforming re-\ntrieval source documents. LlamaIndex (Liu, 2022)\nprovides an interface to generate pseudo-queries for\nretrieval documents, improving matching with real\nqueries. Some works employ contrastive learning\nto bring query and document embeddings closer in\nsemantic space (Li et al., 2023; Xiao et al., 2023;\nZhang et al., 2023a). Post-processing retrieved doc-\numents is another method to enhance generator out-\nput, with techniques like hierarchical prompt sum-\nmarization (Jiang et al., 2023a) and using abstrac-\ntive and extractive compressors (Xu et al., 2023)\nto reduce context length and remove redundancy\n(Wang et al., 2023c).\n2.2 Retriever Enhancement Strategy\nDocument chunking and embedding methods sig-\nnificantly impact retrieval performance. Common\nchunking strategies divide documents into chunks,\nbut determining optimal chunk length can be chal-\nlenging. Small chunks may fragment sentences,\nwhile large chunks might include irrelevant con-\ntext. LlamaIndex (Liu, 2022) optimizes the chunk-\ning method like Small2Big and sliding window.\nRetrieved chunks can be irrelevant and numbers\ncan be large, so reranking is necessary to filter\nirrelevant documents. A common reranking ap-\nproach employs deep language models such as\nBERT (Nogueira et al., 2019), T5 (Nogueira et al.,\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 2, "text": "17717\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Evaluation\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ General Performance\n⚫ Specific Domains\n. Retrieval Capability\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Fine-tune\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Disturb\nRandom\nNormal\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Summarization\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "• Extractive\n• Recomp\n⚫ BM25\n• Contriever\n⚫ Abstractive\n⚫ LongLLMlingua\n• SelectiveContext\n• Recomp\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Repacking\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Sides\nForward\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Reverse\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Large Language Model\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Query Classification\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Retrieval\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "• Original Query\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ BM25\n• Contriever\n⚫ LLM-Embedder\n• Query Rewriting\n• Query Decomposition\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ HYDE\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ Hybrid Search\n⚫ HyDE+Hybrid Search\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Retrieval Source\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Chunking\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Chunking Size\n• Small2big\nSliding Windows\n⚫ Chunk Metadata\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Embedding\n⚫ LLM-Embedder\n⚫ intfloat/e5\n⚫ BAAI/bge\n⚫ Jina-embeddings-v2\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ Gte\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ all-mpnet-base-v2\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Vector Database\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ Milvus\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Faiss\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "• Weaviate\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Reranking\n⚫ DLM-based\n⚫ monoT5\n⚫ monoBERT\n• RankLLAMA\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "⚫ TILDE\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "• Qdrant\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "• Chroma\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "Figure 1: Retrieval-augmented generation workflow. This study investigates the contribution of each component and\nprovides insights into optimal RAG practices through extensive experimentation. The optional methods considered\nfor each component are indicated in bold fonts, while the methods underlined indicate the default choice for\nindividual modules. The methods indicated in blue font denote the best-performing selections identified empirically.\n2020), or LLAMA (Ma et al., 2023b), which re-\nquires slow inference steps during reranking but\ngrants better performance. TILDE (Zhuang and\nZuccon, 2021a,b) achieves efficiency by precom-\nputing and storing the likelihood of query terms,\nranking documents based on their sum.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "2.3 Retriever and Generator Fine-tuning\nFine-tuning within the RAG framework is crucial\nfor optimizing both retrievers and generators. Some\nresearch focuses on fine-tuning the generator to bet-\nter utilize retriever context (Liu et al., 2024b; Luo\net al., 2023; Zhang et al., 2024b), ensuring faith-\nful and robust generated content. Others fine-tune\nthe retriever to learn to retrieve beneficial passages\nfor the generator (Izacard et al., 2022; Shi et al.,\n2023; Zhang et al., 2024a). Holistic approaches\ntreat RAG as an integrated system, fine-tuning both\nretriever and generator together to enhance overall\nperformance (Guu et al., 2020; Lin et al., 2023;\nZamani and Bendersky, 2024), despite increased\ncomplexity and integration challenges.\nSeveral surveys have extensively discussed cur-\nrent RAG systems, covering aspects like text gener-\nation (Cai et al., 2022; Li et al., 2022), integration\nwith LLMs (Gao et al., 2023; Huang and Huang,\n2024), multimodal (Zhao et al., 2023a), and AI-\ngenerated content (Zhao et al., 2024). While these\nsurveys provide comprehensive overviews of ex-\nisting RAG methodologies, selecting the appropri-\nate algorithm for practical implementation remains\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "challenging. In this paper, we focus on best prac-\ntices for applying RAG methods, advancing the\nunderstanding and application of RAG in LLMs.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "3 RAG Workflow\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "In this section, we detail the components of the\nRAG workflow. For each module, we review com-\nmonly used approaches and select the default and\nalternative methods for our final pipeline. Section\n4 will discuss best practices. Figure 1 presents the\nworkflow and methods for each module. Detailed\nexperimental setups, including datasets, hyperpa-\nrameters, and results are provided in Appendix A.\n3.1 Query Classification\nNot all queries require to be retrieval-augmented\ndue to the inherent capabilities of LLMs. While\nRAG can enhance information accuracy and re-\nduce hallucinations, frequent retrieval costs longer\nresponse time. Therefore, we begin by classifying\nqueries to determine retrieval necessity. Queries\nrequiring retrieval proceed through the RAG mod-\nules; others are handled directly by LLMs.\nRetrieval is generally recommended when\nknowledge beyond the model's parameters is\nneeded. However, the need for retrieval varies by\ntask. For instance, an LLM trained up to 2023 can\nhandle a translation request for \"Sora was devel-\noped by OpenAI\" without retrieval. Conversely,\nan introduction request for the same topic would\nrequire retrieval to provide relevant information.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 3, "text": "17718\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 4, "text": "To address this issue, we propose classifying\ntasks by type to determine if a query needs retrieval.\nWe categorize 15 tasks based on whether they pro-\nvide sufficient information, with specific tasks and\nexamples illustrated in Figure 2. For tasks entirely\nbased on user-given information, we denote as \"suf-\nficient\", which need not retrieval; otherwise, we\ndenote as \"insufficient”, where retrieval may be\nnecessary. We created a dataset consisting of 111K\nsamples covering 15 different types of tasks, with\n64K samples labeled as \"retrieval required\" and\n47K samples as \"no retrieval required\". A classifier\nwas trained to automate this decision-making pro-\ncess. Specific experimental results are presented\nin Appendix A.1. Section 4 explores the impact\nof query classification on the workflow, comparing\nscenarios with and without classification.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 4, "text": "3.2 Chunking\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 4, "text": "Chunking documents into smaller segments is cru-\ncial for enhancing retrieval precision and avoiding\nlength issues in LLMs. This process can be ap-\nplied at various levels of granularity, such as token,\nsentence, and semantic levels.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 4, "text": "• Token-level Chunking is straightforward but\nmay split sentences, affecting retrieval quality.\n• Semantic-level Chunking uses LLMs to deter-\nmine breakpoints, context-preserving but time-\nconsuming.\n• Sentence-level Chunking balances preserving\ntext semantics with simplicity and efficiency.\nIn this study, we use sentence-level chunking, bal-\nancing simplicity and semantic preservation. We\nexamine chunking from four dimensions:\nChunk Size Chunk size significantly impacts\nperformance. Larger chunks provide more context,\nenhancing comprehension but increasing process\ntime. Smaller chunks improve retrieval recall and\nreduce time but may lack sufficient context.\nChunking Techniques Advanced techniques\nsuch as small-to-big and sliding window improve\nretrieval quality by organizing chunk block relation-\nships. Small-sized blocks are used to match queries,\nand larger blocks that include the small ones along\nwith contextual information are returned.\nMetadata Addition Enhancing chunk blocks\nwith metadata like titles, keywords, and hypo-\nthetical questions can improve retrieval, provide\nmore ways to post-process retrieved texts, and help\nLLMs better understand retrieved information.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 4, "text": "Embedding Model Choosing the right embed-\nding model is crucial for effective semantic match-\ning of queries and chunk blocks. Based on the\nevaluation module of FlagEmbedding¹, we select\nthe LLM-Embedder (Zhang et al., 2023a) for its\nbalance of performance and size.\nA detailed study on metadata inclusion will\nbe addressed in future work. Further discussion\non chunk size influence, advanced chunking tech-\nniques, and comparative experiments on different\nembedding models are presented in Appendix A.2.\n3.3 Vector Databases\nVector databases store embedding vectors with\ntheir metadata, enabling efficient retrieval of docu-\nments relevant to queries through various indexing\nand approximate nearest neighbor (ANN) methods.\nTo select an appropriate vector database for our\nresearch, we evaluated several options based on\nfour key criteria: multiple index types, billion-scale\nvector support, hybrid search, and cloud-native ca-\npabilities. These criteria were chosen for their\nimpact on flexibility, scalability, and ease of de-\nployment in modern, cloud-based infrastructures.\nMultiple index types provide the flexibility to opti-\nmize searches based on different data characteris-\ntics and use cases. Billion-scale vector support is\ncrucial for handling large datasets in LLM applica-\ntions. Hybrid search combines vector search with\ntraditional keyword search, enhancing retrieval ac-\ncuracy. Finally, cloud-native capabilities ensure\nseamless integration, scalability, and management\nin cloud environments. Table 6 presents a detailed\ncomparison of five open-source vector databases:\nWeaviate, Faiss, Chroma, Qdrant, and Milvus.\nOur evaluation indicates that Milvus stands out\nas the most comprehensive solution among the\ndatabases evaluated, meeting all the essential crite-\nria and outperforming other open-source options.\n3.4 Retrieval Methods\nGiven a user query, the retrieval module selects the\ntop-k relevant documents from a pre-built corpus\nbased on the similarity between the query and the\ndocuments. The generation model then uses these\ndocuments to formulate an appropriate response\nto the query. However, original queries often un-\nderperform due to poor expression and lack of se-\nmantic information (Gao et al., 2023), negatively\nimpacting the retrieval process. To address these\nissues, we evaluated three query transformation\nhttps://github.com/FlagOpen/FlagEmbedding\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 4, "text": "17719\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "methods using the LLM-Embedder recommended\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "in Section 3.2 as the query and document encoder:\nQuery Rewriting: Query rewriting refines\nqueries to better match relevant documents.\nInspired by the Rewrite-Retrieve-Read frame-\nwork (Ma et al., 2023a), we prompt an LLM\nto rewrite queries to enhance performance.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "• Query Decomposition: This approach involves\nretrieving documents based on sub-questions de-\nrived from the original query, which is more com-\nplex to comprehend and handle.\n• Pseudo-documents Generation: This approach\ngenerates a hypothetical document based on the\nuser query and uses the embedding of hypotheti-\ncal answers to retrieve similar documents. One\nnotable implement is HyDE (Gao et al., 2022),\nRecent studies, such as Sawarkar et al. (2024),\nindicate that combining lexical-based search with\nvector search significantly enhances performance.\nIn this study, we use BM25 for sparse retrieval and\nContriever(Izacard et al., 2021), an unsupervised\ncontrastive encoder, for dense retrieval, serving as\ntwo robust baselines based on Thakur et al. (2021).\nWe evaluated the performance of different search\nmethods on the TREC DL 2019 and 2020 pas-\nsage ranking datasets. The results presented in\nTable 7 show that supervised methods significantly\noutperformed unsupervised methods. Combining\nwith HyDE and hybrid search, LLM-Embedder\nachieves the highest scores. However, query rewrit-\ning and query decomposition did not enhance re-\ntrieval performance as effectively. Considering the\nbest performance and tolerated latency, we recom-\nmend Hybrid Search with HyDE as the default\nretrieval method. Taking efficiency into consider-\nation, Hybrid Search combines sparse retrieval\n(BM25) and dense retrieval (Original embedding)\nand achieves notable performance with relatively\nlow latency. Additional implementation details and\nexperiments on the HyDE and hyperparameters of\nhybrid search are presented in Appendix A.3.\n3.5 Reranking Methods\nAfter initial retrieval, a reranking phase is em-\nployed to further enhance the relevancy of the re-\ntrieved documents, ensuring that the most pertinent\ninformation appears on top. By leveraging more\nprecise methods, documents are reordered more\neffectively, increasing the similarity between the\nquery and the top-ranked documents.\nWe consider two approaches in our reranking\nmodule: DLM Reranking, which utilizes classi-\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "fication, and TILDE Reranking, which focuses\non query likelihoods. These approaches prioritize\nperformance and efficiency, respectively.\n• DLM Reranking: Rerankers utilizing deep\nlanguage models (DLMs) (Ma et al., 2023b;\nNogueira et al., 2020, 2019) are a representa-\ntive method, generally providing the best perfor-\nmance, albeit with reduced efficiency. Models\nare fine-tuned to predict the target tokens \"true\"\nor \"false\" based on the relevancy of the user\nquery and candidate document. The model is\nfine-tuned with the query and document concate-\nnated as input, labeled accordingly. At inference,\ndocuments are then ranked by the probability of\nthe \"true\" token for each query.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "TILDE Reranking: Conventional query likeli-\nhood models (Santos et al., 2020; Zhuang et al.,\n2021) calculate conditional probabilities of query\nterms based on the likelihoods of its preceding\ntokens, but lack efficiency. TILDE (Zhuang and\nZuccon, 2021a,b) instead independently consid-\ners each query term and predicts the probabilities\nof tokens across the entire vocabulary. With the\ncandidate documents preprocessed at indexing,\nrapid reranking can be done by summing the\npre-calculated log probabilities corresponding to\nthe query tokens for each document. TILDEv2\nfurther enhances efficiency and greatly reduces\nindex size by indexing only document-present to-\nkens, using NCE loss, and document expansion.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "Our experiments were conducted on the MS\nMARCO Passage ranking dataset (Bajaj et al.,\n2016). We followed and made modifications to the\nimplementation provided by PyGaggle (Nogueira\net al., 2020) and TILDE, using the models monoT5,\nmonoBERT, RankLLaMA and TILDEv2. Rerank-\ning results are shown in Table 10. We recommend\nmonoT5 as a comprehensive method balancing\nperformance and efficiency. RankLLAMA is suit-\nable for achieving the best performance, while\nTILDEV2 is ideal for the quickest experience on a\nfixed collection. Details on the experimental setup\nand results are presented in Appendix A.4.\n3.6 Document Repacking\nThe performance of subsequent processes, such\nas LLM response generation, may be affected by\nthe order documents are provided. To address this\nissue, we incorporate a compact repacking mod-\nule into the workflow after reranking, featuring\nthree repacking methods: \"forward”, “reverse\"\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 5, "text": "17720\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "and \"sides\". \"Forward” repacks documents by de-\nscending the relevancy scores from the reranking\nphase, whereas \"reverse\" arranges them in ascend-\ning order. Inspired by (Liu et al., 2024a), which\nconcluded that optimal performance is achieved\nwhen relevant information is placed at the head or\ntail of the input, we also include a \"sides\" option.\nAs the repacking method utilized primarily\naffects subsequent modules, we select the best\nrepacking method in Section 4 by testing it in com-\nbination with other modules. Here, we choose\n\"sides\" as the default repacking method.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "3.7 Summarization\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "Retrieval results may contain redundant or unnec-\nessary information, potentially preventing LLMs\nfrom generating accurate responses. Additionally,\nlong prompts can slow down the inference pro-\ncess. Therefore, efficient methods to summarize re-\ntrieved documents are crucial in the RAG pipeline.\nSummarization tasks can be extractive or ab-\nstractive. Extractive methods segment text into\nsentences, then score and rank them based on im-\nportance. Abstractive compressors synthesize in-\nformation from multiple documents to rephrase and\ngenerate a cohesive summary. These tasks can be\nquery-based or non-query-based. In this paper, as\nRAG retrieves information relevant to queries, we\nfocus exclusively on query-based methods.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "Recomp: Recomp (Xu et al., 2023) has extrac-\ntive and abstractive compressors. The extractive\ncompressor selects useful sentences, while the\nabstractive compressor synthesizes information\nfrom multiple documents.\nLongLLMLingua:\nLongLLMLingua (Jiang\net al., 2023b) improves LLMLingua by focusing\non key information related to the query.\nWe evaluate these methods on three benchmark\ndatasets: NQ, TriviaQA, and HotpotQA. Compara-\ntive results of different summarization methods are\nshown in Table 11. We recommend Recomp for its\noutstanding performance. LongLLMLingua does\nnot perform well but demonstrates better general-\nization capabilities as it was not trained on these\nexperimental datasets. Therefore, we consider it\nas an alternative method. Additional implemen-\ntation details and discussions on non-query-based\nmethods are provided in Appendix A.5.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "3.8\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "Generator Fine-tuning\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "In this section, we focus on fine-tuning the gener-\nator while leaving retriever fine-tuning for future\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "exploration. We aim to investigate the impact of\nfine-tuning, particularly the influence of relevant or\nirrelevant contexts on the generator's performance.\nFormally, we denote x as the query fed into the\nRAG system, and D as the contexts for this input.\nThe fine-tuning loss of the generator is the negative\nlog-likelihood of the ground-truth output y.\nTo explore the impact of fine-tuning, especially\nrelevant and irrelevant contexts, we define dgold as\na context relevant to the query, and drandom as a\nrandomly retrieved context. We train the model by\nvarying the composition of D as follows:\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": ".\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "Dg: The augmented context consists of query-\nrelevant documents, denoted as Dg = {dgold}.\n• Dr: The context contains one randomly sampled\ndocument, denoted as Dr\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "=\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "{drandom}.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "• Dgr: The augmented context comprises a rel-\nevant document and a randomly-selected one,\ndenoted as Dgr = {dgold, drandom}.\n• Dgg: The augmented context consists of two\ncopies of a query-relevant document, denoted as\nDgg = {dgold, dgold}.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "We denote the base LM generator not fine-tuned\nas M, and the model fine-tuned under the cor-\nresponding D as Mg, Mr, Mgr, Mgg. We fine-\ntuned our model on several QA and reading com-\nprehension datasets. Ground-truth coverage is used\nas our evaluation metric since QA task answers\nare relatively short. Specifically, we adopted a\nmore lenient approach to the Exact Match (EM)\nscore, which evaluates the performance based on\nthe presence of the gold response in the model's\noutput. We select Llama-2-7B (Touvron et al.,\n2023b) as the base model. Similar to training,\nwe evaluate all trained models on validation sets\nwith Dg, Dr, Dgr, and Dø, where Do indicates\ninference without retrieval. Figure 3 presents our\nmain results. Models trained with a mix of rele-\nvant and random documents (Mgr) perform best\nwhen provided with either gold or mixed contexts.\nThis suggests that mixing relevant and random\ncontexts during training can enhance the gener-\nator's robustness to irrelevant information while\nensuring effective utilization of relevant contexts.\nTherefore, we identify the practice of augment-\ning with a few relevant and randomly-selected\ndocuments during training as the best approach.\nDetailed dataset information, hyperparameters and\nexperimental results can be found in Appendix A.6.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 6, "text": "17721\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "4 Searching for Best RAG Practices\nIn the following section, we investigate the opti-\nmal practices for implementing RAG. To begin\nwith, we used the default practice identified in Sec-\ntion 3 for each module. Following the workflow\ndepicted in Figure 1, we sequentially optimized\nindividual modules and selected the most effective\noption among alternatives. This iterative process\ncontinued until we determined the best method\nfor implementing the final summarization module.\nBased on Section 3.8, we used the Llama2-7B-Chat\nmodel fine-tuned where each query was augmented\nby a few random-selected and relevant documents\nas the generator. We used Milvus to build a vector\ndatabase that includes 10 million text of English\nWikipedia and 4 million text of medical data. We\nalso investigated the impact of removing the Query\nClassification, Reranking, and Summarization mod-\nules to assess their contributions.\n4.1 Comprehensive Evaluation\nWe conducted extensive experiments across vari-\nous NLP tasks and datasets to assess the perfor-\nmance of RAG systems. Specifically: (I) Com-\nmonsense Reasoning; (II) Fact Checking; (III)\nOpen-Domain QA; (IV) MultiHop QA; (V) Med-\nical QA. For further details on the tasks and their\ncorresponding datasets, please refer to Appendix\nA.7. Furthermore, we evaluated the RAG capa-\nbilities on subsets extracted from these datasets,\nemploying the metrics recommended in RAGAS\n(Shahul et al., 2023), including Faithfulness, Con-\ntext Relevancy, Answer Relevancy, and Answer\nCorrectness. Additionally, we measured Retrieval\nSimilarity by computing the cosine similarity be-\ntween retrieved documents and gold documents.\nWe used accuracy as the evaluation metric for\nthe tasks of Commonsense Reasoning, Fact Check-\ning, and Medical QA. For Open-Domain QA and\nMultihop QA, we employed token-level F1 score\nand Exact Match (EM) score. The final RAG score\nwas calculated by averaging the aforementioned\nfive RAG capabilities. Consistently, the same cor-\npus constructed in Section 3 was used for all tasks.\nWe followed Trivedi et al. (2022) and sub-sampled\nup to 500 examples from each dataset.\n4.2 Results and Analysis\nBased on the experimental results presented in Ta-\nble 1, the following key insights emerge:\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "Query Classification Module: This module is\ncrucial for both effectiveness and efficiency, lead-\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "17722\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 7, "text": "ing to an average improvement in the overall\nscore from 0.428 to 0.443 and a reduction in la-\ntency time from 16.41 to 11.58 seconds per query.\nThe query classification method distinguishes be-\ntween queries that require retrieval operations\nand those that do not, based on the completeness\nof information within the queries. This selective\nretrieval strategy avoids unnecessary operations,\nsignificantly enhancing both performance and\nresponse time.\nRetrieval Module: The combination of dense re-\ntrieval and the classical BM25 algorithm demon-\nstrates superior performance due to their com-\nplementary strengths. While dense retrieval ex-\ncels at identifying semantic relationships (e.g.,\nlinking terms like \"bad guy\" and \"villain\"), it\nstruggles with rare terminologies and out-of-\nvocabulary (OOV) words. BM25, however, is\nadept at matching specific terms, compensating\nfor these weaknesses. This hybrid approach bal-\nances the strengths of both methods, enhancing\nretrieval robustness. Moreover, the use of gen-\nerated pseudo-documents minimizes semantic\nmismatches between the query and relevant doc-\numents. While the \"Hybrid with HyDE\" method\nachieved the highest RAG score of 0.58, it came\nat a computational cost of 11.71 seconds per\nquery. In practice, the \"Hybrid\" or \"Original\"\nmethods are recommended, as they maintain\ncomparable performance with reduced latency.\nReranking Module: Reranking is critical to\nmaintaining high-quality results, as demonstrated\nby a performance drop in its absence. Among\nDLM-based rerankers, monoT5 significantly out-\nperformed monoBERT and RankLLAMA. This\nsuperiority can be attributed to monoT5's larger\nparameter set and more extensive training data, as\nwell as its encoder-decoder architecture, which\nprovides enhanced natural language understand-\ning compared to the decoder-only LLAMA model.\nMonoT5's effectiveness in boosting the relevance\nof retrieved documents affirms the necessity of\nreranking in improving the quality of generated\nresponses.\n• Repacking Module: The Reverse configuration\nexhibited superior performance, achieving an\nRAG score of 0.560. This highlights the impor-\ntance of positioning more relevant context closer\nto the query to yield optimal results.\nSummarization Module: The Recomp extrac-\ntive summarization method demonstrated supe-\nrior performance over LongLLMLingua, an ab-\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Med RAG\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Avg.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Score Acc Score Score F1 Latency\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Commonsense Fact Check\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "ODQA\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Multihop\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Method\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Acc\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Acc\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "EM F1 Score EM F1\nwithout retrieval\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ baseline\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.537\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.560\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.373 0.413 0.428 0.167 0.173 0.182 0.360\nclassification module, Hybrid with HyDE, monoT5, sides, Recomp\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.351 0.292\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "1.27\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.505 0.391 0.450 0.478 0.212 0.255 0.254 0.528 0.540 0.422 0.353 16.58\n0.595 0.393 0.450 0.479 0.207 0.257 0.254 0.460 0.580 0.443 0.353 11.71\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "w/o classification\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ classification\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.719\n0.727\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "with classification, retrieval module, monoT5, sides, Recomp\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ HYDE\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.718\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.595\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ Original\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.721\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.585\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ Hybrid\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.718\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ Hybrid + HyDE\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.727\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.595\n0.595\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.320 0.373 0.380 0.170 0.213 0.222 0.400 0.545 0.398 0.293\n0.300 0.350 0.363 0.153 0.197 0.206 0.390 0.486 0.383 0.273\n0.347 0.397 0.418 0.190 0.240 0.233 0.750 0.498 0.429 0.318\n0.393 0.450 0.479 0.207 0.257 0.254 0.460 0.580 0.443 0.353\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "11.58\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "1.44\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "1.45\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "11.71\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "w/o reranking\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ monoT5\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ monoBERT\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ RankLLaMA\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ TILDEV2\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ sides\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ forward\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ reverse\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "with classification, Hybrid with HyDE, reranking module, sides, Recomp\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.720\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.591\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.727\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.723\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.723\n0.725\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.595\n0.593\n0.597\n0.588\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "10.31\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.365 0.429 0.435 0.211 0.260 0.253 0.512 0.530 0.430 0.334\n0.393 0.450 0.479 0.207 0.257 0.253 0.460 0.580 0.443 0.353 11.71\n0.383 0.443 0.463 0.217 0.259 0.253 0.482 0.551 0.438 0.351 11.65\n0.382 0.443 0.459 0.197 0.240 0.237 0.454 0.558 0.431 0.342 13.51\n0.394 0.456 0.473 0.209 0.255 0.249 0.486 0.536 0.440 0.355 11.26\nwith classification, Hybrid with HyDE, monoT5, repacking module, Recomp\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.727\n0.722\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.728\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.595\n0.599\n0.592\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.393 0.450 0.479 0.207 0.257 0.253 0.460 0.580 0.443 0.353 11.71\n0.379 0.437 0.458 0.215 0.260 0.254 0.472 0.542 0.437 0.349 11.68\n0.387 0.445 0.473 0.219 0.263 0.260 0.532 0.560 0.446 0.354 11.70\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "with classification, Hybrid with HyDE, monoT5, reverse, summarization module\nw/o summarization\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ Recomp\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "+ LongLLMLingua\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.729\n0.728\n0.713\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.591\n0.592\n0.581\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "0.402 0.457 0.468 0.205 0.252 0.245 0.528 0.533 0.441 0.355 10.97\n0.387 0.445 0.473 0.219 0.263 0.260 0.532 0.560 0.446 0.354 11.70\n0.362 0.423 0.432 0.199 0.245 0.245 0.530 0.539 0.426 0.334 16.17\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "Table 1: Results of the search for optimal RAG practices. Modules enclosed in a boxed module are under\ninvestigation to determine the best method. The underlined method represents the selected implementation. For\nthe two QA tasks, ODQA and MultiHop, we use GPT to score them simultaneously. The \"Avg\" (average score) is\ncalculated based on the Acc, EM, and RAG scores for all tasks, while the average latency is measured in seconds\nper query. The best scores are highlighted in bold.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "5 Discussion\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "stractive summarization method. Our experi- ing high-quality responses across different tasks.\nments revealed that LongLLMLingua occasion-\nally distorts semantics and produces incoherent\ncontent due to its rewriting approach. Recomp,\non the other hand, preserves the integrity of the\noriginal content, making it better suited for RAG\napplications. Although comparable results can\nbe achieved with lower latency by removing the\nsummarization module, Recomp remains the pre-\nferred choice for scenarios where addressing the\ngenerator's maximum length constraint is crucial.\nIn time-sensitive applications, removing summa-\nrization could effectively reduce response time.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "The experimental results demonstrate that each\nmodule contributes uniquely to the overall perfor-\nmance of the RAG system. The query classifica-\ntion module enhances accuracy and reduces latency,\nwhile the retrieval and reranking modules signif-\nicantly improve the system's ability to handle di-\nverse queries. The repacking and summarization\nmodules further refine the system's output, ensur-\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "5.1 Best Practices for Implementing RAG\nAccording to our experimental findings, we suggest\ntwo distinct recipes or practices for implementing\nRAG systems, each customized to address specific\nrequirements: one focusing on maximizing perfor-\nmance, and the other on striking a balance between\nefficiency and efficacy.\nBest Performance Practice: To achieve the high-\nest performance, it is recommended to incorporate\nquery classification module, use the “Hybrid with\nHYDE\" method for retrieval, employ monoT5 for\nreranking, opt for Reverse for repacking, and lever-\nage Recomp for summarization. This configuration\nyielded the highest average score of 0.483, albeit\nwith a computationally-intensive process.\nBalanced Efficiency Practice: To achieve a bal-\nance between performance and efficiency, it is rec-\nommended to incorporate the query classification\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 8, "text": "17723\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "module, implement the Hybrid method for retrieval,\nuse TILDEV2 for reranking, opt for Reverse for\nrepacking, and employ Recomp for summarization.\nGiven that the retrieval module accounts for the\nmajority of processing time in the system, transi-\ntioning to the Hybrid method while keeping other\nmodules unchanged can substantially reduce la-\ntency while preserving a comparable performance.\n5.2 Generalization of Best Practices\nWhile the above best practices demonstrate strong\nperformance in our experiments, we acknowledge\nthat they may not be universally optimal across\nall tasks and contexts. Therefore, we emphasize\nthe importance of the comprehensive evaluation\nframework, which assesses system performance\nacross general, domain-specific, and task-specific\ncapabilities, and the three-step strategy to identify\nthe most effective practices:\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "Empirical Comparison of Candidate Imple-\nmentations: For each module, we compare mul-\ntiple candidate methods to determine the best-\nperforming options.\nModule Integration: After selecting the optimal\nmethod for each module, we evaluate how they\ninteract when integrated into the full workflow.\n• Evaluation of Module Combinations: Finally,\nwe assess the performance of different module\ncombinations to identify opportunities for im-\nproving system efficiency and effectiveness.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "5.3\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "Multimodal Extension\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "We have extended RAG to multimodal applications.\nSpecifically, we have incorporated text2image and\nimage2text retrieval capabilities into the system\nwith a substantial collection of paired image and\ntextual descriptions as a retrieval source. As de-\npicted in Figure 4, the text2image capability speeds\nup the image generation process when a user query\naligns well with the textual descriptions of stored\nimages (i.e., \"retrieval as generation\" strategy),\nwhile the image2text functionality comes into play\nwhen a user provides an image and engages in con-\nversation about the input image. These multimodal\nRAG capabilities offer the following advantages:\n⚫ Groundedness: Retrieval methods provide in-\nformation from verified multimodal materials,\nthereby ensuring authenticity and specificity. In\ncontrast, on-the-fly generation relies on models\nto generate new content, which can occasionally\nresult in factual errors or inaccuracies.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "•\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "Efficiency: Retrieval methods are typically more\nefficient, especially when the answer already\nexists in stored materials. Conversely, genera-\ntion methods may require more computational\nresources to produce new content, particularly\nfor images or lengthy texts.\nMaintainability: Generation models often ne-\ncessitate careful fine-tuning to tailor them for new\napplications. In contrast, retrieval-based methods\ncan be improved to address new demands by sim-\nply enlarging the size and enhancing the quality\nof retrieval sources.\nWe adopted the experimental setup from (Koh\net al., 2024). Specifically, we used the PartiPrompts\ndataset to prompt the stable diffusion model to gen-\nerate images and to retrieve images from the CC3M\ndataset. We then use the openai/clip-vit-large-\npatch 142 to compute CLIP Similarity between the\nprompts and both types of images (PRO2GEN and\nPRO2RET) and compute consumed time in both\nmethods. The figure 5 represents Groundedness of\nthe \"retrieval as generation\" strategy, as the genera-\ntion model is uncontrollable and may lack relevant\nknowledge. As demonstrated in Table 15, the \"re-\ntrieval as generation” strategy greatly reduces time\nconsumption while maintaining the quality of the\nimages and we can improve the performance of\nretrieval by expanding the search sources which\ndemonstrates the Efficiency and Maintainability\nof this strategy.\nFurthermore, we plan to broaden the application\nof this strategy to include other modalities, such\nas video and speech, while also exploring efficient\nand effective cross-modal retrieval techniques.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "6 Conclusion\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "In this study, we aim to identify optimal practices\nfor implementing retrieval-augmented generation\nin order to improve the quality and reliability of\ncontent produced by large language models. We\nsystematically assessed a range of potential solu-\ntions for each module within the RAG framework\nand recommended the most effective approach for\neach module. Furthermore, we introduced a com-\nprehensive evaluation benchmark for RAG systems\nand conducted extensive experiments to determine\nthe best practices among various alternatives. Our\nfindings not only contribute to a deeper understand-\ning of retrieval-augmented generation systems but\nalso establish a foundation for future research.\n2https://huggingface.co/openai/clip-vit-large-patch14\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 9, "text": "17724\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 10, "text": "Limitations\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
{"page": 10, "text": "We have evaluated the impact of various methods\nfor fine-tuning LLM generators. Previous stud-\nies have demonstrated the feasibility of training\nboth the retriever and generator jointly. We would\nlike to explore this possibility in the future. In\nthis study, we embraced the principle of modular\ndesign to simplify the search for optimal RAG im-\nplementations, thereby reducing complexity. Due\nto the daunting costs associated with constructing\nvector databases and conducting experiments, our\nevaluation was limited to investigating the effec-\ntiveness and influence of representative chunking\ntechniques within the chunking module. It would\nbe intriguing to further explore the impact of differ-\nent chunking techniques on the entire RAG systems.\nWhile we have discussed the application of RAG\nin the domain of NLP and extended its scope to\nimage generation, an enticing avenue for future ex-\nploration would involve expanding this research to\nother modalities such as speech and video.\n", "metadata": {"pdf_path": "data/raw/papers/2024.emnlp-main.981.pdf", "title": "Searching for Best Practices in Retrieval-Augmented Generation", "date": "11/12/2024", "type": "pdf", "author": "Wang et al", "reference_page": 10}}
