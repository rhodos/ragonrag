{"id": "49cd8842-8449-4f3c-90ac-5795568d683a", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 0, "text": "Retrieval-Augmented Generation (RAG) | Pinecone New announcement: Ash Ashutosh, accomplished entrepreneur and tech veteran, joins Pinecone as new CEO - Learn more Dismiss ← Learn Retrieval-Augmented Generation (RAG) Jenna Pederson Jun 12, 2025 Core Components Share: Not only are foundation models stuck in the past, but they intentionally produce natural-sounding and varied responses. Both of these can lead to confidently inaccurate and irrelevant output. This behavior is known as “hallucination.” In this article, we’ll explore the limitations of foundation models and how retrieval-augmented generation (RAG) can address these limitations so chat, search, and agentic workflows can all benefit. Limitations of foundation models Products built on top of foundation models alone are brilliant yet flawed as foundation models have multiple limitations: Knowledge cutoffs When you ask current models about recent events – like asking about last week’s NBA basketball game or how to use features in the latest iPhone model - they may confidently provide outdated or completely fabricated information, the hallucinations we mentioned earlier. Models are trained on massive datasets containing years of human knowledge and creative output from code repositories, books, websites, conversations, scientific papers, and more. But after a model is trained, this data is frozen at a specific point in time, the “cutoff”. This cutoff creates a knowledge gap , leading them to generate plausible but incorrect responses when asked about recent developments. Lack depth in domain-specific knowledge Foundation models have broad knowledge, but can lack depth in specialized domains. High quality datasets might not exist publicly", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "a1556f53-784b-440e-8e70-b3e1556f6b93", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 1, "text": "specific point in time, the “cutoff”. This cutoff creates a knowledge gap , leading them to generate plausible but incorrect responses when asked about recent developments. Lack depth in domain-specific knowledge Foundation models have broad knowledge, but can lack depth in specialized domains. High quality datasets might not exist publicly for a domain, not necessarily because they are private, but because they are highly specialized. Consider a medical model that knows about anatomy, disease, and surgical techniques, but struggles with rare genetic conditions and cutting edge therapies. This data might exist publicly to be used during training, but it may not appear enough to train the model correctly. It also requires expert-level knowledge during the training process to contextualize the information. This limitation can result in responses that are incomplete or irrelevant. Lack private or proprietary data In the case of general-purpose, public models, the data ( your data) does not exist publicly and is inaccessible during training. This means that models don’t know the specifics of your business, whether that be internal company processes and policies, personnel data or email communications, or even the trade secrets of your company. And for good reason: if this data had been included in the training, anyone using the model would potentially gain access to your company’s private and proprietary data. Again, this limitation can result in incomplete or irrelevant responses, limiting the usefulness of the model for your custom business purpose. Loses trust Models typically cannot cite their sources related to a", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "b04358f1-0fd4-4435-bdc8-b6030104be4a", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 2, "text": "in the training, anyone using the model would potentially gain access to your company’s private and proprietary data. Again, this limitation can result in incomplete or irrelevant responses, limiting the usefulness of the model for your custom business purpose. Loses trust Models typically cannot cite their sources related to a specific response. Without citations or references, the user either has to trust the response or validate the claim themselves. Given that models are trained on vast amounts of public data, there is a chance that the generated response is the result of an unauthoritative source. When inaccurate, irrelevant, and useless information is generated, users will lose trust in the model itself, even when this behavior is inherent in how foundation models work. Output generation is probabilistic Hallucinations are often a symptom of the limitations just described. However, models are trained on a diverse set of data that can contain contradictions, errors, and ambiguous data (in addition to the correct data). Because of this, models assign probabilities to all possible continuations, even the wrong ones. Because of sampling randomness like temperature and top k combined with how a user constructs a prompt (maybe it’s too vague or contains misleading context), models may choose the wrong continuation. The result is output that can contain hallucinations. Additionally, models don’t always distinguish between what they know vs what they don’t know, sounding confident even when incomplete, inaccurate, or irrelevant. Hallucinations can produce unwanted behaviors and even be dangerous. For example, an inaccurate but highly", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "99a4d340-5282-4942-81b4-50d3c0fc8ed4", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 3, "text": "may choose the wrong continuation. The result is output that can contain hallucinations. Additionally, models don’t always distinguish between what they know vs what they don’t know, sounding confident even when incomplete, inaccurate, or irrelevant. Hallucinations can produce unwanted behaviors and even be dangerous. For example, an inaccurate but highly convincing medical report could lead to life-threatening treatments or no treatment at all. These foundation model limitations can impact your business bottom line and erode the trust of your users. Retrieval-augmented generation can address these limitations. What is Retrieval-Augmented Generation? Retrieval-augmented generation, or RAG, is a technique that uses authoritative, external data to improve the accuracy, relevancy, and usefulness of a model’s output. It does this through the following four core components, which we’ll cover in more detail later in this article: Ingestion: authoritative data like company proprietary data is loaded into a data source, like a Pinecone vector database Retrieval: relevant data is retrieved from an external data source based on a user query Augmentation: the retrieved data and the user query are combined into a prompt to provide the model with context for the generation step Generation: the model generates output from the augmented prompt, using the context to drive a more accurate and relevant response. Traditional RAG By combining relevant data from an external data source with the user’s query and providing it to the model as context for the generation step, the model will use it to generate a more accurate and relevant output. RAG provides", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "c10860b7-ddc9-4b0b-aca6-e179591aa419", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 4, "text": "to drive a more accurate and relevant response. Traditional RAG By combining relevant data from an external data source with the user’s query and providing it to the model as context for the generation step, the model will use it to generate a more accurate and relevant output. RAG provides the following benefits: Access to real-time data and proprietary or domain-specific data: bring in knowledge relevant to your situation - current events, news, social media, customer data, proprietary data Builds trust: more relevant and accurate results are more likely to earn trust and source citations allow human review More control: control over which sources are used, real-time data access, authorization to data, guardrails/safety/compliance, traceability/source citations, retrieval strategies, cost, tune each component independently of the others Cost-effective compared to alternatives like training/re-training your own model, fine-tuning, or stuffing the context window: foundation models are costly to produce and require specialized knowledge to create, as is fine-tuning; the larger the context sent to the model, the higher the cost RAG in support of agentic workflows But this traditional RAG approach is simple, often with a vector database and a one-shot prompt with context sent to the model to generate output. With the rise of AI agents, agents are now orchestrators of the core RAG components to: construct more effective queries access additional retrieval tools evaluate the accuracy and relevance of the retrieved context apply reasoning to validate retrieved information, to trust or discard it. These operations can be performed by an agent", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "633ce831-d821-47b4-b121-b158298e8b21", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 5, "text": "rise of AI agents, agents are now orchestrators of the core RAG components to: construct more effective queries access additional retrieval tools evaluate the accuracy and relevance of the retrieved context apply reasoning to validate retrieved information, to trust or discard it. These operations can be performed by an agent or agents as part of a larger, iterative plan. Agents as orchestrators of RAG bring even more opportunities for review, revision of queries, reasoning or validation of context, allowing them to make better decisions, take more informed actions, and generate more accurate and relevant output. Now that we’ve covered what RAG is, let’s take a deeper dive into how it works. How does Retrieval-Augmented Generation work? RAG brings accuracy and relevancy to LLM output by relying on authoritative data sources like proprietary, domain-specific data and real-time information. But before we dig into how it does that, let’s ask the questions: do you even need RAG and how will you know it’s working? This is where ground truth evaluations come in. In order to properly deploy any application, you need to know when it's working. AI applications are no different, and so identifying a set of queries and their expected answers is critical to knowing if your application is working. Maintaining that evaluation set is also critical to knowing where to improve over time, and whether those improvements are working. RAG itself is just one optimization, but there are others like query rewriting, chunk expansion, knowledge graphs and more. With a", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "94b7d805-3697-454f-bd67-0013feb03a41", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 6, "text": "is critical to knowing if your application is working. Maintaining that evaluation set is also critical to knowing where to improve over time, and whether those improvements are working. RAG itself is just one optimization, but there are others like query rewriting, chunk expansion, knowledge graphs and more. With a good baseline, you can move on to implementing the four main components of RAG: Ingestion In simple traditional RAG, you’ll retrieve data from a vector database like Pinecone, using semantic search to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the query. We’ll use Pinecone as an example here, but the concept applies to all vector databases. But before we can retrieve the data, you have to ingest the data. Here are steps to get data into your database: Chunk the data During the ingestion step, you’ll load your authoritative data as vectors into Pinecone. You may have structured or unstructured data in the form of text, PDFs, emails, internal wikis, or databases. After cleaning the data, you may need to chunk it by dividing each piece of data, or document, into smaller chunks. Depending on the kind of data you have, the types of queries your users have, and how the results will be used in your application, you’ll need to choose a chunking strategy . Create vector embeddings Then, using an embedding model, you’ll embed each chunk and load it into the vector database. The embedding model is", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "8af0ce0d-232f-4a5a-bed8-939df46c9cc4", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 7, "text": "you have, the types of queries your users have, and how the results will be used in your application, you’ll need to choose a chunking strategy . Create vector embeddings Then, using an embedding model, you’ll embed each chunk and load it into the vector database. The embedding model is a special type of LLM that converts the data chunk into a vector embedding, a numerical representation of the data’s meaning. This allows computers to search for similar items based on the vector representation of the stored data. Load data into a vector database Once you have vectors, you’ll load them into a vector database. This ingestion step most likely happens offline, independently of your application and your user’s workflow. However, if your data changes, for instance, product inventory is updated, you can update the index in real-time to provide up-to-date information to your users. Now that your vector database contains the vector embeddings of your source data, the next step is retrieval. Retrieval A simple approach to retrieval would use semantic search alone. But by using hybrid search, combining both semantic search (with dense vectors) and lexical search (with sparse vectors ), you can improve the retrieval results even more. This becomes relevant when your users don’t always use the same language to talk about a topic (semantic search) and they refer to internal, domain-specific language (lexical or keyword search) like acronyms, product names, or team names. During retrieval, we’ll create a vector embedding from the user’s query to", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "f3aa1c8d-ef15-42eb-a77f-5a7a4246a059", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 8, "text": "more. This becomes relevant when your users don’t always use the same language to talk about a topic (semantic search) and they refer to internal, domain-specific language (lexical or keyword search) like acronyms, product names, or team names. During retrieval, we’ll create a vector embedding from the user’s query to use for searching against the vectors in the database. In hybrid search , you’ll query either a single hybrid index or both a dense and a sparse index. Then we combine and de-duplicate the results and use a reranking model to rerank them based on a unified relevance score, and return the most relevant matches. Augmentation Now that you have the most relevant matches from the retrieval step, you’ll create an augmented prompt with both the search results and the user’s query to send to the LLM. This is where the magic happens. An augmented prompt might look like this: [CODEBLOCK] QUESTION: <the user's question> CONTEXT: <the search results to use as context> Using the CONTEXT provided, answer the QUESTION. Keep your answer grounded in the facts of the CONTEXT. If the CONTEXT doesn't contain the answer to the QUESTION, say you don't know. [/CODEBLOCK] By sending both the search results and the user’s question as context to the LLM, you are encouraging it to use the more accurate and relevant info from the search results during the next generation step. Generation Using the augmented prompt, the LLM now has access to the most pertinent and grounding facts from your", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "ac3d4935-3b2b-4a3b-8023-0918024fcfea", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 9, "text": "results and the user’s question as context to the LLM, you are encouraging it to use the more accurate and relevant info from the search results during the next generation step. Generation Using the augmented prompt, the LLM now has access to the most pertinent and grounding facts from your vector database so your application can provide an accurate answer for your user, reducing the likelihood of hallucination. But RAG is no longer simply about searching for the right piece of information to inform a model response. With agentic RAG, it's about deciding which questions to ask, which tools to use, when to use them, and then aggregating results to ground answers. Agentic RAG In this simple version, the LLM is the agent and decides which retrieval tools to use and when, and how to query those tools. Wrapping up Retrieval-augmented generation has evolved from a buzzword to an indispensable foundation for AI applications. It blends the broad capabilities of foundation models with your company’s authoritative and proprietary knowledge. With AI agents handling more complex use cases, from those supporting professionals servicing complex manufacturing equipment to delivering domain-specific agents at scale , RAG is not just relevant in 2025. It’s critical for building accurate, relevant, and responsible AI applications that go beyond information retrieval. As AI agents become more autonomous and handle more complex workflows, they’ll need to ground their reasoning in your private and domain-specific data through RAG. The question is no longer whether to implement RAG , but", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "8dfa1a1b-61a4-4028-be90-21b752f30a67", "source": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "chunk_index": 10, "text": "critical for building accurate, relevant, and responsible AI applications that go beyond information retrieval. As AI agents become more autonomous and handle more complex workflows, they’ll need to ground their reasoning in your private and domain-specific data through RAG. The question is no longer whether to implement RAG , but how to architect it most effectively for your unique use case and data requirements. Want to dig into a RAG code example? Create a free Pinecone account and check out our example notebooks to implement retrieval-augmented generation with Pinecone or get started with Pinecone Assistant , to build production-grade chat and agent-based applications quickly. Share: Was this article helpful? Yes No Recommended for you Further Reading Learn Jun 25, 2025 Beyond the hype: Why RAG remains essential for modern AI Jenna Pederson Learn Oct 25, 2024 Building a reliable, curated, and accurate RAG system with Cleanlab and Pinecone Matt Turk Engineering Jan 16, 2024 RAG makes LLMs better and equal Amnon , Roy , Ilai , Nathan , Amir", "metadata": {"url": "https://www.pinecone.io/learn/retrieval-augmented-generation/", "title": "Retrieval-Augmented Generation (RAG)", "date": "6/12/2025", "type": "blog", "author": "Pinecone"}}
{"id": "3c3231ba-e2ec-41d8-ae9f-a6b1f333247a", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 0, "text": "Chunking Strategies for LLM Applications | Pinecone New announcement: Ash Ashutosh, accomplished entrepreneur and tech veteran, joins Pinecone as new CEO - Learn more Dismiss ← Learn Chunking Strategies for LLM Applications Roie Schwaber-Cohen , Arjun Patel Jun 28, 2025 Core Components Share: What is chunking? In the context of building LLM-related applications, chunking is the process of breaking down large text into smaller segments called chunks. It’s an essential preprocessing technique that helps optimize the relevance of the content ultimately stored in a vector database. The trick lies in finding chunks that are big enough to contain meaningful information, while small enough to enable performant applications and low latency responses for workloads such as retrieval augmented generation and agentic workflows. In this post, we’ll explore several chunking methods and discuss the tradeoffs needed when choosing a chunking size and method. Finally, we’ll give some recommendations for determining the best chunk size and method that will be appropriate for your application. Start using Pinecone for free Pinecone is the developer-favorite vector database that's fast and easy to use at any scale. Sign up free View Examples Why do we need chunking for our applications? There are two big reasons why chunking is necessary for any application involving vector databases or LLMs: to ensure embedding models can fit the data into their context windows, and to ensure the chunks themselves contain the information necessary for search. All embedding models have context windows, which determine the amount of information in tokens that", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "e33d975e-e63f-4459-aa2e-5e8934011c6d", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 1, "text": "chunking is necessary for any application involving vector databases or LLMs: to ensure embedding models can fit the data into their context windows, and to ensure the chunks themselves contain the information necessary for search. All embedding models have context windows, which determine the amount of information in tokens that can be processed into a single fixed size vector. Exceeding this context window may means the excess tokens are truncated, or thrown away, before being processed into a vector. This is potentially harmful as important context could be removed from the representation of the text, which prevents it from being surfaced during a search. Furthermore, it isn’t enough just to right-size your data for a model; the resulting chunks must contain information that is relevant to search over. If the chunk contains a set of sentences that aren’t useful without context, they may not be surfaced when querying! Chunking’s role in semantic search For example, in semantic search, we index a corpus of documents, with each document containing valuable information on a specific topic. Due to the way embedding models work, those documents will need to be chunked, and similarity is determined by chunk-level comparisons to the input query vector. Then, these similar chunks are returned back to the user. By finding an effective chunking strategy, we can ensure our search results accurately capture the essence of the user’s query. If our chunks are too small or too large, it may lead to imprecise search results or missed opportunities to", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "3dff5793-73fd-469d-841b-724290ac6622", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 2, "text": "Then, these similar chunks are returned back to the user. By finding an effective chunking strategy, we can ensure our search results accurately capture the essence of the user’s query. If our chunks are too small or too large, it may lead to imprecise search results or missed opportunities to surface relevant content. As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant. Chunking’s role for agentic applications and retrieval-augmented generation Agents may need access to up-to-date information from databases in order to call tools, make decisions, and respond to user queries. Chunks returned from searches over databases consume context during a session, and ground the agent’s responses. We use the embedded chunks to build the context based on a knowledge base the agent has access to. This context grounds the agent in trusted information. Similar to how semantic search relies on a good chunking strategy to provide usable outputs, agentic applications need meaningful chunks of information in order to proceed. If an agent is misinformed, or provided information without sufficient context, it may waste tokens generating hallucinations or calling the wrong tools. The Role of Chunking for Long Context LLMs In some cases, like when using o1 or Claude 4 Sonnet with a 200k context window,", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "756ea5e0-3bbb-4c2a-884d-466ee50619e3", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 3, "text": "in order to proceed. If an agent is misinformed, or provided information without sufficient context, it may waste tokens generating hallucinations or calling the wrong tools. The Role of Chunking for Long Context LLMs In some cases, like when using o1 or Claude 4 Sonnet with a 200k context window, un-chunked documents may still fit in context. Still, using large chunks may increase latency and cost in downstream responses. Moreover, long context embedding and LLM models suffer fro m the lost-in-the-middle problem , where relevant information buried inside long documents is missed, even when included in generation. The solution to this problem is ensuring the optimal amount of information is passed to a downstream LLM, which necessarily reduces latency and ensures quality. What should we think about when choosing a chunking strategy? Several variables play a role in determining the best chunking strategy, and these variables vary depending on the use case. Here are some key aspects to keep in mind: What kind of data is being chunked? Are you working with long documents, such as articles or books, or shorter content, like tweets, product descriptions, or chat messages? Small documents may not need to be chunked at all, while larger ones may exhibit certain structure that will inform chunking strategy, such as sub-headers or chapters. Which embedding model are you using? Different embedding models have differing capacities for information, especially on specialized domains like code, finance, medical, or legal information. And, the way these models are trained can strongly", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "947f7275-61a3-4ceb-a92e-0db5c34e5029", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 4, "text": "larger ones may exhibit certain structure that will inform chunking strategy, such as sub-headers or chapters. Which embedding model are you using? Different embedding models have differing capacities for information, especially on specialized domains like code, finance, medical, or legal information. And, the way these models are trained can strongly affect how they perform in practice. After choosing an appropriate model for your domain, be sure to adapt your chunking strategy to align with expected document types the model has been trained on. What are your expectations for the length and complexity of user queries? Will they be short and specific or long and complex? This may inform the way you choose to chunk your content as well so that there’s a closer correlation between the embedded query and embedded chunks. How will the retrieved results be utilized within your specific application? For example, will they be used for semantic search, question answering, retrieval augmented generation, or even an agentic workflow? For example, the amount of information a human may review from a search result may be smaller or larger than what an LLM may need to generate a response. These users determine how your data should be represented within the vector database. Answering these questions beforehand will allow you to choose a chunking strategy that balances performance and accuracy. Embedding short and long content When we embed content, we can expect distinct behaviors depending on whether the content is short (like sentences) or long (like paragraphs or entire documents).", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "26991fca-0860-4d63-b281-6e6242dbfdce", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 5, "text": "the vector database. Answering these questions beforehand will allow you to choose a chunking strategy that balances performance and accuracy. Embedding short and long content When we embed content, we can expect distinct behaviors depending on whether the content is short (like sentences) or long (like paragraphs or entire documents). When a sentence is embedded, the resulting vector focuses on the sentence’s specific meaning. This could be handy in situations where the vector search is used for (sentence-level) classification, recommendation systems, or applications that allow for searches over shorter summaries before longer documents are processed. The search process is then, finding sentences similar in meaning to query sentences or questions. In cases where sentences themselves are considered individual documents, you wouldn’t need to chunk at all! When a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text. Larger input text sizes, on the other hand, may introduce noise or dilute the significance of individual sentences or phrases, making finding precise matches when querying the index more difficult. These chunks help support use cases such as question-answering, where answers may be a few paragraphs or more. Many modern AI applications work with longer documents, which almost always require chunking. Chunking methods Fixed-size chunking This is the most common and straightforward approach to chunking: we simply decide", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "5f4fc1d1-883d-43da-bb1f-a3dacd71db62", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 6, "text": "more difficult. These chunks help support use cases such as question-answering, where answers may be a few paragraphs or more. Many modern AI applications work with longer documents, which almost always require chunking. Chunking methods Fixed-size chunking This is the most common and straightforward approach to chunking: we simply decide the number of tokens in our chunk, and use this number to break up our documents into fixed size chunks. Usually, this number is the max context window size of the embedding model (such as 1024 for llama-text-embed-v2, or 8196 for text-embedding-3-small). Keep in mind that different embedding models may tokenize text differently, so you will need to estimate token counts accurately. Fixed-sized chunking will be the best path in most cases, and we recommend starting here and iterating only after determining it insufficient. “Content-aware” Chunking Although fixed-size chunking is quite easy to implement, it can ignore critical structure within documents that can be used to inform relevant chunks. Content-aware chunking refers to strategies that adhere to structure to help inform the meaning of our chunks. Simple Sentence and Paragraph splitting As we mentioned before, some embedding models are optimized for embedding sentence-level content. But sometimes, sentences need to be mined from larger text datasets that aren’t preprocessed. In these cases, it’s necessary to use sentence chunking, and there are several approaches and tools available to do this: Naive splitting: The most naive approach would be to split sentences by periods (“.”), new lines, or white space. NLTK : The", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "b1ece572-07f5-40b6-ab5a-137a06b43906", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 7, "text": "mined from larger text datasets that aren’t preprocessed. In these cases, it’s necessary to use sentence chunking, and there are several approaches and tools available to do this: Naive splitting: The most naive approach would be to split sentences by periods (“.”), new lines, or white space. NLTK : The Natural Language Toolkit (NLTK) is a popular Python library for working with human language data. It provides a trained sentence tokenizer that can split the text into sentences, helping to create more meaningful chunks. spaCy : spaCy is another powerful Python library for NLP tasks. It offers a sophisticated sentence segmentation feature that can efficiently divide the text into separate sentences, enabling better context preservation in the resulting chunks. Recursive Character Level Chunking LangChain implements a RecursiveCharacterTextSplitter that tries to split text using separators in a given order. The default behavior of the splitter uses the [\"\\n\\n\", \"\\n\", \" \", \"\"] separators to break paragraphs, sentences and words depending on a given chunk size. This is a great middle ground between always splitting on a specific character and using a more semantic splitter, while also ensuring fixed chunk sizes when possible. Document structure-based chunking When chunking large documents such as PDFs, DOCX, HTML, code snippets, Markdown files and LaTex, specialized chunking methods can help preserve the original structure of the content during chunk creation. PDF documents contain loads of headers, text, tables, and other bits and pieces that require preprocessing to chunk. LangChain has some handy utilities to help process", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "c970c9fc-9c30-4316-a127-1b09ee7f5437", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 8, "text": "PDFs, DOCX, HTML, code snippets, Markdown files and LaTex, specialized chunking methods can help preserve the original structure of the content during chunk creation. PDF documents contain loads of headers, text, tables, and other bits and pieces that require preprocessing to chunk. LangChain has some handy utilities to help process these documents, while Pinecone Assistant can chunk and processes these for you HTML, from scraped web pages can contain tags (<p> for paragraphs, or <title> for titles) that can inform text to be broken up or identified, like on product pages or blog posts. Roll your own parser, or use LangChain splitters here to process these for chunking Markdown : Markdown is a lightweight markup language commonly used for formatting text. By recognizing the Markdown syntax (e.g., headings, lists, and code blocks), you can intelligently divide the content based on its structure and hierarchy, resulting in more semantically coherent chunks. LaTex : LaTeX is a document preparation system and markup language often used for academic papers and technical documents. By parsing the LaTeX commands and environments, you can create chunks that respect the logical organization of the content (e.g., sections, subsections, and equations), leading to more accurate and contextually relevant results. Semantic Chunking A new experimental technique for approaching chunking was first introduced by Greg Kamradt . In his notebook , Kamradt rightfully points to the fact that a global chunking size may be too trivial of a mechanism to take into account the meaning of segments within the document.", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "ea77282e-37c6-47fd-a3f6-e54b4ff49f3e", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 9, "text": "results. Semantic Chunking A new experimental technique for approaching chunking was first introduced by Greg Kamradt . In his notebook , Kamradt rightfully points to the fact that a global chunking size may be too trivial of a mechanism to take into account the meaning of segments within the document. If we use this type of mechanism, we can’t know if we’re combining segments that have anything to do with one another. Luckily, if you’re building an application with LLMs, you most likely already have the ability to create embeddings - and embeddings can be used to extract the semantic meaning present in your data. This semantic analysis can be used to create chunks that are made up of sentences that talk about the same theme or topic. Semantic chunking involves breaking a document into sentences, grouping each sentence with its surrounding sentences, and generating embeddings for these groups. By comparing the semantic distance between each group and its predecessor, you can identify where the topic or theme shifts, which defines the chunk boundaries. You can learn more about applying semantic chunking with Pinecone here. Contextual Chunking with LLMs Sometimes, it’s not possible to chunk information from a larger complex document without losing the context entirely. This can happen when the documents are many hundreds of pages, change topics frequently, or require understanding from many related portions of the document. Anthropic introduced contextual retrieval in 2024 to help address this problem. Anthropic prompted a Claude instance with an entire document", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "9ef71e31-1978-4b81-bb15-b04f5bd226a9", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 10, "text": "document without losing the context entirely. This can happen when the documents are many hundreds of pages, change topics frequently, or require understanding from many related portions of the document. Anthropic introduced contextual retrieval in 2024 to help address this problem. Anthropic prompted a Claude instance with an entire document and it’s chunk, in order to generate a contextualized description, which is appended to the chunk and then embedded. The description helps retain the high-level summary meaning of the document to the chunk, which exposes this information to incoming queries. To avoid processing the document each time, it’s cached within the prompt for all necessary chunks. You can learn more about contextual retrieval in our video here and our code example here. Figuring out the best chunking strategy for your application Here are some pointers to help decide a strategy if fixed chunking doesn’t easily apply to your use case. Selecting a Range of Chunk Sizes - Once your data is preprocessed, the next step is to choose a range of potential chunk sizes to test. As mentioned previously, the choice should take into account the nature of the content (e.g., short messages or lengthy documents), the embedding model you’ll use, and its capabilities (e.g., token limits). The objective is to find a balance between preserving context and maintaining accuracy. Start by exploring a variety of chunk sizes, including smaller chunks (e.g., 128 or 256 tokens) for capturing more granular semantic information and larger chunks (e.g., 512 or 1024 tokens)", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "b822cd89-9130-408e-9e68-7a0622a3198d", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 11, "text": "use, and its capabilities (e.g., token limits). The objective is to find a balance between preserving context and maintaining accuracy. Start by exploring a variety of chunk sizes, including smaller chunks (e.g., 128 or 256 tokens) for capturing more granular semantic information and larger chunks (e.g., 512 or 1024 tokens) for retaining more context. Evaluating the Performance of Each Chunk Size - In order to test various chunk sizes, you can either use multiple indices or a single index with multiple namespaces . With a representative dataset, create the embeddings for the chunk sizes you want to test and save them in your index (or indices). You can then run a series of queries for which you can evaluate quality, and compare the performance of the various chunk sizes. This is most likely to be an iterative process, where you test different chunk sizes against different queries until you can determine the best-performing chunk size for your content and expected queries. Post-processing chunks with chunk expansion It’s important to remember that you aren’t entirely married to your chunking strategy. When querying chunked data in a vector database, the retrieved information is typically the top semantically similar chunks given a user query. But users, agents or LLMs may need more surrounding context in order to adequately interpret the chunk. Chunk expansion is an easy way to post-process chunked data from a database, by retrieving neighboring chunks within a window for each chunk in a retrieved set of chunks. Chunks could be", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "b409b0df-6d32-4962-8043-705e299a27d9", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 12, "text": "query. But users, agents or LLMs may need more surrounding context in order to adequately interpret the chunk. Chunk expansion is an easy way to post-process chunked data from a database, by retrieving neighboring chunks within a window for each chunk in a retrieved set of chunks. Chunks could be expanded to paragraphs, pages, or even whole documents depending on your use case. Coupling a chunking strategy with a good chunk expansion on querying can ensure low latency searches without compromising on context. Wrapping up Chunking your content is may appear straightforward in most cases - but it could present some challenges when you start wandering off the beaten path. There’s no one-size-fits-all solution to chunking, so what works for one use case may not work for another. Want to get started experimenting with chunking strategies? Create a free Pinecone account and check out our example notebooks to implement chunking via various applications like semantic search, retrieval augmented generation or agentic applications with Pinecone. Share: Was this article helpful? Yes No Recommended for you Further Reading Jul 15, 2025 What is Context Engineering? 8 min read Jun 25, 2025 Beyond the hype: Why RAG remains essential for modern AI 7 min read roughly-explained Jun 12, 2025 Retrieval-Augmented Generation (RAG) 13 min read", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "6935fd3a-557c-4680-9127-16b17991d21a", "source": "https://www.pinecone.io/learn/chunking-strategies/", "chunk_index": 13, "text": "min read roughly-explained Jun 12, 2025 Retrieval-Augmented Generation (RAG) 13 min read", "metadata": {"url": "https://www.pinecone.io/learn/chunking-strategies/", "title": "Chunking Strategies for LLM Applications", "date": "6/28/2025", "type": "blog", "author": "Pinecone"}}
{"id": "cfdf1f77-2837-4f78-bb8b-445554d07e02", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 0, "text": "Introduction to Retrieval Augmented Generation (RAG) | Weaviate ← Back to Blogs Skip to main content Weaviate's Query Agent is Generally Available – Read the Announcement (Sep 17) Despite the steady release of increasingly larger and smarter models, state-of-the-art generative large language models (LLMs) still have a big problem: they struggle with tasks that require specialized knowledge. This lack of specialized knowledge can lead to issues like hallucinations, where the model generates inaccurate or fabricated information. Retrieval-Augmented Generation (RAG) helps mitigate this by allowing the model to pull in real-time, niche data from external sources, enhancing its ability to provide accurate and detailed responses. Despite these limitations, generative models are impactful tools that automate mundane processes, assist us in our everyday work, and enable us to interact with data in new ways. So how can we leverage their broad knowledge while also making them work for our specific use cases? The answer lies in providing generative models with task-specific data. In this article, we take a deep dive into Retrieval Augmented Generation (RAG), a framework that enhances the capabilities of generative models by allowing them to reference external data. We’ll explore the limitations of generative models that led to the creation of RAG, explain how RAG works, and break down the architecture behind RAG pipelines. We’ll also get practical and outline some real-world RAG use cases, suggest concrete ways to implement RAG, introduce you to a few advanced RAG techniques, and discuss RAG evaluation methods. note LLM is a broad", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "7ff424f0-22bb-48a5-a457-646dc22b85c2", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 1, "text": "creation of RAG, explain how RAG works, and break down the architecture behind RAG pipelines. We’ll also get practical and outline some real-world RAG use cases, suggest concrete ways to implement RAG, introduce you to a few advanced RAG techniques, and discuss RAG evaluation methods. note LLM is a broad term that refers to language models trained on large datasets that are capable of performing a variety of text- and language-related tasks. LLMs that generate novel text in response to a user prompt, like those used in chatbots, are called generative LLMs, or generative models . LLMs that encode text data in the semantic space are called embedding models . Thus, we use the terms generative model and embedding model to distinguish between these two types of models in this article. Limitations of generative models ​ Generative models are trained on large datasets, including (but not limited to) social media posts, books, scholarly articles and scraped webpages, allowing them to acquire a sense of general knowledge. As a result, these models can generate human-like text, respond to diverse questions, and assist with tasks like answering, summarizing, and creative writing. However, training datasets for generative models are inevitably incomplete, as they lack information on niche topics and new developments beyond the dataset’s cutoff date. Generative models also lack access to proprietary data from internal databases or repositories. Furthermore, when these models don’t know the answer to a question, they often guess, and sometimes not very well. This tendency to generate incorrect", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "dcf27188-bfc1-4446-a582-0c63fd410854", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 2, "text": "lack information on niche topics and new developments beyond the dataset’s cutoff date. Generative models also lack access to proprietary data from internal databases or repositories. Furthermore, when these models don’t know the answer to a question, they often guess, and sometimes not very well. This tendency to generate incorrect or fabricated information in a convincing manner is known as hallucination, and can cause real reputational damage in client-facing AI applications. The key to enhancing performance on specialized tasks and reducing hallucinations is to provide generative models with additional information not found in their training data. This is where RAG comes in. What is Retrieval Augmented Generation (RAG)? ​ Retrieval-Augmented Generation (RAG) is a framework that augments the general knowledge of a generative LLM by providing it with additional data relevant to the task at hand retrieved from an external data source. External data sources can include internal databases, files, and repositories, as well as publicly available data such as news articles, websites, or other online content. Access to this data empowers the model to respond more factually, cite its sources in its responses, and avoid “guessing” when prompted about information not found in the model’s original training dataset. Common use cases for RAG include retrieving up-to-date information, accessing specialized domain knowledge, and answering complex, data-driven queries. RAG architecture ​ The basic parts of a RAG pipeline can be broken down into three components : an external knowledge source, a prompt template, and a generative model. Together, these components enable", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "37165fd6-2b51-45f6-9fde-8f0f5805ae0c", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 3, "text": "use cases for RAG include retrieving up-to-date information, accessing specialized domain knowledge, and answering complex, data-driven queries. RAG architecture ​ The basic parts of a RAG pipeline can be broken down into three components : an external knowledge source, a prompt template, and a generative model. Together, these components enable LLM-powered applications to generate more accurate responses by leveraging valuable task-specific data. External knowledge source ​ Without access to external knowledge, a generative model is limited to generating responses based only on its parametric knowledge , which is learned during the model training phase. With RAG, we have the opportunity to incorporate external knowledge sources , also referred to as non-parametric knowledge , in our pipeline. External data sources are often task-specific and likely beyond the scope of the model’s original training data, or its parametric knowledge. Furthermore, they are often stored in vector databases and can vary widely in topic and format. Popular sources of external data include internal company databases, legal codes and documents, medical and scientific literature, and scraped webpages. Private data sources can be used in RAG as well. Personal AI assistants, like Microsoft’s Copilot, leverage multiple sources of personal data including, emails, documents, and instant messages to provide tailored responses and automate tasks more efficiently. Prompt template ​ Prompts are the tools we use to communicate our requests to generative models. Prompts may contain several elements, but generally include a query, instructions, and context that guides the model in generating a relevant response. Prompt templates", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "c706d459-eea3-4dcb-87a3-c4a35f588104", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 4, "text": "messages to provide tailored responses and automate tasks more efficiently. Prompt template ​ Prompts are the tools we use to communicate our requests to generative models. Prompts may contain several elements, but generally include a query, instructions, and context that guides the model in generating a relevant response. Prompt templates provide a structured way to generate standardized prompts, in which various queries and contexts can be inserted. In a RAG pipeline, relevant data is retrieved from an external data source and inserted into prompt templates, thus augmenting the prompt. Essentially, prompt templates act as the bridge between the external data and the model, providing the model with contextually relevant information during inference to generate an accurate response. prompt_template = \"Context information is below.\\n\" \"---------------------\\n\" \"{context_str}\\n\" \"---------------------\\n\" \"Given the context information and not prior knowledge, \" \"answer the query.\\n\" \"Query: {query_str}\\n\" \"Answer: \" Generative large language model (LLM) ​ The final component in RAG is the generative LLM, or generative model, which is used to generate a final response to the user’s query. The augmented prompt, enriched with information from the external knowledge base, is sent to the model, which generates a response that combines the model's internal knowledge with the newly retrieved data. Now that we’ve covered the RAG architecture and its key components, let’s see how they come together in a RAG workflow. How does RAG work? ​ RAG is a multi-step framework that works in two stages. First, the external knowledge is preprocessed and prepared for retrieval during", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "8aa8d443-9ac2-4cd2-b838-1c2430f33ceb", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 5, "text": "newly retrieved data. Now that we’ve covered the RAG architecture and its key components, let’s see how they come together in a RAG workflow. How does RAG work? ​ RAG is a multi-step framework that works in two stages. First, the external knowledge is preprocessed and prepared for retrieval during the ingestion stage. Next, during the inference stage, the model retrieves relevant data from the external knowledge base, augments it with the user’s prompt, and generates a response. Now, let’s take a closer look at each of these stages. Stage 1: Ingestion ​ First, the external knowledge source needs to be prepared. Essentially, the external data needs to be cleaned and transformed into a format that the model can understand. This is called the ingestion stage . During ingestion, text or image data is transformed from its raw format into embeddings through a process called vectorization . Once embeddings are generated, they need to be stored in a manner that allows them to be retrieved at a later time. Most commonly, these embeddings are stored in a vector database, which allows for quick, efficient retrieval of the information for downstream tasks. Stage 2: Inference ​ After external data is encoded and stored, it’s ready to be retrieved during inference , when the model generates a response or answers a question. Inference is broken down into three steps: retrieval, augmentation, and generation. Retrieval ​ The inference stage starts with retrieval, in which data is retrieved from an external knowledge source in", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "7026a438-b600-40f5-9b5a-97fe72130248", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 6, "text": "and stored, it’s ready to be retrieved during inference , when the model generates a response or answers a question. Inference is broken down into three steps: retrieval, augmentation, and generation. Retrieval ​ The inference stage starts with retrieval, in which data is retrieved from an external knowledge source in relation to a user query. Retrieval methods vary in format and complexity, however in the naive RAG schema, in which external knowledge is embedded and stored in a vector database, similarity search is the simplest form of retrieval. To perform similarity search, the user query must be first embedded in the same multi-dimensional space as the external data, which allows for direct comparison between the query and embedded external data. During similarity search , the distance between the query and external data points is calculated, returning those with the shortest distance and completing the retrieval process. Augmentation ​ Once the most relevant data points from the external data source have been retrieved, the augmentation process integrates this external information by inserting it into a predefined prompt template. Generation ​ After the augmented prompt is injected into the model’s context window, it proceeds to generate the final response to the user’s prompt. In the generation phase, the model combines both its internal language understanding and the augmented external data to produce a coherent, contextually appropriate answer. This step involves crafting the response in a fluent, natural manner while drawing on the enriched information to ensure that the output is both accurate", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "050b0a3c-7c98-4fdf-9061-2c79f67a821b", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 7, "text": "prompt. In the generation phase, the model combines both its internal language understanding and the augmented external data to produce a coherent, contextually appropriate answer. This step involves crafting the response in a fluent, natural manner while drawing on the enriched information to ensure that the output is both accurate and relevant to the user's query. While augmentation is about incorporating external facts, generation is about transforming that combined knowledge into a well-formed, human-like output tailored to the specific request. RAG use cases ​ Now that we’ve covered what RAG is, how it works, and its architecture, let’s explore some practical use cases to see how this framework is applied in real-world scenarios. Augmenting generative LLMs with up-to-date, task-specific data boosts their accuracy, relevance, and ability to handle specialized tasks. Consequently, RAG is widely used for real-time information retrieval, creating content recommendation systems, and building personal AI assistants. Real-time information retrieval ​ When used alone, generative models are limited to retrieving only information found in their training dataset. When used in the context of RAG, however, these models can retrieve data and information from external sources, ensuring more accurate and up-to-date responses. One such example is ChatGPT-4o’s ability to access and retrieve information directly from the web in real-time. This is an example of a RAG use case that leverages an external data source that is not embedded in a vector database and can be especially useful in responding to user queries regarding the news or other current events, such", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "f26a42b6-7616-4109-833a-03c5bb4bf2c2", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 8, "text": "and retrieve information directly from the web in real-time. This is an example of a RAG use case that leverages an external data source that is not embedded in a vector database and can be especially useful in responding to user queries regarding the news or other current events, such as stock prices, travel advisories, and weather updates. Content recommendation systems ​ Content recommendation systems analyze user data and preferences to suggest relevant products or content to users. Traditionally, these systems required sophisticated ensemble models and massive user preference datasets. RAG simplifies recommendation systems directly integrating external, contextually relevant user data with the model's general knowledge, allowing it to generate personalized recommendations. Personal AI assistants ​ Our personal data, including files, emails, Slack messages, and notes are a valuable source of data for generative models. Running RAG over our personal data enables us to interact with it in a conversational way, increasing efficiency and allowing for the automation of mundane tasks. With AI assistants, such as Microsoft’s Copilot and Notion’s Ask AI, we can use simple prompts to search for relevant documents, write personalized emails, summarize documents and meeting notes, schedule meetings, and more. How to implement RAG ​ Now that we know how RAG works, let’s explore how to build a functional RAG pipeline. RAG can be implemented through a number of different frameworks, which simplify the building process by providing pre-built tools and modules for integrating individual RAG components as well as external services like vector databases, embedding", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "6b613e15-7049-431b-b14c-7d3deb571dd0", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 9, "text": "that we know how RAG works, let’s explore how to build a functional RAG pipeline. RAG can be implemented through a number of different frameworks, which simplify the building process by providing pre-built tools and modules for integrating individual RAG components as well as external services like vector databases, embedding generation tools, and other APIs. LangChain, LlamaIndex, and DSPy are all robust open source Python libraries with highly engaged communities that offer powerful tools and integrations for building and optimizing RAG pipelines and LLM applications. LangChain provides building blocks, components, and third-party integrations to aid in the development of LLM-powered applications. It can be used with LangGraph for building agentic RAG pipelines and LangSmith for RAG evaluation. LlamaIndex is a framework that offers tools to build LLM-powered applications integrated with external data sources. LlamaIndex maintains the LlamaHub , a rich repository of data loaders, agent tools, datasets, and other components, that streamline the creation of RAG pipelines. DSPy is a modular framework for optimizing LLM pipelines. Both LLMs and RMs (Retrieval Models) can be configured within DSPy, allowing for seamless optimization of RAG pipelines. note Weaviate provides integrations and recipes for each of these frameworks. For specific examples, take a look at our notebooks that show how to build RAG pipelines with Weaviate and LlamaIndex and DSPy . If you’re looking for a way to get up and running with RAG quickly, check out Verba , an open source out-of-the-box RAG application with a shiny, pre-built frontend. Verba enables you", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "ea2f8bd5-0ee4-4833-8a74-a222f011f147", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 10, "text": "look at our notebooks that show how to build RAG pipelines with Weaviate and LlamaIndex and DSPy . If you’re looking for a way to get up and running with RAG quickly, check out Verba , an open source out-of-the-box RAG application with a shiny, pre-built frontend. Verba enables you to visually explore datasets, extract insights, and build customizable RAG pipelines in just a few easy steps, without having to learn an entirely new framework. Verba is a multifunctional tool that can be used as a playground for testing and experimenting with RAG pipelines as well as for personal tasks like assisting with research, analyzing internal documents, and streamlining various RAG-related tasks. Your browser does not support the video tag. Out-of-the-box RAG implementation with Verba RAG techniques ​ The vanilla RAG workflow is generally composed of an external data source embedded in a vector database retrieved via similarity search. However, there are several ways to enhance RAG workflows to yield more accurate and robust results, which collectively are referred to as advanced RAG. Functionality of RAG pipelines can be further extended by incorporating the use of graph databases and agents, which enable even more advanced reasoning and dynamic data retrieval. In this next section, we’ll go over some common advanced RAG techniques and give you an overview of Agentic RAG and Graph RAG. Advanced RAG ​ Advanced RAG techniques can be deployed at various stages in the pipeline. Pre-retrieval strategies like metadata filtering and text chunking can help improve the", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "df6fa153-474a-49bd-84d4-5786cc17c468", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 11, "text": "In this next section, we’ll go over some common advanced RAG techniques and give you an overview of Agentic RAG and Graph RAG. Advanced RAG ​ Advanced RAG techniques can be deployed at various stages in the pipeline. Pre-retrieval strategies like metadata filtering and text chunking can help improve the retrieval efficiency and relevance by narrowing down the search space and ensuring only the most relevant sections of data are considered. Employing more advanced retrieval techniques, such as hybrid search , which combines the strengths of similarity search with keyword search, can also yield more robust retrieval results. Finally, re-ranking retrieved results with a ranker model and using a generative LLM fine-tuned on domain-specific data help improve the quality of generated results. For a more in-depth exploration of this topic, check out our blog post on advanced RAG techniques . Agentic RAG ​ AI agents are autonomous systems that can interpret information, formulate plans, and make decisions. When added to a RAG pipeline, agents can reformulate user queries and re-retrieve more relevant information if initial results are inaccurate or irrelevant. Agentic RAG can also handle more complex queries that require multi-step reasoning, like comparing information across multiple documents, asking follow-up questions, and iteratively adjusting retrieval and generation strategies. To take a closer look at a RAG pipeline that incorporates agents, check out this blog on Agentic RAG . Graph RAG ​ While traditional RAG excels at simple question answering tasks that can be resolved by retrieval alone, it is unable", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "a7a74e7c-e674-4f31-aa2f-cb1d723f1706", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 12, "text": "questions, and iteratively adjusting retrieval and generation strategies. To take a closer look at a RAG pipeline that incorporates agents, check out this blog on Agentic RAG . Graph RAG ​ While traditional RAG excels at simple question answering tasks that can be resolved by retrieval alone, it is unable to answer questions and draw conclusions over an entire external knowledge base. Graph RAG aims to solve this by using a generative model to create a knowledge graph that extracts and stores the relationships between key entities and can then be added as a data source to the RAG pipeline. This enables the RAG system to respond to queries asking to compare and summarize multiple documents and data sources. For more information on building graph RAG pipelines, take a look at Microsoft’s GraphRAG package and documentation . How to evaluate RAG ​ RAG is a multi-stage, multi-step framework that requires both holistic and granular evaluation . This approach ensures both component-level reliability and high-level accuracy. In this section, we’ll explore both of these evaluation approaches and touch on RAGAS, a popular evaluation framework. Component-level evaluation ​ On a component-level, RAG evaluation generally focuses on assessing the quality of the retriever and the generator, as they both play critical roles in producing accurate and relevant responses. Evaluation of the retriever centers around accuracy and relevance. In this context, accuracy measures how precisely the retriever selects information that directly addresses the query, while relevance assesses how closely the retrieved data aligns with", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "79add31c-d711-459f-be45-71f8e0459ff1", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 13, "text": "and the generator, as they both play critical roles in producing accurate and relevant responses. Evaluation of the retriever centers around accuracy and relevance. In this context, accuracy measures how precisely the retriever selects information that directly addresses the query, while relevance assesses how closely the retrieved data aligns with the specific needs and context of the query. On the other hand, evaluation of the generator focuses on faithfulness and correctness. Faithfulness evaluates whether the response generated by the model accurately represents the information from the relevant documents and checks how consistent the response is with the original sources. Correctness assesses whether the generated response is truly factual and aligns with the ground truth or expected answer based on the query's context. End-to-end evaluation ​ Although the retriever and the generator are two distinct components, they rely on each other to produce coherent responses to user queries. Calculating Answer Semantic Similarity is a simple and efficient method of assessing how well the retriever and generator work together. Answer Semantic Similarity calculates the semantic similarity between generated responses and ground truth samples. Generated responses with a high degree of similarity to ground truth samples are indicative of a pipeline that can retrieve relevant information and generate contextually appropriate responses. note RAG evaluation frameworks offer structured methods, tools, or platforms to evaluate RAG pipelines. RAGAS (Retrieval Augmented Generation Assessment) is an especially popular framework, as it offers a suite of metrics to assess retrieval relevance, generation quality, and faithfulness without requiring human-labeled", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "9952c3ad-cd62-4ae1-a0b8-7c4df8be8ef9", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 14, "text": "retrieve relevant information and generate contextually appropriate responses. note RAG evaluation frameworks offer structured methods, tools, or platforms to evaluate RAG pipelines. RAGAS (Retrieval Augmented Generation Assessment) is an especially popular framework, as it offers a suite of metrics to assess retrieval relevance, generation quality, and faithfulness without requiring human-labeled data. Listen to this episode of the Weaviate podcast to learn more about how RAGAS works and advanced techniques for optimizing RAGAS scores, straight from the creators themselves. RAG vs. fine-tuning ​ RAG is only one of several methods to expand the capabilities and mitigate the limitations of generative LLMs. Fine-tuning LLMs is a particularly popular technique for tailoring models to perform highly specialized tasks by training them on domain-specific data. While fine-tuning may be ideal for certain use cases, such as training a LLM to adopt a specific tone or writing style, RAG is often the lowest-hanging fruit for improving model accuracy, reducing hallucinations, and tailoring LLMs for specific tasks. The beauty of RAG lies in the fact that the weights of the underlying generative model don’t need to be updated, which can be costly and time-consuming. RAG allows models to access external data dynamically, improving accuracy without costly retraining. This makes it a practical solution for applications needing real-time information. In the next section, we’ll dive deeper into the architecture of RAG and how its components work together to create a powerful retrieval-augmented system. Summary ​ In this article, we introduced you to RAG, a framework that leverages", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "879c1bbd-a748-45b6-bafc-55c12e37991d", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 15, "text": "retraining. This makes it a practical solution for applications needing real-time information. In the next section, we’ll dive deeper into the architecture of RAG and how its components work together to create a powerful retrieval-augmented system. Summary ​ In this article, we introduced you to RAG, a framework that leverages task-specific external knowledge to improve the performance of applications powered by generative models. We learned about the different components of RAG pipelines, including external knowledge sources, prompt templates, and generative models as well as how they work together in retrieval, augmentation, and generation. We also discussed popular RAG use cases and frameworks for implementation, such as LangChain, LlamaIndex, and DSPy. Finally, we touched on some specialized RAG techniques, including advanced RAG methods, agentic RAG, and graph RAG as well as methods for evaluating RAG pipelines. At a minimum, each section in this post deserves its own individual blog post, if not an entire chapter in a book. As a result, we’ve put together a resource guide with academic papers, blog posts, YouTube videos, tutorials, notebooks, and recipes to help you learn more about the topics, frameworks, and methods presented in this article. Resource guide ​ 📄 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Original RAG paper) 👩‍🍳 Getting Started with RAG in DSPy (Recipe) 👩‍🍳 Naive RAG with LlamaIndex (Recipe) 📝 Advanced RAG Techniques (Blog post) 📒 Agentic RAG with Multi-Document Agents (Notebook) 📝 An Overview of RAG Evaluation (Blog post) 📄 Evaluation of Retrieval-Augmented Generation: A Survey (Academic paper) Ready", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "34eb3965-a0c1-4c52-8bcb-ee3a7ebb7f40", "source": "https://weaviate.io/blog/introduction-to-rag", "chunk_index": 16, "text": "NLP Tasks (Original RAG paper) 👩‍🍳 Getting Started with RAG in DSPy (Recipe) 👩‍🍳 Naive RAG with LlamaIndex (Recipe) 📝 Advanced RAG Techniques (Blog post) 📒 Agentic RAG with Multi-Document Agents (Notebook) 📝 An Overview of RAG Evaluation (Blog post) 📄 Evaluation of Retrieval-Augmented Generation: A Survey (Academic paper) Ready to start building? ​ Check out the Quickstart tutorial , or build amazing apps with a free trial of Weaviate Cloud (WCD) . GitHub Forum Slack X (Twitter) Don't want to miss another blog post? Sign up for our bi-weekly newsletter to stay updated! By submitting, I agree to the Terms of Service and Privacy Policy . Limitations of generative models What is Retrieval Augmented Generation (RAG)? RAG architecture External knowledge source Prompt template Generative large language model (LLM) How does RAG work? Stage 1: Ingestion Stage 2: Inference RAG use cases Real-time information retrieval Content recommendation systems Personal AI assistants How to implement RAG RAG techniques Advanced RAG Agentic RAG Graph RAG How to evaluate RAG Component-level evaluation End-to-end evaluation RAG vs. fine-tuning Summary Resource guide", "metadata": {"url": "https://weaviate.io/blog/introduction-to-rag", "title": "Introduction to Retrieval Augmented Generation (RAG)", "date": "10/15/2024", "type": "blog", "author": "Weaviate"}}
{"id": "573d414b-9286-403c-ad21-f75b4fe6ac0a", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 0, "text": "Chunking Strategies to Improve Your RAG Performance | Weaviate ← Back to Blogs Skip to main content Weaviate's Query Agent is Generally Available – Read the Announcement (Sep 17) If you’re building AI applications with Large Language Models (LLMs), grounding generated text responses with your specific data is key to accurate answers. Retrieval-Augmented Generation (RAG) connects a large language model to an outside knowledge source, like a vector database . This lets it find relevant facts before creating a response. The quality of your retrieval process is one of the biggest factors influencing your application's performance. Many developers focus on picking the right vector database or embedding model. But the most important step is often how you prepare the data itself. This is where chunking comes in. In this post, we’ll review some essential chunking strategies, from fundamentals to advanced techniques, their trade-offs, and tips for choosing the right approach for your RAG application. What is Chunking? ​ In simple terms, chunking is the process of breaking down large documents into smaller, manageable pieces called chunks . This is a crucial first step when preparing data for use with Large Language Models (LLMs). The main reason is that LLMs have a limited context window, meaning they can only focus on a certain amount of text at once. If there is too much text within the context window, important details are lost, resulting in incomplete or inaccurate answers. Chunking solves this by creating smaller, focused pieces of content that an LLM", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "50ac7f7f-5665-4412-b06c-facf1b61f9f8", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 1, "text": "a limited context window, meaning they can only focus on a certain amount of text at once. If there is too much text within the context window, important details are lost, resulting in incomplete or inaccurate answers. Chunking solves this by creating smaller, focused pieces of content that an LLM can use to answer the users query without getting lost in irrelevant information. The size, content, and semantic boundaries of each chunk influence retrieval performance, and so deciding which technique to use can have a huge downstream impact on your RAG system’s performance. Why is Chunking so Important for RAG? ​ Chunking is arguably the most important factor for RAG performance . How you split your documents affects your system’s ability to find relevant information and give accurate answers. When a RAG system performs poorly, the issue is often not the retriever—it’s the chunks . Even a perfect retrieval system fails if it searches over poorly prepared data. This creates a fundamental challenge: your chunks need to be easy for vector search to find, while also giving the LLM enough context to create useful answers. 1. Optimizing for Retrieval Accuracy ​ The first step is making sure your system can find the right information in your vector database. Vector search does this by comparing user queries with the embeddings of your chunks. Here's the problem with chunks that are too large : they often mix multiple ideas together, and subtopics can get lost or muddled. Think of it like trying", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "95073edc-6038-47d3-b36d-59ce7ccf7123", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 2, "text": "find the right information in your vector database. Vector search does this by comparing user queries with the embeddings of your chunks. Here's the problem with chunks that are too large : they often mix multiple ideas together, and subtopics can get lost or muddled. Think of it like trying to describe a book by averaging all its chapters. This creates a noisy, “averaged” embedding that doesn’t clearly represent any single topic, making it hard for the vector retrieval step to find all the relevant context. Chunks that are small and focused capture one clear idea. This results in a precise embedding that can encode all the nuanced parts of the content. This makes it much easier for your system to find the right information. 2. Preserving Context for Generation ​ After your system finds the best chunks, they’re passed to the LLM. This is where context quality determines the quality of the outputted response. Here's a simple test: if a chunk makes sense to you when read alone, it will make sense to the LLM too. Chunks that are too small fail this test. Imagine reading a single sentence from the middle of a research paper—even humans would struggle to understand what's happening without more context. Chunks that are too large create a different problem. LLM performance degrades with longer context inputs due to attention dilution and the \"lost in the middle\" effect, where models have trouble accessing information buried in the middle of long contexts while still handling", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "73fd488b-94cd-4374-bc10-9b52ae204301", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 3, "text": "to understand what's happening without more context. Chunks that are too large create a different problem. LLM performance degrades with longer context inputs due to attention dilution and the \"lost in the middle\" effect, where models have trouble accessing information buried in the middle of long contexts while still handling the beginning and end reasonably well. As context length increases, the model's attention gets spread too thin across all the input, making it less accurate at finding relevant information, causing more errors in reasoning, and increasing the likelihood of hallucinating responses. The Chunking Sweet Spot ​ You want to preserve the author's \"train of thought\" while creating chunks that are small enough for precise retrieval but complete enough to give the LLM full context. This is part of context engineering : preparing the input to the LLM in a way that it can understand it and generate accurate responses. When you get this balance right, several things improve: Improves Retrieval Quality: By creating focused, semantically complete chunks, you enable the retrieval system to pinpoint the most precise context for a query. Manages the LLM's Context Window: Effective chunking ensures only relevant data gets passed to the LLM, helping to avoid too long context lengths that can confuse the model Reduces Hallucinations: By providing the model with small, highly relevant chunks, you ground its response in factual data and minimize the risk that it will invent information. Enhances Efficiency and Reduces Cost: Processing smaller chunks is faster and more computationally efficient,", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "827907ae-1c73-4069-b831-5d2a5d0fd455", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 4, "text": "too long context lengths that can confuse the model Reduces Hallucinations: By providing the model with small, highly relevant chunks, you ground its response in factual data and minimize the risk that it will invent information. Enhances Efficiency and Reduces Cost: Processing smaller chunks is faster and more computationally efficient, which leads to quicker response times and lower costs from LLM usage. info If you’re looking for a hands-on Python tutorial, check out this unit in the Weaviate Academy: https://docs.weaviate.io/academy/py/standalone/chunking Pre-Chunking vs Post-Chunking ​ Now that we covered the fundamental dilemma of chunking, we can explore when to perform the chunking step in our RAG pipeline. This decision leads to two primary strategies: the standard pre-chunking and a more advanced alternative, post-chunking . Pre-chunking is the most common method. It processes documents asynchronously by breaking them into smaller pieces before embedding and storing them in the vector database. This approach requires upfront decisions about chunk size and boundaries, but enables fast retrieval at query time since all chunks are pre-computed and indexed. Post-chunking takes a different approach by embedding entire documents first, then performing chunking at query time only on the documents that are actually retrieved . The chunked results can be cached, so the system becomes faster over time as frequently accessed documents build up cached chunks. This method avoids chunking documents that may never be queried while allowing for more dynamic, context-aware chunking strategies based on the specific query. However, it introduces latency on first access and requires", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "f8497b31-21f4-40a0-b534-06dc0fa79604", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 5, "text": "can be cached, so the system becomes faster over time as frequently accessed documents build up cached chunks. This method avoids chunking documents that may never be queried while allowing for more dynamic, context-aware chunking strategies based on the specific query. However, it introduces latency on first access and requires additional infrastructure decisions. info We built a post-chunking strategy into Elysia, our open source agentic RAG framework. You can read more about that here . Chunking Strategies ​ The best chunking strategy depends on the type of documents you are working with and the needs of your RAG application. The methods below are designed primarily for text-based documents. For other formats, like PDFs, additional steps are needed to convert them into clean text. info Working with PDFs Before chunking a PDF, you need clean, structured text. PDFs are a visual format, so extracting text can be tricky. Columns, tables, headers, or scanned pages can make text extraction unreliable. For scanned documents, Optical Character Recognition (OCR) is required to get any text. Pro Tip : The most reliable approach is first to convert PDFs into a structured format like Markdown. This preprocessing step ensures you have clean, logically ordered text before applying any of the chunking strategies below. Simple Chunking Techniques ​ Fixed-Size Chunking (or Token Chunking) ​ Fixed-size chunking is the simplest and most straightforward approach. It splits text into chunks of a predetermined size , often measured in tokens (pieces of text that the model processes) or characters. This", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "8e053d12-256d-41a5-831c-590005c93c88", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 6, "text": "applying any of the chunking strategies below. Simple Chunking Techniques ​ Fixed-Size Chunking (or Token Chunking) ​ Fixed-size chunking is the simplest and most straightforward approach. It splits text into chunks of a predetermined size , often measured in tokens (pieces of text that the model processes) or characters. This method is easy to implement but does not respect the semantic structure of the text. As a result, it can cut off in the middle of sentences or even words, resulting in awkward breaks. A common solution is chunk overlap , where some tokens from the end of one chunk are repeated at the beginning of the next. This preserves context that might otherwise be lost at chunk boundaries. Key considerations: Chunk Size: A common starting point is a chunk size that aligns with the context window of the embedding model. Smaller chunks can be better for capturing fine-grained details, while larger chunks might be more suitable for understanding broader themes. Chunk Overlap: A typical overlap is between 10% and 20% of the chunk size. info When to use: Quick prototyping and getting a baseline for how well your RAG system performs. It's the easiest place to start, especially when you're dealing with documents that don't have a consistent structure or when you're not sure what you're working with yet. Just make sure to use a decent overlap - 10-20% - so you don't lose important context when information gets split across chunks. Code example: from typing import List import", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "0a3bd73a-c1f2-4107-9301-00eb3b3a741c", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 7, "text": "you're dealing with documents that don't have a consistent structure or when you're not sure what you're working with yet. Just make sure to use a decent overlap - 10-20% - so you don't lose important context when information gets split across chunks. Code example: from typing import List import re # Split the text into units (words, in this case) def word_splitter ( source_text : str ) - > List [ str ] : source_text = re . sub ( \"\\s+\" , \" \" , source_text ) # Replace multiple whitespces return re . split ( \"\\s\" , source_text ) # Split by single whitespace def get_chunks_fixed_size_with_overlap ( text : str , chunk_size : int , overlap_fraction : float = 0.2 ) - > List [ str ] : text_words = word_splitter ( text ) overlap_int = int ( chunk_size * overlap_fraction ) chunks = [ ] for i in range ( 0 , len ( text_words ) , chunk_size ) : chunk_words = text_words [ max ( i - overlap_int , 0 ) : i + chunk_size ] chunk = \" \" . join ( chunk_words ) chunks . append ( chunk ) return chunks Recursive Chunking ​ Recursive chunking is a more nuanced approach. It splits text using a prioritized list of common separators, such as double newlines (for paragraphs) or single newlines (for sentences). It first tries to split the text by the highest-priority separator (paragraphs). If any resulting chunk is still too large, the algorithm", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "f184d299-293c-4640-97fb-d6ce8e321739", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 8, "text": "Recursive chunking is a more nuanced approach. It splits text using a prioritized list of common separators, such as double newlines (for paragraphs) or single newlines (for sentences). It first tries to split the text by the highest-priority separator (paragraphs). If any resulting chunk is still too large, the algorithm recursively applies the next separator (sentences) to that specific chunk. This method adapts to the document’s structure, keeping structurally related units together as much as possible. It avoids the abrupt cuts of fixed-size chunking and ensures that chunks retain the structure of their original format. info Recommended for: Unstructured text documents, such as articles, blog posts, and research papers. This is usually a solid default choice because it respects the natural organization of the text, rather than splitting it randomly. Code example: from typing import List def recursive_chunking ( text : str , max_chunk_size : int = 1000 ) - > List [ str ] # Base case: if text is small enough, return as single chunk if len ( text ) <= max_chunk_size : return [ text . strip ( ) ] if text . strip ( ) else [ ] # Try separators in priority order separators = [ \"\\n\\n\" , \"\\n\" , \". \" , \" \" ] for separator in separators : if separator in text : parts = text . split ( separator ) chunks = [ ] current_chunk = \"\" for part in parts : # Check if adding this part would exceed the", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "546ff384-146b-4045-a72a-f5114897aed6", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 9, "text": "[ \"\\n\\n\" , \"\\n\" , \". \" , \" \" ] for separator in separators : if separator in text : parts = text . split ( separator ) chunks = [ ] current_chunk = \"\" for part in parts : # Check if adding this part would exceed the limit test_chunk = current_chunk + separator + part if current_chunk else part if len ( test_chunk ) <= max_chunk_size : current_chunk = test_chunk else : # Save current chunk and start new one if current_chunk : chunks . append ( current_chunk . strip ( ) ) current_chunk = part # Add the final chunk if current_chunk : chunks . append ( current_chunk . strip ( ) ) # Recursively process any chunks that are still too large final_chunks = [ ] for chunk in chunks : if len ( chunk ) > max_chunk_size : final_chunks . extend ( recursive_chunking ( chunk , max_chunk_size ) ) else : final_chunks . append ( chunk ) return [ chunk for chunk in final_chunks if chunk ] # Fallback: split by character limit if no separators work return [ text [ i : i + max_chunk_size ] for i in range ( 0 , len ( text ) , max_chunk_size ) ] Document-Based Chunking ​ Document-based chunking uses the intrinsic structure of a document . Instead of relying on generic separators, it parses the document based on its format-specific elements. For example: Markdown : Split by headings ( # , ## ) to capture", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "adf77e5f-5635-457b-8086-9a36cdd314f6", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 10, "text": "len ( text ) , max_chunk_size ) ] Document-Based Chunking ​ Document-based chunking uses the intrinsic structure of a document . Instead of relying on generic separators, it parses the document based on its format-specific elements. For example: Markdown : Split by headings ( # , ## ) to capture sections or subsections. HTML : Split by tags ( <p> , <div> ) to preserve logical content blocks. PDF : After preprocessing (e.g., OCR or conversion to Markdown), split by headers, paragraphs, tables, or other structural elements. Programming Code : Split by functions or classes (e.g., def in Python) to maintain logical units of code. With this method, chunks stay aligned with the document’s logical organization, which often also correlates with semantic meaning as well. Both LangChain and LlamaIndex offer specialized splitters for various document types, including Markdown, code, and JSON. info When to use: Highly structured documents where the format can easily define logical separations. Ideal for Markdown, HTML, source code, or any document with clear structural markers. Code example: from typing import List import re def markdown_document_chunking ( text : str ) - > List [ str ] : # Split by markdown headers (# ## ### etc.) header_pattern = r'^#{1,6}\\s+.+$' lines = text . split ( '\\n' ) chunks = [ ] current_chunk = [ ] for line in lines : # Check if this line is a header if re . match ( header_pattern , line , re . MULTILINE ) : # Save previous chunk", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "e93e8105-68ab-4ae3-b236-d4f9997a6aea", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 11, "text": "header_pattern = r'^#{1,6}\\s+.+$' lines = text . split ( '\\n' ) chunks = [ ] current_chunk = [ ] for line in lines : # Check if this line is a header if re . match ( header_pattern , line , re . MULTILINE ) : # Save previous chunk if it has content if current_chunk : chunk_text = '\\n' . join ( current_chunk ) . strip ( ) if chunk_text : chunks . append ( chunk_text ) # Start new chunk with this header current_chunk = [ line ] else : # Add line to current chunk current_chunk . append ( line ) # Add final chunk if current_chunk : chunk_text = '\\n' . join ( current_chunk ) . strip ( ) if chunk_text : chunks . append ( chunk_text ) return chunks Advanced Chunking Techniques ​ Semantic Chunking (Context-Aware Chunking) ​ Semantic chunking shifts from traditional rule-based splitting to meaning-based segmentation. Instead of relying on character counts or document structure, this more advanced technique divides text based on its semantic similarity. The process involves: Sentence Segmentation : Breaking the text into individual sentences Embedding Generation : Converting each sentence into a vector embedding Similarity Analysis : Comparing the embeddings to detect semantic breakpoints (places where the topic changes) Chunk Formation : Creating new chunks between these breakpoints The result is a set of highly coherent semantic chunks, each containing a self-contained idea or topic. This method works well for dense, unstructured text where you want to preserve the", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "4b41aa14-c099-4dd9-bfdd-5c8d6e60c6a1", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 12, "text": "embeddings to detect semantic breakpoints (places where the topic changes) Chunk Formation : Creating new chunks between these breakpoints The result is a set of highly coherent semantic chunks, each containing a self-contained idea or topic. This method works well for dense, unstructured text where you want to preserve the logical flow of arguments or narratives. info Recommended for: Dense, unstructured text to preserve the complete semantic context of an idea. This method works well for academic papers, legal documents, or long stories. These texts do not always use clear separators like paragraphs to show topic changes. This approach is great when you're dealing with complex content where the semantic boundaries don't line up neatly with the document structure. LLM-Based Chunking ​ LLM-based chunking uses a large language model (LLM) to decide how to split the text. Instead of relying on fixed rules or vector-based similarity scores, the LLM processes the document and generates semantically coherent chunks, often also adding additional context, summaries, or other information. This can be done by: Identifying propositions (breaking text into clear, logical statements) Summarizing sections into smaller, meaning-preserving chunks Highlighting key points to ensure the most relevant information is captured The result is a set of chunks that preserve semantic meaning more accurately than traditional methods. This makes LLM-based chunking one of the most powerful strategies for retrieval-augmented generation (RAG). info When to use: High-value, complex documents where retrieval quality is critical and budget is less of a concern. Ideal for legal contracts, research", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "542b6d5e-2e9e-4b97-8469-2d2adea0dacf", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 13, "text": "set of chunks that preserve semantic meaning more accurately than traditional methods. This makes LLM-based chunking one of the most powerful strategies for retrieval-augmented generation (RAG). info When to use: High-value, complex documents where retrieval quality is critical and budget is less of a concern. Ideal for legal contracts, research papers, compliance documents, or enterprise knowledge bases. This approach can produce chunks that summarize or highlight key ideas, but it comes with trade-offs. It is the most computationally expensive and slowest method compared to other chunking techniques. Agentic Chunking ​ Agentic chunking takes the concept of LLM-based chunking one step further. Instead of applying a single method, an AI agent dynamically decides how to split your documents. It looks at the whole document, including its structure, density, and content. Then it decides on the best chunking strategy or mix of strategies to use. For example, the agent might see that a document is a Markdown file. Then it splits the file by headers. It might also find that a denser document needs a propositional approach. It can even enrich chunks with metadata tags for more advanced retrieval. These 'LLM-powered methods' can create very clear and context-rich chunks. However, they use a lot of computing power and cost more. They often need many calls to a strong model for each document. info When to use: High-stakes RAG systems where you need the best possible chunks and cost isn't a dealbreaker. Perfect when you need custom chunking strategies tailored to each document's", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "6107f8d4-3ba0-484f-a17d-ee49599b6b31", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 14, "text": "a lot of computing power and cost more. They often need many calls to a strong model for each document. info When to use: High-stakes RAG systems where you need the best possible chunks and cost isn't a dealbreaker. Perfect when you need custom chunking strategies tailored to each document's unique characteristics. Late Chunking ​ Late chunking is a slightly different type of technique that aims to solve a common problem in other chunking strategies: context loss . In other chunking techniques, when you split a document first and then create embeddings, each chunk becomes isolated. This can result in ambiguous or lost context within the chunk that was explained or referenced earlier in a document. Late chunking works backwards. Instead of splitting first, you start by feeding the entire document into a long-context embedding model. This creates detailed, token-level embeddings that understand the full picture. Only then do you split the document into chunks. When you create the embedding for each chunk, you use the token embeddings that were already created with full context. You simply average the relevant token embeddings for that chunk. This means every chunk retains context about the whole document. info When to use: Use this in RAG systems where retrieval quality depends on understanding relationships between chunks and the whole document. This is very useful for technical documents, research papers, or legal texts. These documents have sections that refer to ideas, methods, or definitions mentioned elsewhere. This helps capture connections between different parts of", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "ce4f63d0-036b-45ad-a3e7-2c098eaf84cf", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 15, "text": "this in RAG systems where retrieval quality depends on understanding relationships between chunks and the whole document. This is very useful for technical documents, research papers, or legal texts. These documents have sections that refer to ideas, methods, or definitions mentioned elsewhere. This helps capture connections between different parts of the document that regular chunking methods miss. Hierarchical Chunking ​ Hierarchical chunking can be a game-changer for very large and complex documents. The idea is pretty straightforward: You create multiple layers of chunks at different levels of detail. At the top layer , you create large chunks that summarize broad sections or themes, like the title and abstract. At the next layers , you split those sections into progressively smaller chunks that capture finer details such as arguments, examples, or definitions. This lets your RAG system start with the high-level overview and then drill down into specifics when users need more detail. LlamaIndex's HierarchicalNodeParser makes it easy to implement this approach. info When to use: Very large and complex documents, such as textbooks, legal contracts, or extensive technical manuals. This strategy is ideal when you need to answer both high-level, summary-based questions and highly specific, detailed queries. It gives you a good middle ground between broad context and granular access without the full complexity of hierarchical chunking, though it's more involved than basic splitting methods. Adaptive Chunking ​ Adaptive chunking techniques dynamically adjust key parameters (like chunk size and overlap) based on the document's content. Instead of applying a single,", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "93b0e773-57f0-4cf9-877e-c908af469417", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 16, "text": "a good middle ground between broad context and granular access without the full complexity of hierarchical chunking, though it's more involved than basic splitting methods. Adaptive Chunking ​ Adaptive chunking techniques dynamically adjust key parameters (like chunk size and overlap) based on the document's content. Instead of applying a single, fixed rule for an entire document, this method treats the text as a varied landscape. It might use machine learning models to analyze the semantic density and structure of different sections. For example, it could automatically create smaller, more granular chunks for a complex, information-rich paragraph to capture fine-grained details, while using larger chunks for a more general, introductory section. The goal is to create chunks whose size and boundaries are tailored to the specific content they contain, leading to more precise and context-aware retrieval. This differs from Agentic Chunking, where the agent decides which chunking strategy to use , rather than just adjusting the parameters of one. info When to use: Documents with varied and inconsistent internal structures. Think of a long report that contains dense, technical paragraphs alongside sparse, narrative sections. An adaptive strategy excels here because it avoids the \"one-size-fits-all\" problem. It can create small, granular chunks for the complex parts to capture every detail, and larger chunks for the simpler text to preserve context, all within the same document. How to Choose the Best Chunking Strategy ​ There is no single \"best\" chunking method; the optimal strategy always depends on your specific use case. But before", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "2c38e8cc-a918-4d72-8133-5b2fdab86016", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 17, "text": "for the complex parts to capture every detail, and larger chunks for the simpler text to preserve context, all within the same document. How to Choose the Best Chunking Strategy ​ There is no single \"best\" chunking method; the optimal strategy always depends on your specific use case. But before diving into different techniques, the most important question to ask is: “Does my data need chunking at all?” Chunking is designed to break down long, unstructured documents. If your data source already has small, complete pieces of information like FAQs, product descriptions, or social media posts, you usually do not need to chunk them. Chunking can even cause problems. The goal is to create meaningful semantic units, and if your data is already in that format, you're ready for the embedding stage. Once you've confirmed that your documents are long enough to benefit from chunking, you can use the following questions to guide your choice of strategy: What is the nature of my documents? Are they highly structured (like code or JSON), or are they unstructured narrative text? What level of detail does my RAG system need? Does it need to retrieve specific, granular facts or summarize broader concepts? Which embedding model am I using? What are the size of the output vectors (more dimensions increases the ability for more granular information to be stored)?? How complex will my user queries be? Will they be simple questions that need small, targeted chunks, or complex ones that require more context? Chunking", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "9780a9fe-2a26-4902-8171-b8eb16654ab9", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 18, "text": "embedding model am I using? What are the size of the output vectors (more dimensions increases the ability for more granular information to be stored)?? How complex will my user queries be? Will they be simple questions that need small, targeted chunks, or complex ones that require more context? Chunking Strategy How It Works Complexity Best For Examples Fixed-Size (or Token) Splits by token or character count. Low Small or simple docs, or when speed matters most Meeting notes, short blog posts, emails, simple FAQs Recursive Splits text by repeatedly dividing it until it fits the desired chunk size, often preserving some structure. Medium Documents where some structure should be maintained but speed is still important Research articles, product guides, short reports Document-Based Treats each document as a single chunk or splits only at document boundaries. Low Collections of short, standalone documents News articles, customer support tickets, short contracts Semantic Splits text at natural meaning boundaries (topics, ideas). Medium-High Technical, academic, or narrative documents Scientific papers, textbooks, novels, whitepapers LLM-Based Uses a language model to decide chunk boundaries based on context, meaning, or task needs. High Complex text where meaning-aware chunking improves downstream tasks like summarization or Q&A Long reports, legal opinions, medical records Agentic Lets an AI agent decide how to split based on meaning and structure. Very High Complex, nuanced documents that require custom strategies Regulatory filings, multi-section contracts, corporate policies Late Embeds the whole document first, then derives chunk embeddings from it. High Use cases where chunks", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "89e05777-fafa-4a60-a730-ca7c7748848f", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 19, "text": "legal opinions, medical records Agentic Lets an AI agent decide how to split based on meaning and structure. Very High Complex, nuanced documents that require custom strategies Regulatory filings, multi-section contracts, corporate policies Late Embeds the whole document first, then derives chunk embeddings from it. High Use cases where chunks need awareness of full document context Case studies, comprehensive manuals, long-form analysis reports Hierarchical Breaks text into multiple levels (sections → paragraphs → sentences). Keeps structure intact. Medium Large, structured docs like manuals, reports, or contracts Employee handbooks, government regulations, software documentation Adaptive Adjusts chunk size and overlap dynamically with ML or heuristics. High Mixed datasets with varying structures and lengths Data from multiple sources: blogs, PDFs, emails, technical docs Code Splits by logical code blocks (functions, classes, modules) while preserving syntax. Medium Source code, scripts, or programming documentation Python modules, JavaScript projects, API docs, Jupyter notebooks Tools and Libraries for Chunking ​ When setting up a data ingestion pipeline for your RAG application, you often face a classic trade-off with chunking: you can rely on a specialized library for speed and ease, or build the logic yourself to have full control. Using a Library ​ Luckily , you don’t have to start from scratch. The LLM community often turns to two powerful open-source libraries: LangChain and LlamaIndex, each with a different approach to chunking: LangChain: A broad framework for building LLM applications. Its flexible TextSplitters make it easy to integrate chunking as part of a larger system, like a", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "069f2865-5ce1-4974-bf03-e3016d612137", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 20, "text": "have to start from scratch. The LLM community often turns to two powerful open-source libraries: LangChain and LlamaIndex, each with a different approach to chunking: LangChain: A broad framework for building LLM applications. Its flexible TextSplitters make it easy to integrate chunking as part of a larger system, like a multi-step AI agent. Best for: modular workflows where chunking is just one piece of the puzzle. LlamaIndex: Designed specifically for RAG pipelines. Its sophisticated NodeParsers produce “Nodes,” optimized for ingestion and retrieval. Best for: high-performance, data-centric retrieval systems. Manual Implementation ​ The alternative to using a library is to implement the chunking logic yourself. Strategies like fixed-size or recursive chunking are straightforward to code in Python, giving you complete authority over how your data is processed and avoiding the need to add external dependencies to your project. Best for: Projects where you want to avoid adding large libraries, need to implement a highly custom chunking strategy, or require full transparency in your data pipeline. How to Optimize The Chunk Size for RAG in Production ​ Optimizing chunk size in a production setting takes many tests and reviews. Here are some steps you can take: Begin with a common baseline strategy, such as fixed-size chunking. A good place to start is a chunk size of 512 tokens and a chunk overlap of 50-100 tokens. This gives you a solid baseline that's easy to reproduce and compare other chunking strategies against. Experiment with different chunking approaches by tweaking parameters like chunk size", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "533036cc-4246-4990-b217-42fcec30f1b6", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 21, "text": "such as fixed-size chunking. A good place to start is a chunk size of 512 tokens and a chunk overlap of 50-100 tokens. This gives you a solid baseline that's easy to reproduce and compare other chunking strategies against. Experiment with different chunking approaches by tweaking parameters like chunk size and overlap to find what works best for your data. Test how well your retrieval works by running typical queries and checking metrics like hit rate, precision, and recall to see which strategy delivers. Involve humans to review both the retrieved chunks and LLM-generated responses - their feedback will catch things metrics might miss. Continuously monitor the performance of your RAG system in production and be prepared to iterate on your chunking strategy as needed. info Dive deeper into Advanced RAG Techniques with our free eBook: https://weaviate.io/ebooks/advanced-rag-techniques Summary ​ Understanding the different chunking strategies is the first step, but the best way to master them is by seeing them in action. If you're looking for a practical example, check out Verba , our open-source RAG application. You can even fork the repository, load your own data, and start experimenting today. Open source is only possible because of you and the community 💚, so if you have questions or want to troubleshoot, join the conversation in the Weaviate Community Slack or Forum . See you there! Ready to start building? ​ Check out the Quickstart tutorial , or build amazing apps with a free trial of Weaviate Cloud (WCD) . GitHub", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "1f58306e-2e3a-4020-956d-d016c108030f", "source": "https://weaviate.io/blog/chunking-strategies-for-rag", "chunk_index": 22, "text": "community 💚, so if you have questions or want to troubleshoot, join the conversation in the Weaviate Community Slack or Forum . See you there! Ready to start building? ​ Check out the Quickstart tutorial , or build amazing apps with a free trial of Weaviate Cloud (WCD) . GitHub Forum Slack X (Twitter) Don't want to miss another blog post? Sign up for our bi-weekly newsletter to stay updated! By submitting, I agree to the Terms of Service and Privacy Policy . What is Chunking? Why is Chunking so Important for RAG? 1. Optimizing for Retrieval Accuracy 2. Preserving Context for Generation The Chunking Sweet Spot Pre-Chunking vs Post-Chunking Chunking Strategies Simple Chunking Techniques Fixed-Size Chunking (or Token Chunking) Recursive Chunking Document-Based Chunking Advanced Chunking Techniques Semantic Chunking (Context-Aware Chunking) LLM-Based Chunking Agentic Chunking Late Chunking Hierarchical Chunking Adaptive Chunking How to Choose the Best Chunking Strategy Tools and Libraries for Chunking Using a Library Manual Implementation How to Optimize The Chunk Size for RAG in Production Summary", "metadata": {"url": "https://weaviate.io/blog/chunking-strategies-for-rag", "title": "Chunking Strategies to Improve Your RAG Performance", "date": "9/4/2025", "type": "blog", "author": "Weaviate"}}
{"id": "3cd6c031-34aa-44bf-863d-43bd1a55be94", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 0, "text": "Context In a recent overview on the state of large language models (LLMs), Karpathy described LLMs as the kernel process of a new kind of operating system. Just as modern computers have RAM and access to files, LLMs have a context window that can be loaded with information retrieved from numerous data sources. Retrieval is a core component of the new LLM operating system This retrieved information is loaded into the context window and used in LLM output generation, a process typically called retrieval augmented generation (RAG). RAG is one of the most important concepts in LLM app development because it is an easy way to pass external information to an LLM with advantages over more complex / complex fine-tuning on problems that require factual recall . Typically, RAG systems involve: a question (often from a user) that determines what information to retrieve, a process of retrieving that information from a data source (or sources), and a process of passing the retrieved information directly to the LLM as part of the prompt (see an example prompt in LangChain hub here ). Challenge The landscape of RAG methods has expanded greatly in recent months, resulting in some degree of overload or confusion among users about where to start and how to think about the various approaches. Over the past few months, we have worked to group RAG concepts into a few categories and have released guides for each. Below we'll provide a round-up of these concepts and present some future work.", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "c2fe27ae-5bd3-4c47-9337-9eeb4bbdb69a", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 1, "text": "confusion among users about where to start and how to think about the various approaches. Over the past few months, we have worked to group RAG concepts into a few categories and have released guides for each. Below we'll provide a round-up of these concepts and present some future work. Major RAG themes Query Transformations A first question to ask when thinking about RAG: how can we make retrieval robust to variability in user input? For example, user questions may be poorly worded for the challenging task of retrieval. Query transformations are a set of approaches focused on modifying the user input in order to improve retrieval. Query expansion Consider the question \"Who won a championship more recently, the Red Sox or the Patriots?\" Answering this can benefit from asking two specific sub-questions: \"When was the last time the Red Sox won a championship?\" \"When was the last time the Patriots won a championship?\" Query expansion decomposes the input into sub-questions, each of which is a more narrow retrieval challenge. The m ulti-query retriever performs sub-question generation, retrieval, and returns the unique union of the retrieved docs. RAG fusion builds on by ranking of the returned docs from each of the sub-questions. Step-back prompting offers a third approach in this vein, generating a step-back question to ground an answer synthesis in higher-level concepts or principles (see paper ). For example, a question about physics can be stepped-back into a question (and LLM-generated answer) about the physical principles behind the user", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "d148039d-9e2b-496d-999e-3a57d1485313", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 2, "text": "the sub-questions. Step-back prompting offers a third approach in this vein, generating a step-back question to ground an answer synthesis in higher-level concepts or principles (see paper ). For example, a question about physics can be stepped-back into a question (and LLM-generated answer) about the physical principles behind the user query. Query re-writing To address poorly framed or worded user inputs, Rewrite-Retrieve-Read (see paper ) is an approach re-writes user questions in order to improve retrieval. Query compression In some RAG applications, such as WebLang (our open source research assistant), a user question follows a broader chat conversation. In order to properly answer the question, the full conversational context may be required. To address this, we use this prompt to compress chat history into a final question for retrieval. Further reading See our blog post on query transformations See our blog post on OpenAI's RAG strategies Routing A second question to ask when thinking about RAG: where does the data live? In many RAG demos, data lives in a single vectorstore but this is often not the case in production settings. When operating across a set of various datastores, incoming queries need to be routed. LLMs can be used to support dynamic query routing effectively (see here ), as discussed in our recent review of OpenAI's RAG strategies . Query Construction A third question to ask when thinking about RAG: what syntax is needed to query the data? While routed questions are in natural language, data is stored in sources", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "1ffb5143-8640-47f8-be6f-5ef4d49de341", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 3, "text": "support dynamic query routing effectively (see here ), as discussed in our recent review of OpenAI's RAG strategies . Query Construction A third question to ask when thinking about RAG: what syntax is needed to query the data? While routed questions are in natural language, data is stored in sources such as relational or graph databases that require specific syntax to retrieve. And even vectorstores utilize structured metadata for filtering. In all cases, natural language from the query needs to be converted into a query syntax for retrieval. Text-to-SQL Considerable effort has focused on translating natural language into SQL requests. Text-to-SQL can be done easily ( here ) by providing an LLM the natural language question along with relevant table information; open source LLMs have proven effective at this task, enabling data privacy (see our templates here and here ). Mixed type (structured and unstructured) data storage in relational databases is increasingly common (see here ); an embedded document column can be included using the open-source pgvector extension for PostgreSQL. It's also possible to interact with this semi-structured data using natural language, marrying the expressiveness of SQL with semantic search (see our cookbook and template ). Text-to-Cypher While vector stores readily handle unstructured data, they don't understand the relationships between vectors. While SQL databases can model relationships, schema changes can be disruptive and costly. Knowledge graphs can address these challenges by modeling the relationships between data and extending the types of relationships without a major overhaul. They are desirable for", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "c6e1a436-3424-4828-8493-a7fe7d4aaedb", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 4, "text": "readily handle unstructured data, they don't understand the relationships between vectors. While SQL databases can model relationships, schema changes can be disruptive and costly. Knowledge graphs can address these challenges by modeling the relationships between data and extending the types of relationships without a major overhaul. They are desirable for data that has many-to-many relationships or hierarchies that are difficult to represent in tabular form. Like relational databases, graph databases benefit from a natural language interface using text-to-Cypher, a structured query language designed to provide a visual way of matching patterns and relationships (see templates here and here ). Text-to-metadata filters Vectorstores equipped with metadata filtering enable structured queries to filter embedded unstructured documents. The self-query retriever can translate natural language into these structured queries with metadata filters using a specification for the metadata fields present in the vectorstore (see our self-query template ). Further reading See our blog post on query construction Indexing A fourth question to ask when thinking about RAG: how to design my index? For vectorstores, there is considerable opportunity to tune parameters like the chunk size and / or the document embedding strategy to support variable data types. Chunk size In our review of OpenAI's RAG strategies , we highlight the notable boost in performance that they saw simply from experimenting with the chunk size during document embedding. This makes sense, because chunk size controls how much information we load into the context window (or \"RAM\" in our LLM OS analogy). Since this is a", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "7712689f-6ddf-4e72-86f3-6d389b05d57d", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 5, "text": "RAG strategies , we highlight the notable boost in performance that they saw simply from experimenting with the chunk size during document embedding. This makes sense, because chunk size controls how much information we load into the context window (or \"RAM\" in our LLM OS analogy). Since this is a central step in index building, we have an open source Streamlit app where you can test various chunk sizes to gain some intuition; in particular, it's worth examining where the document is split using various split sizes or strategies and whether semantically related content is unnaturally split. Document embedding strategy One of the simplest and most useful ideas in index design is to decouple what you embed (for retrieval) from what you pass to the LLM (for answer synthesis). For example, consider a large passage of text with lots of redundant detail. We can embed a few different representations of this to improve retrieval, such as a summary or small chunks to narrow the scope of information that is embedded . In either case, we can then retrieve the full text to pass to the LLM. These can be implemented using multi-vector and parent-document retriever, respectively. The multi-vector retriever also works well for semi-structured documents that contain a mix of text and tables (see our cookbook and template ). In these cases, it's possible to extract each table, produce a summary of the table that is well suited for retrieval, but return the raw table to the LLM for answer", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "88168cf4-acd8-48bf-865b-118f54482d68", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 6, "text": "works well for semi-structured documents that contain a mix of text and tables (see our cookbook and template ). In these cases, it's possible to extract each table, produce a summary of the table that is well suited for retrieval, but return the raw table to the LLM for answer synthesis. We can take this one step further: with the advent of multi-modal LLMs , it's possible to use generate and embed image summaries as one means of image retrieval for documents that contain text and images (see diagram below). This may be appropriate for cases where multi-modal embeddings are not expected to reliably retrieve the images, as may be the case with complex figures or table. As an example, in our cookbook we use this approach with figures from a financial analysis blog ( @jaminball 's Clouded Judgement). However, we also have another cookbook using open source ( OpenCLIP ) multi-modal embeddings for retrieval of images based on more straightforward visual concepts. Further reading See our blog post on multi-vector retriever Post-Processing A final question to ask when thinking about RAG: how to combine the documents that I have retrieved? This is important, because the context window has limited size and redundant documents (e.g., from different sources) will utilize tokens without providing unique information to the LLM. A number of approaches for document post-processing (e.g., to improve diversity or filter for recency) have emerged, some of which we discuss in our blog post on OpenAI's RAG strategies . Re-ranking", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "e9e9a598-e0b1-4b29-a0df-b6e7846f3cf6", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 7, "text": "size and redundant documents (e.g., from different sources) will utilize tokens without providing unique information to the LLM. A number of approaches for document post-processing (e.g., to improve diversity or filter for recency) have emerged, some of which we discuss in our blog post on OpenAI's RAG strategies . Re-ranking The Cohere ReRank endpoint can be used for document compression (reduce redundancy) in cases where we are retrieving a large number of documents. Relatedly, RAG-fusion uses reciprocal rank fusion (see blog and implementation ) to ReRank documents returned from a retriever (similar to multi-query ). Classification OpenAI classified each retrieved document based upon its content and then chose a different prompt depending on that classification. This marries tagging of text for classification with logical routing (in this case, for the prompt) based on a tag. Future Plans Going forward, we will focus on at least two areas that extend these themes. Open source Many of these tasks to improve RAG are narrow and well-defined. For example, query expansion (sub-question generation) or structured query construction for metadata filtering are narrow, well-defined tasks that also may be done repeatedly. In turn, they may not require large (and most costly) generalist models to achieve acceptable performance. Instead, smaller open source models (potentially with fine-tuning) may be sufficient. We will be releasing a series of templates that showcases how to use open source models into the RAG stack where appropriate. Benchmarks Hand-in-hand with our effort to test open source LLMs, we recently launched public", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "7df520dd-c3ab-442b-92ed-d7e89d0aa30d", "source": "https://blog.langchain.com/deconstructing-rag/", "chunk_index": 8, "text": "achieve acceptable performance. Instead, smaller open source models (potentially with fine-tuning) may be sufficient. We will be releasing a series of templates that showcases how to use open source models into the RAG stack where appropriate. Benchmarks Hand-in-hand with our effort to test open source LLMs, we recently launched public datasets that can serve ground truth for evaluation. We will be expanding these to include some more specific RAG challenges and using them to assess the merits of the above approaches as well as the incorporation of open source LLMs. Join our newsletter Updates from the LangChain team and community Enter your email Subscribe Processing your application... Success! Please check your inbox and click the link to confirm your subscription. Sorry, something went wrong. Please try again.", "metadata": {"url": "https://blog.langchain.com/deconstructing-rag/", "title": "Deconstructing RAG", "date": "11/30/2023", "type": "blog", "author": "LangChain"}}
{"id": "584479b7-48e8-46b3-9a70-1adf86c260c6", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 0, "text": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval | Haystack 💼 Announcing Haystack Enterprise: Best Practices and Support Run the Example 🧑‍🍳 Go to Cookbooks Retrieval RAG Advanced Use Cases Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval Use LLMs to extract metadata from queries to use as filters that improve retrieval in RAG applications. May 13, 2024 This is part one of the Advanced Use Cases series: 1️⃣ Extract Metadata from Queries to Improve Retrieval 2️⃣ Query Expansion 3️⃣ Query Decomposition 4️⃣ Automated Metadata Enrichment In Retrieval-Augmented Generation (RAG) applications, the retrieval step, which provides relevant context to your large language model (LLM), is vital for generating high-quality responses. There are possible ways of improving retrieval and metadata filtering is one of the easiest ways. Metadata filtering , the approach of limiting the search space based on some concrete metadata, can really enhance the quality of the retrieved documents. Here are some advantages of using metadata filtering: Relevance : Metadata filtering narrows down the information being retrieved. This ensures that the generated responses align with the specific query or topic. Accuracy : Filtering based on metadata such as domain, source, date, or topic guarantees that the information used for generation is accurate and trustworthy. This is particularly important for applications where accuracy is paramount. For instance, if you need information about a specific year, using the year as a metadata filter will retrieve only pertinent data. Efficiency : Eliminating irrelevant or low-quality information boosts the efficiency of", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "f118f7a0-52a0-4525-b3c2-ee2f87489d1f", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 1, "text": "used for generation is accurate and trustworthy. This is particularly important for applications where accuracy is paramount. For instance, if you need information about a specific year, using the year as a metadata filter will retrieve only pertinent data. Efficiency : Eliminating irrelevant or low-quality information boosts the efficiency of your RAG application, reduces the amount of processing needed, and speeds up retrieval response times. You have two options for applying the metadata filter: you can either specify it directly when running the pipeline or, you can extract it from the query itself. In this article, we’ll focus on extracting filters from a query to improve the quality of generated responses in RAG applications. Let’s get started. Introduction to Metadata Filters First things first, what is metadata? Metadata (or meta tag) is actually data about your data, used to categorize, sort, and filter information based on various attributes such as date, topic, source, or any other information that you find relevant. After incorporating meta information into your data, you can apply filters to queries used with Retrievers to limit the scope of your search based on this metadata and ensure that your answers come from a specific slice of your data. Imagine that you have following Documents in your document store: [CODEBLOCK] documents = [ Document ( content = \"Some text about revenue increase\" , meta ={ \"year\" : 2022 , \"company\" : \"Nvidia\" , \"name\" : \"A\" }), Document ( content = \"Some text about revenue increase\" , meta", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "02fed360-c8a5-412f-a7b3-c4a5370a8942", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 2, "text": "data. Imagine that you have following Documents in your document store: [CODEBLOCK] documents = [ Document ( content = \"Some text about revenue increase\" , meta ={ \"year\" : 2022 , \"company\" : \"Nvidia\" , \"name\" : \"A\" }), Document ( content = \"Some text about revenue increase\" , meta ={ \"year\" : 2023 , \"company\" : \"Nvidia\" , \"name\" : \"B\" }), Document ( content = \"Some text about revenue increase\" , meta ={ \"year\" : 2022 , \"company\" : \"BMW\" , \"name\" : \"C\" }), Document ( content = \"Some text about revenue increase\" , meta ={ \"year\" : 2023 , \"company\" : \"BMW\" , \"name\" : \"D\" }), Document ( content = \"Some text about revenue increase\" , meta ={ \"year\" : 2022 , \"company\" : \"Mercedes\" , \"name\" : \"E\" }), Document ( content = \"Some text about revenue increase\" , meta ={ \"year\" : 2023 , \"company\" : \"Mercedes\" , \"name\" : \"F\" }), ] [/CODEBLOCK] When the query is “ Causes of the revenue increase ”, the retriever returns all documents as they all contain some information about revenue. However, the metadata filter below ensures that any returned document by the retriever has a value of 2022 in the year metadata field and either BMW or Mercedes in the company metadata field. So, only documents with name “ C ” and “ E ” are retrieved. [CODEBLOCK] pipeline . run ( data ={ \"retriever\" :{ \"query\" : \"Causes of the revenue increase\" , \"filters\"", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "7c5167bb-6246-4dcd-bd0b-126502975fcc", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 3, "text": "value of 2022 in the year metadata field and either BMW or Mercedes in the company metadata field. So, only documents with name “ C ” and “ E ” are retrieved. [CODEBLOCK] pipeline . run ( data ={ \"retriever\" :{ \"query\" : \"Causes of the revenue increase\" , \"filters\" : { \"operators\" : \"AND\" , \"conditions\" : [ { \"field\" : \"meta.year\" , \"operator\" : \"==\" , \"value\" : \"2022\" }, { \"field\" : \"meta.company\" , \"operator\" : \"in\" , \"value\" : [ \"BMW\" , \"Mercedes\" ]} ] } } } ) [/CODEBLOCK] In this example, we pass the filter explicitly, but sometimes, the query itself might contain information that can be used as a metadata filter during the querying process. In this case, we need to preprocess the query to extract filters before we use it with a retriever. Extracting Metadata Filters from a Query In LLM-based applications, queries are written in natural language. From time to time, they include valuable hints that can be used as metadata filters to improve the retrieval. We can extract these hints, formulate them as metadata filters and use them with the retriever alongside the query. For instance, when the query is “ What was the revenue of Nvidia in 2022? ”, we can extract 2022 as years and Nvidia as companies . Based on this information, formulated metadata filter to use with a retriever should look like: [CODEBLOCK] \"filters\" : { \"operators\" : \"AND\" , \"conditions\" : [ { \"field\" :", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "de3e66cc-9f2f-40df-915f-75974ee86a84", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 4, "text": "“ What was the revenue of Nvidia in 2022? ”, we can extract 2022 as years and Nvidia as companies . Based on this information, formulated metadata filter to use with a retriever should look like: [CODEBLOCK] \"filters\" : { \"operators\" : \"AND\" , \"conditions\" : [ { \"field\" : \"meta.years\" , \"operator\" : \"==\" , \"value\" : \"2022\" }, { \"field\" : \"meta.companies\" , \"operator\" : \"==\" , \"value\" : \"Nvidia\" } ] } [/CODEBLOCK] Thankfully, LLMs are highly capable of extracting structured information from unstructured text. Let’s see step-by-step how we can implement a custom component that uses an LLM to extract keywords, phrases, or entities from the query and formulate the metadata filter. Implementing QueryMetadataExtractor 🧑‍🍳 You can find and run all the code in our cookbook Extrating Metadata Filter from a Query We start by creating a custom component , QueryMetadataExtractor , which takes query and metadata_fields as inputs and outputs filters . This component encapsulates a generative pipeline, made up of PromptBuilder and OpenAIGenerator . The pipeline instructs the LLM to extract keywords, phrases, or entities from a given query which can then be used as metadata filters. In the prompt, we include instructions to ensure the output format is in JSON and provide metadata_fields along with the query to ensure the correct entities are extracted from the query. Once the pipeline is initialized in the init method of the component, we post-process the LLM output in the run method. This step ensures the extracted", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "0836d354-e8cf-430d-af1d-e6ae83f78c00", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 5, "text": "ensure the output format is in JSON and provide metadata_fields along with the query to ensure the correct entities are extracted from the query. Once the pipeline is initialized in the init method of the component, we post-process the LLM output in the run method. This step ensures the extracted metadata is correctly formatted to be used as a metadata filter. [CODEBLOCK] import json from typing import Dict , List from haystack import Pipeline , component from haystack.components.builders import PromptBuilder from haystack.components.generators import OpenAIGenerator @component () class QueryMetadataExtractor : def __init__ ( self ): prompt = \"\"\" You are part of an information system that processes users queries. Given a user query you extract information from it that matches a given list of metadata fields. The information to be extracted from the query must match the semantics associated with the given metadata fields. The information that you extracted from the query will then be used as filters to narrow down the search space when querying an index. Just include the value of the extracted metadata without including the name of the metadata field. The extracted information in 'Extracted metadata' must be returned as a valid JSON structure. ### Example 1: Query: \"What was the revenue of Nvidia in 2022?\" Metadata fields: {\"company\", \"year\"} Extracted metadata fields: {\"company\": \"nvidia\", \"year\": 2022} ### Example 2: Query: \"What were the most influential publications in 2023 regarding Alzheimer's disease?\" Metadata fields: {\"disease\", \"year\"} Extracted metadata fields: {\"disease\": \"Alzheimer\", \"year\": 2023} ### Example 3: Query:", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "def09017-e018-4ba3-8771-ddf36e47a841", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 6, "text": "Query: \"What was the revenue of Nvidia in 2022?\" Metadata fields: {\"company\", \"year\"} Extracted metadata fields: {\"company\": \"nvidia\", \"year\": 2022} ### Example 2: Query: \"What were the most influential publications in 2023 regarding Alzheimer's disease?\" Metadata fields: {\"disease\", \"year\"} Extracted metadata fields: {\"disease\": \"Alzheimer\", \"year\": 2023} ### Example 3: Query: \"{{query}}\" Metadata fields: \"{{metadata_fields}}\" Extracted metadata fields: \"\"\" self . pipeline = Pipeline () self . pipeline . add_component ( name = \"builder\" , instance = PromptBuilder ( prompt )) self . pipeline . add_component ( name = \"llm\" , instance = OpenAIGenerator ( model = \"gpt-3.5-turbo\" )) self . pipeline . connect ( \"builder\" , \"llm\" ) @component . output_types ( filters = Dict [ str , str ]) def run ( self , query : str , metadata_fields : List [ str ]): result = self . pipeline . run ({ 'builder' : { 'query' : query , 'metadata_fields' : metadata_fields }}) metadata = json . loads ( result [ 'llm' ][ 'replies' ][ 0 ]) # this can be done with specific data structures and in a more sophisticated way filters = [] for key , value in metadata . items (): field = f \"meta. { key } \" filters . append ({ f \"field\" : field , \"operator\" : \"==\" , \"value\" : value }) return { \"filters\" : { \"operator\" : \"AND\" , \"conditions\" : filters }} [/CODEBLOCK] First, let’s test the QueryMetadataExtractor in isolation, passing a query and a list of metadata fields.", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "c56f75b0-b3f3-46ed-9709-90f8cc9abef3", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 7, "text": "key } \" filters . append ({ f \"field\" : field , \"operator\" : \"==\" , \"value\" : value }) return { \"filters\" : { \"operator\" : \"AND\" , \"conditions\" : filters }} [/CODEBLOCK] First, let’s test the QueryMetadataExtractor in isolation, passing a query and a list of metadata fields. [CODEBLOCK] extractor = QueryMetadataExtractor () query = \"What were the most influential publications in 2022 regarding Parkinson's disease?\" metadata_fields = { \"disease\" , \"year\" } result = extractor . run ( query , metadata_fields ) print ( result ) [/CODEBLOCK] The result should look like this: [CODEBLOCK] { 'filters' : { 'operator' : 'AND' , 'conditions' : [ { 'field' : 'meta.disease' , 'operator' : '==' , 'value' : 'Alzheimers' }, { 'field' : 'meta.year' , 'operator' : '==' , 'value' : 2023} ]} } [/CODEBLOCK] Notice that the QueryMetadataExtractor has extracted the metadata fields from the query and returned them in a format that can be used as filters passed directly to a Retriever . By default, the QueryMetadataExtractor will use all metadata fields as conditions together with an AND operator. Using QueryMetadataExtractor in a Pipeline Now, let’s plug the QueryMetadataExtractor into a Pipeline with a Retriever connected to a DocumentStore to see how it works in practice. We start by creating a InMemoryDocumentStore and adding some documents to it. We include info about “year” and “disease” in the “meta” field of each document. [CODEBLOCK] from haystack import Document from haystack.document_stores.in_memory import InMemoryDocumentStore from haystack.document_stores.types import DuplicatePolicy documents =", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "f94c38a0-5584-4334-ba5d-42fb80986e56", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 8, "text": "a DocumentStore to see how it works in practice. We start by creating a InMemoryDocumentStore and adding some documents to it. We include info about “year” and “disease” in the “meta” field of each document. [CODEBLOCK] from haystack import Document from haystack.document_stores.in_memory import InMemoryDocumentStore from haystack.document_stores.types import DuplicatePolicy documents = [ Document ( content = \"some publication about Alzheimer prevention research done over 2023 patients study\" , meta ={ \"year\" : 2022 , \"disease\" : \"Alzheimer\" , \"author\" : \"Michael Butter\" }), Document ( content = \"some text about investigation and treatment of Alzheimer disease\" , meta ={ \"year\" : 2023 , \"disease\" : \"Alzheimer\" , \"author\" : \"John Bread\" }), Document ( content = \"A study on the effectiveness of new therapies for Parkinson's disease\" , meta ={ \"year\" : 2022 , \"disease\" : \"Parkinson\" , \"author\" : \"Alice Smith\" } ), Document ( content = \"An overview of the latest research on the genetics of Parkinson's disease and its implications for treatment\" , meta ={ \"year\" : 2023 , \"disease\" : \"Parkinson\" , \"author\" : \"David Jones\" } ) ] document_store = InMemoryDocumentStore ( bm25_algorithm = \"BM25Plus\" ) document_store . write_documents ( documents = documents , policy = DuplicatePolicy . OVERWRITE ) [/CODEBLOCK] We then create a pipeline consisting of the QueryMetadataExtractor and a InMemoryBM25Retriever connected to the InMemoryDocumentStore created above. Learn about connecting components and creating pipelines in Docs: Creating Pipelines . [CODEBLOCK] from haystack import Pipeline , Document from haystack.components.retrievers.in_memory import InMemoryBM25Retriever retrieval_pipeline = Pipeline ()", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "68f9d4b4-d990-467b-a508-a61fa7cc527f", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 9, "text": "DuplicatePolicy . OVERWRITE ) [/CODEBLOCK] We then create a pipeline consisting of the QueryMetadataExtractor and a InMemoryBM25Retriever connected to the InMemoryDocumentStore created above. Learn about connecting components and creating pipelines in Docs: Creating Pipelines . [CODEBLOCK] from haystack import Pipeline , Document from haystack.components.retrievers.in_memory import InMemoryBM25Retriever retrieval_pipeline = Pipeline () metadata_extractor = QueryMetadataExtractor () retriever = InMemoryBM25Retriever ( document_store = document_store ) retrieval_pipeline . add_component ( instance = metadata_extractor , name = \"metadata_extractor\" ) retrieval_pipeline . add_component ( instance = retriever , name = \"retriever\" ) retrieval_pipeline . connect ( \"metadata_extractor.filters\" , \"retriever.filters\" ) [/CODEBLOCK] Now define a query and metadata fields and pass them to the pipeline: [CODEBLOCK] query = \"publications 2023 Alzheimer's disease\" metadata_fields = { \"year\" , \"author\" , \"disease\" } retrieval_pipeline . run ( data ={ \"metadata_extractor\" : { \"query\" : query , \"metadata_fields\" : metadata_fields }, \"retriever\" :{ \"query\" : query }}) [/CODEBLOCK] This returns only documents whose metadata field year = 2023 and disease = Alzheimer [CODEBLOCK] { 'documents' : [ Document ( id = e3b0bfd497a9f83397945583e77b293429eb5bdead5680cc8f58dd4337372aa3 , content : 'some text about investigation and treatment of Alzheimer disease' , meta : { 'year' : 2023 , 'disease' : 'Alzheimer' , 'author' : 'John Bread' }, score : 2.772588722239781 )] } [/CODEBLOCK] Conclusion Metadata filtering stands out as a powerful technique for improving the relevance and accuracy of retrieved documents, thus enabling the generation of high-quality responses in RAG applications. Using the custom component QueryMetadataExtractor we implemented, we can extract filters from user queries and", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "76afa65b-14a6-4781-8f2f-2ae4a69efee0", "source": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "chunk_index": 10, "text": "}, score : 2.772588722239781 )] } [/CODEBLOCK] Conclusion Metadata filtering stands out as a powerful technique for improving the relevance and accuracy of retrieved documents, thus enabling the generation of high-quality responses in RAG applications. Using the custom component QueryMetadataExtractor we implemented, we can extract filters from user queries and directly use them with Retrievers. This article was part one of the Advanced Use Cases series. If you want to stay on top of the latest Haystack developments, you can subscribe to our newsletter or join our Discord community 💙 David Batista Senior NLP Engineer Bilge Yücel Developer Relations Engineer", "metadata": {"url": "https://haystack.deepset.ai/blog/extracting-metadata-filter", "title": "Advanced Retrieval: Extract Metadata from Queries to Improve Retrieval", "date": "5/13/2024", "type": "blog", "author": "Haystack"}}
{"id": "b8cad2c5-c862-4879-944a-1d1a1b776add", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 0, "text": "Advanced RAG: Query Expansion | Haystack 💼 Announcing Haystack Enterprise: Best Practices and Support Run the Example 🧑‍🍳 Go to Cookbooks Retrieval RAG Advanced Use Cases Advanced RAG: Query Expansion Expand keyword queries to improve recall and provide more context to RAG. August 14, 2024 This is part one of the Advanced Use Cases series: 1️⃣ Extract Metadata from Queries to Improve Retrieval 2️⃣ Query Expansion 3️⃣ Query Decomposition 4️⃣ Automated Metadata Enrichment The quality of RAG (retrieval augmented generation) highly depends on the quality of the first step in the process: retrieval. The generation step can only be as good as the context its working on, which it will receive as a result of a retrieval step. However, retrieval is also in turn dependent on the query that it receives. There are multiple types of retrieval: keyword based, semantic search (embedding) based, hybrid, or even in some cases simply based on the results of a query to an API (for example, the results of websearch and so on). But at the end of the day, in the majority of cases, there’s a human behind a keyboard typing a query, and humans are not guaranteed to produce good quality queries for the results they intend to get. In this article, we’ll walk you through a very simple yet effective technique that allows us to make sure we are retrieving more of, and more relevant bits of context to a given query: query expansion. TL;DR: Query expansion increases the number of", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "035bf0b4-1dcc-4d18-9f3f-55b2143c6685", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 1, "text": "for the results they intend to get. In this article, we’ll walk you through a very simple yet effective technique that allows us to make sure we are retrieving more of, and more relevant bits of context to a given query: query expansion. TL;DR: Query expansion increases the number of results, so it increases recall (vs precision). In general, BM25 favors precision while embedding retrieval favors recall (See this explanation by Nils Reimers ). So, it makes sense to use BM25+query expansion to increase recall in cases where you want to rely on keyword search. Query Expansion Query expansion is a technique where we take the user query, and generate a certain number of similar queries. For example: User Query: “open source NLP frameworks” After Query Expansion: [”natural language processing tools”, “free nlp libraries”, “open-source language processing platforms”, “NLP software with open-source code”, “open source NLP frameworks”] This helps improve retrieval results, and in turn the quality of RAG results in cases where: The user query is vague or poorly formed. In cases of keyword-based retrieval, it also allows you to cover your bases with queries of similar meaning or synonyms. Take ‘global warming’ as an example, query expansion would allow us to make sure we’re also doing keyword search for ‘climate change’ or similar queries. Let’s start by importing the experimental QueryExpander component. This component is using an OpenAI model ( gpt-4o-mini in this case) to generate a certain number of additional queries that are similar to the original", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "79586dba-12da-492e-82c6-4c729359f48d", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 2, "text": "allow us to make sure we’re also doing keyword search for ‘climate change’ or similar queries. Let’s start by importing the experimental QueryExpander component. This component is using an OpenAI model ( gpt-4o-mini in this case) to generate a certain number of additional queries that are similar to the original user query. It returns queries, which include the original query plus the generated similar ones: [CODEBLOCK] expander = QueryExpander () expander . run ( query = \"open source nlp frameworks\" , number = 4 ) [/CODEBLOCK] This would result in the component returning queries that include the original query + 4 expanded queries: [CODEBLOCK] { 'queries' : [ 'natural language processing tools' , 'free nlp libraries' , 'open-source language processing platforms' , 'NLP software with open-source code' , 'open source nlp frameworks' ]} [/CODEBLOCK] Retrieval With Query Expansion Let’s look at what happens if we use query expansion as a step in our retrieval pipeline. Let’s look at this through a very simple and small demo. To this end, I used some dummy data. Here’s the list of documents I used: [CODEBLOCK] documents = [ Document(content=\"The effects of climate are many including loss of biodiversity\"), Document(content=\"The impact of climate change is evident in the melting of the polar ice caps.\"), Document(content=\"Consequences of global warming include the rise in sea levels.\"), Document(content=\"One of the effects of environmental changes is the change in weather patterns.\"), Document(content=\"There is a global call to reduce the amount of air travel people take.\"), Document(content=\"Air travel is", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "b4567129-0818-477f-a052-f7764eeea5ab", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 3, "text": "is evident in the melting of the polar ice caps.\"), Document(content=\"Consequences of global warming include the rise in sea levels.\"), Document(content=\"One of the effects of environmental changes is the change in weather patterns.\"), Document(content=\"There is a global call to reduce the amount of air travel people take.\"), Document(content=\"Air travel is one of the core contributors to climate change.\"), Document(content=\"Expect warm climates in Turkey during the summer period.\"), ] [/CODEBLOCK] When asking to retrieve the top 3 documents to the query “climate change” using the InMemoryBM25Retriever (so, we’re doing keyword search) here’s what we get as our top 3 candidates: [CODEBLOCK] 'Air travel is one of the core contributors to climate change.' 'The impact of climate change is evident in the melting of the polar ice caps.' 'The effects of climate are many including loss of biodiversity' [/CODEBLOCK] There are 2 things to notice here: We’re only asking for 3 documents, and we’re getting 3 relevant documents to the query “climate change”. In this sense, this retrieval is completely valid and has done a good job. But, because we’re using the query “climate change” in combination with a keyword retriever, we are actually missing out on some documents that may be even more relevant to the query. For example, the document with “global warming” is completely left out. You can start to see how this could impact the results you get in cases where users are typing vague queries or keywords into the search box. Now, let’s add query expansion to", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "27d8d467-64ed-4b47-941f-090841bd720c", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 4, "text": "even more relevant to the query. For example, the document with “global warming” is completely left out. You can start to see how this could impact the results you get in cases where users are typing vague queries or keywords into the search box. Now, let’s add query expansion to the mix. We will be using a custom retriever this time called the MultiQueryInMemoryBM25Retriever which can accept a list of queries instead of a single query (see the cookbook for the full code). Here’s the retrieval pipeline that we create: [CODEBLOCK] query_expander = QueryExpander () retriever = MultiQueryInMemoryBM25Retriever ( InMemoryBM25Retriever ( document_store = doc_store )) expanded_retrieval_pipeline = Pipeline () expanded_retrieval_pipeline . add_component ( \"expander\" , query_expander ) expanded_retrieval_pipeline . add_component ( \"keyword_retriever\" , retriever ) expanded_retrieval_pipeline . connect ( \"expander.queries\" , \"keyword_retriever.queries\" ) [/CODEBLOCK] Now, we can run this pipeline, again with the same query “climate change” [CODEBLOCK] expanded_retrieval_pipeline . run ({ \"expander\" : { \"query\" : \"climate change\" }}, include_outputs_from =[ \"expander\" ]) [/CODEBLOCK] And we get the following results. The query expander has created the following queries : [CODEBLOCK] 'expander': {'queries': ['global warming consequences', 'environmental impact of climate change', 'effects of climate variability', 'implications of climate crisis', 'consequences of greenhouse gas emissions', 'climate change']}} [/CODEBLOCK] Note that you may get different results because your QueryExpander may generate different queries And we’ve received the following documents from the retrieval pipeline: [CODEBLOCK] 'Consequences of global warming include the rise in sea levels.' 'The impact of climate change is evident in", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "f46bf889-4aa9-4a00-9335-80c45ce9e6cb", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 5, "text": "of greenhouse gas emissions', 'climate change']}} [/CODEBLOCK] Note that you may get different results because your QueryExpander may generate different queries And we’ve received the following documents from the retrieval pipeline: [CODEBLOCK] 'Consequences of global warming include the rise in sea levels.' 'The impact of climate change is evident in the melting of the polar ice caps.', 'There is a global call to reduce the amount of air travel people take.' 'The effects of climate are many including loss of biodiversity' 'One of the effects of environmental changes is the change in weather patterns.' 'Air travel is one of the core contributors to climate change.' [/CODEBLOCK] Notice how we’re able to add context about ‘global warming’ and ‘effects of environmental change’. Using Query Expansion for RAG In the example cookbook, we’ve also added a section on using query expansion for RAG on Wikipedia pages. We index the following wikipedia pages into an InMemoryDocumentStore : [CODEBLOCK] \"Electric_vehicle\", \"Dam\", \"Electric_battery\", \"Tree\", \"Solar_panel\", \"Nuclear_power\", \"Wind_power\", \"Hydroelectricity\", \"Coal\", \"Natural_gas\", \"Greenhouse_gas\", \"Renewable_energy\", \"Fossil_fuel\" [/CODEBLOCK] And then, we construct a RAG pipeline. For our resulting prompt to the LLM, we also indicate what the original query from the user was. [CODEBLOCK] template = \"\"\" You are part of an information system that summarises related documents. You answer a query using the textual content from the documents retrieved for the following query. You build the summary answer based only on quoting information from the documents. You should reference the documents you used to support your answer. ###", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "b593fe31-b235-4061-8b51-387d97ec4239", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 6, "text": "part of an information system that summarises related documents. You answer a query using the textual content from the documents retrieved for the following query. You build the summary answer based only on quoting information from the documents. You should reference the documents you used to support your answer. ### Original Query: \"{{query}}\" Retrieved Documents: {{documents}} Summary Answer: \"\"\" query_expander = QueryExpander () retriever = MultiQueryInMemoryBM25Retriever ( InMemoryBM25Retriever ( document_store = doc_store )) prompt_builder = PromptBuilder ( template = template ) llm = OpenAIGenerator () query_expanded_rag_pipeline = Pipeline () query_expanded_rag_pipeline . add_component ( \"expander\" , query_expander ) query_expanded_rag_pipeline . add_component ( \"keyword_retriever\" , retriever ) query_expanded_rag_pipeline . add_component ( \"prompt\" , prompt_builder ) query_expanded_rag_pipeline . add_component ( \"llm\" , llm ) query_expanded_rag_pipeline . connect ( \"expander.queries\" , \"keyword_retriever.queries\" ) query_expanded_rag_pipeline . connect ( \"keyword_retriever.documents\" , \"prompt.documents\" ) query_expanded_rag_pipeline . connect ( \"prompt\" , \"llm\" ) [/CODEBLOCK] Running this pipeline with the simple query “green energy sources” with the query expander, we’re able to get a response constructed from Wikipedia pages including “Electric Vehicle”, “Wind Power”, “Renewable Energy”, “Fossil Fuel” and “Nuclear Power”. Without the MultiQueryInMemoryBM25Retriever , we rely on the top k results from a single pass of BM25 retrieval on the query “green energy sources” resulting in a response constructed from the pages “Renewable energy”, “Wind Power” and “Fossil Fuel” Wrapping Up Query Expansion is a great technique that will allow you to get a wider range of relevant resources while still using keyword search. While semantic search is", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "50829c8a-d15e-4155-8bf7-31d670a60da5", "source": "https://haystack.deepset.ai/blog/query-expansion", "chunk_index": 7, "text": "on the query “green energy sources” resulting in a response constructed from the pages “Renewable energy”, “Wind Power” and “Fossil Fuel” Wrapping Up Query Expansion is a great technique that will allow you to get a wider range of relevant resources while still using keyword search. While semantic search is a great option, it does require the use of an embedding model, and the existence of embeddings for the data source we will perform search on. This makes keyword based search quite an attractive option for faster, cheaper retrieval. This does however mean that we heavily rely on the quality of the provided query. Query expansion allows you to navigate this issue by generating similar queries to the user query. In my opinion, one of the main advantages of this technique is that it allows you to avoid embedding documentation at each update, while still managing to increase the relevance of retrieved documents at query time. Keyword retrieval doesn’t require any extra embedding step, so the only inferencing happening at retrieval time in this scenario is when we ask an LLM to generate a certain number of similar queries. Tuana Çelik", "metadata": {"url": "https://haystack.deepset.ai/blog/query-expansion", "title": "Advanced RAG: Query Expansion", "date": "8/14/2024", "type": "blog", "author": "Haystack"}}
{"id": "e897cf41-ac8c-4c31-a7dc-c84339a166d3", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 0, "text": "Advanced RAG: Query Decomposition & Reasoning | Haystack 💼 Announcing Haystack Enterprise: Best Practices and Support Run the Example 🧑‍🍳 Go to Cookbooks Retrieval RAG Advanced Use Cases Advanced RAG: Query Decomposition & Reasoning Decompose queries that are multiples in disguise and have an LLM reason about the final answer. September 30, 2024 This is part one of the Advanced Use Cases series: 1️⃣ Extract Metadata from Queries to Improve Retrieval 2️⃣ Query Expansion 3️⃣ Query Decomposition 4️⃣ Automated Metadata Enrichment Sometimes a single question is multiple questions in disguise. For example: “Did Microsoft or Google make more money last year?”. To get to the correct answer for this seemingly simple question, we actually have to break it down: “How much money did Google make last year?” and “How much money did Microsoft make last year?”. Only if we know the answer to these 2 questions can we reason about the final answer. This is where query decomposition comes in. This is a technique for retrieval augmented generation (RAG) based AI applications that follows a simple approach: Decompose the original question into smaller questions that can be answered independently to each other. Let’s call these ‘sub questions’ here on out. Reason about the final answer to the original question, based on each sub-answer. While for many query/dataset combinations, this may not be required, for some, it very well may be. At the end of the day, often one query results in one retrieval step. If within that one single retrieval", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "f896bcf6-d630-4fff-b99f-08dce49dfeeb", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 1, "text": "Reason about the final answer to the original question, based on each sub-answer. While for many query/dataset combinations, this may not be required, for some, it very well may be. At the end of the day, often one query results in one retrieval step. If within that one single retrieval step we are unable to have the retriever return both the money Microsoft made last year and Google, then the system will struggle to produce an accurate final response. This method ensures that we are: retrieving the relevant context for each sub question. reasoning about the final answer given each answer based on the contexts retrieved for each sub question. In this article, I’ll be going through some key steps that allow you to achieve this. You can find the full working example and code in the linked recipe from our cookbook . Here, I’ll only show the most relevant parts of the code. 🚀 I’m sneaking something extra into this article. I saw the opportunity to try out the structured output functionality (currently in beta) by OpenAI to create this example. For this step, I extended the OpenAIGenerator in Haystack to be able to work with Pydantic schemas. More on this in the next step. Let’s try build a full pipeline that makes use of query decomposition and reasoning. We’ll use a dataset about Game of Thrones (a classic for Haystack) which you can find preprocessed and chunked on Tuana/game-of-thrones on Hugging Face Datasets. Defining our Questions Structure Our", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "bfc66c8d-8a0b-4dc8-9bb4-bd3224e15a6b", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 2, "text": "on this in the next step. Let’s try build a full pipeline that makes use of query decomposition and reasoning. We’ll use a dataset about Game of Thrones (a classic for Haystack) which you can find preprocessed and chunked on Tuana/game-of-thrones on Hugging Face Datasets. Defining our Questions Structure Our first step is to create a structure within which we can contain the subquestions, and each of their answers. This will be used by our OpenAIGenerator to produce a structured output. [CODEBLOCK] from pydantic import BaseModel class Question ( BaseModel ): question : str answer : Optional [ str ] = None class Questions ( BaseModel ): questions : list [ Question ] [/CODEBLOCK] The structure is simple, we have Questions made up of a list of Question . Each Question has the question string as well as an optional answer to that question. Defining the Prompt for Query Decomposition Next up, we need to get an LLM to decompose a question and produce multiple questions. Here, we will start making use of our Questions schema. [CODEBLOCK] splitter_prompt = \"\"\" You are a helpful assistant that prepares queries that will be sent to a search component. Sometimes, these queries are very complex. Your job is to simplify complex queries into multiple queries that can be answered in isolation to eachother. If the query is simple, then keep it as it is. Examples 1. Query: Did Microsoft or Google make more money last year? Decomposed Questions: [Question(question='How much profit did Microsoft", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "0aed9dd1-31a2-4721-8f51-395ea4b38458", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 3, "text": "very complex. Your job is to simplify complex queries into multiple queries that can be answered in isolation to eachother. If the query is simple, then keep it as it is. Examples 1. Query: Did Microsoft or Google make more money last year? Decomposed Questions: [Question(question='How much profit did Microsoft make last year?', answer=None), Question(question='How much profit did Google make last year?', answer=None)] 2. Query: What is the capital of France? Decomposed Questions: [Question(question='What is the capital of France?', answer=None)] 3. Query: {{question}} Decomposed Questions: \"\"\" builder = PromptBuilder ( splitter_prompt ) llm = OpenAIGenerator ( model = \"gpt-4o-mini\" , generation_kwargs ={ \"response_format\" : Questions }) [/CODEBLOCK] Answering Each Sub Question First, let’s build a pipeline that uses the splitter_prompt to decompose our question: [CODEBLOCK] query_decomposition_pipeline = Pipeline () query_decomposition_pipeline . add_component ( \"prompt\" , PromptBuilder ( splitter_prompt )) query_decomposition_pipeline . add_component ( \"llm\" , OpenAIGenerator ( model = \"gpt-4o-mini\" , generation_kwargs ={ \"response_format\" : Questions })) query_decomposition_pipeline . connect ( \"prompt\" , \"llm\" ) question = \"Who has more siblings, Jamie or Sansa?\" result = query_decomposition_pipeline . run ({ \"prompt\" :{ \"question\" : question }}) print ( result [ \"llm\" ][ \"structured_reply\" ]) [/CODEBLOCK] This produces the following Questions ( List[Question] ) [CODEBLOCK] questions=[Question(question='How many siblings does Jamie have?', answer=None), Question(question='How many siblings does Sansa have?', answer=None)] [/CODEBLOCK] Now, we have to fill in the answer fields. For this step, we need to have a separate prompt and two custom components: The CohereMultiTextEmbedder which can take multiple questions rather", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "15fb89b5-ceab-41e3-921e-38016411bd30", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 4, "text": "( List[Question] ) [CODEBLOCK] questions=[Question(question='How many siblings does Jamie have?', answer=None), Question(question='How many siblings does Sansa have?', answer=None)] [/CODEBLOCK] Now, we have to fill in the answer fields. For this step, we need to have a separate prompt and two custom components: The CohereMultiTextEmbedder which can take multiple questions rather than a single one like the CohereTextEmbedder . The MultiQueryInMemoryEmbeddingRetriever which can again, take multiple questions and their embeddings, returning question_context_pairs . Each pair contains the question and documents that are relevant to that question. Next, we need to construct a prompt that can instruct a model to answer each subquestion: [CODEBLOCK] multi_query_template = \"\"\" You are a helpful assistant that can answer complex queries. Here is the original question you were asked: {{question}} And you have split the task into the following questions: { % f or pair in question_context_pairs %} {{pair.question}} { % e ndfor %} Here are the question and context pairs for each question. For each question, generate the question answer pair as a structured output { % f or pair in question_context_pairs %} Question: {{pair.question}} Context: {{pair.documents}} { % e ndfor %} Answers: \"\"\" multi_query_prompt = PromptBuilder ( multi_query_template ) [/CODEBLOCK] Let’s build a pipeline that can answer each individual sub question. We will call this the query_decomposition_pipeline : [CODEBLOCK] query_decomposition_pipeline = Pipeline () query_decomposition_pipeline . add_component ( \"prompt\" , PromptBuilder ( splitter_prompt )) query_decomposition_pipeline . add_component ( \"llm\" , OpenAIGenerator ( model = \"gpt-4o-mini\" , generation_kwargs ={ \"response_format\" : Questions })) query_decomposition_pipeline . add_component", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "51b8c27b-d525-4ff7-8beb-3cf61116589b", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 5, "text": "that can answer each individual sub question. We will call this the query_decomposition_pipeline : [CODEBLOCK] query_decomposition_pipeline = Pipeline () query_decomposition_pipeline . add_component ( \"prompt\" , PromptBuilder ( splitter_prompt )) query_decomposition_pipeline . add_component ( \"llm\" , OpenAIGenerator ( model = \"gpt-4o-mini\" , generation_kwargs ={ \"response_format\" : Questions })) query_decomposition_pipeline . add_component ( \"embedder\" , CohereMultiTextEmbedder ( model = \"embed-multilingual-v3.0\" )) query_decomposition_pipeline . add_component ( \"multi_query_retriever\" , MultiQueryInMemoryEmbeddingRetriever ( InMemoryEmbeddingRetriever ( document_store = document_store ))) query_decomposition_pipeline . add_component ( \"multi_query_prompt\" , PromptBuilder ( multi_query_template )) query_decomposition_pipeline . add_component ( \"query_resolver_llm\" , OpenAIGenerator ( model = \"gpt-4o-mini\" , generation_kwargs ={ \"response_format\" : Questions })) query_decomposition_pipeline . connect ( \"prompt\" , \"llm\" ) query_decomposition_pipeline . connect ( \"llm.structured_reply\" , \"embedder.questions\" ) query_decomposition_pipeline . connect ( \"embedder.embeddings\" , \"multi_query_retriever.query_embeddings\" ) query_decomposition_pipeline . connect ( \"llm.structured_reply\" , \"multi_query_retriever.queries\" ) query_decomposition_pipeline . connect ( \"llm.structured_reply\" , \"embedder.questions\" ) query_decomposition_pipeline . connect ( \"multi_query_retriever.question_context_pairs\" , \"multi_query_prompt.question_context_pairs\" ) query_decomposition_pipeline . connect ( \"multi_query_prompt\" , \"query_resolver_llm\" ) [/CODEBLOCK] Running this pipeline with the original question “Who has more siblings, Jamie or Sansa?”, results in the following structured output: [CODEBLOCK] question = \"Who has more siblings, Jamie or Sansa?\" result = query_decomposition_pipeline . run ({ \"prompt\" :{ \"question\" : question }, \"multi_query_prompt\" : { \"question\" : question }}) print ( result [ \"query_resolver_llm\" ][ \"structured_reply\" ]) [/CODEBLOCK] [CODEBLOCK] questions=[Question(question='How many siblings does Jamie have?', answer='2 (Cersei Lannister, Tyrion Lannister)'), Question(question='How many siblings does Sansa have?', answer='5 (Robb Stark, Arya Stark, Bran Stark, Rickon Stark, Jon Snow)')] [/CODEBLOCK] Reasoning About the Final", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "bbb913bb-e74c-4fa5-b575-5b8fa49c654d", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 6, "text": "\"multi_query_prompt\" : { \"question\" : question }}) print ( result [ \"query_resolver_llm\" ][ \"structured_reply\" ]) [/CODEBLOCK] [CODEBLOCK] questions=[Question(question='How many siblings does Jamie have?', answer='2 (Cersei Lannister, Tyrion Lannister)'), Question(question='How many siblings does Sansa have?', answer='5 (Robb Stark, Arya Stark, Bran Stark, Rickon Stark, Jon Snow)')] [/CODEBLOCK] Reasoning About the Final Answer The final step we have to take is to reason about the ultimate answer to the original question. Again, we create a prompt that will instruct an LLM to do this. Given we have the questions output that contains each sub question and answer , we will make these inputs to this final prompt. [CODEBLOCK] reasoning_template = \"\"\" You are a helpful assistant that can answer complex queries. Here is the original question you were asked: {{question}} You have split this question up into simpler questions that can be answered in isolation. Here are the questions and answers that you've generated { % f or pair in question_answer_pair %} {{pair}} { % e ndfor %} Reason about the final answer to the original query based on these questions and aswers Final Answer: \"\"\" resoning_prompt = PromptBuilder ( reasoning_template ) [/CODEBLOCK] To be able to augment this prompt with the question answer pairs, we will have to extend our previous pipeline and connect the structured_reply from the previous LLM, to the question_answer_pair input of this prompt. [CODEBLOCK] query_decomposition_pipeline . add_component ( \"reasoning_prompt\" , PromptBuilder ( reasoning_template )) query_decomposition_pipeline . add_component ( \"reasoning_llm\" , OpenAIGenerator ( model = \"gpt-4o-mini\" )) query_decomposition_pipeline", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "62db206b-a0e9-4f22-ac33-d733d383465f", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 7, "text": "question answer pairs, we will have to extend our previous pipeline and connect the structured_reply from the previous LLM, to the question_answer_pair input of this prompt. [CODEBLOCK] query_decomposition_pipeline . add_component ( \"reasoning_prompt\" , PromptBuilder ( reasoning_template )) query_decomposition_pipeline . add_component ( \"reasoning_llm\" , OpenAIGenerator ( model = \"gpt-4o-mini\" )) query_decomposition_pipeline . connect ( \"query_resolver_llm.structured_reply\" , \"reasoning_prompt.question_answer_pair\" ) query_decomposition_pipeline . connect ( \"reasoning_prompt\" , \"reasoning_llm\" ) [/CODEBLOCK] Now, let’s run this final pipeline and see what results we get: [CODEBLOCK] question = \"Who has more siblings, Jamie or Sansa?\" result = query_decomposition_pipeline . run ({ \"prompt\" :{ \"question\" : question }, \"multi_query_prompt\" : { \"question\" : question }, \"reasoning_prompt\" : { \"question\" : question }}, include_outputs_from =[ \"query_resolver_llm\" ]) print ( \"The original query was split and resolved: \\n \" ) for pair in result [ \"query_resolver_llm\" ][ \"structured_reply\" ]. questions : print ( pair ) print ( \" \\n So the original query is answered as follows: \\n \" ) print ( result [ \"reasoning_llm\" ][ \"replies\" ][ 0 ]) [/CODEBLOCK] 🥁 Drum roll please: [CODEBLOCK] The original query was split and resolved: question='How many siblings does Jaime have?' answer='Jaime has one sister (Cersei) and one younger brother (Tyrion), making a total of 2 siblings.' question='How many siblings does Sansa have?' answer='Sansa has five siblings: one older brother (Robb), one younger sister (Arya), and two younger brothers (Bran and Rickon), as well as one older illegitimate half-brother (Jon Snow).' So the original query is answered as follows: To determine who", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "178526fc-08e9-4d79-ac44-d11e29f9cf03", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 8, "text": "a total of 2 siblings.' question='How many siblings does Sansa have?' answer='Sansa has five siblings: one older brother (Robb), one younger sister (Arya), and two younger brothers (Bran and Rickon), as well as one older illegitimate half-brother (Jon Snow).' So the original query is answered as follows: To determine who has more siblings between Jaime and Sansa, we need to compare the number of siblings each has based on the provided answers. From the answers: - Jaime has 2 siblings (Cersei and Tyrion). - Sansa has 5 siblings (Robb, Arya, Bran, Rickon, and Jon Snow). Since Sansa has 5 siblings and Jaime has 2 siblings, we can conclude that Sansa has more siblings than Jaime. Final Answer: Sansa has more siblings than Jaime. [/CODEBLOCK] Wrapping up Given the right instructions, LLMs are good at breaking down tasks. Query decomposition is a great way we can make sure we do that for questions that are multiple questions in disguise. In this article, you learned how to implement this technique with a twist 🙂 Let us know what you think about using structured outputs for these sorts of use cases. And check out the Haystack experimental repo to see what new features we’re working on. Tuana Çelik", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "16a21b67-f5c3-498a-90c5-192d116cd734", "source": "https://haystack.deepset.ai/blog/query-decomposition", "chunk_index": 9, "text": "we’re working on. Tuana Çelik", "metadata": {"url": "https://haystack.deepset.ai/blog/query-decomposition", "title": "Advanced RAG: Query Decomposition & Reasoning", "date": "9/30/2024", "type": "blog", "author": "Haystack"}}
{"id": "005d1bbb-08c1-494a-b8e2-7278f501c3c5", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 0, "text": "Advanced RAG: Automated Structured Metadata Enrichment Open in Colab Download Last Updated: April 11, 2025 by Tuana Celik ( LI , Twitter ) This is part one of the Advanced Use Cases series: 1️⃣ Extract Metadata from Queries to Improve Retrieval cookbook & full article 2️⃣ Query Expansion cookbook & full article 3️⃣ Query Decomposition cookbook & the full article 4️⃣ Automated Metadata Enrichment In this example, you’ll see how you can make use of structured outputs which is an option for some LLMs, and a custom Haystack component, to automate the enrichment of metadata from documents. You will see how you can define your own metadata fields as a Pydantic Model, as well as the data types each field should have. Finally, you will get a custom MetadataEnricher to extract the required fields and add them to the document meta information. In this example, we will be enriching metadata with information relating the funding announements. Once we populate the metadata of a document with our own fields, we are able to use Metadata Filtering during the retrieval step of RAG pipelines. We can even combine this with Metadata Extraction from Queries to Improve Retrieval to be very precise about what documents we are providing as context to an LLM. 📺 Code Along Install requirements [CODEBLOCK] ! pip install haystack - ai ! pip install trafilatura [/CODEBLOCK] [CODEBLOCK] from haystack import Document , component , Pipeline from haystack.components.builders import PromptBuilder from haystack.components.converters import HTMLToDocument from haystack.components.fetchers import LinkContentFetcher from haystack.components.generators", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "619a01a8-e5b1-47ef-9dee-f50c5308fba6", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 1, "text": "what documents we are providing as context to an LLM. 📺 Code Along Install requirements [CODEBLOCK] ! pip install haystack - ai ! pip install trafilatura [/CODEBLOCK] [CODEBLOCK] from haystack import Document , component , Pipeline from haystack.components.builders import PromptBuilder from haystack.components.converters import HTMLToDocument from haystack.components.fetchers import LinkContentFetcher from haystack.components.generators import OpenAIGenerator from haystack.components.preprocessors import DocumentSplitter from haystack.dataclasses import ChatMessage , StreamingChunk from openai import Stream from openai.types.chat import ChatCompletion , ChatCompletionChunk from typing import List , Any , Dict , Optional , Callable , Union from pydantic import BaseModel [/CODEBLOCK] 🧪 Experimental Addition to the OpenAIGenerator for Structured Output Support 🚀 This is the same extension to the OpenAIGenerator that was used in the Advanced RAG: Query Decomposition and Reasoning example Let’s extend the OpenAIGeneraotor to be able to make use of the strctured output option by OpenAI . Below, we extend the class to call self.client.beta.chat.completions.parse if the user has provides a respose_format in generation_kwargs . This will allow us to provifde a Pydantic Model to the gnerator and request our generator to respond with structured outputs that adhere to this Pydantic schema. [CODEBLOCK] class OpenAIGenerator ( OpenAIGenerator ): def __init__ ( self , ** kwargs ): super (). __init__ (** kwargs ) @component . output_types ( replies = List [ str ], meta = List [ Dict [ str , Any ]], structured_reply = BaseModel ) def run ( self , prompt : str , streaming_callback : Optional [ Callable [[ StreamingChunk ], None ]] = None", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "8bd2e6e2-cbca-43cc-9fa9-66e6526afe2a", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 2, "text": "(). __init__ (** kwargs ) @component . output_types ( replies = List [ str ], meta = List [ Dict [ str , Any ]], structured_reply = BaseModel ) def run ( self , prompt : str , streaming_callback : Optional [ Callable [[ StreamingChunk ], None ]] = None , generation_kwargs : Optional [ Dict [ str , Any ]] = None ,): generation_kwargs = {** self . generation_kwargs , **( generation_kwargs or {})} if \"response_format\" in generation_kwargs . keys (): message = ChatMessage . from_user ( prompt ) if self . system_prompt : messages = [ ChatMessage . from_system ( self . system_prompt ), message ] else : messages = [ message ] streaming_callback = streaming_callback or self . streaming_callback openai_formatted_messages = [ message . to_openai_dict_format () for message in messages ] completion : Union [ Stream [ ChatCompletionChunk ], ChatCompletion ] = self . client . beta . chat . completions . parse ( model = self . model , messages = openai_formatted_messages , ** generation_kwargs ) completions = [ self . _build_structured_message ( completion , choice ) for choice in completion . choices ] for response in completions : self . _check_finish_reason ( response ) return { \"replies\" : [ message . text for message in completions ], \"meta\" : [ message . meta for message in completions ], \"structured_reply\" : completions [ 0 ]. text } else : return super (). run ( prompt , streaming_callback , generation_kwargs ) def _build_structured_message ( self , completion", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "646b2726-6584-4c3b-a0e3-5cdf28051477", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 3, "text": "{ \"replies\" : [ message . text for message in completions ], \"meta\" : [ message . meta for message in completions ], \"structured_reply\" : completions [ 0 ]. text } else : return super (). run ( prompt , streaming_callback , generation_kwargs ) def _build_structured_message ( self , completion : Any , choice : Any ) -> ChatMessage : chat_message = ChatMessage . from_assistant ( choice . message . parsed or \"\" ) chat_message . meta . update ( { \"model\" : completion . model , \"index\" : choice . index , \"finish_reason\" : choice . finish_reason , \"usage\" : dict ( completion . usage ), } ) return chat_message [/CODEBLOCK] [CODEBLOCK] import os from getpass import getpass if \"OPENAI_API_KEY\" not in os . environ : os . environ [ \"OPENAI_API_KEY\" ] = getpass ( \"OpenAI API Key:\" ) [/CODEBLOCK] [CODEBLOCK] OpenAI API Key:·········· [/CODEBLOCK] Custom MetadataEnricher We create a custom Haystack component that is able ti accept metadata_model and prompt . If no prompt is provided, it usees the DEFAULT_PROMPT . This component returns documents enriched with the requested metadata fileds. [CODEBLOCK] DEFAULT_PROMPT = \"\"\" Given the contents of the documents, extract the requested metadata. The requested metadata is {{ metadata_model }} Document: {{document}} Metadata: \"\"\" @component class MetadataEnricher : def __init__ ( self , metadata_model : BaseModel , prompt : str = DEFAULT_PROMPT ): self . metadata_model = metadata_model self . metadata_prompt = prompt builder = PromptBuilder ( self . metadata_prompt ) llm = OpenAIGenerator ( generation_kwargs", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "92a33607-b8fb-46fa-b445-3ded9a1b9430", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 4, "text": "is {{ metadata_model }} Document: {{document}} Metadata: \"\"\" @component class MetadataEnricher : def __init__ ( self , metadata_model : BaseModel , prompt : str = DEFAULT_PROMPT ): self . metadata_model = metadata_model self . metadata_prompt = prompt builder = PromptBuilder ( self . metadata_prompt ) llm = OpenAIGenerator ( generation_kwargs ={ \"response_format\" : metadata_model }) self . pipeline = Pipeline () self . pipeline . add_component ( name = \"builder\" , instance = builder ) self . pipeline . add_component ( name = \"llm\" , instance = llm ) self . pipeline . connect ( \"builder\" , \"llm\" ) @component . output_types ( documents = List [ Document ]) def run ( self , documents : List [ Document ]): documents_with_meta = [] for document in documents : result = self . pipeline . run ({ 'builder' : { 'document' : document . content , 'metadata_model' : self . metadata_model }}) metadata = result [ 'llm' ][ 'structured_reply' ] document . meta . update ( metadata . dict ()) documents_with_meta . append ( document ) return { \"documents\" : documents_with_meta } [/CODEBLOCK] Define Metadata Fields as a Pydantic Model For automatic metadata enrichment, we want to be able to provide a structure describing what fields we want to extract, as well as what types they should be. Below, I have defined a Metadata model, with 4 fields. 💡 Note: In some cases, it might make sense to make each field optional, or provide default values. [CODEBLOCK] class Metadata ( BaseModel", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "08dedb3a-3464-4e46-961c-ed60b0b0e42f", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 5, "text": "a structure describing what fields we want to extract, as well as what types they should be. Below, I have defined a Metadata model, with 4 fields. 💡 Note: In some cases, it might make sense to make each field optional, or provide default values. [CODEBLOCK] class Metadata ( BaseModel ): company : str year : int funding_value : int funding_currency : str [/CODEBLOCK] Next, we initialize a MetadataEnricher and provide Metadata as the metadata_model we want to abide by. [CODEBLOCK] enricher = MetadataEnricher ( metadata_model = Metadata ) [/CODEBLOCK] Build an Automated Metadata Enrichment Pipeline Now that we have our enricher , we can use it in a pipeline. Below is an example of a pipeline that fetches the contents of some URLs (in this case, urls that contain information about funding announcements). The pipeline then adds the requested metadata fields to each Document ’s meta field 👇 [CODEBLOCK] pipeline = Pipeline () pipeline . add_component ( \"fetcher\" , LinkContentFetcher ()) pipeline . add_component ( \"converter\" , HTMLToDocument ()) pipeline . add_component ( \"enricher\" , enricher ) pipeline . connect ( \"fetcher\" , \"converter\" ) pipeline . connect ( \"converter.documents\" , \"enricher.documents\" ) pipeline . run ({ \"fetcher\" : { \"urls\" : [ 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/' , 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html' ]}}) [/CODEBLOCK] [CODEBLOCK] {'enricher': {'documents': [Document(id=5844517120556b13f92430ea8af9837714ede1b351580c43c2ddce9b646cb6cb, content: 'Deepset, a platform for building enterprise apps powered by large language models akin to ChatGPT, t...', meta: {'content_type': 'text/html', 'url': 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/', 'company': 'Deepset', 'year': 2023, 'funding_value': 30000000, 'funding_currency': 'USD'}), Document(id=8cdcb63a4e006b1cac902ebc2e012cd95156d188777e0d0c8bd407a92f4491c7, content: 'Arize AI Raises $38 Million", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "cde43f12-e5f6-457f-a071-7477dd09ffe7", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 6, "text": "{ \"urls\" : [ 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/' , 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html' ]}}) [/CODEBLOCK] [CODEBLOCK] {'enricher': {'documents': [Document(id=5844517120556b13f92430ea8af9837714ede1b351580c43c2ddce9b646cb6cb, content: 'Deepset, a platform for building enterprise apps powered by large language models akin to ChatGPT, t...', meta: {'content_type': 'text/html', 'url': 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/', 'company': 'Deepset', 'year': 2023, 'funding_value': 30000000, 'funding_currency': 'USD'}), Document(id=8cdcb63a4e006b1cac902ebc2e012cd95156d188777e0d0c8bd407a92f4491c7, content: 'Arize AI Raises $38 Million Series B To Scale Machine Learning Observability Platform As companies t...', meta: {'content_type': 'text/html', 'url': 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html', 'company': 'Arize AI', 'year': 2022, 'funding_value': 38000000, 'funding_currency': 'USD'})]}} [/CODEBLOCK] [CODEBLOCK] pipeline . show () [/CODEBLOCK] Extra: Metadata Inheritance This is just an extra step to show how metadata that belongs to a document is inherited by the document chunks if you use a component such as the DocumentSplitter . [CODEBLOCK] pipeline . add_component ( \"splitter\" , DocumentSplitter ()) pipeline . connect ( \"enricher\" , \"splitter\" ) [/CODEBLOCK] [CODEBLOCK] <haystack.core.pipeline.pipeline.Pipeline object at 0x77ff80aee8c0> 🚅 Components - fetcher: LinkContentFetcher - converter: HTMLToDocument - enricher: MetadataEnricher - splitter: DocumentSplitter 🛤️ Connections - fetcher.streams -> converter.sources (List[ByteStream]) - converter.documents -> enricher.documents (List[Document]) - enricher.documents -> splitter.documents (List[Document]) [/CODEBLOCK] [CODEBLOCK] pipeline . run ({ \"fetcher\" : { \"urls\" : [ 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/' , 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html' ]}}) [/CODEBLOCK] [CODEBLOCK] {'splitter': {'documents': [Document(id=9611aa2bdb658163d8f6964220052065936fcd036dd24743d1b34ce79d25bc5a, content: 'Deepset, a platform for building enterprise apps powered by large language models akin to ChatGPT, t...', meta: {'content_type': 'text/html', 'url': 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/', 'company': 'Deepset', 'year': 2023, 'funding_value': 30000000, 'funding_currency': 'USD', 'source_id': '5844517120556b13f92430ea8af9837714ede1b351580c43c2ddce9b646cb6cb', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0}), Document(id=6bffbcf9f1cd1a3940628d1450c9ba9a8c9a092136896d295b35af3175caffbf, content: 'unfortunate state of affairs is likely contributing to challenges around AI development within the e...', meta: {'content_type': 'text/html', 'url':", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "0b1414e1-ff75-4465-86c6-80be2890798a", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 7, "text": "by large language models akin to ChatGPT, t...', meta: {'content_type': 'text/html', 'url': 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/', 'company': 'Deepset', 'year': 2023, 'funding_value': 30000000, 'funding_currency': 'USD', 'source_id': '5844517120556b13f92430ea8af9837714ede1b351580c43c2ddce9b646cb6cb', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0}), Document(id=6bffbcf9f1cd1a3940628d1450c9ba9a8c9a092136896d295b35af3175caffbf, content: 'unfortunate state of affairs is likely contributing to challenges around AI development within the e...', meta: {'content_type': 'text/html', 'url': 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/', 'company': 'Deepset', 'year': 2023, 'funding_value': 30000000, 'funding_currency': 'USD', 'source_id': '5844517120556b13f92430ea8af9837714ede1b351580c43c2ddce9b646cb6cb', 'page_number': 1, 'split_id': 1, 'split_idx_start': 1256}), Document(id=da372f9bc2292f487f0aad372053a531744d9549a46f8ac197c216abaa4d99d0, content: 'to end users, and perform analyses of the LLMs’ accuracy while continuously monitoring their perform...', meta: {'content_type': 'text/html', 'url': 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/', 'company': 'Deepset', 'year': 2023, 'funding_value': 30000000, 'funding_currency': 'USD', 'source_id': '5844517120556b13f92430ea8af9837714ede1b351580c43c2ddce9b646cb6cb', 'page_number': 1, 'split_id': 2, 'split_idx_start': 2609}), Document(id=f316cd275e8bc763de41d128dbdbd81e1baad2693b0102f6951c4f46aa8f6048, content: 'predicts that the sector for MLOps will reach $23.1 billion by 2031, up from around $1 billion in 20...', meta: {'content_type': 'text/html', 'url': 'https://techcrunch.com/2023/08/09/deepset-secures-30m-to-expand-its-llm-focused-mlops-offerings/', 'company': 'Deepset', 'year': 2023, 'funding_value': 30000000, 'funding_currency': 'USD', 'source_id': '5844517120556b13f92430ea8af9837714ede1b351580c43c2ddce9b646cb6cb', 'page_number': 1, 'split_id': 3, 'split_idx_start': 3997}), Document(id=c7ff4e0d7af8aaa16f3195cb1f9096bb1cf8e7d985190fa6746c278b1d8457e8, content: 'Arize AI Raises $38 Million Series B To Scale Machine Learning Observability Platform As companies t...', meta: {'content_type': 'text/html', 'url': 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html', 'company': 'Arize AI', 'year': 2022, 'funding_value': 38000000, 'funding_currency': 'USD', 'source_id': '8cdcb63a4e006b1cac902ebc2e012cd95156d188777e0d0c8bd407a92f4491c7', 'page_number': 1, 'split_id': 0, 'split_idx_start': 0}), Document(id=646f4d43ee20fcbe35c97bd04e8fd6edd4ad5c9af63fe4a63267f5df1807254f, content: 'by humans. Launched in 2020, Arize's ML observability platform is already counted on by a growing li...', meta: {'content_type': 'text/html', 'url': 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html', 'company': 'Arize AI', 'year': 2022, 'funding_value': 38000000, 'funding_currency': 'USD', 'source_id': '8cdcb63a4e006b1cac902ebc2e012cd95156d188777e0d0c8bd407a92f4491c7', 'page_number': 1, 'split_id': 1, 'split_idx_start': 1360}), Document(id=09382ae0ab9adbd7860199b0d86e8ca044787eda860dfb181e3b243c6584a427, content: 'what happened, and improve overall model performance,\" says Morgan Gerlak, Partner at TCV. \"Like oth...', meta: {'content_type': 'text/html', 'url': 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html',", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "ba348438-1cd9-464a-a3a5-e597ea238277", "source": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "chunk_index": 8, "text": "counted on by a growing li...', meta: {'content_type': 'text/html', 'url': 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html', 'company': 'Arize AI', 'year': 2022, 'funding_value': 38000000, 'funding_currency': 'USD', 'source_id': '8cdcb63a4e006b1cac902ebc2e012cd95156d188777e0d0c8bd407a92f4491c7', 'page_number': 1, 'split_id': 1, 'split_idx_start': 1360}), Document(id=09382ae0ab9adbd7860199b0d86e8ca044787eda860dfb181e3b243c6584a427, content: 'what happened, and improve overall model performance,\" says Morgan Gerlak, Partner at TCV. \"Like oth...', meta: {'content_type': 'text/html', 'url': 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html', 'company': 'Arize AI', 'year': 2022, 'funding_value': 38000000, 'funding_currency': 'USD', 'source_id': '8cdcb63a4e006b1cac902ebc2e012cd95156d188777e0d0c8bd407a92f4491c7', 'page_number': 1, 'split_id': 2, 'split_idx_start': 2697}), Document(id=94497826791e38f15016a2360d7e3eea5e242770f51dd11be69fb21210e89c9a, content: 'you are going to be left behind,\" notes Brett Wilson, Co-Founder and General Partner at Swift Ventur...', meta: {'content_type': 'text/html', 'url': 'https://www.prnewswire.com/news-releases/arize-ai-raises-38-million-series-b-to-scale-machine-learning-observability-platform-301620603.html', 'company': 'Arize AI', 'year': 2022, 'funding_value': 38000000, 'funding_currency': 'USD', 'source_id': '8cdcb63a4e006b1cac902ebc2e012cd95156d188777e0d0c8bd407a92f4491c7', 'page_number': 1, 'split_id': 3, 'split_idx_start': 3967})]}} [/CODEBLOCK]", "metadata": {"url": "https://haystack.deepset.ai/cookbook/metadata_enrichment", "title": "Advanced RAG: Automated Structured Metadata Enrichment", "date": "4/11/2025", "type": "blog", "author": "Haystack"}}
{"id": "b5464c15-d703-4d5c-8bfa-1bd58fb91f2c", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 0, "text": "Retrieval Augmented Generation (RAG) | Cohere Retrieval Augmented Generation (RAG) is a method for generating text using additional information fetched from an external data source, which can greatly increase the accuracy of the response. When used in conjunction with Command family of models, the Chat API makes it easy to generate text that is grounded on supplementary documents, thus minimizing hallucinations. A quick example To call the Chat API with RAG, pass the following parameters as a minimum: model for the model ID messages for the user’s query. documents for defining the documents to be used as the context for the response. The code snippet below, for example, will produce a grounded answer to \"Where do the tallest penguins live?\" , along with inline citations based on the provided documents. Cohere platform Private deployment PYTHON 1 # ! pip install -U cohere 2 import cohere 3 4 co = cohere.ClientV2( 5 \"COHERE_API_KEY\" 6 ) # Get your free API key here: https://dashboard.cohere.com/api-keys PYTHON 1 # Retrieve the documents 2 documents = [ 3 { 4 \"data\": { 5 \"title\": \"Tall penguins\", 6 \"snippet\": \"Emperor penguins are the tallest.\", 7 } 8 }, 9 { 10 \"data\": { 11 \"title\": \"Penguin habitats\", 12 \"snippet\": \"Emperor penguins only live in Antarctica.\", 13 } 14 }, 15 { 16 \"data\": { 17 \"title\": \"What are animals?\", 18 \"snippet\": \"Animals are different from plants.\", 19 } 20 }, 21 ] 22 23 # Add the user message 24 message = \"Where do the tallest", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "911b262a-7882-44df-a08b-76ce36bf57eb", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 1, "text": "habitats\", 12 \"snippet\": \"Emperor penguins only live in Antarctica.\", 13 } 14 }, 15 { 16 \"data\": { 17 \"title\": \"What are animals?\", 18 \"snippet\": \"Animals are different from plants.\", 19 } 20 }, 21 ] 22 23 # Add the user message 24 message = \"Where do the tallest penguins live?\" 25 26 messages = [{\"role\": \"user\", \"content\": message}] 27 28 response = co.chat( 29 model=\"command-a-03-2025\", 30 messages=messages, 31 documents=documents, 32 ) 33 34 print(response.message.content[0].text) 35 36 print(response.message.citations) The resulting generation is \"The tallest penguins are emperor penguins, which live in Antarctica.\" . The model was able to combine partial information from multiple sources and ignore irrelevant documents to arrive at the full answer. Nice 🐧 ❄️! Example response: 1 # response.message.content[0].text 2 Emperor penguins are the tallest penguins. They only live in Antarctica. 3 4 # response.message.citations 5 [Citation(start=0, 6 end=16, 7 text='Emperor penguins', 8 sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})]), 9 Citation(start=25, 10 end=42, 11 text='tallest penguins.', 12 sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})]), 13 Citation(start=61, 14 end=72, 15 text='Antarctica.', 16 sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})])] The response also includes inline citations that reference the first two documents, since they hold the answers. Read more about using and customizing RAG citations here Three steps of RAG The RAG workflow generally consists of 3 steps : Generating search queries for finding relevant documents. What does", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "2167edd6-aa04-45c7-9704-ab5c633dca04", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 2, "text": "'Penguin habitats'})])] The response also includes inline citations that reference the first two documents, since they hold the answers. Read more about using and customizing RAG citations here Three steps of RAG The RAG workflow generally consists of 3 steps : Generating search queries for finding relevant documents. What does the model recommend looking up before answering this question? Fetching relevant documents from an external data source using the generated search queries. Performing a search to find some relevant information. Generating a response with inline citations using the fetched documents. Generating a response using the fetched documents. This response will contain inline citations, which you can decide to leverage or ignore . Example: Using RAG to identify the definitive 90s boy band In this section, we will use the three step RAG workflow to finally settle the score between the notorious boy bands Backstreet Boys and NSYNC. We ask the model to provide an informed answer to the question \"Who is more popular: Nsync or Backstreet Boys?\" Step 1: Generating search queries First, the model needs to generate an optimal set of search queries to use for retrieval. There are different possible approaches to do this. In this example, we’ll take a tool use approach. Here, we build a tool that takes a user query and returns a list of relevant document snippets for that query. The tool can generate zero, one or multiple search queries depending on the user query. PYTHON 1 message = \"Who is more popular: Nsync", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "8244ee51-9b98-4d13-9a10-044a72e095ec", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 3, "text": "take a tool use approach. Here, we build a tool that takes a user query and returns a list of relevant document snippets for that query. The tool can generate zero, one or multiple search queries depending on the user query. PYTHON 1 message = \"Who is more popular: Nsync or Backstreet Boys?\" 2 3 # Define the query generation tool 4 query_gen_tool = [ 5 { 6 \"type\": \"function\", 7 \"function\": { 8 \"name\": \"internet_search\", 9 \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\", 10 \"parameters\": { 11 \"type\": \"object\", 12 \"properties\": { 13 \"queries\": { 14 \"type\": \"array\", 15 \"items\": {\"type\": \"string\"}, 16 \"description\": \"a list of queries to search the internet with.\", 17 } 18 }, 19 \"required\": [\"queries\"], 20 }, 21 }, 22 } 23 ] 24 25 # Define a system message to optimize search query generation 26 instructions = \"Write a search query that will find helpful information for answering the user's question accurately. If you need more than one search query, write a list of search queries. If you decide that a search is very unlikely to find information that would be useful in constructing a response to the user, you should instead directly answer.\" 27 28 # Generate search queries (if any) 29 import json 30 31 search_queries = [] 32 33 res = co.chat( 34 model=\"command-a-03-2025\", 35 messages=[ 36 {\"role\": \"system\", \"content\": instructions}, 37 {\"role\": \"user\", \"content\": message}, 38 ], 39 tools=query_gen_tool, 40", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "96666481-3769-4bf1-908c-318ded7fdabf", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 4, "text": "a response to the user, you should instead directly answer.\" 27 28 # Generate search queries (if any) 29 import json 30 31 search_queries = [] 32 33 res = co.chat( 34 model=\"command-a-03-2025\", 35 messages=[ 36 {\"role\": \"system\", \"content\": instructions}, 37 {\"role\": \"user\", \"content\": message}, 38 ], 39 tools=query_gen_tool, 40 ) 41 42 if res.message.tool_calls: 43 for tc in res.message.tool_calls: 44 queries = json.loads(tc.function.arguments)[\"queries\"] 45 search_queries.extend(queries) 46 47 print(search_queries) # Sample response ['popularity of NSync', 'popularity of Backstreet Boys'] Indeed, to generate a factually accurate answer to the question “Who is more popular: Nsync or Backstreet Boys?”, looking up popularity of NSync and popularity of Backstreet Boys first would be helpful. Customizing the generation of search queries You can then update the system message and/or the tool definition to generate queries that are more relevant to your use case. For example, you can update the system message to encourage a longer list of search queries to be generated. PYTHON 1 instructions = \"Write a search query that will find helpful information for answering the user's question accurately. If you need more than one search query, write a list of search queries. If you decide that a search is very unlikely to find information that would be useful in constructing a response to the user, you should instead directly answer.\" Example response: 1 ['NSync popularity', 'Backstreet Boys popularity', 'NSync vs Backstreet Boys popularity comparison', 'Which boy band is more popular NSync or Backstreet Boys', 'NSync and Backstreet Boys fan base size", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "7f6560c0-47d8-43e6-88e0-7895dc60f686", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 5, "text": "to find information that would be useful in constructing a response to the user, you should instead directly answer.\" Example response: 1 ['NSync popularity', 'Backstreet Boys popularity', 'NSync vs Backstreet Boys popularity comparison', 'Which boy band is more popular NSync or Backstreet Boys', 'NSync and Backstreet Boys fan base size comparison', 'Who has sold more albums NSync or Backstreet Boys', 'NSync and Backstreet Boys chart performance comparison'] Step 2: Fetching relevant documents The next step is to fetch documents from the relevant data source using the generated search queries. For example, to answer the question about the two pop sensations NSYNC and Backstreet Boys , one might want to use an API from a web search engine, and fetch the contents of the websites listed at the top of the search results. We won’t go into details of fetching data in this guide, since it’s very specific to the search API you’re querying. However we should mention that breaking up documents into small chunks of ±400 words will help you get the best performance from Cohere models. Context length limit When trying to stay within the context length limit, you might need to omit some of the documents from the request. To make sure that only the least relevant documents are omitted, we recommend using the Rerank endpoint endpoint which will sort the documents by relevancy to the query. The lowest ranked documents are the ones you should consider dropping first. Formatting documents The Chat endpoint supports a few different", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "6e6c890e-d088-426d-b019-36ce51d90099", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 6, "text": "request. To make sure that only the least relevant documents are omitted, we recommend using the Rerank endpoint endpoint which will sort the documents by relevancy to the query. The lowest ranked documents are the ones you should consider dropping first. Formatting documents The Chat endpoint supports a few different options for structuring documents in the documents parameter: List of objects with data object: Each document is passed as a data object (with an optional id field to be used in citations). The data object is a string-any dictionary containing the document’s contents. For example, a web search document can contain a title , text , and url for the document’s title, text, and URL. List of objects with data string: Each document is passed as a data string (with an optional id field to be used in citations). List of strings: Each document is passed as a string. The following examples demonstrate the options mentioned above. 'data' object 'data' string string PYTHON 1 documents = [ 2 { 3 \"data\": { 4 \"title\": \"Tall penguins\", 5 \"snippet\": \"Emperor penguins are the tallest.\", 6 } 7 } 8 ] The id field will be used in citation generation as the reference document IDs. If no id field is passed in an API call, the API will automatically generate the IDs based on the documents position in the list. For more information, see the guide on using custom IDs . Step 3: Generating a response with citations In the final step,", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "1986e6c9-e5e3-4040-8ac2-5e5c3b578c66", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 7, "text": "reference document IDs. If no id field is passed in an API call, the API will automatically generate the IDs based on the documents position in the list. For more information, see the guide on using custom IDs . Step 3: Generating a response with citations In the final step, we will be calling the Chat API again, but this time passing along the documents you acquired in Step 2. We recommend using a few descriptive keys such as \"title\" , \"snippet\" , or \"last updated\" and only including semantically relevant data. The keys and the values will be formatted into the prompt and passed to the model. PYTHON 1 documents = [ 2 { 3 \"data\": { 4 \"title\": \"CSPC: Backstreet Boys Popularity Analysis - ChartMasters\", 5 \"snippet\": \"↓ Skip to Main Content\\n\\nMusic industry – One step closer to being accurate\\n\\nCSPC: Backstreet Boys Popularity Analysis\\n\\nHernán Lopez Posted on February 9, 2017 Posted in CSPC 72 Comments Tagged with Backstreet Boys, Boy band\\n\\nAt one point, Backstreet Boys defined success: massive albums sales across the globe, great singles sales, plenty of chart topping releases, hugely hyped tours and tremendous media coverage.\\n\\nIt is true that they benefited from extraordinarily good market conditions in all markets. After all, the all-time record year for the music business, as far as revenues in billion dollars are concerned, was actually 1999. That is, back when this five men group was at its peak.\", 6 } 7 }, 8 { 9 \"data\": { 10 \"title\": \"CSPC: NSYNC", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "c2386ef0-0366-4677-b562-83ed77bb1a54", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 8, "text": "in all markets. After all, the all-time record year for the music business, as far as revenues in billion dollars are concerned, was actually 1999. That is, back when this five men group was at its peak.\", 6 } 7 }, 8 { 9 \"data\": { 10 \"title\": \"CSPC: NSYNC Popularity Analysis - ChartMasters\", 11 \"snippet\": \"↓ Skip to Main Content\\n\\nMusic industry – One step closer to being accurate\\n\\nCSPC: NSYNC Popularity Analysis\\n\\nMJD Posted on February 9, 2018 Posted in CSPC 27 Comments Tagged with Boy band, N'Sync\\n\\nAt the turn of the millennium three teen acts were huge in the US, the Backstreet Boys, Britney Spears and NSYNC. The latter is the only one we haven’t study so far. It took 15 years and Adele to break their record of 2,4 million units sold of No Strings Attached in its first week alone.\\n\\nIt wasn’t a fluke, as the second fastest selling album of the Soundscan era prior 2015, was also theirs since Celebrity debuted with 1,88 million units sold.\", 12 } 13 }, 14 { 15 \"data\": { 16 \"title\": \"CSPC: Backstreet Boys Popularity Analysis - ChartMasters\", 17 \"snippet\": \" 1997, 1998, 2000 and 2001 also rank amongst some of the very best years.\\n\\nYet the way many music consumers – especially teenagers and young women’s – embraced their output deserves its own chapter. If Jonas Brothers and more recently One Direction reached a great level of popularity during the past decade, the type of success achieved by Backstreet Boys is in", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "cc6cddf7-b9a4-4f6c-b858-30e1c9718d75", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 9, "text": "very best years.\\n\\nYet the way many music consumers – especially teenagers and young women’s – embraced their output deserves its own chapter. If Jonas Brothers and more recently One Direction reached a great level of popularity during the past decade, the type of success achieved by Backstreet Boys is in a completely different level as they really dominated the business for a few years all over the world, including in some countries that were traditionally hard to penetrate for Western artists.\\n\\nWe will try to analyze the extent of that hegemony with this new article with final results which will more than surprise many readers.\", 18 } 19 }, 20 { 21 \"data\": { 22 \"title\": \"CSPC: NSYNC Popularity Analysis - ChartMasters\", 23 \"snippet\": \" Was the teen group led by Justin Timberlake really that big? Was it only in the US where they found success? Or were they a global phenomenon?\\n\\nAs usual, I’ll be using the Commensurate Sales to Popularity Concept in order to relevantly gauge their results. This concept will not only bring you sales information for all NSYNC‘s albums, physical and download singles, as well as audio and video streaming, but it will also determine their true popularity. If you are not yet familiar with the CSPC method, the next page explains it with a short video. I fully recommend watching the video before getting into the sales figures.\", 24 } 25 }, 26 ] 27 28 # Add the user message 29 message = \"Who is more", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "dc5e79d2-05e4-4433-94c3-526a463032ba", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 10, "text": "If you are not yet familiar with the CSPC method, the next page explains it with a short video. I fully recommend watching the video before getting into the sales figures.\", 24 } 25 }, 26 ] 27 28 # Add the user message 29 message = \"Who is more popular: Nsync or Backstreet Boys?\" 30 messages = [{\"role\": \"user\", \"content\": message}] 31 32 response = co.chat( 33 model=\"command-a-03-2025\", 34 messages=messages, 35 documents=documents, 36 ) 37 38 print(response.message.content[0].text) Example response: 1 Both NSYNC and Backstreet Boys were huge in the US at the turn of the millennium. However, Backstreet Boys achieved a greater level of success than NSYNC. They dominated the music business for a few years all over the world, including in some countries that were traditionally hard to penetrate for Western artists. Their success included massive album sales across the globe, great singles sales, plenty of chart-topping releases, hugely hyped tours and tremendous media coverage. In this RAG setting, Cohere models are trained to generate fine-grained citations, out-of-the-box, alongside their text output. Here, we see a sample list of citations, one for each specific span in its response, where it uses the document(s) to answer the question. For a deeper dive into the citations feature, see the RAG citations guide . PYTHON 1 print(response.message.citations) Example response: 1 # (truncated for brevity) 2 [Citation(start=36, 3 end=81, 4 text='huge in the US at the turn of the millennium.', 5 sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'snippet': \"↓ Skip to Main Content\\n\\nMusic industry", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "b6d2284b-d781-45f3-b765-c8b0ad7ea9e8", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 11, "text": "a deeper dive into the citations feature, see the RAG citations guide . PYTHON 1 print(response.message.citations) Example response: 1 # (truncated for brevity) 2 [Citation(start=36, 3 end=81, 4 text='huge in the US at the turn of the millennium.', 5 sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'snippet': \"↓ Skip to Main Content\\n\\nMusic industry – One step closer ...\", 'title': 'CSPC: NSYNC Popularity Analysis - ChartMasters'})]), 6 Citation(start=107, 7 end=154, 8 text='achieved a greater level of success than NSYNC.', 9 sources=[DocumentSource(type='document', id='doc:2', document={'id': 'doc:2', 'snippet': ' 1997, 1998, 2000 and 2001 also rank amongst some of the very best ...', 'title': 'CSPC: Backstreet Boys Popularity Analysis - ChartMasters'})]), 10 Citation(start=160, 11 end=223, 12 ... 13 ...] Not only will we discover that the Backstreet Boys were the more popular band, but the model can also Tell Me Why , by providing details supported by citations . For a more in-depth RAG example that leverages the Embed and Rerank endpoints for retrieval, see End-to-end example of RAG with Chat, Embed, and Rerank . Caveats It’s worth underscoring that RAG does not guarantee accuracy. It involves giving a model context which informs its replies, but if the provided documents are themselves out-of-date, inaccurate, or biased, whatever the model generates might be as well. What’s more, RAG doesn’t guarantee that a model won’t hallucinate. It greatly reduces the risk, but doesn’t necessarily eliminate it altogether. This is why we put an emphasis on including inline citations, which allow users to verify the information.", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "5831edec-aff3-4f97-a84a-b1039cc64541", "source": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "chunk_index": 12, "text": "the model generates might be as well. What’s more, RAG doesn’t guarantee that a model won’t hallucinate. It greatly reduces the risk, but doesn’t necessarily eliminate it altogether. This is why we put an emphasis on including inline citations, which allow users to verify the information.", "metadata": {"url": "https://docs.cohere.com/docs/retrieval-augmented-generation-rag", "title": "Retrieval Augmented Generation (RAG)", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "362ae530-d5a1-487a-89d3-f201983d46c0", "source": "https://docs.cohere.com/docs/rag-citations", "chunk_index": 0, "text": "RAG Citations | Cohere Accessing citations The Chat endpoint generates fine-grained citations for its RAG response. This capability is included out-of-the-box with the Command family of models. The following sections describe how to access the citations in both the non-streaming and streaming modes. Non-streaming First, define the documents to be passed as the context of the model’s response. Cohere platform Private deployment PYTHON 1 # ! pip install -U cohere 2 import cohere 3 import json 4 5 co = cohere.ClientV2( 6 \"COHERE_API_KEY\" 7 ) # Get your free API key here: https://dashboard.cohere.com/api-keys PYTHON 1 documents = [ 2 { 3 \"data\": { 4 \"title\": \"Tall penguins\", 5 \"snippet\": \"Emperor penguins are the tallest.\", 6 } 7 }, 8 { 9 \"data\": { 10 \"title\": \"Penguin habitats\", 11 \"snippet\": \"Emperor penguins only live in Antarctica.\", 12 } 13 }, 14 ] In the non-streaming mode (using chat to generate the model response), the citations are provided in the message.citations field of the response object. Each citation object contains: start and end : the start and end indices of the text that cites a source(s) text : its corresponding span of text sources : the source(s) that it references PYTHON 1 messages = [ 2 {\"role\": \"user\", \"content\": \"Where do the tallest penguins live?\"} 3 ] 4 5 response = co.chat( 6 model=\"command-a-03-2025\", 7 messages=messages, 8 documents=documents, 9 ) 10 11 print(response.message.content[0].text) 12 13 for citation in response.message.citations: 14 print(citation, \"\\n\") Example response: 1 The tallest penguins are the Emperor penguins.", "metadata": {"url": "https://docs.cohere.com/docs/rag-citations", "title": "RAG Citations", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "54685a92-8926-473e-9c65-67bf9220caba", "source": "https://docs.cohere.com/docs/rag-citations", "chunk_index": 1, "text": "messages = [ 2 {\"role\": \"user\", \"content\": \"Where do the tallest penguins live?\"} 3 ] 4 5 response = co.chat( 6 model=\"command-a-03-2025\", 7 messages=messages, 8 documents=documents, 9 ) 10 11 print(response.message.content[0].text) 12 13 for citation in response.message.citations: 14 print(citation, \"\\n\") Example response: 1 The tallest penguins are the Emperor penguins. They only live in Antarctica. 2 3 start=29 end=46 text='Emperor penguins.' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type='TEXT_CONTENT' 4 5 start=65 end=76 text='Antarctica.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type='TEXT_CONTENT' Streaming In a streaming scenario (using chat_stream to generate the model response), the citations are provided in the citation-start events. Each citation object contains the same fields as the non-streaming scenario . PYTHON 1 messages = [ 2 {\"role\": \"user\", \"content\": \"Where do the tallest penguins live?\"} 3 ] 4 5 response = co.chat_stream( 6 model=\"command-a-03-2025\", 7 messages=messages, 8 documents=documents, 9 ) 10 11 response_text = \"\" 12 citations = [] 13 for chunk in response: 14 if chunk: 15 if chunk.type == \"content-delta\": 16 response_text += chunk.delta.message.content.text 17 print(chunk.delta.message.content.text, end=\"\") 18 if chunk.type == \"citation-start\": 19 citations.append(chunk.delta.message.citations) 20 21 for citation in citations: 22 print(citation, \"\\n\") Example response: 1 The tallest penguins are the Emperor penguins, which only live in Antarctica. 2 3 start=29 end=45 text='Emperor penguins' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type='TEXT_CONTENT' 4 5 start=66 end=77 text='Antarctica.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'snippet': 'Emperor penguins only", "metadata": {"url": "https://docs.cohere.com/docs/rag-citations", "title": "RAG Citations", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "b7780a62-adfe-4132-8636-51bade37f839", "source": "https://docs.cohere.com/docs/rag-citations", "chunk_index": 2, "text": "print(citation, \"\\n\") Example response: 1 The tallest penguins are the Emperor penguins, which only live in Antarctica. 2 3 start=29 end=45 text='Emperor penguins' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type='TEXT_CONTENT' 4 5 start=66 end=77 text='Antarctica.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type='TEXT_CONTENT' Document ID When passing the documents as context, you can optionally add custom IDs to the id field in the document object. These IDs will be used by the endpoint as the citation reference. If you don’t provide the id field, the ID will be auto-generated in the the format of doc:<auto_generated_id> . Example: doc:0 . Here is an example of using custom IDs. Here, we are adding custom IDs 100 and 101 to each of the two documents we are passing as context. PYTHON 1 # ! pip install -U cohere 2 import cohere 3 import json 4 5 co = cohere.ClientV2( 6 \"COHERE_API_KEY\" 7 ) # Get your free API key here: https://dashboard.cohere.com/api-keys 8 9 documents = [ 10 { 11 \"data\": { 12 \"title\": \"Tall penguins\", 13 \"snippet\": \"Emperor penguins are the tallest.\", 14 }, 15 \"id\": \"100\", 16 }, 17 { 18 \"data\": { 19 \"title\": \"Penguin habitats\", 20 \"snippet\": \"Emperor penguins only live in Antarctica.\", 21 }, 22 \"id\": \"101\", 23 }, 24 ] When document IDs are provided, the citation will refer to the documents using these IDs. PYTHON 1 messages = [ 2 {\"role\": \"user\", \"content\":", "metadata": {"url": "https://docs.cohere.com/docs/rag-citations", "title": "RAG Citations", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "f86fe944-47e9-40ff-adc4-9e5156a8f0f5", "source": "https://docs.cohere.com/docs/rag-citations", "chunk_index": 3, "text": "17 { 18 \"data\": { 19 \"title\": \"Penguin habitats\", 20 \"snippet\": \"Emperor penguins only live in Antarctica.\", 21 }, 22 \"id\": \"101\", 23 }, 24 ] When document IDs are provided, the citation will refer to the documents using these IDs. PYTHON 1 messages = [ 2 {\"role\": \"user\", \"content\": \"Where do the tallest penguins live?\"} 3 ] 4 5 response = co.chat( 6 model=\"command-a-03-2025\", 7 messages=messages, 8 documents=documents, 9 ) 10 11 print(response.message.content[0].text) Note the id fields in the citations, which refer to the IDs in the document object. Example response: 1 The tallest penguins are the Emperor penguins, which only live in Antarctica. 2 3 start=29 end=45 text='Emperor penguins' sources=[DocumentSource(type='document', id='100', document={'id': '100', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type='TEXT_CONTENT' 4 5 start=66 end=77 text='Antarctica.' sources=[DocumentSource(type='document', id='101', document={'id': '101', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type='TEXT_CONTENT' In contrast, here’s an example citation when the IDs are not provided. Example response: 1 The tallest penguins are the Emperor penguins, which only live in Antarctica. 2 3 start=29 end=45 text='Emperor penguins' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type='TEXT_CONTENT' 4 5 start=66 end=77 text='Antarctica.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type='TEXT_CONTENT' Citation modes When running RAG in streaming mode, it’s possible to configure how citations are generated and presented. You can choose between fast citations or accurate citations, depending on your latency and precision needs. Accurate citations The model", "metadata": {"url": "https://docs.cohere.com/docs/rag-citations", "title": "RAG Citations", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "efd26699-1e43-40e8-a12c-9fd6cb442657", "source": "https://docs.cohere.com/docs/rag-citations", "chunk_index": 4, "text": "'doc:1', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type='TEXT_CONTENT' Citation modes When running RAG in streaming mode, it’s possible to configure how citations are generated and presented. You can choose between fast citations or accurate citations, depending on your latency and precision needs. Accurate citations The model produces its answer first, and then, after the entire response is generated, it provides citations that map to specific segments of the response text. This approach may incur slightly higher latency, but it ensures the citation indices are more precisely aligned with the final text segments of the model’s answer. This is the default option, or you can explicitly specify it by adding the citation_options={\"mode\": \"accurate\"} argument in the API call. Here is an example using the same list of pre-defined messages as the above. With the citation_options mode set to accurate , we get the citations after the entire response is generated. PYTHON 1 documents = [ 2 { 3 \"data\": { 4 \"title\": \"Tall penguins\", 5 \"snippet\": \"Emperor penguins are the tallest.\", 6 }, 7 \"id\": \"100\", 8 }, 9 { 10 \"data\": { 11 \"title\": \"Penguin habitats\", 12 \"snippet\": \"Emperor penguins only live in Antarctica.\", 13 }, 14 \"id\": \"101\", 15 }, 16 ] 17 18 messages = [ 19 {\"role\": \"user\", \"content\": \"Where do the tallest penguins live?\"} 20 ] 21 22 response = co.chat_stream( 23 model=\"command-a-03-2025\", 24 messages=messages, 25 documents=documents, 26 citation_options={\"mode\": \"accurate\"}, 27 ) 28 29 response_text = \"\" 30 citations = [] 31", "metadata": {"url": "https://docs.cohere.com/docs/rag-citations", "title": "RAG Citations", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "e3e3fdee-c468-46c5-a6f0-e6bb9a3da0ff", "source": "https://docs.cohere.com/docs/rag-citations", "chunk_index": 5, "text": "14 \"id\": \"101\", 15 }, 16 ] 17 18 messages = [ 19 {\"role\": \"user\", \"content\": \"Where do the tallest penguins live?\"} 20 ] 21 22 response = co.chat_stream( 23 model=\"command-a-03-2025\", 24 messages=messages, 25 documents=documents, 26 citation_options={\"mode\": \"accurate\"}, 27 ) 28 29 response_text = \"\" 30 citations = [] 31 for chunk in response: 32 if chunk: 33 if chunk.type == \"content-delta\": 34 response_text += chunk.delta.message.content.text 35 print(chunk.delta.message.content.text, end=\"\") 36 if chunk.type == \"citation-start\": 37 citations.append(chunk.delta.message.citations) 38 39 print(\"\\n\") 40 for citation in citations: 41 print(citation, \"\\n\") Example response: 1 The tallest penguins are the Emperor penguins. They live in Antarctica. 2 3 start=29 end=46 text='Emperor penguins.' sources=[DocumentSource(type='document', id='100', document={'id': '100', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type='TEXT_CONTENT' 4 5 start=60 end=71 text='Antarctica.' sources=[DocumentSource(type='document', id='101', document={'id': '101', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type='TEXT_CONTENT' Fast citations The model generates citations inline, as the response is being produced. In streaming mode, you will see citations injected at the exact moment the model uses a particular piece of external context. This approach provides immediate traceability at the expense of slightly less precision in citation relevance. You can specify it by adding the citation_options={\"mode\": \"fast\"} argument in the API call. With the citation_options mode set to fast , we get the citations inline as the model generates the response. PYTHON 1 documents = [ 2 { 3 \"data\": { 4 \"title\": \"Tall penguins\", 5 \"snippet\": \"Emperor penguins are the tallest.\", 6 }, 7 \"id\": \"100\", 8", "metadata": {"url": "https://docs.cohere.com/docs/rag-citations", "title": "RAG Citations", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "20418521-9206-4df6-8504-46487dc3ab23", "source": "https://docs.cohere.com/docs/rag-citations", "chunk_index": 6, "text": "in the API call. With the citation_options mode set to fast , we get the citations inline as the model generates the response. PYTHON 1 documents = [ 2 { 3 \"data\": { 4 \"title\": \"Tall penguins\", 5 \"snippet\": \"Emperor penguins are the tallest.\", 6 }, 7 \"id\": \"100\", 8 }, 9 { 10 \"data\": { 11 \"title\": \"Penguin habitats\", 12 \"snippet\": \"Emperor penguins only live in Antarctica.\", 13 }, 14 \"id\": \"101\", 15 }, 16 ] 17 18 messages = [ 19 {\"role\": \"user\", \"content\": \"Where do the tallest penguins live?\"} 20 ] 21 22 response = co.chat_stream( 23 model=\"command-a-03-2025\", 24 messages=messages, 25 documents=documents, 26 citation_options={\"mode\": \"fast\"}, 27 ) 28 29 response_text = \"\" 30 for chunk in response: 31 if chunk: 32 if chunk.type == \"content-delta\": 33 response_text += chunk.delta.message.content.text 34 print(chunk.delta.message.content.text, end=\"\") 35 if chunk.type == \"citation-start\": 36 print( 37 f\" [{chunk.delta.message.citations.sources[0].id}]\", 38 end=\"\", 39 ) Example response: 1 The tallest penguins [100] are the Emperor penguins [100] which only live in Antarctica. [101]", "metadata": {"url": "https://docs.cohere.com/docs/rag-citations", "title": "RAG Citations", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "28621d62-a795-4ed3-873f-93db2390fe67", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 0, "text": "Effective Chunking Strategies for RAG | Cohere Ania Bialas Back to Cookbooks Open in GitHub PYTHON 1 %%capture 2 !pip install cohere 3 !pip install -qU langchain-text-splitters 4 !pip install llama-index-embeddings-cohere 5 !pip install llama-index-postprocessor-cohere-rerank PYTHON 1 import requests 2 from typing import List 3 4 from bs4 import BeautifulSoup 5 6 import cohere 7 from getpass import getpass 8 from IPython.display import HTML, display 9 10 from langchain_text_splitters import CharacterTextSplitter 11 from langchain_text_splitters import RecursiveCharacterTextSplitter 12 13 from llama_index.core import Document 14 from llama_index.embeddings.cohere import CohereEmbedding 15 from llama_index.postprocessor.cohere_rerank import CohereRerank 16 from llama_index.core import VectorStoreIndex, ServiceContext PYTHON 1 co_model = 'command-r' 2 co_api_key = getpass(\"Enter Cohere API key: \") 3 co = cohere.Client(api_key=co_api_key) Output Enter Cohere API key: ·········· Introduction Chunking is an essential component of any RAG-based system. This cookbook aims to demonstrate how different chunking strategies affect the results of LLM-generated output. There are multiple considerations that need to be taken into account when designing chunking strategy. Therefore, we begin by providing a framework for these strategies and then jump into a practical example. We will focus our example on transcript calls, which create a unique challenge because of their rich content and the change of people speaking throughout the text. Chunking strategies framework Document splitting By document splitting, we mean deciding on the conditions under which we will break the text. At this stage, we should ask, “Are there any parts of consecutive text we want to ensure we do not break?” . If the", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "12d37f76-1af6-4807-b6f0-66f1df474499", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 1, "text": "people speaking throughout the text. Chunking strategies framework Document splitting By document splitting, we mean deciding on the conditions under which we will break the text. At this stage, we should ask, “Are there any parts of consecutive text we want to ensure we do not break?” . If the answer is “no”, then, the content-independent splitting strategies are helpful. On the other hand, in scenarios like transcripts or meeting notes, we probably would like to keep the content of one speaker together, which might require us to deploy content-dependent strategies. Content-independent splitting strategies We split the document based on some content-independent conditions, among the most popular ones are: splitting by the number of characters, splitting by sentence, splitting by a given character, for example, \\n for paragraphs. The advantage of this scenario is that we do not need to make any assumptions about the text. However, some considerations remain, like whether we want to preserve some semantic structure, for example, sentences or paragraphs. Sentence splitting is better suited if we are looking for small chunks to ensure accuracy. Conversely, paragraphs preserve more context and might be more useful in open-ended questions. Content-dependent splitting strategies On the other hand, there are scenarios in which we care about preserving some text structure. Then, we develop custom splitting strategies based on the document’s content. A prime example is call transcripts. In such scenarios, we aim to ensure that one person’s speech is fully contained within a chunk. Creating chunks from the document", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "647bbcff-c2c7-479c-9183-4c4348891d9b", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 2, "text": "are scenarios in which we care about preserving some text structure. Then, we develop custom splitting strategies based on the document’s content. A prime example is call transcripts. In such scenarios, we aim to ensure that one person’s speech is fully contained within a chunk. Creating chunks from the document splits After the document is split, we need to decide on the desired size of our chunks (the split only defines how we break the document, but we can create bigger chunks from multiple splits). Smaller chunks support more accurate retrieval. However, they might lack context. On the other hand, larger chunks offer more context, but they reduce the effectiveness of the retrieval. It is important to experiment with different settings to find the optimal balance. Overlapping chunks Overlapping chunks is a useful technique to have in the toolbox. Especially when we employ content-independent splitting strategies, it helps us mitigate some of the pitfalls of breaking the document without fully understanding the text. Overlapping guarantees that there is always some buffer between the chunks, and even if an important piece of information might be split in the original splitting strategy, it is more probable that the full information will be captured in the next chunk. The disadvantage of this method is that it creates redundancy. Getting started Designing a robust chunking strategy is as much a science as an art. There are no straightforward answers; the most effective strategies often emerge through experimentation. Therefore, let’s dive straight into an example", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "45932354-8e48-46c5-a373-7d3dfb6e0a78", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 3, "text": "captured in the next chunk. The disadvantage of this method is that it creates redundancy. Getting started Designing a robust chunking strategy is as much a science as an art. There are no straightforward answers; the most effective strategies often emerge through experimentation. Therefore, let’s dive straight into an example to illustrate this concept. Utils PYTHON 1 def set_css(): 2 display(HTML(''' 3 4 ''')) 5 get_ipython().events.register('pre_run_cell', set_css) 6 7 set_css() PYTHON 1 def insert_citations(text: str, citations: List[dict]): 2 \"\"\" 3 A helper function to pretty print citations. 4 \"\"\" 5 offset = 0 6 # Process citations in the order they were provided 7 for citation in citations: 8 # Adjust start/end with offset 9 start, end = citation['start'] + offset, citation['end'] + offset 10 placeholder = \"[\" + \", \".join(doc[4:] for doc in citation[\"document_ids\"]) + \"]\" 11 # ^ doc[4:] removes the 'doc_' prefix, and leaves the quoted document 12 modification = f'{text[start:end]} {placeholder}' 13 # Replace the cited text with its bolded version + placeholder 14 text = text[:start] + modification + text[end:] 15 # Update the offset for subsequent replacements 16 offset += len(modification) - (end - start) 17 18 return text 19 20 def build_retriever(documents, top_n=5): 21 # Create the embedding model 22 embed_model = CohereEmbedding( 23 cohere_api_key=co_api_key, 24 model_name=\"embed-v4.0\", 25 input_type=\"search_query\", 26 ) 27 28 # Load the data, for this example data needs to be in a test file 29 index = VectorStoreIndex.from_documents( 30 documents, 31 embed_model=embed_model 32 ) 33 34 # Create a", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "a04cc39a-f730-4949-a445-61db4ac2344d", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 4, "text": "21 # Create the embedding model 22 embed_model = CohereEmbedding( 23 cohere_api_key=co_api_key, 24 model_name=\"embed-v4.0\", 25 input_type=\"search_query\", 26 ) 27 28 # Load the data, for this example data needs to be in a test file 29 index = VectorStoreIndex.from_documents( 30 documents, 31 embed_model=embed_model 32 ) 33 34 # Create a cohere reranker 35 cohere_rerank = CohereRerank(api_key=co_api_key) 36 37 # Create the retriever 38 retriever = index.as_retriever(node_postprocessors=[cohere_rerank], similarity_top_k=top_n) 39 return retriever Load the data In this example we will work with an 2023 Tesla earning call transcript. PYTHON 1 # Get all investement memos (19) in bvp repository 2 url_path = 'https://www.fool.com/earnings/call-transcripts/2024/01/24/tesla-tsla-q4-2023-earnings-call-transcript/' 3 response = requests.get(url_path) 4 soup = BeautifulSoup(response.content, 'html.parser') 5 6 target_divs = soup.find(\"div\", {\"class\": \"article-body\"}).find_all(\"p\")[2:] 7 print('Length of the script: ', len(target_divs)) 8 9 print() 10 print('Example of processed text:') 11 text = '\\n\\n'.join([div.get_text() for div in target_divs]) 12 print(text[:500]) Output Length of the script: 385 Example of processed text: Martin Viecha Good afternoon, everyone, and welcome to Tesla's fourth-quarter 2023 Q&amp;A webcast. My name is Martin Viecha, VP of investor relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q4 results were announced at about 3 p.m. Central Time in the update that we published at the same link as this webcast. During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and Example 1: Chunking using content-independent strategies Let’s begin with a simple content-independent strategy. We aim to", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "be87efad-7486-4626-85d3-ba5433b654fc", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 5, "text": "in the update that we published at the same link as this webcast. During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and Example 1: Chunking using content-independent strategies Let’s begin with a simple content-independent strategy. We aim to answer the question, Who mentions Jonathan Nolan? . We chose this question as it is easily verifiable and it requires to identify the speaker. The answer to this question can be found in the downloaded transcript, here is the relevant passage: Elon Musk -- Chief Executive Officer and Product Architect Yeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well. PYTHON 1 # Define the question 2 question = \"Who mentions Jonathan Nolan?\" In this case, we are more concerned about accuracy than a verbose answer, so we focus on keeping the chunks small . To ensure that the desired size is not exceeded, we will randomly split the list of characters, in our case [\"\\n\\n\", \"\\n\", \" \", \"\"] . We employ the RecursiveCharacterTextSplitter from LangChain for this task. PYTHON 1 # Define the chunking function 2 def", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "0a7892bc-00d4-473c-af92-c7edf34a5c0a", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 6, "text": "on keeping the chunks small . To ensure that the desired size is not exceeded, we will randomly split the list of characters, in our case [\"\\n\\n\", \"\\n\", \" \", \"\"] . We employ the RecursiveCharacterTextSplitter from LangChain for this task. PYTHON 1 # Define the chunking function 2 def get_chunks(text, chunk_size, chunk_overlap): 3 text_splitter = RecursiveCharacterTextSplitter( 4 chunk_size=chunk_size, 5 chunk_overlap=chunk_overlap, 6 length_function=len, 7 is_separator_regex=False, 8 ) 9 10 documents = text_splitter.create_documents([text]) 11 documents = [Document(text=doc.page_content) for doc in documents] 12 13 return documents Experiment 1 - no overlap In our first experiment we define the chunk size as 500 and allow no overlap between consecutive chunks . Subsequently, we implement the standard RAG pipeline. We feed the chunks into a retriever, selecting the top_n most pertinent to the query chunks, and supply them as context to the generation model. Throughout this pipeline, we leverage Cohere’s endpoints , specifically, co.embed , co.re.rank , and finally, co.chat . PYTHON 1 chunk_size = 500 2 chunk_overlap = 0 3 documents = get_chunks(text, chunk_size, chunk_overlap) 4 retriever = build_retriever(documents) 5 6 source_nodes = retriever.retrieve(question) 7 print('Number of docuemnts: ',len(source_nodes)) 8 source_nodes= [{\"text\": ni.get_content()}for ni in source_nodes] 9 10 11 response = co.chat( 12 message=question, 13 documents=source_nodes, 14 model=co_model 15 ) 16 response = response 17 print(response.text) Output Number of docuemnts: 5 An unknown speaker mentions Jonathan Nolan in a conversation about the creators of Westworld. They mention that Jonathan Nolan and Lisa Joy Nolan are friends of theirs, and that they have invited", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "8853a131-179e-40b6-b9c9-6fea3adda057", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 7, "text": "message=question, 13 documents=source_nodes, 14 model=co_model 15 ) 16 response = response 17 print(response.text) Output Number of docuemnts: 5 An unknown speaker mentions Jonathan Nolan in a conversation about the creators of Westworld. They mention that Jonathan Nolan and Lisa Joy Nolan are friends of theirs, and that they have invited them to visit the lab. A notable feature of co.chat is its ability to ground the model’s answer within the context. This means we can identify which chunks were used to generate the answer. Below, we show the previous output of the model together with the citation reference, where [num] represents the index of the chunk. PYTHON 1 print(insert_citations(response.text, response.citations)) Output An unknown speaker [0] mentions Jonathan Nolan in a conversation about the creators of Westworld. [0] They mention that Jonathan Nolan and Lisa Joy Nolan [0] are friends [0] of theirs, and that they have invited them to visit the lab. [0] Indeed, by printing the cited chunk, we can validate that the text was divided so that the generation model could not provide the correct response. Notably, the speaker’s name is not included in the context, which is why the model refes to an unknown speaker . PYTHON 1 print(source_nodes[0]) Output 1 {'text': \"Yeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "d5e9eb21-0a08-4dbb-b334-2ecffbc954ab", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 8, "text": "1 print(source_nodes[0]) Output 1 {'text': \"Yeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\\n\\nYeah.\\n\\nUnknown speaker\\n\\nWe're not entering Westworld anytime soon.\"} Experiment 2 - allow overlap In the previous experiment, we discovered that the chunks were generated in a way that made it impossible to generate the correct answer. The name of the speaker was not included in the relevant chunk. Therefore, this time to mitigate this issue, we allow for overlap between consecutive chunks . PYTHON 1 chunk_size = 500 2 chunk_overlap = 100 3 documents = get_chunks(text,chunk_size, chunk_overlap) 4 retriever = build_retriever(documents) 5 6 source_nodes = retriever.retrieve(question) 7 print('Number of docuemnts: ',len(source_nodes)) 8 source_nodes= [{\"text\": ni.get_content()}for ni in source_nodes] 9 10 11 response = co.chat( 12 message=question, 13 documents=source_nodes, 14 model=co_model 15 ) 16 response = response 17 print(response.text) Output Number of docuemnts: 5 Elon Musk mentions Jonathan Nolan. Musk is the CEO and Product Architect of the lab that resembles the set of Westworld, a show created by Jonathan Nolan and Lisa Joy Nolan. Again, we can print the text along with the citations. PYTHON 1 print(insert_citations(response.text, response.citations)) Output Elon Musk [0] mentions Jonathan Nolan. Musk", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "80df5b11-aee7-4bbf-85a4-a1d1201f65af", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 9, "text": "Jonathan Nolan. Musk is the CEO and Product Architect of the lab that resembles the set of Westworld, a show created by Jonathan Nolan and Lisa Joy Nolan. Again, we can print the text along with the citations. PYTHON 1 print(insert_citations(response.text, response.citations)) Output Elon Musk [0] mentions Jonathan Nolan. Musk is the CEO and Product Architect [0] of the lab [0] that resembles the set of Westworld [0], a show created by Jonathan Nolan [0] and Lisa Joy Nolan. [0] And investigate the chunks which were used as context to answer the query. PYTHON 1 source_nodes[0] Output 1 {'text': \"Yeah, not the best reference.\\n\\nElon Musk -- Chief Executive Officer and Product Architect\\n\\nYeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\\n\\nYeah.\"} As we can see, by allowing overlap we managed to get the correct answer to our question. Example 2: Chunking using content-dependent strategies In the previous experiment, we provided an example of how using or not using overlapping can affect a model’s performance, particularly in documents such as call transcripts where subjects change frequently. Ensuring that each chunk contains all relevant information is crucial. While we managed to", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "609aed12-be55-4552-847a-18470af7aee1", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 10, "text": "2: Chunking using content-dependent strategies In the previous experiment, we provided an example of how using or not using overlapping can affect a model’s performance, particularly in documents such as call transcripts where subjects change frequently. Ensuring that each chunk contains all relevant information is crucial. While we managed to retrieve the correct information by introducing overlapping into the chunking strategy, this might still not be the optimal approach for transcripts with longer speaker speeches. Therefore, in this experiment, we will adopt a content-dependent strategy. Our proposed approach entails segmenting the text whenever a new speaker begins speaking, which requires preprocessing the text accordingly. Preprocess the text Firstly, let’s observe that in the HTML text, each time the speaker changes, their name is enclosed within <p><strong>Name</strong></p> tags, denoting the speaker’s name in bold letters. To facilitate our text chunking process, we’ll use the above observation and introduce a unique character sequence ### , which we’ll utilize as a marker for splitting the text. PYTHON 1 print('HTML text') 2 print(target_divs[:3]) 3 print('-------------------\\n') 4 5 text_custom = [] 6 for div in target_divs: 7 if div.get_text() is None: 8 continue 9 if str(div).startswith('<p><strong>'): 10 text_custom.append(f'### {div.get_text()}') 11 else: 12 text_custom.append(div.get_text()) 13 14 text_custom = '\\n'.join(text_custom) 15 print(text_custom[:500]) Output HTML text [<p><strong>Martin Viecha</strong></p>, <p>Good afternoon, everyone, and welcome to Tesla's fourth-quarter 2023 Q&amp;A webcast. My name is Martin Viecha, VP of investor relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q4 results were announced", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "c7b073dc-6105-49fe-bf03-7c04430444eb", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 11, "text": "text_custom = '\\n'.join(text_custom) 15 print(text_custom[:500]) Output HTML text [<p><strong>Martin Viecha</strong></p>, <p>Good afternoon, everyone, and welcome to Tesla's fourth-quarter 2023 Q&amp;A webcast. My name is Martin Viecha, VP of investor relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q4 results were announced at about 3 p.m. Central Time in the update that we published at the same link as this webcast.</p>, <p>During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and expectations as of today. Actual events or results could differ materially due to a number of risks and uncertainties, including those mentioned in our most recent filings with the SEC. [Operator instructions] But before we jump into Q&amp;A, Elon has some opening remarks.</p>] ------------------- ### Martin Viecha Good afternoon, everyone, and welcome to Tesla's fourth-quarter 2023 Q&amp;A webcast. My name is Martin Viecha, VP of investor relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q4 results were announced at about 3 p.m. Central Time in the update that we published at the same link as this webcast. During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions an In this approach, we prioritize splitting the text at the appropriate separator, ###. To ensure this behavior, we’ll use CharacterTextSplitter from LangChain , guaranteeing such behavior. From our analysis of the text and", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "adf74e6c-91f4-448f-9eda-ebcf3d0f0b8f", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 12, "text": "will discuss our business outlook and make forward-looking statements. These comments are based on our predictions an In this approach, we prioritize splitting the text at the appropriate separator, ###. To ensure this behavior, we’ll use CharacterTextSplitter from LangChain , guaranteeing such behavior. From our analysis of the text and the fact that we aim to preserve entire speaker speeches intact, we anticipate that most of them will exceed a length of 500. Hence, we’ll increase the chunk size to 1000. PYTHON 1 separator = \"###\" 2 chunk_size = 1000 3 chunk_overlap = 0 4 5 text_splitter = CharacterTextSplitter( 6 separator = separator, 7 chunk_size=chunk_size, 8 chunk_overlap=chunk_overlap, 9 length_function=len, 10 is_separator_regex=False, 11 ) 12 13 documents = text_splitter.create_documents([text_custom]) 14 documents = [Document(text=doc.page_content) for doc in documents] 15 16 retriever = build_retriever(documents) 17 18 source_nodes = retriever.retrieve(question) 19 print('Number of docuemnts: ',len(source_nodes)) 20 source_nodes= [{\"text\": ni.get_content()}for ni in source_nodes] 21 22 response = co.chat( 23 message=question, 24 documents=source_nodes, 25 model=co_model 26 ) 27 response = response 28 print(response.text) Output WARNING:langchain_text_splitters.base:Created a chunk of size 5946, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 4092, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1782, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1392, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 2046, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1152, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "87ab9ce2-bff9-44aa-8502-47401455d7b8", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 13, "text": "which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1392, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 2046, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1152, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1304, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1295, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 2090, which is longer than the specified 1000 WARNING:langchain_text_splitters.base:Created a chunk of size 1251, which is longer than the specified 1000 Number of docuemnts: 5 Elon Musk mentions Jonathan Nolan. Musk is friends with the creators of Westworld, Jonathan Nolan and Lisa Joy Nolan. Below we validate the answer using citations. PYTHON 1 print(insert_citations(response.text, response.citations)) Output Elon Musk [0] mentions Jonathan Nolan. [0] Musk is friends [0] with the creators of Westworld [0], Jonathan Nolan [0] and Lisa Joy Nolan. [0] PYTHON 1 source_nodes[0] Output 1 {'text': \"Elon Musk -- Chief Executive Officer and Product Architect\\nYeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\\nYeah.\\n### Unknown speaker\\nWe're not entering Westworld anytime soon.\\n###", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "ac25a599-6222-406a-8083-d656815c4d83", "source": "https://docs.cohere.com/page/chunking-strategies", "chunk_index": 14, "text": "and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\\nYeah.\\n### Unknown speaker\\nWe're not entering Westworld anytime soon.\\n### Elon Musk -- Chief Executive Officer and Product Architect\\nRight, right. Yeah. I take -- take safety very very seriously.\\n### Martin Viecha\\nThank you. The next question from Norman is: How many Cybertruck orders are in the queue? And when do you anticipate to be able to fulfill existing orders?\"} Discussion This example highlights some of the concerns that arise when implementing chunking strategies. This is a field of ongoing research, and many exciting surveys have been published in domain-specific applications. For example, this paper examines different chunking strategies in finance.", "metadata": {"url": "https://docs.cohere.com/page/chunking-strategies", "title": "Effective Chunking Strategies for RAG", "date": "3/1/2025", "type": "blog", "author": "Cohere"}}
{"id": "76c07d5a-a75b-401b-aea3-263ee758ce3d", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 0, "text": "The 7 Best Vector Databases in 2025 | DataCamp Skip to main content In the realm of Artificial Intelligence (AI), vast amounts of data require efficient handling and processing. As we delve into more advanced applications of AI, such as image recognition, voice search, or recommendation engines, the nature of data becomes more intricate. Here's where vector databases come into play. Unlike traditional databases that store scalar values, vector databases are uniquely designed to handle multi-dimensional data points, often termed vectors. These vectors, representing data in numerous dimensions, can be thought of as arrows pointing in a particular direction and magnitude in space. As the digital age propels us into an era dominated by AI and machine learning, vector databases have emerged as indispensable tools for storing, searching, and analyzing high-dimensional data vectors. This blog aims to provide a comprehensive understanding of vector databases, their ever-growing importance in AI, and a deep dive into the best vector databases available in 2025. Develop AI Applications Learn to build AI applications using the OpenAI API. Start Upskilling For Free What is a Vector Database? A vector database is a specific kind of database that saves information in the form of multi-dimensional vectors representing certain characteristics or qualities. The number of dimensions in each vector can vary widely, from just a few to several thousand, based on the data's intricacy and detail. This data, which could include text, images, audio, and video, is transformed into vectors using various processes like machine learning models,", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "644ef430-7ceb-47d7-98db-f3b76e1f978d", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 1, "text": "representing certain characteristics or qualities. The number of dimensions in each vector can vary widely, from just a few to several thousand, based on the data's intricacy and detail. This data, which could include text, images, audio, and video, is transformed into vectors using various processes like machine learning models, word embeddings, or feature extraction techniques. The primary benefit of a vector database is its ability to swiftly and precisely locate and retrieve data according to their vector proximity or resemblance. This allows for searches rooted in semantic or contextual relevance rather than relying solely on exact matches or set criteria as with conventional databases. For instance, with a vector database, you can: Search for songs that resonate with a particular tune based on melody and rhythm. Discover articles that align with another specific article in theme and perspective. Identify gadgets that mirror the characteristics and reviews of a certain device. How Does a Vector Database Work? Traditional databases store simple data like words and numbers in a table format. Vector databases, however, work with complex data called vectors and use unique methods for searching. While regular databases search for exact data matches, vector databases look for the closest match using specific measures of similarity. Vector databases use special search techniques known as Approximate Nearest Neighbor (ANN) search, which includes methods like hashing and graph-based searches. To really understand how vector databases work and how it is different from traditional relational databases like SQL , we have to first understand", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "86552e82-5fe1-4ac9-b035-188fb1b01a66", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 2, "text": "using specific measures of similarity. Vector databases use special search techniques known as Approximate Nearest Neighbor (ANN) search, which includes methods like hashing and graph-based searches. To really understand how vector databases work and how it is different from traditional relational databases like SQL , we have to first understand the concept of embeddings. Unstructured data, such as text, images, and audio, lacks a predefined format, posing challenges for traditional databases. To leverage this data in artificial intelligence and machine learning applications, it's transformed into numerical representations using embeddings. Embedding is like giving each item, whether it's a word, image, or something else, a unique code that captures its meaning or essence. This code helps computers understand and compare these items in a more efficient and meaningful way. Think of it as turning a complicated book into a short summary that still captures the main points. This embedding process is typically achieved using a special kind of neural network designed for the task. For example, word embeddings convert words into vectors in such a way that words with similar meanings are closer in the vector space. This transformation allows algorithms to understand relationships and similarities between items. Essentially, embeddings serve as a bridge, converting non-numeric data into a form that machine learning models can work with, enabling them to discern patterns and relationships in the data more effectively. How does a vector database work? ( Image source ) Vector Database Applications Vector databases, with their unique capabilities, are carving out", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "5669b076-6bc5-41f0-84c0-c40d6e09b207", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 3, "text": "as a bridge, converting non-numeric data into a form that machine learning models can work with, enabling them to discern patterns and relationships in the data more effectively. How does a vector database work? ( Image source ) Vector Database Applications Vector databases, with their unique capabilities, are carving out niches in a multitude of industries due to their efficiency in implementing \"similarity search.\" Here's a deeper dive into their diverse applications: 1. Enhancing retail experiences In the bustling retail sector, vector databases are reshaping how consumers shop. They enable the creation of advanced recommendation systems, curating personalized shopping experiences. For instance, an online shopper may receive product suggestions not just based on past purchases, but also by analyzing the similarities in product attributes, user behavior, and preferences. 2. Financial data analysis The financial sector is awash with intricate patterns and trends. Vector databases excel in analyzing this dense data, helping financial analysts detect patterns crucial for investment strategies. By recognizing subtle similarities or deviations, they can forecast market movements and devise more informed investment blueprints. 3. Healthcare In the realm of healthcare, personalization is paramount. By analyzing genomic sequences, vector databases enable more tailored medical treatments, ensuring that medical solutions align more closely with individual genetic makeup. 4. Enhancing natural language processing (NLP) applications The digital world is seeing a surge in chatbots and virtual assistants. These AI-driven entities rely heavily on understanding human language. By converting vast text data into vectors, these systems can more accurately comprehend and", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "b9f23fa6-fc53-4b72-9d1e-f387c986f93a", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 4, "text": "solutions align more closely with individual genetic makeup. 4. Enhancing natural language processing (NLP) applications The digital world is seeing a surge in chatbots and virtual assistants. These AI-driven entities rely heavily on understanding human language. By converting vast text data into vectors, these systems can more accurately comprehend and respond to human queries. For example, companies like Talkmap utilize real-time natural language understanding, enabling smoother customer-agent interactions. 5. Media analysis From medical scans to surveillance footage, the capacity to accurately compare and understand images is crucial. Vector databases streamline this by focusing on the essential features of images, filtering out noise and distortions. For instance, in traffic management, images from video feeds can be swiftly analyzed to optimize traffic flow and enhance public safety. 6. Anomaly detection Spotting outliers is as essential as recognizing similarities. Especially in sectors like finance and security, detecting anomalies can mean preventing fraud or preempting a potential security breach. Vector databases offer enhanced capabilities in this domain, making the detection process faster and more precise. Features of a Good Vector Database Vector databases have emerged as powerful tools to navigate the vast terrain of unstructured data, like images, videos, and texts, without relying heavily on human-generated labels or tags. Their capabilities, when integrated with advanced machine learning models, hold the potential to revolutionize numerous sectors, from e-commerce to pharmaceuticals. Here are some of the standout features that make vector databases a game-changer: 1. Scalability and adaptability A robust vector database ensures that as data", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "e9e50264-80b1-4582-bb43-069c8cbe3ef9", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 5, "text": "on human-generated labels or tags. Their capabilities, when integrated with advanced machine learning models, hold the potential to revolutionize numerous sectors, from e-commerce to pharmaceuticals. Here are some of the standout features that make vector databases a game-changer: 1. Scalability and adaptability A robust vector database ensures that as data grows - reaching millions or even billions of elements - it can effortlessly scale across multiple nodes. The best vector databases offer adaptability, allowing users to tune the system based on variations in insertion rate, query rate, and underlying hardware. 2. Multi-user support and data privacy Accommodating multiple users is a standard expectation for databases. However, merely creating a new vector database for each user isn't efficient. Vector databases prioritize data isolation, ensuring that any changes made to one data collection remain unseen to the rest unless shared intentionally by the owner. This not only supports multi-tenancy but also ensures the privacy and security of data. 3. Comprehensive API suite A genuine and effective database offers a full set of APIs and SDKs. This ensures that the system can interact with diverse applications and can be managed effectively. Leading vector databases, like Pinecone, provide SDKs in various programming languages such as Python, Node, Go, and Java, ensuring flexibility in development and management. 4. User-friendly interfaces Reducing the steep learning curve associated with new technologies, user-friendly interfaces in vector databases play a pivotal role. These interfaces offer a visual overview, easy navigation, and accessibility to features that might otherwise remain obscured.", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "03633e89-bb69-4043-8bde-133ffcb8bbe1", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 6, "text": "as Python, Node, Go, and Java, ensuring flexibility in development and management. 4. User-friendly interfaces Reducing the steep learning curve associated with new technologies, user-friendly interfaces in vector databases play a pivotal role. These interfaces offer a visual overview, easy navigation, and accessibility to features that might otherwise remain obscured. 5 Best Vector Databases in 2025 The list is in no particular order - each displays many of the qualities outlined in the section above. 1. Chroma Building LLM Apps using ChromaDB ( Image source ) Chroma is an open-source embedding database. Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs. As we explore in our Chroma DB tutorial , you can easily manage text documents, convert text to embeddings, and do similarity searches. ChromaDB features: LangChain (Python and JavScript) and LlamaIndex support available The same API that runs in Python notebook scales to the production cluster 2. Pinecone Pinecone vector database ( Image source ) Pinecone is a managed vector database platform that has been purpose-built to tackle the unique challenges associated with high-dimensional data. Equipped with cutting-edge indexing and search capabilities, Pinecone empowers data engineers and data scientists to construct and implement large-scale machine learning applications that effectively process and analyze high-dimensional data. Key features of Pinecone include: Fully managed service Highly scalable Real-time data ingestion Low-latency search Integration with LangChain Notably, Pinecone was the only vector database included in the inaugural Fortune 2023 50 AI Innovator list. To learn", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "cefb5e9d-8ed9-4927-bb90-2039356a7b17", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 7, "text": "and implement large-scale machine learning applications that effectively process and analyze high-dimensional data. Key features of Pinecone include: Fully managed service Highly scalable Real-time data ingestion Low-latency search Integration with LangChain Notably, Pinecone was the only vector database included in the inaugural Fortune 2023 50 AI Innovator list. To learn more about Pinecone, check out the Mastering Vector Databases with Pinecone tutorial . 3. Weaviate Weaviate vector database architecture ( Image source ) Weaviate is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML models and scale seamlessly into billions of data objects. Some of the key features of Weaviate are: Weaviate can quickly search the nearest neighbors from millions of objects in just a few milliseconds. With Weaviate, either vectorize data during import or upload your own, leveraging modules that integrate with platforms like OpenAI, Cohere, HuggingFace, and more. From prototypes to large-scale production, Weaviate emphasizes scalability, replication, and security. Apart from fast vector searches, Weaviate offers recommendations, summarizations, and neural search framework integrations. 4. Faiss Faiss is an open-source library for vector search created by Facebook ( Image source ) Faiss is an open-source library for the swift search of similarities and the clustering of dense vectors. It houses algorithms capable of searching within vector sets of varying sizes, even those that might exceed RAM capacity. Additionally, Faiss offers auxiliary code for assessment and adjusting parameters. While it's primarily coded in C++, it fully supports Python/NumPy integration. Some of", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "cbeceb83-3480-495d-a563-3e18055f93da", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 8, "text": "of similarities and the clustering of dense vectors. It houses algorithms capable of searching within vector sets of varying sizes, even those that might exceed RAM capacity. Additionally, Faiss offers auxiliary code for assessment and adjusting parameters. While it's primarily coded in C++, it fully supports Python/NumPy integration. Some of its key algorithms are also available for GPU execution. The primary development of Faiss is undertaken by the Fundamental AI Research group at Meta. 5. Qdrant Qdrant vector database ( Image source ) Qdrant is a vector database and a tool for conducting vector similarity searches. It operates as an API service, enabling searches for the closest high-dimensional vectors. Using Qdrant, you can transform embeddings or neural network encoders into comprehensive applications for tasks like matching, searching, making recommendations, and much more. Here are some key features of Qdrant: Offers OpenAPI v3 specs and ready-made clients for various languages. Uses a custom HNSW algorithm for rapid and accurate searches. Allows results filtering based on associated vector payloads. Supports string matching, numerical ranges, geo-locations, and more. Cloud-native design with horizontal scaling capabilities. Built-in Rust, optimizing resource use with dynamic query planning. 6. Milvus Milvus architecture overview. ( Image source ) Milvus is an open-source vector database that has quickly gained traction for its scalability, reliability, and performance. Designed for similarity search and AI-driven applications, it supports storing and querying massive embedding vectors generated by deep neural networks. Milvus offers the following features: It's able to handle billions of vectors with a", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "a38f7fe5-081e-4065-b83d-d3a4527ceb6a", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 9, "text": "is an open-source vector database that has quickly gained traction for its scalability, reliability, and performance. Designed for similarity search and AI-driven applications, it supports storing and querying massive embedding vectors generated by deep neural networks. Milvus offers the following features: It's able to handle billions of vectors with a distributed architecture. Optimized for high-speed similarity searches with low latency. Supports popular deep learning frameworks such as TensorFlow, PyTorch, and Hugging Face. Offers multiple deployment options, including Kubernetes, Docker, and cloud environments. Backed by a growing open-source community and extensive documentation. Milvus is ideal for applications in recommendation systems, video analysis, and personalized search experiences. 7. pgvector HNSW indexing and searching with pgvector on Amazon Aurora architecture diagram. ( Image source ) pgvector is an extension for PostgreSQL that introduces vector data types and similarity search capabilities to the widely used relational database. By integrating vector search into PostgreSQL, pgvector offers a seamless solution for teams already using traditional databases but looking to add vector search capabilities. Key features of pgvector include: Adds vector-based functionality to a familiar database system, eliminating the need for separate vector databases. Compatible with tools and ecosystems that already rely on PostgreSQL. Supports Approximate Nearest Neighbor (ANN) search for efficient querying of high-dimensional vectors. Simplifies adoption for users familiar with SQL, making it accessible for developers and data engineers alike. pgvector is particularly well-suited for smaller-scale vector search use cases or environments where a single database system is preferred for both relational and vector-based workloads.", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "a27b64c0-66a5-49db-8fac-ebbcd1f51d86", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 10, "text": "Neighbor (ANN) search for efficient querying of high-dimensional vectors. Simplifies adoption for users familiar with SQL, making it accessible for developers and data engineers alike. pgvector is particularly well-suited for smaller-scale vector search use cases or environments where a single database system is preferred for both relational and vector-based workloads. To get started, check out our detailed tutorial on pgvector . Top Vector Databases Comparison Below is a comparison table highlighting the features of the top vector databases discussed before: Feature Chroma Pinecone Weaviate Faiss Qdrant Milvus PGVector Open-source ✅ ❎ ✅ ✅ ✅ ✅ ✅ Primary Use Case LLM Apps Development Managed Vector Database for ML Scalable Vector Storage and Search High-Speed Similarity Search and Clustering Vector Similarity Search High-Performance AI Search Adding Vector Search to PostgreSQL Integration LangChain, LlamaIndex LangChain OpenAI, Cohere, HuggingFace Python/NumPy, GPU Execution OpenAPI v3, Various Language Clients TensorFlow, PyTorch, HuggingFace Built into PostgreSQL ecosystem Scalability Scales from Python notebooks to clusters Highly scalable Seamless scaling to billions of objects Capable of handling sets larger than RAM Cloud-native with horizontal scaling Scales to billions of vectors Depends on PostgreSQL setup Search Speed Fast similarity searches Low-latency search Milliseconds for millions of objects Fast, supports GPU Custom HNSW algorithm for rapid search Optimized for low-latency search Approximate Nearest Neighbor (ANN) Data Privacy Supports multi-user with data isolation Fully managed service Emphasizes security and replication Primarily for research and development Advanced filtering on vector payloads Secure multi-tenant architecture Inherits PostgreSQL’s security Programming Language Python, JavaScript Python Python,", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "6e626c8c-8b80-42ec-a76e-c40310ef9e0e", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 11, "text": "Custom HNSW algorithm for rapid search Optimized for low-latency search Approximate Nearest Neighbor (ANN) Data Privacy Supports multi-user with data isolation Fully managed service Emphasizes security and replication Primarily for research and development Advanced filtering on vector payloads Secure multi-tenant architecture Inherits PostgreSQL’s security Programming Language Python, JavaScript Python Python, Java, Go, others C++, Python Rust C++, Python, Go PostgreSQL extension (SQL-based) The Rise of AI and the Impact of Vector Databases Vector databases specialize in storing high-dimensional vectors, enabling fast and accurate similarity searches. As AI models, especially those in the domain of natural language processing and computer vision, generate and work with these vectors, the need for efficient storage and retrieval systems has become paramount. This is where vector databases come into play, providing a highly optimized environment for these AI-driven applications. A prime example of this relationship between AI and vector databases is observed in the emergence of Large Language Models (LLMs) like GPT-3 . These models are designed to understand and generate human-like text by processing vast amounts of data, transforming them into high-dimensional vectors. Applications built on GPT and similar models rely heavily on vector databases to manage and query these vectors efficiently. The reason for this reliance lies in the sheer volume and complexity of data these models handle. Given the substantial parameter increase, models like GPT-4 generate a vast amount of vectorized data, which can be challenging for conventional databases to process efficiently. This underscores the importance of specialized vector databases capable of", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "9c7f27b7-5584-4047-b795-7140097bc137", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 12, "text": "for this reliance lies in the sheer volume and complexity of data these models handle. Given the substantial parameter increase, models like GPT-4 generate a vast amount of vectorized data, which can be challenging for conventional databases to process efficiently. This underscores the importance of specialized vector databases capable of handling such high-dimensional data. Conclusion The ever-evolving landscape of artificial intelligence and machine learning underscores the indispensability of vector databases in today's data-centric world. These databases, with their unique ability to store, search, and analyze multi-dimensional data vectors, are proving instrumental in powering AI-driven applications, from recommendation systems to genomic analysis. We’ve recently seen an impressive array of vector databases, such as Chroma, Pinecone, Weaviate, Faiss, and Qdrant, each offering distinct capabilities and innovations. As AI continues its ascent, the role of vector databases in shaping the future of data retrieval, processing, and analysis will undoubtedly grow, promising more sophisticated, efficient, and personalized solutions across various sectors. Learn to master vector databases with our Pinecone tutorial , or sign up for our Deep Learning in Python skill track to improve your AI skills and keep up-to-date with the latest developments. Earn a Top AI Certification Demonstrate you can effectively and responsibly use AI. Get Certified, Get Hired FAQs How are vector databases different from traditional relational databases like MySQL or PostgreSQL? Vector databases are designed to handle high-dimensional data, such as embeddings produced by AI models. Unlike relational databases, which rely on structured tables and exact matches, vector databases focus", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "ec459158-4e47-4494-b81d-3c48563c19b0", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 13, "text": "responsibly use AI. Get Certified, Get Hired FAQs How are vector databases different from traditional relational databases like MySQL or PostgreSQL? Vector databases are designed to handle high-dimensional data, such as embeddings produced by AI models. Unlike relational databases, which rely on structured tables and exact matches, vector databases focus on similarity searches, enabling them to retrieve semantically or contextually related data points. Can vector databases replace traditional databases? No, vector databases complement traditional databases rather than replace them. While traditional databases excel at managing structured data and supporting transactional operations, vector databases are specialized tools for handling and searching unstructured, high-dimensional data like text embeddings, images, or audio. What are Approximate Nearest Neighbor (ANN) algorithms, and why are they essential in vector databases? ANN algorithms are specialized methods for quickly finding vectors that are closest to a given query vector in high-dimensional space. They balance speed and accuracy, making them ideal for large datasets where exact nearest neighbor searches would be computationally expensive. Are vector databases suitable for small-scale projects or only for large enterprises? Vector databases are versatile and can be used in both small and large projects. For small-scale projects, open-source solutions like Chroma, Faiss, and Weaviate offer robust capabilities. For enterprise-scale projects, managed platforms like Pinecone provide scalability and performance optimization. How does vector database performance scale with increasing data size? Performance scalability depends on the underlying architecture and indexing techniques, such as HNSW or IVF. Most modern vector databases, including Milvus and Qdrant, are optimized", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "089bdbcb-fcd0-47fc-9926-ddb46a627952", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 14, "text": "offer robust capabilities. For enterprise-scale projects, managed platforms like Pinecone provide scalability and performance optimization. How does vector database performance scale with increasing data size? Performance scalability depends on the underlying architecture and indexing techniques, such as HNSW or IVF. Most modern vector databases, including Milvus and Qdrant, are optimized for distributed architectures, enabling them to scale seamlessly to billions of vectors. Can I use a vector database without deep knowledge of machine learning? Yes, many vector databases, like Pinecone and Chroma, provide user-friendly APIs, SDKs, and integrations with popular frameworks (e.g., LangChain, Hugging Face), allowing non-experts to leverage their capabilities with minimal learning curves. What are the storage requirements for vector databases? Vector databases store embeddings, which can be memory-intensive, especially with high-dimensional data. Storage requirements depend on factors such as vector size, dataset volume, and indexing structure. Solutions like Faiss and Milvus offer optimizations to handle large datasets efficiently, even exceeding available RAM. Are vector databases compatible with cloud-native applications? Yes, many modern vector databases, like Milvus and Qdrant, are designed with cloud-native architectures, offering seamless integration with Kubernetes, Docker, and cloud platforms like AWS and GCP. Author Moez Ali LinkedIn Twitter Data Scientist, Founder & Creator of PyCaret Topics Artificial Intelligence Machine Learning Learn more about AI with these courses! Course Understanding Artificial Intelligence 2 hr 296.7K Learn the basic concepts of Artificial Intelligence, such as machine learning, deep learning, NLP, generative AI, and more. See Details Right Arrow Start Course Course Introduction to Embeddings with the", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "8cbef79a-eefd-4a50-a45f-90770f9e1c54", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 15, "text": "PyCaret Topics Artificial Intelligence Machine Learning Learn more about AI with these courses! Course Understanding Artificial Intelligence 2 hr 296.7K Learn the basic concepts of Artificial Intelligence, such as machine learning, deep learning, NLP, generative AI, and more. See Details Right Arrow Start Course Course Introduction to Embeddings with the OpenAI API 3 hr 12K Unlock more advanced AI applications, like semantic search and recommendation engines, using OpenAI's embedding model! See Details Right Arrow Start Course Course Vector Databases for Embeddings with Pinecone 3 hr 4.2K Discover how the Pinecone vector database is revolutionizing AI application development! See Details Right Arrow Start Course See More Right Arrow Related blog Types of Databases: Relational, NoSQL, Cloud, Vector The main types of databases include relational databases for structured data, NoSQL databases for flexibility, cloud databases for remote access, and vector databases for machine learning applications. Moez Ali 15 min podcast Not Only Vector Databases: Putting Databases at the Heart of AI, with Andi Gutmans, VP and GM of Databases at Google Richie and Andi explore databases and their relationship with AI, key features needed in databases for AI, GCP, AlloyDB, federated queries in Google Cloud, vector and graph databases, practical use cases of AI in databases and much more. podcast The Power of Vector Databases and Semantic Search with Elan Dekel, VP of Product at Pinecone RIchie and Elan explore LLMs, vector databases and the best use-cases for them, semantic search, the tech stack for AI applications, emerging roles within the AI", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "cd7ae44d-6d07-4d08-b702-95030fb1731f", "source": "https://www.datacamp.com/blog/the-top-5-vector-databases", "chunk_index": 16, "text": "of AI in databases and much more. podcast The Power of Vector Databases and Semantic Search with Elan Dekel, VP of Product at Pinecone RIchie and Elan explore LLMs, vector databases and the best use-cases for them, semantic search, the tech stack for AI applications, emerging roles within the AI space, the future of vector databases and AI, and much more. Tutorial An Introduction to Vector Databases For Machine Learning: A Hands-On Guide With Examples Explore vector databases in ML with our guide. Learn to implement vector embeddings and practical applications. Gary Alway Tutorial Mastering Vector Databases with Pinecone Tutorial: A Comprehensive Guide Dive into the world of vector databases with our in-depth tutorial on Pinecone. Discover how to efficiently handle high-dimensional data, understand unstructured data, and harness the power of vector embeddings for AI-driven applications. Moez Ali code-along Vector Databases for Data Science with Weaviate in Python In this code-along, JP shows you how to use Weaviate, a leading open source vector database, to build apps that can understand and manipulate them based on meaning. JP Hwang See More See More", "metadata": {"url": "https://www.datacamp.com/blog/the-top-5-vector-databases", "title": "The Top 7 Vector Databases in 2025", "date": "1/18/2025", "type": "blog", "author": "DataCamp"}}
{"id": "b64cf81b-d55c-4015-92f8-99fd0d4f2cec", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 0, "text": "Contextual Retrieval in AI Systems \\ Anthropic Engineering at Anthropic Introducing Contextual Retrieval Published Sep 19, 2024 For an AI model to be useful in specific contexts, it often needs access to background knowledge. For an AI model to be useful in specific contexts, it often needs access to background knowledge. For example, customer support chatbots need knowledge about the specific business they're being used for, and legal analyst bots need to know about a vast array of past cases. Developers typically enhance an AI model's knowledge using Retrieval-Augmented Generation (RAG). RAG is a method that retrieves relevant information from a knowledge base and appends it to the user's prompt, significantly enhancing the model's response. The problem is that traditional RAG solutions remove context when encoding information, which often results in the system failing to retrieve the relevant information from the knowledge base. In this post, we outline a method that dramatically improves the retrieval step in RAG. The method is called “Contextual Retrieval” and uses two sub-techniques: Contextual Embeddings and Contextual BM25. This method can reduce the number of failed retrievals by 49% and, when combined with reranking, by 67%. These represent significant improvements in retrieval accuracy, which directly translates to better performance in downstream tasks. You can easily deploy your own Contextual Retrieval solution with Claude with our cookbook . A note on simply using a longer prompt Sometimes the simplest solution is the best. If your knowledge base is smaller than 200,000 tokens (about 500 pages of", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "2159fa59-296e-4341-b096-a7553b87c7bf", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 1, "text": "translates to better performance in downstream tasks. You can easily deploy your own Contextual Retrieval solution with Claude with our cookbook . A note on simply using a longer prompt Sometimes the simplest solution is the best. If your knowledge base is smaller than 200,000 tokens (about 500 pages of material), you can just include the entire knowledge base in the prompt that you give the model, with no need for RAG or similar methods. A few weeks ago, we released prompt caching for Claude, which makes this approach significantly faster and more cost-effective. Developers can now cache frequently used prompts between API calls, reducing latency by > 2x and costs by up to 90% (you can see how it works by reading our prompt caching cookbook ). However, as your knowledge base grows, you'll need a more scalable solution. That’s where Contextual Retrieval comes in. A primer on RAG: scaling to larger knowledge bases For larger knowledge bases that don't fit within the context window, RAG is the typical solution. RAG works by preprocessing a knowledge base using the following steps: Break down the knowledge base (the “corpus” of documents) into smaller chunks of text, usually no more than a few hundred tokens; Use an embedding model to convert these chunks into vector embeddings that encode meaning; Store these embeddings in a vector database that allows for searching by semantic similarity. At runtime, when a user inputs a query to the model, the vector database is used to find", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "beae3378-d1c6-41e3-8419-c855dcd4cf79", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 2, "text": "a few hundred tokens; Use an embedding model to convert these chunks into vector embeddings that encode meaning; Store these embeddings in a vector database that allows for searching by semantic similarity. At runtime, when a user inputs a query to the model, the vector database is used to find the most relevant chunks based on semantic similarity to the query. Then, the most relevant chunks are added to the prompt sent to the generative model. While embedding models excel at capturing semantic relationships, they can miss crucial exact matches. Fortunately, there’s an older technique that can assist in these situations. BM25 (Best Matching 25) is a ranking function that uses lexical matching to find precise word or phrase matches. It's particularly effective for queries that include unique identifiers or technical terms. BM25 works by building upon the TF-IDF (Term Frequency-Inverse Document Frequency) concept. TF-IDF measures how important a word is to a document in a collection. BM25 refines this by considering document length and applying a saturation function to term frequency, which helps prevent common words from dominating the results. Here’s how BM25 can succeed where semantic embeddings fail: Suppose a user queries \"Error code TS-999\" in a technical support database. An embedding model might find content about error codes in general, but could miss the exact \"TS-999\" match. BM25 looks for this specific text string to identify the relevant documentation. RAG solutions can more accurately retrieve the most applicable chunks by combining the embeddings and BM25 techniques using", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "c977f745-6817-4468-8fc9-2500b899b966", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 3, "text": "support database. An embedding model might find content about error codes in general, but could miss the exact \"TS-999\" match. BM25 looks for this specific text string to identify the relevant documentation. RAG solutions can more accurately retrieve the most applicable chunks by combining the embeddings and BM25 techniques using the following steps: Break down the knowledge base (the \"corpus\" of documents) into smaller chunks of text, usually no more than a few hundred tokens; Create TF-IDF encodings and semantic embeddings for these chunks; Use BM25 to find top chunks based on exact matches; Use embeddings to find top chunks based on semantic similarity; Combine and deduplicate results from (3) and (4) using rank fusion techniques; Add the top-K chunks to the prompt to generate the response. By leveraging both BM25 and embedding models, traditional RAG systems can provide more comprehensive and accurate results, balancing precise term matching with broader semantic understanding. A Standard Retrieval-Augmented Generation (RAG) system that uses both embeddings and Best Match 25 (BM25) to retrieve information. TF-IDF (term frequency-inverse document frequency) measures word importance and forms the basis for BM25. This approach allows you to cost-effectively scale to enormous knowledge bases, far beyond what could fit in a single prompt. But these traditional RAG systems have a significant limitation: they often destroy context. The context conundrum in traditional RAG In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "52f625db-c86f-415c-ad20-692ce31ae47b", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 4, "text": "in a single prompt. But these traditional RAG systems have a significant limitation: they often destroy context. The context conundrum in traditional RAG In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when individual chunks lack sufficient context. For example, imagine you had a collection of financial information (say, U.S. SEC filings) embedded in your knowledge base, and you received the following question: \"What was the revenue growth for ACME Corp in Q2 2023?\" A relevant chunk might contain the text: \"The company's revenue grew by 3% over the previous quarter.\" However, this chunk on its own doesn't specify which company it's referring to or the relevant time period, making it difficult to retrieve the right information or use the information effectively. Introducing Contextual Retrieval Contextual Retrieval solves this problem by prepending chunk-specific explanatory context to each chunk before embedding (“Contextual Embeddings”) and creating the BM25 index (“Contextual BM25”). Let’s return to our SEC filings collection example. Here's an example of how a chunk might be transformed: [CODEBLOCK] original_chunk = \"The company's revenue grew by 3% over the previous quarter.\" contextualized_chunk = \"This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.\" [/CODEBLOCK] Copy It is worth noting that other approaches to using context to improve retrieval have been proposed in the", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "cb8791e6-af5a-4f99-8ba6-ad2af68001b7", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 5, "text": "chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.\" [/CODEBLOCK] Copy It is worth noting that other approaches to using context to improve retrieval have been proposed in the past. Other proposals include: adding generic document summaries to chunks (we experimented and saw very limited gains), hypothetical document embedding , and summary-based indexing (we evaluated and saw low performance). These methods differ from what is proposed in this post. Implementing Contextual Retrieval Of course, it would be far too much work to manually annotate the thousands or even millions of chunks in a knowledge base. To implement Contextual Retrieval, we turn to Claude. We’ve written a prompt that instructs the model to provide concise, chunk-specific context that explains the chunk using the context of the overall document. We used the following Claude 3 Haiku prompt to generate context for each chunk: [CODEBLOCK] <document> {{WHOLE_DOCUMENT}} </document> Here is the chunk we want to situate within the whole document <chunk> {{CHUNK_CONTENT}} </chunk> Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. [/CODEBLOCK] Copy The resulting contextual text, usually 50-100 tokens, is prepended to the chunk before embedding it and before creating the BM25 index. Here’s what the preprocessing flow looks like in practice: Contextual Retrieval is a", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "01e3c9e0-965e-475d-90c9-0d6319e4f357", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 6, "text": "search retrieval of the chunk. Answer only with the succinct context and nothing else. [/CODEBLOCK] Copy The resulting contextual text, usually 50-100 tokens, is prepended to the chunk before embedding it and before creating the BM25 index. Here’s what the preprocessing flow looks like in practice: Contextual Retrieval is a preprocessing technique that improves retrieval accuracy. If you’re interested in using Contextual Retrieval, you can get started with our cookbook . Using Prompt Caching to reduce the costs of Contextual Retrieval Contextual Retrieval is uniquely possible at low cost with Claude, thanks to the special prompt caching feature we mentioned above. With prompt caching, you don’t need to pass in the reference document for every chunk. You simply load the document into the cache once and then reference the previously cached content. Assuming 800 token chunks, 8k token documents, 50 token context instructions, and 100 tokens of context per chunk, the one-time cost to generate contextualized chunks is $1.02 per million document tokens . Methodology We experimented across various knowledge domains (codebases, fiction, ArXiv papers, Science Papers), embedding models, retrieval strategies, and evaluation metrics. We’ve included a few examples of the questions and answers we used for each domain in Appendix II . The graphs below show the average performance across all knowledge domains with the top-performing embedding configuration (Gemini Text 004) and retrieving the top-20-chunks. We use 1 minus recall@20 as our evaluation metric, which measures the percentage of relevant documents that fail to be retrieved within the top", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "e8458254-9a5b-4536-8f64-4cdebba43890", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 7, "text": "Appendix II . The graphs below show the average performance across all knowledge domains with the top-performing embedding configuration (Gemini Text 004) and retrieving the top-20-chunks. We use 1 minus recall@20 as our evaluation metric, which measures the percentage of relevant documents that fail to be retrieved within the top 20 chunks. You can see the full results in the appendix - contextualizing improves performance in every embedding-source combination we evaluated. Performance improvements Our experiments showed that: Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35% (5.7% → 3.7%). Combining Contextual Embeddings and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 49% (5.7% → 2.9%). Combining Contextual Embedding and Contextual BM25 reduce the top-20-chunk retrieval failure rate by 49%. Implementation considerations When implementing Contextual Retrieval, there are a few considerations to keep in mind: Chunk boundaries: Consider how you split your documents into chunks. The choice of chunk size, chunk boundary, and chunk overlap can affect retrieval performance 1 . Embedding model: Whereas Contextual Retrieval improves performance across all embedding models we tested, some models may benefit more than others. We found Gemini and Voyage embeddings to be particularly effective. Custom contextualizer prompts: While the generic prompt we provided works well, you may be able to achieve even better results with prompts tailored to your specific domain or use case (for example, including a glossary of key terms that might only be defined in other documents in the knowledge base). Number of chunks: Adding more chunks into the", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "c44db47e-d696-49f8-a4ce-a7d25a504563", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 8, "text": "provided works well, you may be able to achieve even better results with prompts tailored to your specific domain or use case (for example, including a glossary of key terms that might only be defined in other documents in the knowledge base). Number of chunks: Adding more chunks into the context window increases the chances that you include the relevant information. However, more information can be distracting for models so there's a limit to this. We tried delivering 5, 10, and 20 chunks, and found using 20 to be the most performant of these options (see appendix for comparisons) but it’s worth experimenting on your use case. Always run evals: Response generation may be improved by passing it the contextualized chunk and distinguishing between what is context and what is the chunk. Further boosting performance with Reranking In a final step, we can combine Contextual Retrieval with another technique to give even more performance improvements. In traditional RAG, the AI system searches its knowledge base to find the potentially relevant information chunks. With large knowledge bases, this initial retrieval often returns a lot of chunks—sometimes hundreds—of varying relevance and importance. Reranking is a commonly used filtering technique to ensure that only the most relevant chunks are passed to the model. Reranking provides better responses and reduces cost and latency because the model is processing less information. The key steps are: Perform initial retrieval to get the top potentially relevant chunks (we used the top 150); Pass the top-N chunks, along", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "dfc48628-ce17-444e-9dca-f5f08e2eaee7", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 9, "text": "only the most relevant chunks are passed to the model. Reranking provides better responses and reduces cost and latency because the model is processing less information. The key steps are: Perform initial retrieval to get the top potentially relevant chunks (we used the top 150); Pass the top-N chunks, along with the user's query, through the reranking model; Using a reranking model, give each chunk a score based on its relevance and importance to the prompt, then select the top-K chunks (we used the top 20); Pass the top-K chunks into the model as context to generate the final result. Combine Contextual Retrieva and Reranking to maximize retrieval accuracy. Performance improvements There are several reranking models on the market. We ran our tests with the Cohere reranker . Voyage also offers a reranker , though we did not have time to test it. Our experiments showed that, across various domains, adding a reranking step further optimizes retrieval. Specifically, we found that Reranked Contextual Embedding and Contextual BM25 reduced the top-20-chunk retrieval failure rate by 67% (5.7% → 1.9%). Reranked Contextual Embedding and Contextual BM25 reduces the top-20-chunk retrieval failure rate by 67%. Cost and latency considerations One important consideration with reranking is the impact on latency and cost, especially when reranking a large number of chunks. Because reranking adds an extra step at runtime, it inevitably adds a small amount of latency, even though the reranker scores all the chunks in parallel. There is an inherent trade-off between reranking more", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "a403ef9d-c30f-4c02-8610-9be044a4d84f", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 10, "text": "reranking is the impact on latency and cost, especially when reranking a large number of chunks. Because reranking adds an extra step at runtime, it inevitably adds a small amount of latency, even though the reranker scores all the chunks in parallel. There is an inherent trade-off between reranking more chunks for better performance vs. reranking fewer for lower latency and cost. We recommend experimenting with different settings on your specific use case to find the right balance. Conclusion We ran a large number of tests, comparing different combinations of all the techniques described above (embedding model, use of BM25, use of contextual retrieval, use of a reranker, and total # of top-K results retrieved), all across a variety of different dataset types. Here’s a summary of what we found: Embeddings+BM25 is better than embeddings on their own; Voyage and Gemini have the best embeddings of the ones we tested; Passing the top-20 chunks to the model is more effective than just the top-10 or top-5; Adding context to chunks improves retrieval accuracy a lot; Reranking is better than no reranking; All these benefits stack : to maximize performance improvements, we can combine contextual embeddings (from Voyage or Gemini) with contextual BM25, plus a reranking step, and adding the 20 chunks to the prompt. We encourage all developers working with knowledge bases to use our cookbook to experiment with these approaches to unlock new levels of performance. Appendix I Below is a breakdown of results across datasets, embedding providers, use", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
{"id": "bca48861-dbc8-4070-837f-7064bec0d818", "source": "https://www.anthropic.com/engineering/contextual-retrieval", "chunk_index": 11, "text": "contextual BM25, plus a reranking step, and adding the 20 chunks to the prompt. We encourage all developers working with knowledge bases to use our cookbook to experiment with these approaches to unlock new levels of performance. Appendix I Below is a breakdown of results across datasets, embedding providers, use of BM25 in addition to embeddings, use of contextual retrieval, and use of reranking for Retrievals @ 20. See Appendix II for the breakdowns for Retrievals @ 10 and @ 5 as well as example questions and answers for each dataset. 1 minus recall @ 20 results across data sets and embedding providers. Acknowledgements Research and writing by Daniel Ford. Thanks to Orowa Sikder, Gautam Mittal, and Kenneth Lien for critical feedback, Samuel Flamini for implementing the cookbooks, Lauren Polansky for project coordination and Alex Albert, Susan Payne, Stuart Ritchie, and Brad Abrams for shaping this blog post. Get the developer newsletter Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox. Please provide your email address if you’d like to receive our monthly developer newsletter. You can unsubscribe at any time.", "metadata": {"url": "https://www.anthropic.com/engineering/contextual-retrieval", "title": "Introducing Contextual Retrieval", "date": "9/19/2024", "type": "blog", "author": "Anthropic"}}
