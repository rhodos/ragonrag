{"id": "53869a1e-ff29-4d4b-b2eb-3c64c958f507", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 0, "text": "Dense Passage Retrieval for Open-Domain Question Answering VladimirKarpukhin∗,BarlasOg˘uz∗,SewonMin†,PatrickLewis, LedellWu,SergeyEdunov,DanqiChen‡,Wen-tauYih FacebookAI †UniversityofWashington ‡PrincetonUniversity {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com sewon@cs.washington.edu danqic@cs.princeton.edu Abstract Retrievalinopen-domainQAisusuallyimple- mented using TF-IDF or BM25 (Robertson and Open-domainquestionansweringreliesonef- Zaragoza, 2009), which matches keywords effi- ficient passage retrieval to select candidate ciently with an inverted index and can be seen contexts,wheretraditionalsparsevectorspace models, such as TF-IDF or BM25, are the de as representing the question and context in high- facto method. In this work, we show that dimensional,sparsevectors(withweighting). Con- retrieval can be practically implemented us- versely,thedense,latentsemanticencodingiscom- ing dense representations alone, where em- plementarytosparserepresentationsbydesign. For beddings are learned from a small number example,synonymsorparaphrasesthatconsistof of questions and passages by a simple dual- completelydifferenttokensmaystillbemappedto encoder framework. When evaluated on a vectorsclosetoeachother. Considerthequestion wide range of open-domain QA datasets, our “Whoisthebadguyinlordoftherings?”,whichcan dense retriever outperforms a strong Lucene- BM25systemgreatlyby9%-19%absolutein beansweredfromthecontext“SalaBakerisbest termsoftop-20passageretrievalaccuracy,and knownforportrayingthevillainSauronintheLord helpsourend-to-endQAsystemestablishnew oftheRingstrilogy.” Aterm-basedsystemwould state-of-the-art on multiple open-domain QA have difficulty retrieving such a context, while benchmarks.1 a dense retrieval system would be able to better 1 Introduction match“badguy”with“villain”andfetchthecor- rectcontext. Denseencodingsarealsolearnable Open-domainquestionanswering(QA)(Voorhees, byadjustingtheembeddingfunctions,whichpro- 1999) is a task that answers factoid questions us- videsadditionalflexibilitytohaveatask-specific ing a large collection of documents. While early representation. Withspecialin-memorydatastruc- QAsystemsareoftencomplicatedandconsistof turesandindexingschemes,retrievalcanbedone multiplecomponents(Ferrucci(2012);Moldovan efficiently using maximum inner product search et al. (2003), inter alia), the advances of reading (MIPS)algorithms(e.g.,ShrivastavaandLi(2014); comprehensionmodelssuggestamuchsimplified Guoetal.(2016)). two-stageframework: (1)acontextretrieverfirst However, it is generally believed that learn- selects a small subset of passages where some ing a good dense vector representation needs a of them contain the answer to the question, and largenumberoflabeledpairsofquestionandcon- then (2) a machine reader", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "b9b21c29-7baa-4723-8c2d-50fafd0b0343", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 1, "text": "(2003), inter alia), the advances of reading (MIPS)algorithms(e.g.,ShrivastavaandLi(2014); comprehensionmodelssuggestamuchsimplified Guoetal.(2016)). two-stageframework: (1)acontextretrieverfirst However, it is generally believed that learn- selects a small subset of passages where some ing a good dense vector representation needs a of them contain the answer to the question, and largenumberoflabeledpairsofquestionandcon- then (2) a machine reader can thoroughly exam- texts. Dense retrieval methods have thus never inetheretrievedcontextsandidentifythecorrect beshowntooutperformTF-IDF/BM25foropen- answer (Chen et al., 2017). Although reducing domainQAbeforeORQA(Leeetal.,2019),which open-domainQAtomachinereadingisaveryrea- proposes a sophisticated inverse cloze task (ICT) sonablestrategy,ahugeperformancedegradation objective, predicting the blocks that contain the isoftenobservedinpractice2,indicatingtheneeds masked sentence, for additional pretraining. The ofimprovingretrieval. questionencoderandthereadermodelarethenfine- ∗Equalcontribution tunedusingpairsofquestionsandanswersjointly. 1The code and trained models have been released at Although ORQA successfully demonstrates that https://github.com/facebookresearch/DPR. denseretrievalcanoutperformBM25,settingnew 2Forinstance,theexactmatchscoreonSQuADv1.1drops fromabove80%tolessthan40%(Yangetal.,2019a). state-of-the-art results on multiple open-domain QAdatasets,italsosuffersfromtwoweaknesses. the extractive QA setting, in which the answer is First,ICTpretrainingiscomputationallyintensive restrictedtoaspanappearinginoneormorepas- anditisnotcompletelyclearthatregularsentences sages in the corpus. Assume that our collection are good surrogates of questions in the objective contains D documents, d ,d ,··· ,d . We first 1 2 D function. Second, becausethecontextencoderis split each of the documents into text passages of notfine-tunedusingpairsofquestionsandanswers, equallengthsasthebasicretrievalunits3andgetM thecorrespondingrepresentationscouldbesubop- totalpassagesinourcorpusC = {p ,p ,...,p }, 1 2 M timal. whereeachpassagep canbeviewedasasequence i (i) (i) (i) In this paper, we address the question: can we oftokensw ,w ,··· ,w . Givenaquestionq, 1 2 |pi| train a better dense embedding model using only (i) (i) (i) thetaskistofindaspanw ,w ,··· ,w from pairsofquestionsandpassages(oranswers),with- s s+1 e oneofthepassagesp thatcananswerthequestion. i outadditionalpretraining? Byleveragingthenow Noticethattocoverawidevarietyofdomains,the standard BERT pretrained model (Devlin et al., corpussizecaneasilyrangefrommillionsofdocu- 2019) and a dual-encoder architecture", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "9e5210a0-0749-4533-a851-54b017fb764f", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 2, "text": "can we oftokensw ,w ,··· ,w . Givenaquestionq, 1 2 |pi| train a better dense embedding model using only (i) (i) (i) thetaskistofindaspanw ,w ,··· ,w from pairsofquestionsandpassages(oranswers),with- s s+1 e oneofthepassagesp thatcananswerthequestion. i outadditionalpretraining? Byleveragingthenow Noticethattocoverawidevarietyofdomains,the standard BERT pretrained model (Devlin et al., corpussizecaneasilyrangefrommillionsofdocu- 2019) and a dual-encoder architecture (Bromley ments(e.g.,Wikipedia)tobillions(e.g.,theWeb). et al., 1994), we focus on developing the right Asaresult,anyopen-domainQAsystemneedsto training scheme using a relatively small number includeanefficientretrievercomponentthatcanse- of question and passage pairs. Through a series lectasmallsetofrelevanttexts,beforeapplyingthe of careful ablation studies, our final solution is reader to extract the answer (Chen et al., 2017).4 surprisingly simple: the embedding is optimized Formally speaking, a retriever R : (q,C) → C F formaximizinginnerproductsofthequestionand isafunctionthattakesasinputaquestionq anda relevantpassagevectors,withanobjectivecompar- corpus C and returns a much smaller filter set of ingallpairsofquestionsandpassagesinabatch. textsC ⊂ C,where|C | = k (cid:28) |C|. Forafixed F F OurDensePassageRetriever(DPR)isexception- k,aretrievercanbeevaluatedinisolationontop-k ally strong. It not only outperforms BM25 by a retrieval accuracy, which is the fraction of ques- largemargin(65.2%vs. 42.9%inTop-5accuracy), tionsforwhichC containsaspanthatanswersthe F but also results in a substantial improvement on question. theend-to-endQAaccuracycomparedtoORQA (41.5%vs. 33.3%)intheopenNaturalQuestions 3 DensePassageRetriever(DPR) setting(Leeetal.,2019;Kwiatkowskietal.,2019). We focus our research in this work on improv- Ourcontributionsaretwofold. First,wedemon- ing the retrieval component in open-domain QA. strate that with the proper training setup, sim- GivenacollectionofM textpassages,thegoalof plyfine-tuningthequestionandpassageencoders our dense passage retriever (DPR) is to index all onexistingquestion-passagepairsissufficientto thepassagesinalow-dimensionalandcontinuous greatly outperform BM25. Our empirical results space, such that it can retrieve efficiently the top alsosuggestthatadditionalpretrainingmaynotbe k passages relevant to the input question for the needed. Second, we verify that, in the context", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "3b2776ab-0a7f-4e16-9550-8ad3da2aa9cd", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 3, "text": "training setup, sim- GivenacollectionofM textpassages,thegoalof plyfine-tuningthequestionandpassageencoders our dense passage retriever (DPR) is to index all onexistingquestion-passagepairsissufficientto thepassagesinalow-dimensionalandcontinuous greatly outperform BM25. Our empirical results space, such that it can retrieve efficiently the top alsosuggestthatadditionalpretrainingmaynotbe k passages relevant to the input question for the needed. Second, we verify that, in the context of readeratrun-time. NotethatM canbeverylarge open-domainquestionanswering,ahigherretrieval (e.g., 21 million passages in our experiments, de- precisionindeedtranslatestoahigherend-to-end scribedinSection4.1)andk isusuallysmall,such QAaccuracy. Byapplyingamodernreadermodel as20–100. tothetopretrievedpassages,weachievecompara- bleorbetterresultsonmultipleQAdatasetsinthe 3.1 Overview open-retrievalsetting,comparedtoseveral,much Our dense passage retriever (DPR) uses a dense complicatedsystems. encoderE (·)whichmapsanytextpassagetoad- P 2 Background dimensionalreal-valuedvectorsandbuildsanindex foralltheM passagesthatwewilluseforretrieval. The problem of open-domain QA studied in this 3Theidealsizeandboundaryofatextpassagearefunc- papercanbedescribedasfollows. Givenafactoid tionsofboththeretrieverandreader.Wealsoexperimented question,suchas“WhofirstvoicedMegonFamily withnaturalparagraphsinourpreliminarytrialsandfoundthat Guy?”or“Wherewasthe8thDalaiLamaborn?”,a usingfixed-lengthpassagesperformsbetterinbothretrieval andfinalQAaccuracy,asobservedbyWangetal.(2019). systemisrequiredtoansweritusingalargecorpus 4Exceptionsinclude(Seoetal.,2019)and(Robertsetal., ofdiversifiedtopics. Morespecifically,weassume 2020),whichretrievesandgeneratestheanswers,respectively. Atrun-time,DPRappliesadifferentencoderE (·) larity)thantheirrelevantones,bylearningabetter Q that maps the input question to a d-dimensional embeddingfunction. vector, and retrieves k passages of which vectors Let D = {(cid:104)q ,p+,p− ,··· ,p− (cid:105)}m be the i i i,1 i,n i=1 are the closest to the question vector. We define training data that consists of m instances. Each thesimilaritybetweenthequestionandthepassage instancecontainsonequestionq andonerelevant i usingthedotproductoftheirvectors: (positive)passagep+,alongwithnirrelevant(neg- i ative)passagesp− . Weoptimizethelossfunction (cid:124) i,j sim(q,p) = E Q (q) E P (p). (1) as the negative log likelihood of the positive pas- sage: Althoughmoreexpressivemodelformsformeasur- ingthesimilaritybetweenaquestionandapassage L(q ,p+,p− ,··· ,p− ) (2) i i i,1 i,n do exist, such as networks consisting of multiple esim(qi,p+ i ) layers of cross attentions, the similarity function = −log . needs to be decomposable so that the represen- esim(qi,p+ i )+ (cid:80)n j=1 esim(qi,p− i,j ) tations of the collection", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "e9b3312d-af13-44f4-bb6c-1a0f4f13b646", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 4, "text": ",p+,p− ,··· ,p− ) (2) i i i,1 i,n do exist, such as networks consisting of multiple esim(qi,p+ i ) layers of cross attentions, the similarity function = −log . needs to be decomposable so that the represen- esim(qi,p+ i )+ (cid:80)n j=1 esim(qi,p− i,j ) tations of the collection of passages can be pre- Positive and negative passages For retrieval computed. Mostdecomposablesimilarityfunctions problems,itisoftenthecasethatpositiveexamples are some transformations of Euclidean distance are available explicitly, while negative examples (L2). For instance, cosine is equivalent to inner needtobeselectedfromanextremelylargepool. productforunitvectorsandtheMahalanobisdis- Forinstance,passagesrelevanttoaquestionmay tanceisequivalenttoL2distanceinatransformed begiveninaQAdataset,orcanbefoundusingthe space. Innerproductsearchhasbeenwidelyused answer. Allotherpassagesinthecollection,while and studied, as well as its connection to cosine notspecifiedexplicitly,canbeviewedasirrelevant similarityandL2distance(MussmannandErmon, bydefault. Inpractice,howtoselectnegativeex- 2016;RamandGray,2012). Asourablationstudy amples is often overlooked but could be decisive findsothersimilarityfunctionsperformcompara- for learning a high-quality encoder. We consider bly (Section 5.2; Appendix B), we thus choose threedifferenttypesofnegatives: (1)Random: any thesimplerinnerproductfunctionandimprovethe random passage from the corpus; (2) BM25: top densepassageretrieverbylearningbetterencoders. passages returned by BM25 which don’t contain Encoders Althoughinprinciplethequestionand the answer but match most question tokens; (3) passageencoderscanbeimplementedbyanyneu- Gold: positivepassagespairedwithotherquestions ralnetworks,inthisworkweusetwoindependent whichappearinthetrainingset. Wewilldiscussthe BERT (Devlin et al., 2019) networks (base, un- impactofdifferenttypesofnegativepassagesand cased) and take the representation at the [CLS] training schemes in Section 5.2. Our best model tokenastheoutput,sod = 768. usesgoldpassagesfromthesamemini-batchand oneBM25negativepassage. Inparticular,re-using Inference During inference time, we apply the gold passages from the same batch as negatives passageencoderE toallthepassagesandindex P can make the computation efficient while achiev- them using FAISS (Johnson et al., 2017) offline. ing great performance. We discuss this approach FAISS is an extremely efficient, open-source li- below. braryforsimilaritysearchandclusteringofdense vectors,whichcaneasilybeappliedtobillionsof In-batch", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "187b6b5c-ccfa-4c0c-bfb9-8df12862153d", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 5, "text": "During inference time, we apply the gold passages from the same batch as negatives passageencoderE toallthepassagesandindex P can make the computation efficient while achiev- them using FAISS (Johnson et al., 2017) offline. ing great performance. We discuss this approach FAISS is an extremely efficient, open-source li- below. braryforsimilaritysearchandclusteringofdense vectors,whichcaneasilybeappliedtobillionsof In-batch negatives Assume that we have B vectors. Givenaquestionq atrun-time,wederive questions in a mini-batch and each one is asso- itsembeddingv = E (q)andretrievethetopk ciatedwitharelevantpassage. LetQandPbethe q Q passageswithembeddingsclosesttov . (B×d)matrixofquestionandpassageembeddings q inabatchofsizeB. S = QPT isa(B ×B)ma- 3.2 Training trixofsimilarityscores,whereeachrowofwhich Trainingtheencoderssothatthedot-productsim- correspondstoaquestion,pairedwithB passages. ilarity(Eq.(1))becomesagoodrankingfunction Inthisway,wereusecomputationandeffectively for retrieval isessentially ametriclearning prob- trainonB2 (q ,p )question/passagepairsineach i j lem (Kulis, 2013). The goal is to create a vector batch. Any(q ,p )pairisapositiveexamplewhen i j spacesuchthatrelevantpairsofquestionsandpas- i = j,andnegativeotherwise. ThiscreatesB train- sageswillhavesmallerdistance(i.e.,highersimi- inginstancesineachbatch,wherethereareB−1 negativepassagesforeachquestion. Dataset Train Dev Test Thetrickofin-batchnegativeshasbeenusedin NaturalQuestions 79,168 58,880 8,757 3,610 the full batch setting (Yih et al., 2011) and more TriviaQA 78,785 60,413 8,837 11,313 WebQuestions 3,417 2,474 361 2,032 recently for mini-batch (Henderson et al., 2017; CuratedTREC 1,353 1,125 133 694 Gillick et al., 2019). It has been shown to be an SQuAD 78,713 70,096 8,886 10,570 effectivestrategyforlearningadual-encodermodel thatbooststhenumberoftrainingexamples. Table1: NumberofquestionsineachQAdataset. The two columns of Train denote the original training ex- 4 ExperimentalSetup amplesinthedatasetandtheactualquestionsusedfor trainingDPRafterfiltering. Seetextformoredetails. In this section, we describe the data we used for experimentsandthebasicsetup. aswellasvariousWebsourcesandisintendedfor 4.1 WikipediaDataPre-processing open-domainQAfromunstructuredcorpora. SQuAD v1.1 (Rajpurkar et al., 2016) is a popu- Following (Lee et al., 2019), we use the English larbenchmarkdatasetforreadingcomprehension. WikipediadumpfromDec.20,2018asthesource AnnotatorswerepresentedwithaWikipediapara- documentsforansweringquestions. Wefirstapply graph,andaskedtowritequestionsthatcouldbe the pre-processing code released", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "2deb0a40-b624-43ca-a311-229234f11643", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 6, "text": "training ex- 4 ExperimentalSetup amplesinthedatasetandtheactualquestionsusedfor trainingDPRafterfiltering. Seetextformoredetails. In this section, we describe the data we used for experimentsandthebasicsetup. aswellasvariousWebsourcesandisintendedfor 4.1 WikipediaDataPre-processing open-domainQAfromunstructuredcorpora. SQuAD v1.1 (Rajpurkar et al., 2016) is a popu- Following (Lee et al., 2019), we use the English larbenchmarkdatasetforreadingcomprehension. WikipediadumpfromDec.20,2018asthesource AnnotatorswerepresentedwithaWikipediapara- documentsforansweringquestions. Wefirstapply graph,andaskedtowritequestionsthatcouldbe the pre-processing code released in DrQA (Chen answered from the given text. Although SQuAD et al., 2017) to extract the clean, text-portion of hasbeenusedpreviouslyforopen-domainQAre- articles from the Wikipedia dump. This step re- search,itisnotidealbecausemanyquestionslack moves semi-structured data, such as tables, info- contextinabsenceoftheprovidedparagraph. We boxes, lists, as well as the disambiguation pages. still include it in our experiments for providing Wethenspliteacharticleintomultiple,disjointtext a fair comparison to previous work and we will blocks of 100 words as passages, serving as our discussmoreinSection5.1. basicretrievalunits,following(Wangetal.,2019), whichresultsin21,015,324passagesintheend.5 Selection of positive passages Because only Eachpassageisalsoprependedwiththetitleofthe pairs of questions and answers are provided in Wikipediaarticlewherethepassageisfrom,along TREC,WebQuestionsandTriviaQA6,weusethe withan[SEP]token. highest-rankedpassagefromBM25thatcontains theanswerasthepositivepassage. Ifnoneofthe 4.2 QuestionAnsweringDatasets top100retrievedpassageshastheanswer,theques- We use the same five QA datasets and train- tion will be discarded. For SQuAD and Natural ing/dev/testing splitting method as in previous Questions, since the original passages have been work(Leeetal.,2019). Belowwebrieflydescribe split and processed differently than our pool of eachdatasetandreferreaderstotheirpaperforthe candidate passages, we match and replace each detailsofdatapreparation. goldpassagewiththecorrespondingpassageinthe Natural Questions (NQ) (Kwiatkowski et al., candidate pool.7 We discard the questions when 2019) was designed for end-to-end question an- the matching is failed due to different Wikipedia swering. The questions were mined from real versionsorpre-processing. Table1showsthenum- Googlesearchqueriesandtheanswerswerespans berofquestionsintraining/dev/testsetsforallthe inWikipediaarticlesidentifiedbyannotators. datasetsandtheactualquestionsusedfortraining TriviaQA(Joshietal.,2017)containsasetoftrivia theretriever. questionswithanswersthatwereoriginallyscraped fromtheWeb. 5 Experiments: PassageRetrieval WebQuestions(WQ)(Berantetal.,2013)consists In", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "24adcee2-045f-4c18-bd86-b17536892086", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 7, "text": "Natural Questions (NQ) (Kwiatkowski et al., candidate pool.7 We discard the questions when 2019) was designed for end-to-end question an- the matching is failed due to different Wikipedia swering. The questions were mined from real versionsorpre-processing. Table1showsthenum- Googlesearchqueriesandtheanswerswerespans berofquestionsintraining/dev/testsetsforallthe inWikipediaarticlesidentifiedbyannotators. datasetsandtheactualquestionsusedfortraining TriviaQA(Joshietal.,2017)containsasetoftrivia theretriever. questionswithanswersthatwereoriginallyscraped fromtheWeb. 5 Experiments: PassageRetrieval WebQuestions(WQ)(Berantetal.,2013)consists In this section, we evaluate the retrieval perfor- of questions selected using Google Suggest API, mance of our Dense Passage Retriever (DPR), wheretheanswersareentitiesinFreebase. alongwithanalysisonhowitsoutputdiffersfrom CuratedTREC (TREC) (Baudisˇ and Sˇedivy`, 2015) sources questions from TREC QA tracks 6WeusetheunfilteredTriviaQAversionanddiscardthe noisyevidencedocumentsminedfromBing. 5However,Wangetal.(2019)alsoproposesplittingdocu- 7Theimprovementofusinggoldcontextsoverpassages mentsintooverlappingpassages,whichwedonotfindadvan- that contain answers is small. See Section 5.2 and Ap- tageouscomparedtothenon-overlappingversion. pendixA. Training Retriever Top-20 Top-100 NQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD None BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0 DPR 78.4 79.4 73.2 79.8 63.2 85.4 85.0 81.4 89.1 77.2 Single BM25+DPR 76.6 79.8 71.0 85.2 71.5 83.8 84.5 80.5 92.7 81.3 DPR 79.4 78.8 75.0 89.1 51.6 86.0 84.7 82.9 93.9 67.6 Multi BM25+DPR 78.0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6 Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained usingindividialorcombinedtrainingdatasets(allthedatasetsexcludingSQuAD).Seetextformoredetails. traditionalretrievalmethods,theeffectsofdifferent 90 trainingschemesandtherun-timeefficiency. TheDPRmodelusedinourmainexperiments 80 istrainedusingthein-batchnegativesetting(Sec- tion3.2)withabatchsizeof128andoneadditional 70 BM25negativepassageperquestion. Wetrained the question and passage encoders for up to 40 60 epochsforlargedatasets(NQ,TriviaQA,SQuAD) and 100 epochs for small datasets (TREC, WQ), 50 with a learning rate of 10−5 using Adam, linear schedulingwithwarm-upanddropoutrate0.1.", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "f55950a1-5723-483e-a0de-f672dcad0890", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 8, "text": "denote that our Dense Passage Retriever (DPR) was trained usingindividialorcombinedtrainingdatasets(allthedatasetsexcludingSQuAD).Seetextformoredetails. traditionalretrievalmethods,theeffectsofdifferent 90 trainingschemesandtherun-timeefficiency. TheDPRmodelusedinourmainexperiments 80 istrainedusingthein-batchnegativesetting(Sec- tion3.2)withabatchsizeof128andoneadditional 70 BM25negativepassageperquestion. Wetrained the question and passage encoders for up to 40 60 epochsforlargedatasets(NQ,TriviaQA,SQuAD) and 100 epochs for small datasets (TREC, WQ), 50 with a learning rate of 10−5 using Adam, linear schedulingwithwarm-upanddropoutrate0.1. 40 20 40 60 80 100 Whileitisgoodtohavetheflexibilitytoadapt k: # of retrieved passages the retriever to each dataset, it would also be de- sirabletoobtainasingleretrieverthatworkswell across the board. To this end, we train a multi- dataset encoder by combining training data from alldatasetsexcludingSQuAD.8InadditiontoDPR, wealsopresenttheresultsofBM25,thetraditional retrievalmethod9 andBM25+DPR,usingalinear combination of their scores as the new ranking function. Specifically, we obtain two initial sets of top-2000 passages based on BM25 and DPR, respectively, and rerank the union of them using BM25(q,p)+λ·sim(q,p)astherankingfunction. Weusedλ = 1.1basedontheretrievalaccuracyin thedevelopmentset. 5.1 MainResults Table 2 compares different passage retrieval sys- temsonfiveQAdatasets,usingthetop-k accuracy (k ∈ {20,100}). With the exception of SQuAD, DPR performs consistently better than BM25 on alldatasets. Thegapisespeciallylargewhenk is small(e.g.,78.4%vs. 59.1%fortop-20accuracy on Natural Questions). When training with mul- 8SQuADislimitedtoasmallsetofWikipediadocuments andthusintroducesunwantedbias.Wewilldiscussthisissue moreinSection5.1. 9Luceneimplementation.BM25parametersb=0.4(doc- umentlengthnormalization)andk = 0.9(termfrequency 1 scaling)aretunedusingdevelopmentsets. )%( ycarucca k-poT BM25 # Train: 1k # Train: 10k # Train: 20k # Train: 40k # Train: all (59k) Figure1: Retrievertop-k accuracywithdifferentnum- bersoftrainingexamplesusedinourdensepassagere- triever vs BM25. The results are measured on the de- velopmentsetofNaturalQuestions. OurDPRtrained using1,000examplesalreadyoutperformsBM25. tiple datasets, TREC, the smallest dataset of the five,benefitsgreatlyfrommoretrainingexamples. Incontrast,NaturalQuestionsandWebQuestions improvemodestlyandTriviaQAdegradesslightly. Resultscanbeimprovedfurtherinsomecasesby combining DPR with BM25 in both single- and multi-datasetsettings. We conjecture that the lower performance on SQuAD is due to two reasons. First, the", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "a0fe033a-7a38-4c3c-8671-e5aaadd5148b", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 9, "text": "bersoftrainingexamplesusedinourdensepassagere- triever vs BM25. The results are measured on the de- velopmentsetofNaturalQuestions. OurDPRtrained using1,000examplesalreadyoutperformsBM25. tiple datasets, TREC, the smallest dataset of the five,benefitsgreatlyfrommoretrainingexamples. Incontrast,NaturalQuestionsandWebQuestions improvemodestlyandTriviaQAdegradesslightly. Resultscanbeimprovedfurtherinsomecasesby combining DPR with BM25 in both single- and multi-datasetsettings. We conjecture that the lower performance on SQuAD is due to two reasons. First, the annota- torswrotequestionsafterseeingthepassage. As a result, there is a high lexical overlap between passagesandquestions,whichgivesBM25aclear advantage. Second, the data was collected from only500+Wikipediaarticlesandthusthedistribu- tion of training examples is extremely biased, as arguedpreviouslybyLeeetal.(2019). 5.2 AblationStudyonModelTraining Tounderstandfurtherhowdifferentmodeltraining optionsaffecttheresults,weconductseveraladdi- tionalexperimentsanddiscussourfindingsbelow. Sample efficiency We explore how many train- Type #N IB Top-5 Top-20 Top-100 ingexamplesareneededtoachievegoodpassage Random 7 (cid:55) 47.0 64.3 77.8 retrievalperformance. Figure1illustratesthetop-k BM25 7 (cid:55) 50.0 63.3 74.8 Gold 7 (cid:55) 42.6 63.1 78.3 retrieval accuracy with respect to different num- bersoftrainingexamples,measuredonthedevel- Gold 7 (cid:51) 51.1 69.1 80.8 Gold 31 (cid:51) 52.1 70.8 82.1 opment set of Natural Questions. As is shown, a Gold 127 (cid:51) 55.8 73.0 83.1 densepassageretrievertrainedusingonly1,000ex- G.+BM25(1) 31+32 (cid:51) 65.0 77.3 84.4 amplesalreadyoutperformsBM25. Thissuggests G.+BM25(2) 31+64 (cid:51) 64.5 76.4 84.0 thatwithageneralpretrainedlanguagemodel,itis G.+BM25(1) 127+128 (cid:51) 65.8 78.0 84.9 possibletotrainahigh-qualitydenseretrieverwith asmallnumberofquestion–passagepairs. Adding Table 3: Comparison of different training schemes, more training examples (from 1k to 59k) further measuredastop-k retrievalaccuracyonNaturalQues- tions (development set). #N: number of negative improvestheretrievalaccuracyconsistently. examples, IB: in-batch training. G.+BM25(1) and G.+BM25(2) denote in-batch training with 1 or 2 ad- In-batch negative training We test different ditional BM25 negatives, which serve as negative pas- trainingschemesonthedevelopmentsetofNatural sagesforallquestionsinthebatch. Questions and summarize the results in Table 3. Thetopblockisthestandard1-of-N trainingset- ting, where each question in", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "d5ab81e6-6ffc-4968-b2d6-0743f7fe36cf", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 10, "text": "number of negative improvestheretrievalaccuracyconsistently. examples, IB: in-batch training. G.+BM25(1) and G.+BM25(2) denote in-batch training with 1 or 2 ad- In-batch negative training We test different ditional BM25 negatives, which serve as negative pas- trainingschemesonthedevelopmentsetofNatural sagesforallquestionsinthebatch. Questions and summarize the results in Table 3. Thetopblockisthestandard1-of-N trainingset- ting, where each question in the batch is paired OurexperimentsonNaturalQuestionsshowthat with a positive passage and its own set of n neg- switchingtodistantly-supervisedpassages(using ative passages (Eq. (2)). We find that the choice thehighest-rankedBM25passagethatcontainsthe of negatives — random, BM25 or gold passages answer), has only a small impact: 1 point lower (positive passages from other questions) — does top-k accuracyforretrieval. AppendixAcontains notimpactthetop-k accuracymuchinthissetting moredetails. whenk ≥ 20. Similarityandloss Besidesdotproduct,cosine Themiddlebockisthein-batchnegativetraining andEuclideanL2distancearealsocommonlyused (Section3.2)setting. Wefindthatusingasimilar asdecomposablesimilarityfunctions. Wetestthese configuration(7goldnegativepassages),in-batch alternatives and find that L2 performs compara- negativetrainingimprovestheresultssubstantially. ble to dot product, and both of them are superior Thekeydifferencebetweenthetwoiswhetherthe to cosine. Similarly, in addition to negative log- goldnegativepassagescomefromthesamebatch likelihood, a popular option for ranking is triplet orfromthewholetrainingset. Effectively,in-batch loss,whichcomparesapositivepassageandanega- negativetrainingisaneasyandmemory-efficient tiveonedirectlywithrespecttoaquestion(Burges waytoreusethenegativeexamplesalreadyinthe et al., 2005). Our experiments show that using batch rather than creating new ones. It produces tripletlossdoesnotaffecttheresultsmuch. More morepairsandthusincreasesthenumberoftrain- detailscanbefoundinAppendixB. ingexamples,whichmightcontributetothegood modelperformance. Asaresult,accuracyconsis- Cross-dataset generalization One interesting tentlyimprovesasthebatchsizegrows. questionregardingDPR’sdiscriminativetraining Finally, we explore in-batch negative training ishowmuchperformancedegradationitmaysuf- withadditional“hard”negativepassagesthathave fer from a non-iid setting. In other words, can high BM25 scores given the question, but do not it still generalize well when directly applied to containtheanswerstring(thebottomblock). These adifferentdatasetwithoutadditionalfine-tuning? additionalpassagesareusedasnegativepassages To test the cross-dataset generalization, we train for all questions in the same batch. We find that DPRonNaturalQuestionsonlyandtestitdirectly addingasingleBM25negativepassageimproves on the smaller", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "bb297eeb-0491-4e39-ab38-509380a11a44", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 11, "text": "a non-iid setting. In other words, can high BM25 scores given the question, but do not it still generalize well when directly applied to containtheanswerstring(thebottomblock). These adifferentdatasetwithoutadditionalfine-tuning? additionalpassagesareusedasnegativepassages To test the cross-dataset generalization, we train for all questions in the same batch. We find that DPRonNaturalQuestionsonlyandtestitdirectly addingasingleBM25negativepassageimproves on the smaller WebQuestions and CuratedTREC theresultsubstantiallywhileaddingtwodoesnot datasets. WefindthatDPRgeneralizeswell,with helpfurther. 3-5pointslossfromthebestperformingfine-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. Impact of gold passages We use passages that 75.0/89.1 for WebQuestions and TREC, respec- match the gold contexts in the original datasets tively),whilestillgreatlyoutperformingtheBM25 (whenavailable)aspositiveexamples(Section4.2). baseline(55.0/70.9). 5.3 QualitativeAnalysis score is chosen as the final answer. The passage selectionmodelservesasarerankerthroughcross- AlthoughDPRperformsbetterthanBM25ingen- attentionbetweenthequestionandthepassage. Al- eral,passagesretrievedbythesetwomethodsdif- thoughcross-attentionisnotfeasibleforretrieving fer qualitatively. Term-matching methods like relevantpassagesinalargecorpusduetoitsnon- BM25 are sensitive to highly selective keywords decomposablenature,ithasmorecapacitythanthe andphrases,whileDPRcaptureslexicalvariations dual-encodermodelsim(q,p)asinEq.(1). Apply- orsemanticrelationshipsbetter. SeeAppendixC ingittoselectingthepassagefromasmallnumber forexamplesandmorediscussion. of retrieved candidates has been shown to work 5.4 Run-timeEfficiency well(Wangetal.,2019,2018;Linetal.,2018). Specifically, let P ∈ RL×h (1 ≤ i ≤ k) be Themainreasonthatwerequirearetrievalcompo- i a BERT (base, uncased in our experiments) rep- nentforopen-domainQAistoreducethenumber resentation for the i-th passage, where L is the ofcandidatepassagesthatthereaderneedstocon- maximumlengthofthepassageandhthehidden sider, which is crucial for answering user’s ques- dimension. Theprobabilitiesofatokenbeingthe tionsinreal-time. Weprofiledthepassageretrieval starting/endingpositionsofananswerspananda speedonaserverwithIntelXeonCPUE5-2698v4 passagebeingselectedaredefinedas: @2.20GHzand512GBmemory. Withthehelpof FAISSin-memoryindexforreal-valuedvectors10, (cid:0) (cid:1) P (s) = softmax P w , (3) DPRcanbemadeincrediblyefficient,processing start,i i start s (cid:0) (cid:1) 995.0questionspersecond,returningtop100pas- P end,i (t) = softmax P i w end t , (4) sagesperquestion. Incontrast,BM25/Lucene(im- P (i) = softmax (cid:0) Pˆ(cid:124) w (cid:1) , (5) selected selected i plementedinJava,usingfileindex)processes23.7 questionspersecondperCPUthread. where Pˆ = [P [CLS] ,...,P [CLS] ] ∈ Rh×k and 1 k", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "76edd2fd-894b-479c-83d5-c54adef98752", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 12, "text": "DPRcanbemadeincrediblyefficient,processing start,i i start s (cid:0) (cid:1) 995.0questionspersecond,returningtop100pas- P end,i (t) = softmax P i w end t , (4) sagesperquestion. Incontrast,BM25/Lucene(im- P (i) = softmax (cid:0) Pˆ(cid:124) w (cid:1) , (5) selected selected i plementedinJava,usingfileindex)processes23.7 questionspersecondperCPUthread. where Pˆ = [P [CLS] ,...,P [CLS] ] ∈ Rh×k and 1 k Ontheotherhand,thetimerequiredforbuilding w ,w ,w ∈ Rh are learnable vectors. start end selected an index for dense vectors is much longer. Com- Wecomputeaspanscoreofthes-thtot-thwords putingdenseembeddingson21-millionpassages fromthei-thpassageasP (s)×P (t),and start,i end,i isresourceintensive,butcanbeeasilyparallelized, a passage selection score of the i-th passage as taking roughly 8.8 hours on 8 GPUs. However, P (i). selected building the FAISS index on 21-million vectors During training, we sample one positive and onasingleservertakes8.5hours. Incomparison, m˜ −1negativepassagesfromthetop100passages building an inverted index using Lucene is much returned by the retrieval system (BM25 or DPR) cheaperandtakesonlyabout30minutesintotal. foreachquestion. m˜ isahyper-parameterandwe usem˜ = 24inalltheexperiments. Thetrainingob- 6 Experiments: QuestionAnswering jectiveistomaximizethemarginallog-likelihood Inthissection,weexperimentwithhowdifferent ofallthecorrectanswerspansinthepositivepas- passageretrieversaffectthefinalQAaccuracy. sage(theanswerstringmayappearmultipletimes inonepassage),combinedwiththelog-likelihood 6.1 End-to-endQASystem ofthepositivepassagebeingselected. Weusethe Weimplementanend-to-endquestionanswering batchsizeof16forlarge(NQ,TriviaQA,SQuAD) system in which we can plug different retriever and4forsmall(TREC,WQ)datasets,andtunek systemsdirectly. Besidestheretriever,ourQAsys- onthedevelopmentset. Forexperimentsonsmall tem consists of a neural reader that outputs the datasets under the Multi setting, in which using answertothequestion. Giventhetopk retrieved other datasets is allowed, we fine-tune the reader passages(upto100inourexperiments),thereader trainedonNaturalQuestionstothetargetdataset. assignsapassageselectionscoretoeachpassage. Allexperimentsweredoneoneight32GBGPUs. In addition, it extracts an answer span from each 6.2 Results passage and assigns a span score. The best span fromthepassagewiththehighestpassageselection Table 4 summarizes our final end-to-end QA re- sults,measuredbyexactmatchwiththereference 10FAISSconfiguration:weusedHNSWindextypeonCPU, answerafterminornormalizationasin(Chenetal., neighborstostorepernode=512,constructiontimesearch depth=200,searchdepth=128. 2017; Lee et al., 2019). From the table, we can Training Model NQ TriviaQA WQ", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "aba0c9f7-97ba-407f-b2b4-008c78b57d8b", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 13, "text": "Allexperimentsweredoneoneight32GBGPUs. In addition, it extracts an answer span from each 6.2 Results passage and assigns a span score. The best span fromthepassagewiththehighestpassageselection Table 4 summarizes our final end-to-end QA re- sults,measuredbyexactmatchwiththereference 10FAISSconfiguration:weusedHNSWindextypeonCPU, answerafterminornormalizationasin(Chenetal., neighborstostorepernode=512,constructiontimesearch depth=200,searchdepth=128. 2017; Lee et al., 2019). From the table, we can Training Model NQ TriviaQA WQ TREC SQuAD Single BM25+BERT(Leeetal.,2019) 26.5 47.1 17.7 21.3 33.2 Single ORQA(Leeetal.,2019) 33.3 45.0 36.4 30.1 20.2 Single HardEM(Minetal.,2019a) 28.1 50.9 - - - Single GraphRetriever(Minetal.,2019b) 34.5 56.0 36.4 - - Single PathRetriever(Asaietal.,2020) 32.6 - - - 56.5 Single REALM (Guuetal.,2020) 39.2 - 40.2 46.8 - Wiki Single REALM (Guuetal.,2020) 40.4 - 40.7 42.9 - News BM25 32.6 52.4 29.9 24.9 38.1 Single DPR 41.5 56.8 34.6 25.9 29.8 BM25+DPR 39.0 57.0 35.2 28.0 36.7 DPR 41.5 56.8 42.4 49.4 24.1 Multi BM25+DPR 38.8 57.9 41.1 50.6 35.8 Table 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers. REALM andREALM arethesamemodelbutpretrainedonWikipediaandCC-News,respectively. Single Wiki News andMultidenotethatourDensePassageRetriever(DPR)istrainedusingindividualorcombinedtrainingdatasets (allexceptSQuAD).ForWQandTRECintheMultisetting,wefine-tunethereadertrainedonNQ. seethathigherretrieveraccuracytypicallyleadsto trained,followingLeeetal.(2019). Thisapproach betterfinalQAresults: inallcasesexceptSQuAD, obtainsascoreof39.8EM,whichsuggeststhatour answersextractedfromthepassagesretrievedby strategyoftrainingastrongretrieverandreaderin DPR are more likely to be correct, compared to isolationcanleverageeffectivelyavailablesupervi- thosefromBM25. ForlargedatasetslikeNQand sion,whileoutperformingacomparablejointtrain- TriviaQA,modelstrainedusingmultipledatasets ingapproachwithasimplerdesign(AppendixD). (Multi)performcomparablytothosetrainedusing Onethingworthnoticingisthatourreaderdoes the individual training set (Single). Conversely, consider more passages compared to ORQA, al- onsmallerdatasetslikeWQandTREC,themulti- thoughitisnotcompletelyclearhowmuchmore datasetsettinghasaclearadvantage. Overall,our timeittakesforinference. WhileDPRprocesses DPR-basedmodelsoutperformthepreviousstate- up to 100 passages for each question, the reader of-the-art results on four out of the five datasets, is able to fit all of them into one batch on a sin- with1%to12%absolutedifferencesinexactmatch gle 32GB GPU, thus the latency remains almost accuracy. Itisinterestingtocontrastourresultsto", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "5358d661-e218-4247-ae48-d7d71d46aa72", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 14, "text": "al- onsmallerdatasetslikeWQandTREC,themulti- thoughitisnotcompletelyclearhowmuchmore datasetsettinghasaclearadvantage. Overall,our timeittakesforinference. WhileDPRprocesses DPR-basedmodelsoutperformthepreviousstate- up to 100 passages for each question, the reader of-the-art results on four out of the five datasets, is able to fit all of them into one batch on a sin- with1%to12%absolutedifferencesinexactmatch gle 32GB GPU, thus the latency remains almost accuracy. Itisinterestingtocontrastourresultsto identicaltothesinglepassagecase(around20ms). those of ORQA (Lee et al., 2019) and also the Theexactimpactonthroughputishardertomea- concurrentlydevelopedapproach,REALM(Guu sure: ORQAuses2-3xlongerpassagescompared et al., 2020). While both methods include addi- to DPR (288 word pieces compared to our 100 tional pretraining tasks and employ an expensive tokens)andthecomputationalcomplexityissuper- end-to-endtrainingregime,DPRmanagestoout- linear in passage length. We also note that we perform them on both NQ and TriviaQA, simply found k = 50 to be optimal for NQ, and k = 10 byfocusingonlearningastrongpassageretrieval leads to only marginal loss in exact match accu- modelusingpairsofquestionsandanswers. The racy(40.8vs. 41.5EMonNQ),whichshouldbe additionalpretrainingtasksarelikelymoreuseful roughlycomparabletoORQA’s5-passagesetup. only when the target training sets are small. Al- 7 RelatedWork thoughtheresultsofDPRonWQandTRECinthe single-datasetsettingarelesscompetitive,adding Passage retrieval has been an important compo- morequestion–answerpairshelpsboosttheperfor- nent for open-domain QA (Voorhees, 1999). It mance,achievingthenewstateoftheart. not only effectively reduces the search space for Tocompareourpipelinetrainingapproachwith answer extraction, but also identifies the support jointlearning,werunanablationonNaturalQues- contextforuserstoverifytheanswer. Strongsparse tions where the retriever and reader are jointly vector space models like TF-IDF or BM25 have beenusedasthestandardmethodappliedbroadly effectivesolutionthatshowsstrongerempiricalper- tovariousQAtasks(e.g.,Chenetal.,2017;Yang formance,withoutrelyingonadditionalpretraining etal.,2019a,b;Nieetal.,2019;Minetal.,2019a; orcomplexjointtrainingschemes. Wolfsonet al., 2020). Augmentingtext-basedre- DPR has also been used as an important mod- trievalwithexternalstructuredinformation,such ule in very recent work. For instance, extending asknowledgegraphandWikipediahyperlinks,has theideaofleveraginghardnegatives,Xiongetal. alsobeenexploredrecently(Minetal.,2019b;Asai (2020a)usetheretrievalmodeltrainedinthepre- etal.,2020). viousiterationtodiscovernewnegativesandcon- structadifferentsetofexamplesineachtraining The use of dense vector representations for re- iteration. Starting from our trained DPR", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "06d7f631-5b1e-4c17-a7d3-95da0ef2e456", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 15, "text": "have beenusedasthestandardmethodappliedbroadly effectivesolutionthatshowsstrongerempiricalper- tovariousQAtasks(e.g.,Chenetal.,2017;Yang formance,withoutrelyingonadditionalpretraining etal.,2019a,b;Nieetal.,2019;Minetal.,2019a; orcomplexjointtrainingschemes. Wolfsonet al., 2020). Augmentingtext-basedre- DPR has also been used as an important mod- trievalwithexternalstructuredinformation,such ule in very recent work. For instance, extending asknowledgegraphandWikipediahyperlinks,has theideaofleveraginghardnegatives,Xiongetal. alsobeenexploredrecently(Minetal.,2019b;Asai (2020a)usetheretrievalmodeltrainedinthepre- etal.,2020). viousiterationtodiscovernewnegativesandcon- structadifferentsetofexamplesineachtraining The use of dense vector representations for re- iteration. Starting from our trained DPR model, trieval has a long history since Latent Semantic they show that the retrieval performance can be Analysis(Deerwesteretal.,1990). Usinglabeled furtherimproved. Recentwork(IzacardandGrave, pairs of queries and documents, discriminatively 2020; Lewis et al., 2020b) have also shown that trained dense encoders have become popular re- DPR can be combined with generation models cently(Yihetal.,2011;Huangetal.,2013;Gillick suchasBART(Lewisetal.,2020a)andT5(Raf- et al., 2019), with applications to cross-lingual fel et al., 2019), achieving good performance on documentretrieval,adrelevanceprediction,Web open-domain QA and other knowledge-intensive searchandentityretrieval. Suchapproachescom- tasks. plementthesparsevectormethodsastheycanpo- tentiallygivehighsimilarityscorestosemantically 8 Conclusion relevanttextpairs,evenwithoutexacttokenmatch- ing. The dense representation alone, however, is Inthiswork,wedemonstratedthatdenseretrieval typicallyinferiortothesparseone. Whilenotthe can outperform and potentially replace the tradi- focusofthiswork,denserepresentationsfrompre- tionalsparseretrievalcomponentinopen-domain trainedmodels,alongwithcross-attentionmecha- questionanswering. Whileasimpledual-encoder nisms,havealsobeenshowneffectiveinpassage approach can be made to work surprisingly well, or dialogue re-ranking tasks (Nogueira and Cho, weshowedthattherearesomecriticalingredients 2019;Humeauetal.,2020). Finally,aconcurrent totrainingadenseretrieversuccessfully. Moreover, work (Khattab and Zaharia, 2020) demonstrates ourempiricalanalysisandablationstudiesindicate the feasibility of full dense retrieval in IR tasks. thatmorecomplexmodelframeworksorsimilarity Insteadofemployingthedual-encoderframework, functionsdonotnecessarilyprovideadditionalval- theyintroducedalate-interactionoperatorontop ues. Asaresultofimprovedretrievalperformance, oftheBERTencoders. weobtainednewstate-of-the-artresultsonmultiple open-domainquestionansweringbenchmarks. Dense retrieval for open-domain QA has been exploredbyDasetal.(2019),whoproposetore- Acknowledgments trieverelevantpassagesiterativelyusingreformu- Wethanktheanonymousreviewersfortheirhelpful latedquestionvectors. Asanalternativeapproach commentsandsuggestions. thatskipspassageretrieval,Seoetal.(2019)pro- posetoencodecandidateanswerphrasesasvectors anddirectlyretrievetheanswerstotheinputques- References tionsefficiently. Usingadditionalpretrainingwith AkariAsai,KazumaHashimoto,HannanehHajishirzi, theobjectivethatmatchessurrogatesofquestions Richard Socher, and Caiming Xiong. 2020. Learn- andrelevantpassages,Leeetal.(2019)jointlytrain ingtoretrievereasoningpathsoverWikipediagraph the question encoder and reader. Their approach forquestionanswering. InInternationalConference outperforms the BM25 plus reader paradigm on onLearningRepresentations(ICLR). multipleopen-domainQAdatasetsinQAaccuracy, Petr Baudisˇ", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "ffd0a9a0-75e6-4f6a-8d24-6b110d0c7475", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 16, "text": "retrieval for open-domain QA has been exploredbyDasetal.(2019),whoproposetore- Acknowledgments trieverelevantpassagesiterativelyusingreformu- Wethanktheanonymousreviewersfortheirhelpful latedquestionvectors. Asanalternativeapproach commentsandsuggestions. thatskipspassageretrieval,Seoetal.(2019)pro- posetoencodecandidateanswerphrasesasvectors anddirectlyretrievetheanswerstotheinputques- References tionsefficiently. Usingadditionalpretrainingwith AkariAsai,KazumaHashimoto,HannanehHajishirzi, theobjectivethatmatchessurrogatesofquestions Richard Socher, and Caiming Xiong. 2020. Learn- andrelevantpassages,Leeetal.(2019)jointlytrain ingtoretrievereasoningpathsoverWikipediagraph the question encoder and reader. Their approach forquestionanswering. InInternationalConference outperforms the BM25 plus reader paradigm on onLearningRepresentations(ICLR). multipleopen-domainQAdatasetsinQAaccuracy, Petr Baudisˇ and Jan Sˇedivy`. 2015. Modeling of the and is further extended by REALM (Guu et al., questionansweringtaskintheyodaqasystem. InIn- 2020),whichincludestuningthepassageencoder ternationalConferenceoftheCross-LanguageEval- asynchronously by re-indexing the passages dur- uation Forum for European Languages, pages 222– ing training. The pretraining objective has also 228.Springer. recently been improved by Xiong et al. (2020b). JonathanBerant,AndrewChou,RoyFrostig,andPercy In contrast, our model provides a simple and yet Liang. 2013. Semantic parsing on Freebase from question-answerpairs. InEmpiricalMethodsinNat- clickthrough data. In ACM International Confer- uralLanguageProcessing(EMNLP). ence on Information and Knowledge Management (CIKM),pages2333–2338. Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sa¨ckinger,andRoopakShah.1994. Signatureverifi- Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, cationusinga“Siamese”timedelayneuralnetwork. and Jason Weston. 2020. Poly-encoders: Architec- InNIPS,pages737–744. turesandpre-trainingstrategiesforfastandaccurate multi-sentencescoring. InInternationalConference Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, onLearningRepresentations(ICLR). Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In GautierIzacardandEdouardGrave.2020. Leveraging Proceedingsofthe22ndinternationalconferenceon passageretrievalwithgenerativemodelsforopendo- Machinelearning,pages89–96. mainquestionanswering. ArXiv,abs/2007.01282. DanqiChen, AdamFisch, JasonWeston, andAntoine Jeff Johnson, Matthijs Douze, and Herve´ Je´gou. 2017. Bordes. 2017. Reading Wikipedia to answer open- Billion-scale similarity search with GPUs. ArXiv, domain questions. In Association for Computa- abs/1702.08734. tionalLinguistics(ACL),pages1870–1879. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Zettlemoyer. 2017. TriviaQA: A large scale dis- and Andrew McCallum. 2019. Multi-step retriever- tantlysupervisedchallengedatasetforreadingcom-", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "015a1648-2d2a-4155-8066-f925c80436ba", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 17, "text": "2017. Bordes. 2017. Reading Wikipedia to answer open- Billion-scale similarity search with GPUs. ArXiv, domain questions. In Association for Computa- abs/1702.08734. tionalLinguistics(ACL),pages1870–1879. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Zettlemoyer. 2017. TriviaQA: A large scale dis- and Andrew McCallum. 2019. Multi-step retriever- tantlysupervisedchallengedatasetforreadingcom- readerinteractionforscalableopen-domainquestion prehension. In Association for Computational Lin- answering. In International Conference on Learn- guistics(ACL),pages1601–1611. ingRepresentations(ICLR). Scott Deerwester, Susan T Dumais, George W Fur- Omar Khattab and Matei Zaharia. 2020. ColBERT: nas, Thomas K Landauer, and Richard Harshman. Efficient and effective passage search via contextu- 1990. Indexing by latent semantic analysis. Jour- alized late interaction over BERT. In ACM SIGIR naloftheAmericansocietyforinformationscience, Conference on Research and Development in Infor- 41(6):391–407. mationRetrieval(SIGIR),pages39–48. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Brian Kulis. 2013. Metric learning: A survey. Foun- Kristina Toutanova. 2019. BERT: Pre-training of dationsandTrendsinMachineLearning,5(4):287– deep bidirectional transformers for language under- 364. standing. In North American Association for Com- putationalLinguistics(NAACL). Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, MichaelCollins, AnkurParikh, ChrisAlberti, DavidAFerrucci.2012. Introductionto“ThisisWat- DanielleEpstein,IlliaPolosukhin,MatthewKelcey, son”. IBM Journal of Research and Development, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, 56(3.4):1–1. Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu- Daniel Gillick, Sayali Kulkarni, Larry Lansing, ral questions: a benchmark for question answering AlessandroPresta,JasonBaldridge,EugeneIe,and research. TransactionsoftheAssociationofCompu- Diego Garcia-Olano. 2019. Learning dense repre- tationalLinguistics(TACL). sentationsforentityretrieval. InComputationalNat- uralLanguageLearning(CoNLL). KentonLee,Ming-WeiChang,andKristinaToutanova. 2019. Latent retrieval for weakly supervised open RuiqiGuo,SanjivKumar,KrzysztofChoromanski,and domainquestionanswering. InAssociationforCom- DavidSimcha.2016. Quantizationbasedfastinner putationalLinguistics(ACL),pages6086–6096. productsearch. InArtificialIntelligenceandStatis- tics,pages482–490. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "989c6b9e-1cf4-4a46-9dba-9a4bebfb5e54", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 18, "text": "Gillick, Sayali Kulkarni, Larry Lansing, ral questions: a benchmark for question answering AlessandroPresta,JasonBaldridge,EugeneIe,and research. TransactionsoftheAssociationofCompu- Diego Garcia-Olano. 2019. Learning dense repre- tationalLinguistics(TACL). sentationsforentityretrieval. InComputationalNat- uralLanguageLearning(CoNLL). KentonLee,Ming-WeiChang,andKristinaToutanova. 2019. Latent retrieval for weakly supervised open RuiqiGuo,SanjivKumar,KrzysztofChoromanski,and domainquestionanswering. InAssociationforCom- DavidSimcha.2016. Quantizationbasedfastinner putationalLinguistics(ACL),pages6086–6096. productsearch. InArtificialIntelligenceandStatis- tics,pages482–490. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa- Levy, Veselin Stoyanov, and Luke Zettlemoyer. supat, and Ming-Wei Chang. 2020. REALM: 2020a. BART:Denoisingsequence-to-sequencepre- Retrieval-augmented language model pre-training. trainingfornaturallanguagegeneration,translation, ArXiv,abs/2002.08909. and comprehension. In Association for Computa- MatthewHenderson,RamiAl-Rfou,BrianStrope,Yun- tionalLinguistics(ACL),pages7871–7880. hsuan Sung, La´szlo´ Luka´cs, Ruiqi Guo, Sanjiv Ku- mar, Balint Miklos, and Ray Kurzweil. 2017. Effi- Patrick Lewis, Ethan Perez, Aleksandara Piktus, cientnaturallanguageresponsesuggestionforsmart Fabio Petroni, Vladimir Karpukhin, Naman Goyal, reply. ArXiv,abs/1705.00652. Heinrich Ku¨ttler, Mike Lewis, Wen-tau Yih, Tim Rockta¨schel, Sebastian Riedel, and Douwe Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Kiela. 2020b. Retrieval-augmented generation for Alex Acero, and Larry Heck. 2013. Learning deep knowledge-intensive NLP tasks. In Advances in structured semantic models for Web search using NeuralInformationProcessingSystems(NeurIPS). YankaiLin,HaozheJi,ZhiyuanLiu,andMaosongSun. Anshumali Shrivastava and Ping Li. 2014. Asymmet- 2018. Denoising distantly supervised open-domain ricLSH(ALSH)forsublineartimemaximuminner question answering. In Association for Computa- product search (MIPS). In Advances in Neural In- tionalLinguistics(ACL),pages1736–1745. formation Processing Systems (NIPS), pages 2321– 2329. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard EM ap- Ellen M Voorhees. 1999. The TREC-8 question an- proach for weakly supervised question answering. swering track report. In TREC, volume 99, pages InEmpiricalMethodsinNaturalLanguageProcess- 77–82. ing(EMNLP). ShuohangWang,MoYu,XiaoxiaoGuo,ZhiguoWang, SewonMin, DanqiChen, LukeZettlemoyer, andHan- Tim", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "05cad822-6bf1-4cef-abcd-f89e3ad3bf12", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 19, "text": "(NIPS), pages 2321– 2329. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard EM ap- Ellen M Voorhees. 1999. The TREC-8 question an- proach for weakly supervised question answering. swering track report. In TREC, volume 99, pages InEmpiricalMethodsinNaturalLanguageProcess- 77–82. ing(EMNLP). ShuohangWang,MoYu,XiaoxiaoGuo,ZhiguoWang, SewonMin, DanqiChen, LukeZettlemoyer, andHan- Tim Klinger, Wei Zhang, Shiyu Chang, Gerry nanehHajishirzi.2019b. Knowledgeguidedtextre- Tesauro, Bowen Zhou, and Jing Jiang. 2018. Rˆ3: trievalandreadingforopendomainquestionanswer- Reinforcedranker-readerforopen-domainquestion ing. ArXiv,abs/1911.03868. answering. In Conference on Artificial Intelligence (AAAI). Dan Moldovan, Marius Pas¸ca, Sanda Harabagiu, and Mihai Surdeanu. 2003. Performance issues and er- ZhiguoWang,PatrickNg,XiaofeiMa,RameshNalla- ror analysis in an open-domain question answering pati, and Bing Xiang. 2019. Multi-passage BERT: system. ACMTransactionsonInformationSystems A globally normalized bert model for open-domain (TOIS),21(2):133–154. questionanswering. InEmpiricalMethodsinNatu- ralLanguageProcessing(EMNLP). Stephen Mussmann and Stefano Ermon. 2016. Learn- ingandinferenceviamaximuminnerproductsearch. Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard- In International Conference on Machine Learning ner, Yoav Goldberg, Daniel Deutch, and Jonathan (ICML),pages2587–2596. Berant. 2020. Break it down: A question under- standing benchmark. Transactions of the Associa- YixinNie,SongheWang,andMohitBansal.2019. Re- tionofComputationalLinguistics(TACL). vealingtheimportanceofsemanticretrievalforma- chinereadingatscale. InEmpiricalMethodsinNat- Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, uralLanguageProcessing(EMNLP). JialinLiu,PaulBennett,JunaidAhmed,andArnold Overwijk. 2020a. Approximate nearest neighbor RodrigoNogueiraandKyunghyunCho.2019. Passage negativecontrastivelearningfordensetextretrieval. re-rankingwithBERT. ArXiv,abs/1901.04085. ArXiv,abs/2007.00808. ColinRaffel,NoamShazeer,AdamRoberts,Katherine Wenhan Xiong, Hankang Wang, and William Yang Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wang.2020b. Progressivelypretraineddensecorpus WeiLi, andPeterJLiu.2019. Exploringthelimits index for open-domain question answering. ArXiv, of transfer learning with a unified text-to-text trans- former. ArXiv,abs/1910.10683. abs/2005.00038. PranavRajpurkar,JianZhang,KonstantinLopyrev,and WeiYang,YuqingXie,AileenLin,XingyuLi,Luchen Percy Liang. 2016. SQuAD: 100,000+ questions Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a. for machine comprehension of text. In", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "edcede32-4180-4073-9ffb-678738e3dca4", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 20, "text": "Narang, Michael Matena, Yanqi Zhou, Wang.2020b. Progressivelypretraineddensecorpus WeiLi, andPeterJLiu.2019. Exploringthelimits index for open-domain question answering. ArXiv, of transfer learning with a unified text-to-text trans- former. ArXiv,abs/1910.10683. abs/2005.00038. PranavRajpurkar,JianZhang,KonstantinLopyrev,and WeiYang,YuqingXie,AileenLin,XingyuLi,Luchen Percy Liang. 2016. SQuAD: 100,000+ questions Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a. for machine comprehension of text. In Empirical End-to-end open-domain question answering with MethodsinNaturalLanguageProcessing(EMNLP), bertserini. InNorthAmericanAssociationforCom- pages2383–2392. putationalLinguistics(NAACL),pages72–77. ParikshitRamandAlexanderGGray.2012. Maximum WeiYang,YuqingXie,LuchenTan,KunXiong,Ming inner-product search using cone trees. In Proceed- Li, and Jimmy Lin. 2019b. Data augmentation for ings of the 18th ACM SIGKDD international con- bertfine-tuninginopen-domainquestionanswering. ference on Knowledge discovery and data mining, ArXiv,abs/1904.06652. pages931–939. Wen-tau Yih, Kristina Toutanova, John C Platt, and AdamRoberts,ColinRaffel,andNoamShazeer.2020. Christopher Meek. 2011. Learning discriminative Howmuchknowledgecanyoupackintotheparam- projections for text similarity measures. In Com- etersofalanguagemodel? ArXiv,abs/2002.08910. putational Natural Language Learning (CoNLL), pages247–256. Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and be- yond. Foundations and Trends in Information Re- trieval,3(4):333–389. Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh,AliFarhadi,andHannanehHajishirzi.2019. Real-time open-domain question answering with dense-sparsephraseindex. InAssociationforCom- putationalLinguistics(ACL). A DistantSupervision Top-1 Top-5 Top-20 Top-100 WhentrainingourfinalDPRmodelusingNatural Gold 44.9 66.8 78.1 85.0 Questions, we use the passages in our collection Dist. Sup. 43.9 65.3 77.1 84.4 that best match the gold context as the positive passages. As some QA datasets contain only the Table5: Retrievalaccuracyonthedevelopmentsetof Natural Questions, trained on passages that match the question and answer pairs, it is thus interesting goldcontext(Gold)orthetopBM25passagethatcon- to see when using the passages that contain the tainstheanswer(Dist. Sup.). answers as positives (i.e., the distant supervision setting),whetherthereisasignificantperformance Sim Loss RetrievalAccuracy degradation. Using", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "6fd21e77-94b7-4e5e-be42-ae4b8ffce921", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 21, "text": "As some QA datasets contain only the Table5: Retrievalaccuracyonthedevelopmentsetof Natural Questions, trained on passages that match the question and answer pairs, it is thus interesting goldcontext(Gold)orthetopBM25passagethatcon- to see when using the passages that contain the tainstheanswer(Dist. Sup.). answers as positives (i.e., the distant supervision setting),whetherthereisasignificantperformance Sim Loss RetrievalAccuracy degradation. Using the question and answer to- Top-1 Top-5 Top-20 Top-100 getherasthequery,werunLucene-BM25andpick thetoppassagethatcontainstheanswerasthepos- NLL 44.9 66.8 78.1 85.0 DP itive passage. Table 5 shows the performance of Triplet 41.6 65.0 77.2 84.5 DPR when trained using the original setting and NLL 43.5 64.7 76.1 83.1 thedistantsupervisionsetting. L2 Triplet 42.2 66.0 78.1 84.9 B AlternativeSimilarityFunctions& Table6: RetrievalTop-k accuracyonthedevelopment TripletLoss setofNaturalQuestionsusingdifferentsimilarityand lossfunctions. In addition to dot product (DP) and negative log- likelihoodbasedonsoftmax(NLL),wealsoexper- imentwithEuclideandistance(L2)andthetriplet thecorrectanswer,presumablybymatching“body loss. We negate L2 similarity scores before ap- ofwater”withsemanticneighborssuchasseaand plying softmax and change signs of question-to- channel,eventhoughnolexicaloverlapexists. The positiveandquestion-to-negativesimilaritieswhen second example is one where BM25 does better. applyingthetripletlossondotproductscores. The Thesalientphrase“ThorosofMyr”iscritical,and margin value of the triplet loss is set to 1. Ta- DPRisunabletocaptureit. ble6summarizestheresults. Alltheseadditional experimentsareconductedusingthesamehyper- D JointTrainingofRetrieverand parameterstunedforthebaseline(DP,NLL). Reader Notethattheretrievalaccuracyforour“baseline” We fix the passage encoder in our joint-training settings reported in Table 5 (Gold) and Table 6 schemewhileallowingonlythequestionencoder (DP,NLL)isslightlybetterthanthosereportedin to receive backpropagation signal from the com- Table 3. This is due to a better hyper-parameter bined(retriever+reader)lossfunction. Thisallows settingusedintheseanalysisexperiments,which ustoleveragetheHNSW-basedFAISSindexfor isdocumentedinourcoderelease. efficientlow-latencyretrieving,withoutreindexing thepassagesduringmodelupdates. Ourlossfunc- C QualitativeAnalysis tionlargelyfollowsORQA’sapproach,whichuses AlthoughDPRperformsbetterthanBM25ingen- logprobabilitiesofpositivepassagesselectedfrom eral,theretrievedpassagesofthesetworetrievers theretrievermodel,andcorrectspansandpassages actually differ qualitatively. Methods like BM25 selectedfromthereadermodel. Sincethepassage are sensitive to highly selective keywords and encoder is fixed, we could use larger amount of phrases,butcannotcapturelexicalvariationsorse- retrieved passages when calculating the", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
{"id": "713e2ccb-3ed7-4797-bc57-d62f7a53d8b1", "source": "data/raw/papers/2020.emnlp-main.550.pdf", "chunk_index": 22, "text": "due to a better hyper-parameter bined(retriever+reader)lossfunction. Thisallows settingusedintheseanalysisexperiments,which ustoleveragetheHNSW-basedFAISSindexfor isdocumentedinourcoderelease. efficientlow-latencyretrieving,withoutreindexing thepassagesduringmodelupdates. Ourlossfunc- C QualitativeAnalysis tionlargelyfollowsORQA’sapproach,whichuses AlthoughDPRperformsbetterthanBM25ingen- logprobabilitiesofpositivepassagesselectedfrom eral,theretrievedpassagesofthesetworetrievers theretrievermodel,andcorrectspansandpassages actually differ qualitatively. Methods like BM25 selectedfromthereadermodel. Sincethepassage are sensitive to highly selective keywords and encoder is fixed, we could use larger amount of phrases,butcannotcapturelexicalvariationsorse- retrieved passages when calculating the retriever manticrelationshipswell. Incontrast,DPRexcels loss. Specifically,wegettop100passagesforeach atsemanticrepresentation,butmightlacksufficient questioninamini-batchandusethemethodsimilar capacitytorepresentsalientphraseswhichappear toin-batchnegativetraining: allretrievedpassages’ rarely. Table 7 illustrates this phenomenon with vectors participate in the loss calculation for all two examples. In the first example, the top scor- questionsinabatch. Ourtrainingbatchsizeisset ingpassagefromBM25isirrelevant,eventhough to16,whicheffectivelygives1,600passagesper keywordssuchasEnglandandIrelandappearmul- questiontocalculateretrieverloss. Thereaderstill tipletimes. Incomparison, DPRisabletoreturn uses24passagesperquestion,whichareselected Question PassagereceivedbyBM25 PassageretrievedbyDPR Whatisthebodyofwater Title:BritishCycling Title:IrishSea betweenEnglandandIreland? ...EnglandisnotrecognisedasaregionbytheUCI,and ...AnnualtrafficbetweenGreatBritainandIrelandamounts thereisnoEnglishcyclingteamoutsidetheCommonwealth toover12millionpassengersandoftradedgoods.TheIrish Games.Forthoseoccasions,BritishCyclingselectsandsup- SeaisconnectedtotheNorthAtlanticatbothitsnorthern portstheEnglandteam. CyclingisrepresentedontheIsle andsouthernends. Tothenorth,theconnectionisthrough ofManbytheIsleofManCyclingAssociation. Cyclingin theNorthChannelbetweenScotlandandNorthernIreland NorthernIrelandisorganisedunderCyclingUlster,partof andtheMalinSea.ThesouthernendislinkedtotheAtlantic theall-IrelandgoverningbodyCyclingIreland.Until2006, throughtheStGeorge’sChannelbetweenIrelandandPem- arivalgoverningbodyexisted,... brokeshire,andtheCelticSea.... WhoplaysThorosofMyrin Title:NoOne(GameofThrones) Title:Pa˚lSverreHagen GameofThrones? ...Hemaybe”noone,”butthere’sstillenoughofaperson Pa˚lSverreValheimHagen(born6November1980)isaNor- leftinhimtorespect,andadmirewhothisgirlisandwhat wegianstageandscreenactor. HeappearedintheNorwe- she’sbecome.Aryafinallytellsussomethingthatwe’vekind gianfilm”MaxManus”andplayedThorHeyerdahlinthe ofknownallalong,thatshe’snotnoone,she’sAryaStark Oscar-nominated2012film”Kon-Tiki”. PlHagenwasborn ofWinterfell.””NoOne”sawthereintroductionofRichard inStavanger,Norway,thesonofRoarHagen,aNorwegian DormerandPaulKaye,whoportrayedBericDondarrionand cartoonistwhohaslongbeenassociatedwithNorways´largest ThorosofMyr,respectively,inthethirdseason,... daily,”VG”.HelivedinJtten,aneighborhoodinthecityof Stavangerinsouth-westernNorway.... Table7: ExamplesofpassagesreturnedfromBM25andDPR.Correctanswersarewritteninblueandthecontent wordsinthequestionarewritteninbold. fromthetop5positiveandtop30negativepassages (from the set of top 100 passages retrieved from thesamequestion). Thequestionencoder’sinitial stateistakenfromaDPRmodelpreviouslytrained on the NQ dataset. The reader’s initial state is a BERT-basemodel. Intermsoftheend-to-endQA results,ourjoint-trainingschemedoesnotprovide betterresultscomparedtotheusualretriever/reader trainingpipeline,resultinginthesame39.8exact match score on NQ dev as in our regular reader modeltraining.", "metadata": {"filename": "2020.emnlp-main.550.pdf", "type": "pdf"}}
