{"page": 1, "bbox": [{"x": 0.17549078166484833, "y": 0.08494532853364944}, {"x": 0.8274836540222168, "y": 0.08494532853364944}, {"x": 0.8274836540222168, "y": 0.10050462931394577}, {"x": 0.17549078166484833, "y": 0.10050462931394577}], "text": "Dense Passage Retrieval for Open-Domain Question Answering\n"}
{"page": 1, "bbox": [{"x": 0.16240333020687103, "y": 0.12531539797782898}, {"x": 0.846519947052002, "y": 0.12615643441677094}, {"x": 0.846519947052002, "y": 0.22371740639209747}, {"x": 0.16240333020687103, "y": 0.22287636995315552}], "text": "Vladimir Karpukhin, Barlas Oğuz, Sewon Min†, Patrick Lewis,\nLedell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih\nFacebook AI *University of Washington #Princeton University\n{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n"}
{"page": 1, "bbox": [{"x": 0.2665080428123474, "y": 0.26829269528388977}, {"x": 0.34205830097198486, "y": 0.26829269528388977}, {"x": 0.34205830097198486, "y": 0.27754414081573486}, {"x": 0.2665080428123474, "y": 0.27754414081573486}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.14872099459171295, "y": 0.29226240515708923}, {"x": 0.4622248709201813, "y": 0.29226240515708923}, {"x": 0.4622248709201813, "y": 0.5285954475402832}, {"x": 0.14872099459171295, "y": 0.5285954475402832}], "text": "Open-domain question answering relies on ef-\nficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented us-\ning dense representations alone, where em-\nbeddings are learned from a small number\nof questions and passages by a simple dual-\nencoder framework. When evaluated on a\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong Lucene-\nBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks.¹\n"}
{"page": 1, "bbox": [{"x": 0.5098155736923218, "y": 0.2674516439437866}, {"x": 0.8863771557807922, "y": 0.2674516439437866}, {"x": 0.8863771557807922, "y": 0.6820858120918274}, {"x": 0.5098155736923218, "y": 0.6820858120918274}], "text": "Retrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords effi-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense, latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n\"Who is the bad guy in lord of the rings?\", which can\nbe answered from the context \"Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy.\" A term-based system would\nhave difficulty retrieving such a context, while\na dense retrieval system would be able to better\nmatch \"bad guy” with “villain\" and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional flexibility to have a task-specific\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefficiently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)).\n"}
{"page": 1, "bbox": [{"x": 0.12135633826255798, "y": 0.5445752739906311}, {"x": 0.25996431708335876, "y": 0.5445752739906311}, {"x": 0.25996431708335876, "y": 0.554247260093689}, {"x": 0.12135633826255798, "y": 0.554247260093689}], "text": "1 Introduction\n"}
{"page": 1, "bbox": [{"x": 0.12016656994819641, "y": 0.5698065757751465}, {"x": 0.490779310464859, "y": 0.5698065757751465}, {"x": 0.490779310464859, "y": 0.8389402627944946}, {"x": 0.12016656994819641, "y": 0.8389402627944946}], "text": "Open-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions us-\ning a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), inter alia), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context retriever first\nselects a small subset of passages where some\nof them contain the answer to the question, and\nthen (2) a machine reader can thoroughly exam-\nine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\nopen-domain QA to machine reading is a very rea-\nsonable strategy, a huge performance degradation\nis often observed in practice², indicating the needs\nof improving retrieval.\n"}
{"page": 1, "bbox": [{"x": 0.5145746469497681, "y": 0.6879730820655823}, {"x": 0.8863771557807922, "y": 0.6879730820655823}, {"x": 0.8863771557807922, "y": 0.910008430480957}, {"x": 0.5145746469497681, "y": 0.910008430480957}], "text": "However, it is generally believed that learn-\ning a good dense vector representation needs a\nlarge number of labeled pairs of question and con-\ntexts. Dense retrieval methods have thus never\nbe shown to outperform TF-IDF/BM25 for open-\ndomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\nobjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then fine-\ntuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\n"}
{"page": 1, "bbox": [{"x": 0.14812611043453217, "y": 0.8490328192710876}, {"x": 0.2671029269695282, "y": 0.8477712273597717}, {"x": 0.2671029269695282, "y": 0.8566021919250488}, {"x": 0.14812611043453217, "y": 0.85786372423172}], "text": "*Equal contribution\n"}
{"page": 1, "bbox": [{"x": 0.12016656994819641, "y": 0.861648440361023}, {"x": 0.4878048896789551, "y": 0.8612279295921326}, {"x": 0.4878048896789551, "y": 0.9083263278007507}, {"x": 0.12016656994819641, "y": 0.9087468385696411}], "text": "The code and trained models have been released at\nhttps://github.com/facebookresearch/DPR.\n²For instance, the exact match score on SQUAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).\n"}
{"page": 1, "bbox": [{"x": 0.4830458164215088, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.9331371188163757}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6769\n"}
{"page": 1, "bbox": [{"x": 0.16299821436405182, "y": 0.9449117183685303}, {"x": 0.8370018005371094, "y": 0.9449117183685303}, {"x": 0.8370018005371094, "y": 0.9676198363304138}, {"x": 0.16299821436405182, "y": 0.9676198363304138}], "text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6769-6781,\nNovember 16-20, 2020. 2020 Association for Computational Linguistics\n"}
{"page": 2, "bbox": [{"x": 0.5151695609092712, "y": 0.07695542275905609}, {"x": 0.8863771557807922, "y": 0.0786375105381012}, {"x": 0.8851873874664307, "y": 0.22413793206214905}, {"x": 0.5139797925949097, "y": 0.22245584428310394}], "text": "the extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more pas-\nsages in the corpus. Assume that our collection\ncontains D documents, d1, d2,...,dD. We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units³ and get M\ntotal passages in our corpus C {P1, P2, ..., PM},\nwhere each passage p; can be viewed as a sequence\n(2). Given a question q,\n"}
{"page": 2, "bbox": [{"x": 0.7477691769599915, "y": 0.18250630795955658}, {"x": 0.7364664077758789, "y": 0.18250630795955658}, {"x": 0.7364664077758789, "y": 0.178721621632576}, {"x": 0.7477691769599915, "y": 0.178721621632576}], "text": "=\n"}
{"page": 2, "bbox": [{"x": 0.5157644152641296, "y": 0.2052144706249237}, {"x": 0.6603212356567383, "y": 0.20437341928482056}, {"x": 0.6603212356567383, "y": 0.2195121943950653}, {"x": 0.5157644152641296, "y": 0.22035323083400726}], "text": "of tokens w(i), w(i),\n"}
{"page": 2, "bbox": [{"x": 0.7781082391738892, "y": 0.23633305728435516}, {"x": 0.7977394461631775, "y": 0.2359125316143036}, {"x": 0.7977394461631775, "y": 0.23801514506340027}, {"x": 0.7781082391738892, "y": 0.23843565583229065}], "text": "...\n"}
{"page": 2, "bbox": [{"x": 0.8132064342498779, "y": 0.23465096950531006}, {"x": 0.8328375816345215, "y": 0.23507149517536163}, {"x": 0.8328375816345215, "y": 0.24179983139038086}, {"x": 0.8132064342498779, "y": 0.24137930572032928}], "text": "We\n"}
{"page": 2, "bbox": [{"x": 0.8054729104042053, "y": 0.2392767071723938}, {"x": 0.8084473609924316, "y": 0.2392767071723938}, {"x": 0.8084473609924316, "y": 0.24264086782932281}, {"x": 0.8054729104042053, "y": 0.24264086782932281}], "text": "\"\n"}
{"page": 2, "bbox": [{"x": 0.7233789563179016, "y": 0.23969721794128418}, {"x": 0.7263533473014832, "y": 0.23969721794128418}, {"x": 0.7263533473014832, "y": 0.24264086782932281}, {"x": 0.7233789563179016, "y": 0.24264086782932281}], "text": ",\n"}
{"page": 2, "bbox": [{"x": 0.5145746469497681, "y": 0.21488645672798157}, {"x": 0.8875669240951538, "y": 0.2161480188369751}, {"x": 0.8857823014259338, "y": 0.5016821026802063}, {"x": 0.5127900242805481, "y": 0.5004205107688904}], "text": "|pi|\nthe task is to find a span ws\n(\ni) (i)\nWS+1\"\"\nfrom\none of the passages p¿ that can answer the question.\nNotice that to cover a wide variety of domains, the\ncorpus size can easily range from millions of docu-\nments (e.g., Wikipedia) to billions (e.g., the Web).\nAs a result, any open-domain QA system needs to\ninclude an efficient retriever component that can se-\nlect a small set of relevant texts, before applying the\nreader to extract the answer (Chen et al., 2017).4\nFormally speaking, a retriever R : (q,C) → CF\nis a function that takes as input a question q and a\ncorpus C and returns a much smaller filter set of\ntexts CFCC, where |Cƒ| = k < |C|. For a fixed\nk, a retriever can be evaluated in isolation on top-k\nretrieval accuracy, which is the fraction of ques-\ntions for which CF contains a span that answers the\nquestion.\n"}
{"page": 2, "bbox": [{"x": 0.11957168579101562, "y": 0.07737594842910767}, {"x": 0.4913741946220398, "y": 0.07737594842910767}, {"x": 0.4913741946220398, "y": 0.7695542573928833}, {"x": 0.11957168579101562, "y": 0.7695542573928833}], "text": "QA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot fine-tuned using pairs of questions and answers,\nthe corresponding representations could be subop-\ntimal.\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), with-\nout additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\nof question and passage pairs. Through a series\nof careful ablation studies, our final solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective compar-\ning all pairs of questions and passages in a batch.\nOur Dense Passage Retriever (DPR) is exception-\nally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\nOur contributions are twofold. First, we demon-\nstrate that with the proper training setup, sim-\nply fine-tuning the question and passage encoders\non existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\nopen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve compara-\nble or better results on multiple QA datasets in the\nopen-retrieval setting, compared to several, much\ncomplicated systems.\n"}
{"page": 2, "bbox": [{"x": 0.5145746469497681, "y": 0.517241358757019}, {"x": 0.8863771557807922, "y": 0.5168208479881287}, {"x": 0.8869720697402954, "y": 0.713624894618988}, {"x": 0.5151695609092712, "y": 0.7140454053878784}], "text": "3 Dense Passage Retriever (DPR)\nWe focus our research in this work on improv-\ning the retrieval component in open-domain QA.\nGiven a collection of M text passages, the goal of\nour dense passage retriever (DPR) is to index all\nthe passages in a low-dimensional and continuous\nspace, such that it can retrieve efficiently the top\nk passages relevant to the input question for the\nreader at run-time. Note that M can be very large\n(e.g., 21 million passages in our experiments, de-\nscribed in Section 4.1) and k is usually small, such\nas 20-100.\n"}
{"page": 2, "bbox": [{"x": 0.5157644152641296, "y": 0.732127845287323}, {"x": 0.5371802449226379, "y": 0.732127845287323}, {"x": 0.5371802449226379, "y": 0.7405382394790649}, {"x": 0.5157644152641296, "y": 0.7405382394790649}], "text": "3.1\n"}
{"page": 2, "bbox": [{"x": 0.5574063062667847, "y": 0.732127845287323}, {"x": 0.630577027797699, "y": 0.7317073345184326}, {"x": 0.630577027797699, "y": 0.7409588098526001}, {"x": 0.5574063062667847, "y": 0.7413793206214905}], "text": "Overview\n"}
{"page": 2, "bbox": [{"x": 0.11897680163383484, "y": 0.7884777188301086}, {"x": 0.25580012798309326, "y": 0.7884777188301086}, {"x": 0.25580012798309326, "y": 0.7998318076133728}, {"x": 0.11897680163383484, "y": 0.7998318076133728}], "text": "2 Background\n"}
{"page": 2, "bbox": [{"x": 0.5157644152641296, "y": 0.7510513067245483}, {"x": 0.8863771557807922, "y": 0.7514718174934387}, {"x": 0.8863771557807922, "y": 0.9087468385696411}, {"x": 0.5157644152641296, "y": 0.9083263278007507}], "text": "Our dense passage retriever (DPR) uses a dense\nencoder Ep() which maps any text passage to a d-\ndimensional real-valued vectors and builds an index\nfor all the M passages that we will use for retrieval.\n3 The ideal size and boundary of a text passage are func-\ntions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing fixed-length passages performs better in both retrieval\nand final QA accuracy, as observed by Wang et al. (2019).\n*Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which retrieves and generates the answers, respectively.\n"}
{"page": 2, "bbox": [{"x": 0.11957168579101562, "y": 0.8158116340637207}, {"x": 0.48958954215049744, "y": 0.8153910636901855}, {"x": 0.48958954215049744, "y": 0.910008430480957}, {"x": 0.11957168579101562, "y": 0.9104289412498474}], "text": "The problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \"Who first voiced Meg on Family\nGuy?\" or \"Where was the 8th Dalai Lama born?\", a\nsystem is required to answer it using a large corpus\nof diversified topics. More specifically, we assume\n"}
{"page": 2, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5205234885215759, "y": 0.9243061542510986}, {"x": 0.5205234885215759, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6770\n"}
{"page": 3, "bbox": [{"x": 0.5157644152641296, "y": 0.07821699231863022}, {"x": 0.8839976191520691, "y": 0.07527334243059158}, {"x": 0.8845925331115723, "y": 0.10386879742145538}, {"x": 0.5163593292236328, "y": 0.10681244730949402}], "text": "larity) than the irrelevant ones, by learning a better\nembedding function.\n"}
{"page": 3, "bbox": [{"x": 0.6680546998977661, "y": 0.1089150533080101}, {"x": 0.6787626147270203, "y": 0.1089150533080101}, {"x": 0.6787626147270203, "y": 0.11564339697360992}, {"x": 0.6680546998977661, "y": 0.11564339697360992}], "text": "+\n"}
{"page": 3, "bbox": [{"x": 0.8001189827919006, "y": 0.11101765930652618}, {"x": 0.8245092034339905, "y": 0.11101765930652618}, {"x": 0.8245092034339905, "y": 0.1232127845287323}, {"x": 0.8001189827919006, "y": 0.1232127845287323}], "text": "m\ni=1\n"}
{"page": 3, "bbox": [{"x": 0.11897680163383484, "y": 0.07779646664857864}, {"x": 0.48839977383613586, "y": 0.07779646664857864}, {"x": 0.48839977383613586, "y": 0.17031118273735046}, {"x": 0.11897680163383484, "y": 0.17031118273735046}], "text": "At run-time, DPR applies a different encoder EQ (•)\nthat maps the input question to a d-dimensional\nvector, and retrieves k passages of which vectors\nare the closest to the question vector. We define\nthe similarity between the question and the passage\nusing the dot product of their vectors:\n"}
{"page": 3, "bbox": [{"x": 0.5157644152641296, "y": 0.10849453508853912}, {"x": 0.8863771557807922, "y": 0.10807400941848755}, {"x": 0.8863771557807922, "y": 0.20395290851593018}, {"x": 0.5157644152641296, "y": 0.20437341928482056}], "text": "Let D = {{qi, Pi¯‚ Pi¸₁‚· · · ‚ Pin)} be the\ntraining data that consists of m instances. Each\ninstance contains one question q; and one relevant\n(positive) passage pit, along with n irrelevant (neg-\native) passages P₁j. We optimize the loss function\nas the negative log likelihood of the positive pas-\n"}
{"page": 3, "bbox": [{"x": 0.20166566967964172, "y": 0.18671151995658875}, {"x": 0.4057108759880066, "y": 0.1875525712966919}, {"x": 0.4057108759880066, "y": 0.20100925862789154}, {"x": 0.20166566967964172, "y": 0.2001682072877884}], "text": "sim(q, p) Eq(q)'Ep(p).\n"}
{"page": 3, "bbox": [{"x": 0.4663890600204468, "y": 0.18881413340568542}, {"x": 0.4866151213645935, "y": 0.18881413340568542}, {"x": 0.4866151213645935, "y": 0.19974768161773682}, {"x": 0.4663890600204468, "y": 0.19974768161773682}], "text": "(1)\n"}
{"page": 3, "bbox": [{"x": 0.2891136109828949, "y": 0.19259881973266602}, {"x": 0.2891136109828949, "y": 0.1963835209608078}, {"x": 0.27781081199645996, "y": 0.1963835209608078}, {"x": 0.27781081199645996, "y": 0.19259881973266602}], "text": "=\n"}
{"page": 3, "bbox": [{"x": 0.5157644152641296, "y": 0.2106812447309494}, {"x": 0.5538370013237, "y": 0.2106812447309494}, {"x": 0.5538370013237, "y": 0.21825063228607178}, {"x": 0.5157644152641296, "y": 0.21825063228607178}], "text": "sage:\n"}
{"page": 3, "bbox": [{"x": 0.8607971668243408, "y": 0.23549200594425201}, {"x": 0.8828078508377075, "y": 0.23549200594425201}, {"x": 0.8828078508377075, "y": 0.24600504338741302}, {"x": 0.8607971668243408, "y": 0.24600504338741302}], "text": "(2)\n"}
{"page": 3, "bbox": [{"x": 0.5847709774971008, "y": 0.23296888172626495}, {"x": 0.7608566284179688, "y": 0.23549200594425201}, {"x": 0.7602617740631104, "y": 0.2497897446155548}, {"x": 0.5841760635375977, "y": 0.24726660549640656}], "text": "L(qi, P†‚P₁¸¹¨··‚ Pi¸n)\n"}
{"page": 3, "bbox": [{"x": 0.7043426632881165, "y": 0.25651809573173523}, {"x": 0.7787031531333923, "y": 0.2552565038204193}, {"x": 0.7792980074882507, "y": 0.26703113317489624}, {"x": 0.7049375176429749, "y": 0.26829269528388977}], "text": "esim (qi,P)\n"}
{"page": 3, "bbox": [{"x": 0.5853658318519592, "y": 0.26703113317489624}, {"x": 0.6270077228546143, "y": 0.26871320605278015}, {"x": 0.6258179545402527, "y": 0.27964675426483154}, {"x": 0.5841760635375977, "y": 0.27796468138694763}], "text": "― log\n"}
{"page": 3, "bbox": [{"x": 0.6299821734428406, "y": 0.278385192155838}, {"x": 0.8530636429786682, "y": 0.2767031192779541}, {"x": 0.8530636429786682, "y": 0.2931034564971924}, {"x": 0.6299821734428406, "y": 0.2947855293750763}], "text": "esim(qi,p) +1 esim(qi‚Pi,j)\n"}
{"page": 3, "bbox": [{"x": 0.11957168579101562, "y": 0.21867115795612335}, {"x": 0.4913741946220398, "y": 0.21867115795612335}, {"x": 0.490779310464859, "y": 0.7817493677139282}, {"x": 0.11897680163383484, "y": 0.7817493677139282}], "text": "Although more expressive model forms for measur-\ning the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the represen-\ntations of the collection of passages can be pre-\ncomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis dis-\ntance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nfinds other similarity functions perform compara-\nbly (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders.\nEncoders Although in principle the question and\npassage encoders can be implemented by any neu-\nral networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, un-\ncased) and take the representation at the [CLS]\ntoken as the output, so d: == 768.\nInference During inference time, we apply the\npassage encoder Ep to all the passages and index\nthem using FAISS (Johnson et al., 2017) offline.\nFAISS is an extremely efficient, open-source li-\nbrary for similarity search and clustering of dense\nvectors, which can easily be applied to billions of\nvectors. Given a question q at run-time, we derive\nits embedding vq EQ(q) and retrieve the top k\npassages with embeddings closest to vq.\n"}
{"page": 3, "bbox": [{"x": 0.5145746469497681, "y": 0.3090832531452179}, {"x": 0.8857823014259338, "y": 0.3086627423763275}, {"x": 0.8869720697402954, "y": 0.7039529085159302}, {"x": 0.5157644152641296, "y": 0.7043734192848206}], "text": "Positive and negative passages For retrieval\nproblems, it is often the case that positive examples\nare available explicitly, while negative examples\nneed to be selected from an extremely large pool.\nFor instance, passages relevant to a question may\nbe given in a QA dataset, or can be found using the\nanswer. All other passages in the collection, while\nnot specified explicitly, can be viewed as irrelevant\nby default. In practice, how to select negative ex-\namples is often overlooked but could be decisive\nfor learning a high-quality encoder. We consider\nthree different types of negatives: (1) Random: any\nrandom passage from the corpus; (2) BM25: top\npassages returned by BM25 which don't contain\nthe answer but match most question tokens; (3)\nGold: positive passages paired with other questions\nwhich appear in the training set. We will discuss the\nimpact of different types of negative passages and\ntraining schemes in Section 5.2. Our best model\nuses gold passages from the same mini-batch and\none BM25 negative passage. In particular, re-using\ngold passages from the same batch as negatives\ncan make the computation efficient while achiev-\ning great performance. We discuss this approach\nbelow.\n"}
{"page": 3, "bbox": [{"x": 0.2540154755115509, "y": 0.7560975551605225}, {"x": 0.26412850618362427, "y": 0.7560975551605225}, {"x": 0.26412850618362427, "y": 0.7598822712898254}, {"x": 0.2540154755115509, "y": 0.7598822712898254}], "text": "=\n"}
{"page": 3, "bbox": [{"x": 0.1207614541053772, "y": 0.7943649888038635}, {"x": 0.22962522506713867, "y": 0.7952060699462891}, {"x": 0.22962522506713867, "y": 0.8057190775871277}, {"x": 0.1207614541053772, "y": 0.8048780560493469}], "text": "3.2 Training\n"}
{"page": 3, "bbox": [{"x": 0.5145746469497681, "y": 0.7199327349662781}, {"x": 0.8863771557807922, "y": 0.7203532457351685}, {"x": 0.8863771557807922, "y": 0.9087468385696411}, {"x": 0.5145746469497681, "y": 0.9083263278007507}], "text": "In-batch negatives Assume that we have B\nquestions in a mini-batch and each one is asso-\nciated with a relevant passage. Let Q and P be the\n(Bxd) matrix of question and passage embeddings\nin a batch of size B. S = QPT is a (B × B) ma-\ntrix of similarity scores, where each row of which\ncorresponds to a question, paired with B passages.\nIn this way, we reuse computation and effectively\ntrain on B² (qi, pj) question/passage pairs in each\nbatch. Any (qi, pj) pair is a positive example when\ni = j, and negative otherwise. This creates B train-\ning instances in each batch, where there are B 1\n"}
{"page": 3, "bbox": [{"x": 0.12016656994819641, "y": 0.8170731663703918}, {"x": 0.490779310464859, "y": 0.8170731663703918}, {"x": 0.490779310464859, "y": 0.9083263278007507}, {"x": 0.12016656994819641, "y": 0.9083263278007507}], "text": "Training the encoders so that the dot-product sim-\nilarity (Eq. (1)) becomes a good ranking function\nfor retrieval is essentially a metric learning prob-\nlem (Kulis, 2013). The goal is to create a vector\nspace such that relevant pairs of questions and pas-\nsages will have smaller distance (i.e., higher simi-\n"}
{"page": 3, "bbox": [{"x": 0.4836407005786896, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.9331371188163757}, {"x": 0.4836407005786896, "y": 0.9331371188163757}], "text": "6771\n"}
{"page": 4, "bbox": [{"x": 0.6894705295562744, "y": 0.0803195983171463}, {"x": 0.7245687246322632, "y": 0.0803195983171463}, {"x": 0.7245687246322632, "y": 0.0874684602022171}, {"x": 0.6894705295562744, "y": 0.0874684602022171}], "text": "Train\n"}
{"page": 4, "bbox": [{"x": 0.7876263856887817, "y": 0.08074011653661728}, {"x": 0.8114217519760132, "y": 0.08074011653661728}, {"x": 0.8114217519760132, "y": 0.08704794198274612}, {"x": 0.7876263856887817, "y": 0.08704794198274612}], "text": "Dev\n"}
{"page": 4, "bbox": [{"x": 0.8477097153663635, "y": 0.0803195983171463}, {"x": 0.8732897043228149, "y": 0.0803195983171463}, {"x": 0.8732897043228149, "y": 0.08788898587226868}, {"x": 0.8477097153663635, "y": 0.08788898587226868}], "text": "Test\n"}
{"page": 4, "bbox": [{"x": 0.5258774757385254, "y": 0.08074011653661728}, {"x": 0.5752528309822083, "y": 0.08074011653661728}, {"x": 0.5752528309822083, "y": 0.08788898587226868}, {"x": 0.5258774757385254, "y": 0.08788898587226868}], "text": "Dataset\n"}
{"page": 4, "bbox": [{"x": 0.6549673080444336, "y": 0.09882254153490067}, {"x": 0.757287323474884, "y": 0.09798149764537811}, {"x": 0.757287323474884, "y": 0.10555088520050049}, {"x": 0.6549673080444336, "y": 0.10639192909002304}], "text": "79,168 58,880\n"}
{"page": 4, "bbox": [{"x": 0.8393813371658325, "y": 0.09882254153490067}, {"x": 0.8732897043228149, "y": 0.09840201586484909}, {"x": 0.8732897043228149, "y": 0.10597140341997147}, {"x": 0.8393813371658325, "y": 0.10639192909002304}], "text": "3,610\n"}
{"page": 4, "bbox": [{"x": 0.7781082391738892, "y": 0.09924305975437164}, {"x": 0.8120166659355164, "y": 0.09924305975437164}, {"x": 0.8120166659355164, "y": 0.10639192909002304}, {"x": 0.7781082391738892, "y": 0.10639192909002304}], "text": "8,757\n"}
{"page": 4, "bbox": [{"x": 0.655562162399292, "y": 0.1105971410870552}, {"x": 0.8120166659355164, "y": 0.11101765930652618}, {"x": 0.8120166659355164, "y": 0.11858704686164856}, {"x": 0.655562162399292, "y": 0.11816652864217758}], "text": "78,785 60,413 8,837\n"}
{"page": 4, "bbox": [{"x": 0.8334324955940247, "y": 0.11101765930652618}, {"x": 0.8732897043228149, "y": 0.11101765930652618}, {"x": 0.8732897043228149, "y": 0.11816652864217758}, {"x": 0.8334324955940247, "y": 0.11816652864217758}], "text": "11,313\n"}
{"page": 4, "bbox": [{"x": 0.7900059223175049, "y": 0.12237174063920975}, {"x": 0.8108268976211548, "y": 0.12237174063920975}, {"x": 0.8108268976211548, "y": 0.12910008430480957}, {"x": 0.7900059223175049, "y": 0.12910008430480957}], "text": "361\n"}
{"page": 4, "bbox": [{"x": 0.8387864232063293, "y": 0.12237174063920975}, {"x": 0.8726948499679565, "y": 0.12195122241973877}, {"x": 0.8726948499679565, "y": 0.12910008430480957}, {"x": 0.8387864232063293, "y": 0.12952060997486115}], "text": "2,032\n"}
{"page": 4, "bbox": [{"x": 0.6627007722854614, "y": 0.12237174063920975}, {"x": 0.7578822374343872, "y": 0.12195122241973877}, {"x": 0.7578822374343872, "y": 0.12952060997486115}, {"x": 0.6627007722854614, "y": 0.12994112074375153}], "text": "3,417 2,474\n"}
{"page": 4, "bbox": [{"x": 0.5252825617790222, "y": 0.09798149764537811}, {"x": 0.6353361010551453, "y": 0.09798149764537811}, {"x": 0.6353361010551453, "y": 0.15433137118816376}, {"x": 0.5252825617790222, "y": 0.15433137118816376}], "text": "Natural Questions\nTriviaQA\nWebQuestions\nCuratedTREC\nSQUAD\n"}
{"page": 4, "bbox": [{"x": 0.12016656994819641, "y": 0.07821699231863022}, {"x": 0.48839977383613586, "y": 0.0786375105381012}, {"x": 0.48839977383613586, "y": 0.18713204562664032}, {"x": 0.12016656994819641, "y": 0.18671151995658875}], "text": "negative passages for each question.\nThe trick of in-batch negatives has been used in\nthe full batch setting (Yih et al., 2011) and more\nrecently for mini-batch (Henderson et al., 2017;\nGillick et al., 2019). It has been shown to be an\neffective strategy for learning a dual-encoder model\nthat boosts the number of training examples.\n"}
{"page": 4, "bbox": [{"x": 0.7906008362770081, "y": 0.13414634764194489}, {"x": 0.8108268976211548, "y": 0.13414634764194489}, {"x": 0.8108268976211548, "y": 0.1408746838569641}, {"x": 0.7906008362770081, "y": 0.1408746838569641}], "text": "133\n"}
{"page": 4, "bbox": [{"x": 0.8506841063499451, "y": 0.13456685841083527}, {"x": 0.8732897043228149, "y": 0.13456685841083527}, {"x": 0.8732897043228149, "y": 0.14129520952701569}, {"x": 0.8506841063499451, "y": 0.14129520952701569}], "text": "694\n"}
{"page": 4, "bbox": [{"x": 0.655562162399292, "y": 0.1337258219718933}, {"x": 0.7560975551605225, "y": 0.13330529630184174}, {"x": 0.7560975551605225, "y": 0.1534903347492218}, {"x": 0.655562162399292, "y": 0.15391084551811218}], "text": "1,353 1,125\n78,713\n"}
{"page": 4, "bbox": [{"x": 0.7168352007865906, "y": 0.14592094719409943}, {"x": 0.8114217519760132, "y": 0.1463414579629898}, {"x": 0.8114217519760132, "y": 0.1534903347492218}, {"x": 0.7168352007865906, "y": 0.15306980907917023}], "text": "70.096 8,886\n"}
{"page": 4, "bbox": [{"x": 0.8334324955940247, "y": 0.14592094719409943}, {"x": 0.8726948499679565, "y": 0.1463414579629898}, {"x": 0.8726948499679565, "y": 0.1534903347492218}, {"x": 0.8334324955940247, "y": 0.15306980907917023}], "text": "10,570\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.17283432185649872}, {"x": 0.8851873874664307, "y": 0.1732548326253891}, {"x": 0.8851873874664307, "y": 0.22708158195018768}, {"x": 0.5151695609092712, "y": 0.2266610562801361}], "text": "Table 1: Number of questions in each QA dataset. The\ntwo columns of Train denote the original training ex-\namples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n"}
{"page": 4, "bbox": [{"x": 0.11897680163383484, "y": 0.20395290851593018}, {"x": 0.3224271237850189, "y": 0.20437341928482056}, {"x": 0.3224271237850189, "y": 0.21698907017707825}, {"x": 0.11897680163383484, "y": 0.21656854450702667}], "text": "4 Experimental Setup\n"}
{"page": 4, "bbox": [{"x": 0.1207614541053772, "y": 0.23170731961727142}, {"x": 0.48899465799331665, "y": 0.23170731961727142}, {"x": 0.48899465799331665, "y": 0.25988224148750305}, {"x": 0.1207614541053772, "y": 0.25988224148750305}], "text": "In this section, we describe the data we used for\nexperiments and the basic setup.\n"}
{"page": 4, "bbox": [{"x": 0.11957168579101562, "y": 0.2767031192779541}, {"x": 0.1427721530199051, "y": 0.2767031192779541}, {"x": 0.1427721530199051, "y": 0.28511354327201843}, {"x": 0.11957168579101562, "y": 0.28511354327201843}], "text": "4.1\n"}
{"page": 4, "bbox": [{"x": 0.5145746469497681, "y": 0.25651809573173523}, {"x": 0.8869720697402954, "y": 0.257359117269516}, {"x": 0.8857823014259338, "y": 0.46131202578544617}, {"x": 0.5133848786354065, "y": 0.460470974445343}], "text": "as well as various Web sources and is intended for\nopen-domain QA from unstructured corpora.\nSQUAD v1.1 (Rajpurkar et al., 2016) is a popu-\nlar benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia para-\ngraph, and asked to write questions that could be\nanswered from the given text. Although SQUAD\nhas been used previously for open-domain QA re-\nsearch, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n"}
{"page": 4, "bbox": [{"x": 0.11897680163383484, "y": 0.27628257870674133}, {"x": 0.4901844263076782, "y": 0.27628257870674133}, {"x": 0.4901844263076782, "y": 0.5336416959762573}, {"x": 0.11897680163383484, "y": 0.5336416959762573}], "text": "Wikipedia Data Pre-processing\nFollowing (Lee et al., 2019), we use the English\nWikipedia dump from Dec. 20, 2018 as the source\ndocuments for answering questions. We first apply\nthe pre-processing code released in DrQA (Chen\net al., 2017) to extract the clean, text-portion of\narticles from the Wikipedia dump. This step re-\nmoves semi-structured data, such as tables, info-\nboxes, lists, as well as the disambiguation pages.\nWe then split each article into multiple, disjoint text\nblocks of 100 words as passages, serving as our\nbasic retrieval units, following (Wang et al., 2019),\nwhich results in 21,015,324 passages in the end.5\nEach passage is also prepended with the title of the\nWikipedia article where the passage is from, along\nwith an [SEP] token.\n"}
{"page": 4, "bbox": [{"x": 0.12016656994819641, "y": 0.5517241358757019}, {"x": 0.14336703717708588, "y": 0.5517241358757019}, {"x": 0.14336703717708588, "y": 0.5601345896720886}, {"x": 0.12016656994819641, "y": 0.5601345896720886}], "text": "4.2\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.4768713116645813}, {"x": 0.8869720697402954, "y": 0.47729185223579407}, {"x": 0.8863771557807922, "y": 0.7451639771461487}, {"x": 0.5145746469497681, "y": 0.7447434663772583}], "text": "Selection of positive passages Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA6, we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the ques-\ntion will be discarded. For SQUAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the num-\nber of questions in training/dev/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n"}
{"page": 4, "bbox": [{"x": 0.6311719417572021, "y": 0.653069794178009}, {"x": 0.6383105516433716, "y": 0.653069794178009}, {"x": 0.6383105516433716, "y": 0.6589571237564087}, {"x": 0.6311719417572021, "y": 0.6589571237564087}], "text": "7\n"}
{"page": 4, "bbox": [{"x": 0.11838191747665405, "y": 0.550462543964386}, {"x": 0.4901844263076782, "y": 0.5500420331954956}, {"x": 0.490779310464859, "y": 0.9091673493385315}, {"x": 0.11897680163383484, "y": 0.9095878601074219}], "text": "Question Answering Datasets\nWe use the same five QA datasets and train-\ning/dev/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\nNatural Questions (NQ) (Kwiatkowski et al.,\n2019) was designed for end-to-end question an-\nswering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\nTriviaQA (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\nWebQuestions (WQ) (Berant et al., 2013) consists\nof questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\nCurated TREC (TREC) (Baudiš and Šedivỳ,\n2015) sources questions from TREC QA tracks\n\"However, Wang et al. (2019) also propose splitting docu-\nments into overlapping passages, which we do not find advan-\ntageous compared to the non-overlapping version.\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.7645080089569092}, {"x": 0.8185603618621826, "y": 0.7632464170455933}, {"x": 0.8185603618621826, "y": 0.7771236300468445}, {"x": 0.5151695609092712, "y": 0.7783852219581604}], "text": "5 Experiments: Passage Retrieval\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.7918418645858765}, {"x": 0.8851873874664307, "y": 0.7914213538169861}, {"x": 0.8851873874664307, "y": 0.9079058170318604}, {"x": 0.5151695609092712, "y": 0.9083263278007507}], "text": "In this section, we evaluate the retrieval perfor-\nmance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Ap-\npendix A.\n"}
{"page": 4, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5193337202072144, "y": 0.9243061542510986}, {"x": 0.5193337202072144, "y": 0.9331371188163757}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6772\n"}
{"page": 5, "bbox": [{"x": 0.1421772688627243, "y": 0.07947855442762375}, {"x": 0.27424153685569763, "y": 0.0786375105381012}, {"x": 0.27424153685569763, "y": 0.08830950409173965}, {"x": 0.1421772688627243, "y": 0.08915054798126221}], "text": "Training Retriever\n"}
{"page": 5, "bbox": [{"x": 0.7037477493286133, "y": 0.0803195983171463}, {"x": 0.7555027008056641, "y": 0.07905802875757217}, {"x": 0.7560975551605225, "y": 0.08873002231121063}, {"x": 0.7043426632881165, "y": 0.08999159187078476}], "text": "Top-100\n"}
{"page": 5, "bbox": [{"x": 0.4259369373321533, "y": 0.0803195983171463}, {"x": 0.4705532491207123, "y": 0.0803195983171463}, {"x": 0.4705532491207123, "y": 0.08957106620073318}, {"x": 0.4259369373321533, "y": 0.08957106620073318}], "text": "Top-20\n"}
{"page": 5, "bbox": [{"x": 0.31766805052757263, "y": 0.09209419786930084}, {"x": 0.3396787643432617, "y": 0.09209419786930084}, {"x": 0.3396787643432617, "y": 0.1000841036438942}, {"x": 0.31766805052757263, "y": 0.1000841036438942}], "text": "NQ\n"}
{"page": 5, "bbox": [{"x": 0.3587150573730469, "y": 0.09167367219924927}, {"x": 0.8607971668243408, "y": 0.09167367219924927}, {"x": 0.8607971668243408, "y": 0.10092514753341675}, {"x": 0.3587150573730469, "y": 0.10092514753341675}], "text": "TriviaQA WQ TREC SQUAD NQ TriviaQA WQ TREC SQUAD\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.11101765930652618}, {"x": 0.5675193071365356, "y": 0.11101765930652618}, {"x": 0.5675193071365356, "y": 0.11690495908260345}, {"x": 0.5425342321395874, "y": 0.11690495908260345}], "text": "68.8\n"}
{"page": 5, "bbox": [{"x": 0.14158238470554352, "y": 0.11017661541700363}, {"x": 0.17370612919330597, "y": 0.1105971410870552}, {"x": 0.17370612919330597, "y": 0.11774600297212601}, {"x": 0.14158238470554352, "y": 0.11732548475265503}], "text": "None\n"}
{"page": 5, "bbox": [{"x": 0.43248066306114197, "y": 0.10975609719753265}, {"x": 0.5080309510231018, "y": 0.1105971410870552}, {"x": 0.5080309510231018, "y": 0.11816652864217758}, {"x": 0.43248066306114197, "y": 0.11732548475265503}], "text": "55.0 70.9\n"}
{"page": 5, "bbox": [{"x": 0.5978584289550781, "y": 0.1105971410870552}, {"x": 0.6228435635566711, "y": 0.1105971410870552}, {"x": 0.6228435635566711, "y": 0.11732548475265503}, {"x": 0.5978584289550781, "y": 0.11732548475265503}], "text": "73.7\n"}
{"page": 5, "bbox": [{"x": 0.8245092034339905, "y": 0.1105971410870552}, {"x": 0.8494943380355835, "y": 0.1105971410870552}, {"x": 0.8494943380355835, "y": 0.11732548475265503}, {"x": 0.8245092034339905, "y": 0.11732548475265503}], "text": "80.0\n"}
{"page": 5, "bbox": [{"x": 0.3152885138988495, "y": 0.11101765930652618}, {"x": 0.3408685326576233, "y": 0.11101765930652618}, {"x": 0.3408685326576233, "y": 0.11732548475265503}, {"x": 0.3152885138988495, "y": 0.11732548475265503}], "text": "59.1\n"}
{"page": 5, "bbox": [{"x": 0.3741820454597473, "y": 0.11101765930652618}, {"x": 0.4003569185733795, "y": 0.11101765930652618}, {"x": 0.4003569185733795, "y": 0.11732548475265503}, {"x": 0.3741820454597473, "y": 0.11732548475265503}], "text": "66.9\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.1105971410870552}, {"x": 0.682331919670105, "y": 0.1105971410870552}, {"x": 0.682331919670105, "y": 0.11774600297212601}, {"x": 0.6561570763587952, "y": 0.11774600297212601}], "text": "76.7\n"}
{"page": 5, "bbox": [{"x": 0.7150505781173706, "y": 0.1105971410870552}, {"x": 0.7894110679626465, "y": 0.11017661541700363}, {"x": 0.7894110679626465, "y": 0.11774600297212601}, {"x": 0.7150505781173706, "y": 0.11816652864217758}], "text": "71.1 84.1\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.11101765930652618}, {"x": 0.25163593888282776, "y": 0.11101765930652618}, {"x": 0.25163593888282776, "y": 0.11774600297212601}, {"x": 0.21475312113761902, "y": 0.11774600297212601}], "text": "BM25\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.128679558634758}, {"x": 0.24390244483947754, "y": 0.12910008430480957}, {"x": 0.24390244483947754, "y": 0.13624894618988037}, {"x": 0.21475312113761902, "y": 0.1358284205198288}], "text": "DPR\n"}
{"page": 5, "bbox": [{"x": 0.655562162399292, "y": 0.12910008430480957}, {"x": 0.6829268336296082, "y": 0.12910008430480957}, {"x": 0.6829268336296082, "y": 0.1358284205198288}, {"x": 0.655562162399292, "y": 0.1358284205198288}], "text": "85.0\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.12825904786586761}, {"x": 0.6234384179115295, "y": 0.12952060997486115}, {"x": 0.6234384179115295, "y": 0.13666947185993195}, {"x": 0.5425342321395874, "y": 0.13540790975093842}], "text": "63.2 85.4\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.12952060997486115}, {"x": 0.34205830097198486, "y": 0.12952060997486115}, {"x": 0.34205830097198486, "y": 0.1358284205198288}, {"x": 0.3158833980560303, "y": 0.1358284205198288}], "text": "78.4\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.12910008430480957}, {"x": 0.5074360370635986, "y": 0.12910008430480957}, {"x": 0.5074360370635986, "y": 0.13624894618988037}, {"x": 0.43307554721832275, "y": 0.13624894618988037}], "text": "73.2 79.8\n"}
{"page": 5, "bbox": [{"x": 0.3741820454597473, "y": 0.12952060997486115}, {"x": 0.4003569185733795, "y": 0.12952060997486115}, {"x": 0.4003569185733795, "y": 0.13624894618988037}, {"x": 0.3741820454597473, "y": 0.13624894618988037}], "text": "79.4\n"}
{"page": 5, "bbox": [{"x": 0.8239143490791321, "y": 0.12952060997486115}, {"x": 0.8506841063499451, "y": 0.12952060997486115}, {"x": 0.8506841063499451, "y": 0.13624894618988037}, {"x": 0.8239143490791321, "y": 0.13624894618988037}], "text": "77.2\n"}
{"page": 5, "bbox": [{"x": 0.7144556641578674, "y": 0.12952060997486115}, {"x": 0.7894110679626465, "y": 0.12994112074375153}, {"x": 0.7894110679626465, "y": 0.13666947185993195}, {"x": 0.7144556641578674, "y": 0.13624894618988037}], "text": "81.4 89.1\n"}
{"page": 5, "bbox": [{"x": 0.14158238470554352, "y": 0.13498738408088684}, {"x": 0.18024985492229462, "y": 0.13498738408088684}, {"x": 0.18024985492229462, "y": 0.14381833374500275}, {"x": 0.14158238470554352, "y": 0.14381833374500275}], "text": "Single\n"}
{"page": 5, "bbox": [{"x": 0.7150505781173706, "y": 0.14045415818691254}, {"x": 0.7894110679626465, "y": 0.14045415818691254}, {"x": 0.7894110679626465, "y": 0.1480235457420349}, {"x": 0.7150505781173706, "y": 0.1480235457420349}], "text": "80.5 92.7\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.14045415818691254}, {"x": 0.6817370653152466, "y": 0.1408746838569641}, {"x": 0.6817370653152466, "y": 0.1480235457420349}, {"x": 0.6561570763587952, "y": 0.14760303497314453}], "text": "84.5\n"}
{"page": 5, "bbox": [{"x": 0.8239143490791321, "y": 0.14045415818691254}, {"x": 0.8500892519950867, "y": 0.1408746838569641}, {"x": 0.8500892519950867, "y": 0.1480235457420349}, {"x": 0.8239143490791321, "y": 0.14760303497314453}], "text": "81.3\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.14003364741802216}, {"x": 0.2980368733406067, "y": 0.1408746838569641}, {"x": 0.2980368733406067, "y": 0.1484440714120865}, {"x": 0.21475312113761902, "y": 0.14760303497314453}], "text": "BM25 + DPR\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.14003364741802216}, {"x": 0.5086258053779602, "y": 0.1408746838569641}, {"x": 0.5086258053779602, "y": 0.1484440714120865}, {"x": 0.43307554721832275, "y": 0.14760303497314453}], "text": "71.0 85.2\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.1408746838569641}, {"x": 0.34205830097198486, "y": 0.14129520952701569}, {"x": 0.34205830097198486, "y": 0.1480235457420349}, {"x": 0.3158833980560303, "y": 0.14760303497314453}], "text": "76.6\n"}
{"page": 5, "bbox": [{"x": 0.3747769296169281, "y": 0.14129520952701569}, {"x": 0.39916715025901794, "y": 0.14129520952701569}, {"x": 0.39916715025901794, "y": 0.14760303497314453}, {"x": 0.3747769296169281, "y": 0.14760303497314453}], "text": "79.8\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.14129520952701569}, {"x": 0.5687090754508972, "y": 0.14129520952701569}, {"x": 0.5687090754508972, "y": 0.14760303497314453}, {"x": 0.5425342321395874, "y": 0.14760303497314453}], "text": "71.5\n"}
{"page": 5, "bbox": [{"x": 0.597263514995575, "y": 0.14129520952701569}, {"x": 0.6234384179115295, "y": 0.14129520952701569}, {"x": 0.6234384179115295, "y": 0.14760303497314453}, {"x": 0.597263514995575, "y": 0.14760303497314453}], "text": "83.8\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.1589571088552475}, {"x": 0.3408685326576233, "y": 0.1589571088552475}, {"x": 0.3408685326576233, "y": 0.16568544507026672}, {"x": 0.3158833980560303, "y": 0.16568544507026672}], "text": "79.4\n"}
{"page": 5, "bbox": [{"x": 0.3747769296169281, "y": 0.15853658318519592}, {"x": 0.39976203441619873, "y": 0.1589571088552475}, {"x": 0.39976203441619873, "y": 0.1661059707403183}, {"x": 0.3747769296169281, "y": 0.16568544507026672}], "text": "78.8\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.15811605751514435}, {"x": 0.5074360370635986, "y": 0.1589571088552475}, {"x": 0.5074360370635986, "y": 0.16652649641036987}, {"x": 0.43307554721832275, "y": 0.16568544507026672}], "text": "75.0 89.1\n"}
{"page": 5, "bbox": [{"x": 0.5966686606407166, "y": 0.1589571088552475}, {"x": 0.6228435635566711, "y": 0.1589571088552475}, {"x": 0.6228435635566711, "y": 0.16568544507026672}, {"x": 0.5966686606407166, "y": 0.16568544507026672}], "text": "86.0\n"}
{"page": 5, "bbox": [{"x": 0.8245092034339905, "y": 0.15811605751514435}, {"x": 0.8512790203094482, "y": 0.1589571088552475}, {"x": 0.8506841063499451, "y": 0.16652649641036987}, {"x": 0.8239143490791321, "y": 0.16568544507026672}], "text": "67.6\n"}
{"page": 5, "bbox": [{"x": 0.21415823698043823, "y": 0.15937763452529907}, {"x": 0.24330756068229675, "y": 0.15937763452529907}, {"x": 0.24330756068229675, "y": 0.16568544507026672}, {"x": 0.21415823698043823, "y": 0.16568544507026672}], "text": "DPR\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.15937763452529907}, {"x": 0.5687090754508972, "y": 0.15937763452529907}, {"x": 0.5687090754508972, "y": 0.16568544507026672}, {"x": 0.5425342321395874, "y": 0.16568544507026672}], "text": "51.6\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.15937763452529907}, {"x": 0.6817370653152466, "y": 0.15937763452529907}, {"x": 0.6817370653152466, "y": 0.16568544507026672}, {"x": 0.6561570763587952, "y": 0.16568544507026672}], "text": "84.7\n"}
{"page": 5, "bbox": [{"x": 0.7144556641578674, "y": 0.1589571088552475}, {"x": 0.7882212996482849, "y": 0.15937763452529907}, {"x": 0.7882212996482849, "y": 0.16652649641036987}, {"x": 0.7144556641578674, "y": 0.1661059707403183}], "text": "82.9 93.9\n"}
{"page": 5, "bbox": [{"x": 0.14158238470554352, "y": 0.16484440863132477}, {"x": 0.17430101335048676, "y": 0.1644238829612732}, {"x": 0.17430101335048676, "y": 0.171572744846344}, {"x": 0.14158238470554352, "y": 0.17199327051639557}], "text": "Multi\n"}
{"page": 5, "bbox": [{"x": 0.7150505781173706, "y": 0.17073170840740204}, {"x": 0.7894110679626465, "y": 0.17073170840740204}, {"x": 0.7894110679626465, "y": 0.17746004462242126}, {"x": 0.7150505781173706, "y": 0.17746004462242126}], "text": "82.3 94.1\n"}
{"page": 5, "bbox": [{"x": 0.3741820454597473, "y": 0.17073170840740204}, {"x": 0.39916715025901794, "y": 0.1711522340774536}, {"x": 0.39916715025901794, "y": 0.17746004462242126}, {"x": 0.3741820454597473, "y": 0.17703953385353088}], "text": "79.9\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.1711522340774536}, {"x": 0.3408685326576233, "y": 0.1711522340774536}, {"x": 0.3408685326576233, "y": 0.17746004462242126}, {"x": 0.3158833980560303, "y": 0.17746004462242126}], "text": "78.0\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.1711522340774536}, {"x": 0.5687090754508972, "y": 0.1711522340774536}, {"x": 0.5687090754508972, "y": 0.17746004462242126}, {"x": 0.5425342321395874, "y": 0.17746004462242126}], "text": "66.2\n"}
{"page": 5, "bbox": [{"x": 0.5966686606407166, "y": 0.1711522340774536}, {"x": 0.6228435635566711, "y": 0.1711522340774536}, {"x": 0.6228435635566711, "y": 0.17746004462242126}, {"x": 0.5966686606407166, "y": 0.17746004462242126}], "text": "83.9\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.1711522340774536}, {"x": 0.682331919670105, "y": 0.1711522340774536}, {"x": 0.682331919670105, "y": 0.17746004462242126}, {"x": 0.6561570763587952, "y": 0.17746004462242126}], "text": "84.4\n"}
{"page": 5, "bbox": [{"x": 0.8245092034339905, "y": 0.1711522340774536}, {"x": 0.8494943380355835, "y": 0.1711522340774536}, {"x": 0.8494943380355835, "y": 0.17746004462242126}, {"x": 0.8245092034339905, "y": 0.17746004462242126}], "text": "78.6\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.1711522340774536}, {"x": 0.2980368733406067, "y": 0.1711522340774536}, {"x": 0.2980368733406067, "y": 0.17788057029247284}, {"x": 0.21475312113761902, "y": 0.17788057029247284}], "text": "BM25 + DPR\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.1711522340774536}, {"x": 0.5068411827087402, "y": 0.1711522340774536}, {"x": 0.5068411827087402, "y": 0.17788057029247284}, {"x": 0.43307554721832275, "y": 0.17788057029247284}], "text": "74.7 88.5\n"}
{"page": 5, "bbox": [{"x": 0.12016656994819641, "y": 0.1984861195087433}, {"x": 0.8828078508377075, "y": 0.1984861195087433}, {"x": 0.8828078508377075, "y": 0.2375946193933487}, {"x": 0.12016656994819641, "y": 0.2375946193933487}], "text": "Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved\npassages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQUAD). See text for more details.\n"}
{"page": 5, "bbox": [{"x": 0.5466983914375305, "y": 0.2674516439437866}, {"x": 0.5585960745811462, "y": 0.2674516439437866}, {"x": 0.5585960745811462, "y": 0.27207738161087036}, {"x": 0.5466983914375305, "y": 0.27207738161087036}], "text": "90\n"}
{"page": 5, "bbox": [{"x": 0.5472933053970337, "y": 0.29899075627326965}, {"x": 0.5580011606216431, "y": 0.29899075627326965}, {"x": 0.5580011606216431, "y": 0.3036164939403534}, {"x": 0.5472933053970337, "y": 0.3036164939403534}], "text": "80\n"}
{"page": 5, "bbox": [{"x": 0.5300416350364685, "y": 0.3944491147994995}, {"x": 0.5306365489959717, "y": 0.3019343912601471}, {"x": 0.543724000453949, "y": 0.3019343912601471}, {"x": 0.5431290864944458, "y": 0.3944491147994995}], "text": "Top-k accuracy (%)\n"}
{"page": 5, "bbox": [{"x": 0.5615704655647278, "y": 0.3603868782520294}, {"x": 0.5615704655647278, "y": 0.3696383535861969}, {"x": 0.5466983914375305, "y": 0.3696383535861969}, {"x": 0.5466983914375305, "y": 0.3603868782520294}], "text": "8\n"}
{"page": 5, "bbox": [{"x": 0.7638310790061951, "y": 0.357443243265152}, {"x": 0.8239143490791321, "y": 0.3570227026939392}, {"x": 0.8239143490791321, "y": 0.3751051425933838}, {"x": 0.7638310790061951, "y": 0.37552565336227417}], "text": "BM25\n# Train: 1k\n"}
{"page": 5, "bbox": [{"x": 0.7644259333610535, "y": 0.3801513910293579}, {"x": 0.8310529589653015, "y": 0.3805719017982483}, {"x": 0.8310529589653015, "y": 0.38645920157432556}, {"x": 0.7644259333610535, "y": 0.3860386908054352}], "text": "# Train: 10k\n"}
{"page": 5, "bbox": [{"x": 0.7644259333610535, "y": 0.3910849392414093}, {"x": 0.8304580450057983, "y": 0.3910849392414093}, {"x": 0.8304580450057983, "y": 0.39739277958869934}, {"x": 0.7644259333610535, "y": 0.39739277958869934}], "text": "# Train: 20k\n"}
{"page": 5, "bbox": [{"x": 0.5472933053970337, "y": 0.39402860403060913}, {"x": 0.5585960745811462, "y": 0.39402860403060913}, {"x": 0.5585960745811462, "y": 0.39865434169769287}, {"x": 0.5472933053970337, "y": 0.39865434169769287}], "text": "50\n"}
{"page": 5, "bbox": [{"x": 0.7644259333610535, "y": 0.40285953879356384}, {"x": 0.8304580450057983, "y": 0.40285953879356384}, {"x": 0.8304580450057983, "y": 0.40790581703186035}, {"x": 0.7644259333610535, "y": 0.40790581703186035}], "text": "# Train: 40k\n"}
{"page": 5, "bbox": [{"x": 0.7650208473205566, "y": 0.4129520654678345}, {"x": 0.8542534112930298, "y": 0.41337257623672485}, {"x": 0.8542534112930298, "y": 0.4201009273529053}, {"x": 0.7650208473205566, "y": 0.4196804165840149}], "text": "# Train: all (59k)\n"}
{"page": 5, "bbox": [{"x": 0.5466983914375305, "y": 0.42556771636009216}, {"x": 0.5585960745811462, "y": 0.42556771636009216}, {"x": 0.5585960745811462, "y": 0.4301934540271759}, {"x": 0.5466983914375305, "y": 0.4301934540271759}], "text": "40\n"}
{"page": 5, "bbox": [{"x": 0.6145151853561401, "y": 0.43145501613616943}, {"x": 0.6270077228546143, "y": 0.43145501613616943}, {"x": 0.6270077228546143, "y": 0.4360807538032532}, {"x": 0.6145151853561401, "y": 0.4360807538032532}], "text": "20\n"}
{"page": 5, "bbox": [{"x": 0.672218918800354, "y": 0.43145501613616943}, {"x": 0.6847114562988281, "y": 0.43145501613616943}, {"x": 0.6847114562988281, "y": 0.4360807538032532}, {"x": 0.672218918800354, "y": 0.4360807538032532}], "text": "40\n"}
{"page": 5, "bbox": [{"x": 0.7293277978897095, "y": 0.43145501613616943}, {"x": 0.7418203353881836, "y": 0.43145501613616943}, {"x": 0.7418203353881836, "y": 0.4360807538032532}, {"x": 0.7293277978897095, "y": 0.4360807538032532}], "text": "60\n"}
{"page": 5, "bbox": [{"x": 0.7870315313339233, "y": 0.43145501613616943}, {"x": 0.7995240688323975, "y": 0.43145501613616943}, {"x": 0.7995240688323975, "y": 0.4360807538032532}, {"x": 0.7870315313339233, "y": 0.4360807538032532}], "text": "80\n"}
{"page": 5, "bbox": [{"x": 0.8417608737945557, "y": 0.43145501613616943}, {"x": 0.8590124845504761, "y": 0.43145501613616943}, {"x": 0.8590124845504761, "y": 0.4360807538032532}, {"x": 0.8417608737945557, "y": 0.4360807538032532}], "text": "100\n"}
{"page": 5, "bbox": [{"x": 0.6258179545402527, "y": 0.43818333745002747}, {"x": 0.8036882877349854, "y": 0.4407064616680145}, {"x": 0.8030933737754822, "y": 0.449957937002182}, {"x": 0.6258179545402527, "y": 0.44743481278419495}], "text": "k: # of retrieved passages\n"}
{"page": 5, "bbox": [{"x": 0.11957168579101562, "y": 0.26703113317489624}, {"x": 0.4913741946220398, "y": 0.26703113317489624}, {"x": 0.4913741946220398, "y": 0.6816652417182922}, {"x": 0.11957168579101562, "y": 0.6816652417182922}], "text": "traditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Sec-\ntion 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQUAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10-5 using Adam, linear\nscheduling with warm-up and dropout rate 0.1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be de-\nsirable to obtain a single retriever that works well\nacross the board. To this end, we train a multi-\ndataset encoder by combining training data from\nall datasets excluding SQUAD. In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\nof top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25(q,p) + sim(q, p) as the ranking function.\nWe used \\ = 1.1 based on the retrieval accuracy in\nthe development set.\n"}
{"page": 5, "bbox": [{"x": 0.5157644152641296, "y": 0.4638351500034332}, {"x": 0.8863771557807922, "y": 0.4638351500034332}, {"x": 0.8863771557807922, "y": 0.5319596529006958}, {"x": 0.5157644152641296, "y": 0.5319596529006958}], "text": "Figure 1: Retriever top-k accuracy with different num-\nbers of training examples used in our dense passage re-\ntriever vs BM25. The results are measured on the de-\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n"}
{"page": 5, "bbox": [{"x": 0.2373587191104889, "y": 0.6417157053947449}, {"x": 0.24033313989639282, "y": 0.6417157053947449}, {"x": 0.24033313989639282, "y": 0.6438183188438416}, {"x": 0.2373587191104889, "y": 0.6438183188438416}], "text": ".\n"}
{"page": 5, "bbox": [{"x": 0.5145746469497681, "y": 0.5630782246589661}, {"x": 0.8851873874664307, "y": 0.5630782246589661}, {"x": 0.8851873874664307, "y": 0.8195962905883789}, {"x": 0.5145746469497681, "y": 0.8195962905883789}], "text": "tiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\nWe conjecture that the lower performance on\nSQUAD is due to two reasons. First, the annota-\ntors wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\nonly 500+ Wikipedia articles and thus the distribu-\ntion of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n"}
{"page": 5, "bbox": [{"x": 0.12016656994819641, "y": 0.6968040466308594}, {"x": 0.1427721530199051, "y": 0.6968040466308594}, {"x": 0.1427721530199051, "y": 0.7047939300537109}, {"x": 0.12016656994819641, "y": 0.7047939300537109}], "text": "5.1\n"}
{"page": 5, "bbox": [{"x": 0.16180844604969025, "y": 0.696383535861969}, {"x": 0.26531827449798584, "y": 0.6968040466308594}, {"x": 0.26531827449798584, "y": 0.7052144408226013}, {"x": 0.16180844604969025, "y": 0.7047939300537109}], "text": "Main Results\n"}
{"page": 5, "bbox": [{"x": 0.11897680163383484, "y": 0.7165685296058655}, {"x": 0.490779310464859, "y": 0.7165685296058655}, {"x": 0.490779310464859, "y": 0.9087468385696411}, {"x": 0.11897680163383484, "y": 0.9087468385696411}], "text": "Table 2 compares different passage retrieval sys-\ntems on five QA datasets, using the top-k accuracy\n(k = {20, 100}). With the exception of SQUAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when k is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\non Natural Questions). When training with mul-\n8SQUAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\nmore in Section 5.1.\n'Lucene implementation. BM25 parameters b = 0.4 (doc-\nument length normalization) and k₁ = 0.9 (term frequency\nscaling) are tuned using development sets.\n"}
{"page": 5, "bbox": [{"x": 0.5157644152641296, "y": 0.8397813439369202}, {"x": 0.8845925331115723, "y": 0.8406223654747009}, {"x": 0.8839976191520691, "y": 0.9095878601074219}, {"x": 0.5151695609092712, "y": 0.9087468385696411}], "text": "5.2 Ablation Study on Model Training\nTo understand further how different model training\noptions affect the results, we conduct several addi-\ntional experiments and discuss our findings below.\n"}
{"page": 5, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5199286341667175, "y": 0.9238856434822083}, {"x": 0.5199286341667175, "y": 0.9322960376739502}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6773\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.0803195983171463}, {"x": 0.6299821734428406, "y": 0.0803195983171463}, {"x": 0.6299821734428406, "y": 0.0874684602022171}, {"x": 0.612135648727417, "y": 0.0874684602022171}], "text": "#N\n"}
{"page": 6, "bbox": [{"x": 0.6799523830413818, "y": 0.07947855442762375}, {"x": 0.871505081653595, "y": 0.0803195983171463}, {"x": 0.871505081653595, "y": 0.08999159187078476}, {"x": 0.6799523830413818, "y": 0.08915054798126221}], "text": "IB Top-5 Top-20 Top-100\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.07947855442762375}, {"x": 0.5514574646949768, "y": 0.08074011653661728}, {"x": 0.5508626103401184, "y": 0.09041211009025574}, {"x": 0.5193337202072144, "y": 0.08915054798126221}], "text": "Туре\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.09840201586484909}, {"x": 0.7965496778488159, "y": 0.09882254153490067}, {"x": 0.7965496778488159, "y": 0.10555088520050049}, {"x": 0.7703747749328613, "y": 0.10513035953044891}], "text": "64.3\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.09882254153490067}, {"x": 0.6192742586135864, "y": 0.09882254153490067}, {"x": 0.6192742586135864, "y": 0.10555088520050049}, {"x": 0.612135648727417, "y": 0.10555088520050049}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.09924305975437164}, {"x": 0.5693039894104004, "y": 0.09924305975437164}, {"x": 0.5693039894104004, "y": 0.10555088520050049}, {"x": 0.5199286341667175, "y": 0.10555088520050049}], "text": "Random\n"}
{"page": 6, "bbox": [{"x": 0.682331919670105, "y": 0.09924305975437164}, {"x": 0.7418203353881836, "y": 0.09924305975437164}, {"x": 0.7418203353881836, "y": 0.10555088520050049}, {"x": 0.682331919670105, "y": 0.10555088520050049}], "text": "X 47.0\n"}
{"page": 6, "bbox": [{"x": 0.8322427272796631, "y": 0.09924305975437164}, {"x": 0.8572278618812561, "y": 0.09924305975437164}, {"x": 0.8572278618812561, "y": 0.10555088520050049}, {"x": 0.8322427272796631, "y": 0.10555088520050049}], "text": "77.8\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.11017661541700363}, {"x": 0.8584176301956177, "y": 0.1105971410870552}, {"x": 0.8584176301956177, "y": 0.11774600297212601}, {"x": 0.8328375816345215, "y": 0.11732548475265503}], "text": "74.8\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.1105971410870552}, {"x": 0.6198691129684448, "y": 0.1105971410870552}, {"x": 0.6198691129684448, "y": 0.11732548475265503}, {"x": 0.612135648727417, "y": 0.11732548475265503}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.10933557897806168}, {"x": 0.7953599095344543, "y": 0.1105971410870552}, {"x": 0.7947649955749512, "y": 0.11858704686164856}, {"x": 0.7150505781173706, "y": 0.11732548475265503}], "text": "50.0 63.3\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.11101765930652618}, {"x": 0.5580011606216431, "y": 0.11101765930652618}, {"x": 0.5580011606216431, "y": 0.11774600297212601}, {"x": 0.5199286341667175, "y": 0.11774600297212601}], "text": "BM25\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.12237174063920975}, {"x": 0.6198691129684448, "y": 0.12237174063920975}, {"x": 0.6198691129684448, "y": 0.128679558634758}, {"x": 0.612135648727417, "y": 0.128679558634758}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.12237174063920975}, {"x": 0.8590124845504761, "y": 0.12237174063920975}, {"x": 0.8590124845504761, "y": 0.12910008430480957}, {"x": 0.8328375816345215, "y": 0.12910008430480957}], "text": "78.3\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.12237174063920975}, {"x": 0.5496728420257568, "y": 0.12237174063920975}, {"x": 0.5496728420257568, "y": 0.12952060997486115}, {"x": 0.5199286341667175, "y": 0.12952060997486115}], "text": "Gold\n"}
{"page": 6, "bbox": [{"x": 0.6817370653152466, "y": 0.12237174063920975}, {"x": 0.7947649955749512, "y": 0.12237174063920975}, {"x": 0.7947649955749512, "y": 0.12952060997486115}, {"x": 0.6817370653152466, "y": 0.12952060997486115}], "text": "✓ 42.6 63.1\n"}
{"page": 6, "bbox": [{"x": 0.7697799205780029, "y": 0.14045415818691254}, {"x": 0.7935752272605896, "y": 0.14003364741802216}, {"x": 0.7941701412200928, "y": 0.14760303497314453}, {"x": 0.7703747749328613, "y": 0.1480235457420349}], "text": "69.1\n"}
{"page": 6, "bbox": [{"x": 0.7144556641578674, "y": 0.1408746838569641}, {"x": 0.7394407987594604, "y": 0.14045415818691254}, {"x": 0.7394407987594604, "y": 0.14760303497314453}, {"x": 0.7144556641578674, "y": 0.1480235457420349}], "text": "51.1\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.1408746838569641}, {"x": 0.6192742586135864, "y": 0.1408746838569641}, {"x": 0.6192742586135864, "y": 0.14760303497314453}, {"x": 0.612135648727417, "y": 0.14760303497314453}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.6811421513557434, "y": 0.14129520952701569}, {"x": 0.6924449801445007, "y": 0.14129520952701569}, {"x": 0.6924449801445007, "y": 0.14718250930309296}, {"x": 0.6811421513557434, "y": 0.14718250930309296}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.14129520952701569}, {"x": 0.8584176301956177, "y": 0.14129520952701569}, {"x": 0.8584176301956177, "y": 0.14760303497314453}, {"x": 0.8328375816345215, "y": 0.14760303497314453}], "text": "80.8\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.15264928340911865}, {"x": 0.7953599095344543, "y": 0.15222875773906708}, {"x": 0.7953599095344543, "y": 0.1589571088552475}, {"x": 0.7703747749328613, "y": 0.15937763452529907}], "text": "70.8\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.14045415818691254}, {"x": 0.5502676963806152, "y": 0.14045415818691254}, {"x": 0.5502676963806152, "y": 0.171572744846344}, {"x": 0.5199286341667175, "y": 0.171572744846344}], "text": "Gold\nGold\nGold\n"}
{"page": 6, "bbox": [{"x": 0.6811421513557434, "y": 0.15264928340911865}, {"x": 0.6912552118301392, "y": 0.15264928340911865}, {"x": 0.6912552118301392, "y": 0.15937763452529907}, {"x": 0.6811421513557434, "y": 0.15937763452529907}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.15264928340911865}, {"x": 0.8566329479217529, "y": 0.15264928340911865}, {"x": 0.8566329479217529, "y": 0.15937763452529907}, {"x": 0.8328375816345215, "y": 0.15937763452529907}], "text": "82.1\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.15306980907917023}, {"x": 0.6252231001853943, "y": 0.15306980907917023}, {"x": 0.6252231001853943, "y": 0.15937763452529907}, {"x": 0.612135648727417, "y": 0.15937763452529907}], "text": "31\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.15306980907917023}, {"x": 0.7412254810333252, "y": 0.15306980907917023}, {"x": 0.7412254810333252, "y": 0.15937763452529907}, {"x": 0.7150505781173706, "y": 0.15937763452529907}], "text": "52.1\n"}
{"page": 6, "bbox": [{"x": 0.8322427272796631, "y": 0.16400335729122162}, {"x": 0.8566329479217529, "y": 0.16358284652233124}, {"x": 0.8566329479217529, "y": 0.1711522340774536}, {"x": 0.8322427272796631, "y": 0.171572744846344}], "text": "83.1\n"}
{"page": 6, "bbox": [{"x": 0.6127305030822754, "y": 0.1644238829612732}, {"x": 0.6341463327407837, "y": 0.1644238829612732}, {"x": 0.6341463327407837, "y": 0.171572744846344}, {"x": 0.6127305030822754, "y": 0.171572744846344}], "text": "127\n"}
{"page": 6, "bbox": [{"x": 0.6817370653152466, "y": 0.16484440863132477}, {"x": 0.6930398344993591, "y": 0.16484440863132477}, {"x": 0.6930398344993591, "y": 0.1711522340774536}, {"x": 0.6817370653152466, "y": 0.1711522340774536}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.7144556641578674, "y": 0.16484440863132477}, {"x": 0.740630567073822, "y": 0.16484440863132477}, {"x": 0.740630567073822, "y": 0.1711522340774536}, {"x": 0.7144556641578674, "y": 0.1711522340774536}], "text": "55.8\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.16484440863132477}, {"x": 0.7965496778488159, "y": 0.16484440863132477}, {"x": 0.7965496778488159, "y": 0.171572744846344}, {"x": 0.7703747749328613, "y": 0.171572744846344}], "text": "73.0\n"}
{"page": 6, "bbox": [{"x": 0.11957168579101562, "y": 0.07821699231863022}, {"x": 0.490779310464859, "y": 0.07821699231863022}, {"x": 0.490779310464859, "y": 0.28343144059181213}, {"x": 0.11957168579101562, "y": 0.28343144059181213}], "text": "Sample efficiency We explore how many train-\ning examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top-k\nretrieval accuracy with respect to different num-\nbers of training examples, measured on the devel-\nopment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 ex-\namples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question-passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n"}
{"page": 6, "bbox": [{"x": 0.7697799205780029, "y": 0.18334734439849854}, {"x": 0.7971445322036743, "y": 0.1837678700685501}, {"x": 0.7965496778488159, "y": 0.19133725762367249}, {"x": 0.7697799205780029, "y": 0.1909167319536209}], "text": "77.3\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.18460892140865326}, {"x": 0.8578227162361145, "y": 0.18460892140865326}, {"x": 0.8578227162361145, "y": 0.19049622118473053}, {"x": 0.8328375816345215, "y": 0.19049622118473053}], "text": "84.4\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.18460892140865326}, {"x": 0.7418203353881836, "y": 0.18460892140865326}, {"x": 0.7418203353881836, "y": 0.1909167319536209}, {"x": 0.7150505781173706, "y": 0.1909167319536209}], "text": "65.0\n"}
{"page": 6, "bbox": [{"x": 0.518738865852356, "y": 0.18166527152061462}, {"x": 0.6924449801445007, "y": 0.1804036945104599}, {"x": 0.6930398344993591, "y": 0.2195121943950653}, {"x": 0.5193337202072144, "y": 0.22077375650405884}], "text": "G.+BM25(1) 31+32 ✓\nG.+BM25\nG.+BM25(1) 127+128 ✓\n"}
{"page": 6, "bbox": [{"x": 0.6817370653152466, "y": 0.19764508306980133}, {"x": 0.6930398344993591, "y": 0.19764508306980133}, {"x": 0.6930398344993591, "y": 0.20437341928482056}, {"x": 0.6817370653152466, "y": 0.20437341928482056}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.1980656087398529}, {"x": 0.8590124845504761, "y": 0.1980656087398529}, {"x": 0.8590124845504761, "y": 0.20437341928482056}, {"x": 0.8328375816345215, "y": 0.20437341928482056}], "text": "84.0\n"}
{"page": 6, "bbox": [{"x": 0.6127305030822754, "y": 0.1980656087398529}, {"x": 0.6508030891418457, "y": 0.1980656087398529}, {"x": 0.6508030891418457, "y": 0.20479394495487213}, {"x": 0.6127305030822754, "y": 0.20479394495487213}], "text": "31+64\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.19680403172969818}, {"x": 0.7959547638893127, "y": 0.19722455739974976}, {"x": 0.7959547638893127, "y": 0.21825063228607178}, {"x": 0.7150505781173706, "y": 0.2178301066160202}], "text": "64.5 76.4\n65.8\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.21026071906089783}, {"x": 0.7953599095344543, "y": 0.21110177040100098}, {"x": 0.7947649955749512, "y": 0.21825063228607178}, {"x": 0.7697799205780029, "y": 0.21740958094596863}], "text": "78.0\n"}
{"page": 6, "bbox": [{"x": 0.8322427272796631, "y": 0.21110177040100098}, {"x": 0.8578227162361145, "y": 0.21110177040100098}, {"x": 0.8578227162361145, "y": 0.2178301066160202}, {"x": 0.8322427272796631, "y": 0.2178301066160202}], "text": "84.9\n"}
{"page": 6, "bbox": [{"x": 0.5151695609092712, "y": 0.23801514506340027}, {"x": 0.8857823014259338, "y": 0.23843565583229065}, {"x": 0.8857823014259338, "y": 0.33473506569862366}, {"x": 0.5151695609092712, "y": 0.3343145549297333}], "text": "Table 3: Comparison of different training schemes,\nmeasured as top-k retrieval accuracy on Natural Ques-\ntions (development set). #N: number of negative\nexamples, IB: in-batch training. G.+BM25 (¹) and\nG.+BM25 (2) denote in-batch training with 1 or 2 ad-\nditional BM25 negatives, which serve as negative pas-\nsages for all questions in the batch.\n"}
{"page": 6, "bbox": [{"x": 0.5145746469497681, "y": 0.36375105381011963}, {"x": 0.8839976191520691, "y": 0.36164844036102295}, {"x": 0.8851873874664307, "y": 0.4533221125602722}, {"x": 0.5157644152641296, "y": 0.4554247260093689}], "text": "Our experiments on Natural Questions show that\nswitching to distantly-supervised passages (using\nthe highest-ranked BM25 passage that contains the\nanswer), has only a small impact: 1 point lower\ntop-k accuracy for retrieval. Appendix A contains\nmore details.\n"}
{"page": 6, "bbox": [{"x": 0.11897680163383484, "y": 0.30109336972236633}, {"x": 0.4913741946220398, "y": 0.30109336972236633}, {"x": 0.4913741946220398, "y": 0.8473507165908813}, {"x": 0.11897680163383484, "y": 0.8473507165908813}], "text": "In-batch negative training We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of-N training set-\nting, where each question in the batch is paired\nwith a positive passage and its own set of n neg-\native passages (Eq. (2)). We find that the choice\nof negatives random, BM25 or gold passages\n(positive passages from other questions) — does\nnot impact the top-k accuracy much in this setting\nwhen k > 20.\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\nor from the whole training set. Effectively, in-batch\nnegative training is an easy and memory-efficient\nway to reuse the negative examples already in the\nbatch rather than creating new ones. It produces\nmore pairs and thus increases the number of train-\ning examples, which might contribute to the good\nmodel performance. As a result, accuracy consis-\ntently improves as the batch size grows.\nFinally, we explore in-batch negative training\nwith additional \"hard\" negative passages that have\nhigh BM25 scores given the question, but do not\ncontain the answer string (the bottom block). These\nadditional passages are used as negative passages\nfor all questions in the same batch. We find that\nadding a single BM25 negative passage improves\nthe result substantially while adding two does not\nhelp further.\n"}
{"page": 6, "bbox": [{"x": 0.5151695609092712, "y": 0.4693019390106201}, {"x": 0.8869720697402954, "y": 0.4697224497795105}, {"x": 0.8857823014259338, "y": 0.9087468385696411}, {"x": 0.5139797925949097, "y": 0.9083263278007507}], "text": "Similarity and loss Besides dot product, cosine\nand Euclidean L2 distance are also commonly used\nas decomposable similarity functions. We test these\nalternatives and find that L2 performs compara-\nble to dot product, and both of them are superior\nto cosine. Similarly, in addition to negative log-\nlikelihood, a popular option for ranking is triplet\nloss, which compares a positive passage and a nega-\ntive one directly with respect to a question (Burges\net al., 2005). Our experiments show that using\ntriplet loss does not affect the results much. More\ndetails can be found in Appendix B.\nCross-dataset generalization One interesting\nquestion regarding DPR's discriminative training\nis how much performance degradation it may suf-\nfer from a non-iid setting. In other words, can\nit still generalize well when directly applied to\na different dataset without additional fine-tuning?\nTo test the cross-dataset generalization, we train\nDPR on Natural Questions only and test it directly\non the smaller WebQuestions and CuratedTREC\ndatasets. We find that DPR generalizes well, with\n3-5 points loss from the best performing fine-tuned\nmodel in top-20 retrieval accuracy (69.9/86.3 vs.\n75.0/89.1 for WebQuestions and TREC, respec-\ntively), while still greatly outperforming the BM25\nbaseline (55.0/70.9).\n"}
{"page": 6, "bbox": [{"x": 0.11957168579101562, "y": 0.8654331564903259}, {"x": 0.48958954215049744, "y": 0.8654331564903259}, {"x": 0.48958954215049744, "y": 0.9091673493385315}, {"x": 0.11957168579101562, "y": 0.9091673493385315}], "text": "Impact of gold passages We use passages that\nmatch the gold contexts in the original datasets\n(when available) as positive examples (Section 4.2).\n"}
{"page": 6, "bbox": [{"x": 0.4830458164215088, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6774\n"}
{"page": 7, "bbox": [{"x": 0.11897680163383484, "y": 0.07779646664857864}, {"x": 0.4913741946220398, "y": 0.07821699231863022}, {"x": 0.4913741946220398, "y": 0.20815812051296234}, {"x": 0.11897680163383484, "y": 0.20773759484291077}], "text": "5.3 Qualitative Analysis\nAlthough DPR performs better than BM25 in gen-\neral, passages retrieved by these two methods dif-\nfer qualitatively. Term-matching methods like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\nor semantic relationships better. See Appendix C\nfor examples and more discussion.\n"}
{"page": 7, "bbox": [{"x": 0.5145746469497681, "y": 0.07695542275905609}, {"x": 0.8869720697402954, "y": 0.07695542275905609}, {"x": 0.8869720697402954, "y": 0.3486122786998749}, {"x": 0.5145746469497681, "y": 0.3486122786998749}], "text": "score is chosen as the final answer. The passage\nselection model serves as a reranker through cross-\nattention between the question and the passage. Al-\nthough cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its non-\ndecomposable nature, it has more capacity than the\ndual-encoder model sim(q, p) as in Eq. (1). Apply-\ning it to selecting the passage from a small number\nof retrieved candidates has been shown to work\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let P¿ € R¹×h (1 ≤ i ≤ k) be\na BERT (base, uncased in our experiments) rep-\nresentation for the i-th passage, where L is the\nmaximum length of the passage and h the hidden\ndimension. The probabilities of a token being the\nstarting/ending positions of an answer span and a\npassage being selected are defined as:\n"}
{"page": 7, "bbox": [{"x": 0.12016656994819641, "y": 0.2232968807220459}, {"x": 0.14396192133426666, "y": 0.22371740639209747}, {"x": 0.14396192133426666, "y": 0.2338099181652069}, {"x": 0.12016656994819641, "y": 0.23338940739631653}], "text": "5.4\n"}
{"page": 7, "bbox": [{"x": 0.16240333020687103, "y": 0.2232968807220459}, {"x": 0.3188578188419342, "y": 0.22455845773220062}, {"x": 0.3188578188419342, "y": 0.23507149517536163}, {"x": 0.16240333020687103, "y": 0.2338099181652069}], "text": "Run-time Efficiency\n"}
{"page": 7, "bbox": [{"x": 0.6728137731552124, "y": 0.36417156457901}, {"x": 0.7638310790061951, "y": 0.3658536672592163}, {"x": 0.7632361650466919, "y": 0.37804877758026123}, {"x": 0.672218918800354, "y": 0.3763667047023773}], "text": "softmax (Pi\n"}
{"page": 7, "bbox": [{"x": 0.6418798565864563, "y": 0.37047940492630005}, {"x": 0.6525877714157104, "y": 0.37047940492630005}, {"x": 0.6525877714157104, "y": 0.37426409125328064}, {"x": 0.6418798565864563, "y": 0.37426409125328064}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.782272458076477, "y": 0.3608073890209198}, {"x": 0.825698971748352, "y": 0.3696383535861969}, {"x": 0.8197501301765442, "y": 0.3839360773563385}, {"x": 0.7763236165046692, "y": 0.37552565336227417}], "text": "(start) s'\n"}
{"page": 7, "bbox": [{"x": 0.8607971668243408, "y": 0.36711522936820984}, {"x": 0.881618082523346, "y": 0.36711522936820984}, {"x": 0.881618082523346, "y": 0.37804877758026123}, {"x": 0.8607971668243408, "y": 0.37804877758026123}], "text": "(3)\n"}
{"page": 7, "bbox": [{"x": 0.7644259333610535, "y": 0.37047940492630005}, {"x": 0.7775133848190308, "y": 0.37089991569519043}, {"x": 0.7775133848190308, "y": 0.37552565336227417}, {"x": 0.7644259333610535, "y": 0.37552565336227417}], "text": "W\n"}
{"page": 7, "bbox": [{"x": 0.6716240048408508, "y": 0.3843565881252289}, {"x": 0.8143962025642395, "y": 0.3877207636833191}, {"x": 0.8138012886047363, "y": 0.4020185172557831}, {"x": 0.6710291504859924, "y": 0.39865434169769287}], "text": "softmax (P₂Wend) +\n"}
{"page": 7, "bbox": [{"x": 0.8607971668243408, "y": 0.3877207636833191}, {"x": 0.8822129964828491, "y": 0.3877207636833191}, {"x": 0.8822129964828491, "y": 0.39865434169769287}, {"x": 0.8607971668243408, "y": 0.39865434169769287}], "text": "(4)\n"}
{"page": 7, "bbox": [{"x": 0.5466983914375305, "y": 0.3658536672592163}, {"x": 0.6252231001853943, "y": 0.36501261591911316}, {"x": 0.6264128684997559, "y": 0.4213624894618988}, {"x": 0.5478881597518921, "y": 0.42220354080200195}], "text": "Pstart,i\nstart,i \n(s)\nPend,i(t)\nPselected (i)\n"}
{"page": 7, "bbox": [{"x": 0.6412849426269531, "y": 0.39192599058151245}, {"x": 0.6531826257705688, "y": 0.39192599058151245}, {"x": 0.6531826257705688, "y": 0.3965517282485962}, {"x": 0.6412849426269531, "y": 0.3965517282485962}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.672218918800354, "y": 0.40622371435165405}, {"x": 0.8834027647972107, "y": 0.40790581703186035}, {"x": 0.8834027647972107, "y": 0.4234651029109955}, {"x": 0.672218918800354, "y": 0.4217830002307892}], "text": "softmax (PTW selected),, (5)\n"}
{"page": 7, "bbox": [{"x": 0.6418798565864563, "y": 0.41337257623672485}, {"x": 0.6525877714157104, "y": 0.41337257623672485}, {"x": 0.6525877714157104, "y": 0.41715726256370544}, {"x": 0.6418798565864563, "y": 0.41715726256370544}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.11957168579101562, "y": 0.24432295560836792}, {"x": 0.490779310464859, "y": 0.24432295560836792}, {"x": 0.490779310464859, "y": 0.5954583883285522}, {"x": 0.11957168579101562, "y": 0.5954583883285522}], "text": "The main reason that we require a retrieval compo-\nnent for open-domain QA is to reduce the number\nof candidate passages that the reader needs to con-\nsider, which is crucial for answering user's ques-\ntions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors 10,\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 pas-\nsages per question. In contrast, BM25/Lucene (im-\nplemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Com-\nputing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\non a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n"}
{"page": 7, "bbox": [{"x": 0.6395003199577332, "y": 0.4390243887901306}, {"x": 0.674003541469574, "y": 0.43860387802124023}, {"x": 0.6745984554290771, "y": 0.4495374262332916}, {"x": 0.6400951743125916, "y": 0.449957937002182}], "text": "[CLS]\n"}
{"page": 7, "bbox": [{"x": 0.5151695609092712, "y": 0.4444911777973175}, {"x": 0.5812016725540161, "y": 0.44196805357933044}, {"x": 0.5817965269088745, "y": 0.453742653131485}, {"x": 0.5163593292236328, "y": 0.45626577734947205}], "text": "where p\n"}
{"page": 7, "bbox": [{"x": 0.6085663437843323, "y": 0.4478553533554077}, {"x": 0.6085663437843323, "y": 0.4520605504512787}, {"x": 0.5978584289550781, "y": 0.4520605504512787}, {"x": 0.5978584289550781, "y": 0.4478553533554077}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.730517566204071, "y": 0.4516400396823883}, {"x": 0.7382510304450989, "y": 0.4516400396823883}, {"x": 0.7382510304450989, "y": 0.45794785022735596}, {"x": 0.730517566204071, "y": 0.45794785022735596}], "text": "k\n"}
{"page": 7, "bbox": [{"x": 0.5139797925949097, "y": 0.4390243887901306}, {"x": 0.8845925331115723, "y": 0.4377628266811371}, {"x": 0.8851873874664307, "y": 0.535744309425354}, {"x": 0.5145746469497681, "y": 0.5370059013366699}], "text": "[P 1,..., P[CLS)]] = Rhxk and\nW start, Wend, Wselected Є Rh are learnable vectors.\nWe compute a span score of the s-th to t-th words\nfrom the i-th passage as Pstart, i (s) × Pend,i(t), and\na passage selection score of the i-th passage as\nPselected (i).\n"}
{"page": 7, "bbox": [{"x": 0.578822135925293, "y": 0.6097561120986938}, {"x": 0.578822135925293, "y": 0.613540768623352}, {"x": 0.5681142210960388, "y": 0.613540768623352}, {"x": 0.5681142210960388, "y": 0.6097561120986938}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.1207614541053772, "y": 0.6122792363166809}, {"x": 0.449137419462204, "y": 0.6122792363166809}, {"x": 0.449137419462204, "y": 0.6248948574066162}, {"x": 0.1207614541053772, "y": 0.6248948574066162}], "text": "6 Experiments: Question Answering\n"}
{"page": 7, "bbox": [{"x": 0.12016656994819641, "y": 0.6396130919456482}, {"x": 0.4878048896789551, "y": 0.6396130919456482}, {"x": 0.4878048896789551, "y": 0.6673675179481506}, {"x": 0.12016656994819641, "y": 0.6673675179481506}], "text": "In this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.5416316390037537}, {"x": 0.8857823014259338, "y": 0.5407905578613281}, {"x": 0.8869720697402954, "y": 0.809924304485321}, {"x": 0.5145746469497681, "y": 0.8107653260231018}], "text": "During training, we sample one positive and\nm 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. m is a hyper-parameter and we\nuse m\n24 in all the experiments. The training ob-\njective is to maximize the marginal log-likelihood\nof all the correct answer spans in the positive pas-\nsage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\nof the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQUAD)\nand 4 for small (TREC, WQ) datasets, and tune k\non the development set. For experiments on small\ndatasets under the Multi setting, in which using\nother datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n"}
{"page": 7, "bbox": [{"x": 0.11838191747665405, "y": 0.6816652417182922}, {"x": 0.4913741946220398, "y": 0.6825063228607178}, {"x": 0.490779310464859, "y": 0.9095878601074219}, {"x": 0.11778703331947327, "y": 0.9087468385696411}], "text": "6.1 End-to-end QA System\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA sys-\ntem consists of a neural reader that outputs the\nanswer to the question. Given the top k retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth =200, search depth = 128.\n"}
{"page": 7, "bbox": [{"x": 0.5163593292236328, "y": 0.8275862336158752}, {"x": 0.6145151853561401, "y": 0.8275862336158752}, {"x": 0.6145151853561401, "y": 0.8359966278076172}, {"x": 0.5163593292236328, "y": 0.8359966278076172}], "text": "6.2 Results\n"}
{"page": 7, "bbox": [{"x": 0.5151695609092712, "y": 0.8486123085021973}, {"x": 0.8857823014259338, "y": 0.8486123085021973}, {"x": 0.8857823014259338, "y": 0.9079058170318604}, {"x": 0.5151695609092712, "y": 0.9079058170318604}], "text": "Table 4 summarizes our final end-to-end QA re-\nsults, measured by exact match with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n"}
{"page": 7, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5199286341667175, "y": 0.9243061542510986}, {"x": 0.5199286341667175, "y": 0.9331371188163757}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6775\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.08158116042613983}, {"x": 0.2986317574977875, "y": 0.07989907264709473}, {"x": 0.2986317574977875, "y": 0.09125315397977829}, {"x": 0.1641879826784134, "y": 0.0929352417588234}], "text": "Training Model\n"}
{"page": 8, "bbox": [{"x": 0.5229030251502991, "y": 0.08116064220666885}, {"x": 0.8387864232063293, "y": 0.08116064220666885}, {"x": 0.8387864232063293, "y": 0.09251471608877182}, {"x": 0.5229030251502991, "y": 0.09251471608877182}], "text": "NQ TriviaQA WQ TREC SQUAD\n"}
{"page": 8, "bbox": [{"x": 0.6627007722854614, "y": 0.10470984131097794}, {"x": 0.7519333958625793, "y": 0.10470984131097794}, {"x": 0.7519333958625793, "y": 0.11269974708557129}, {"x": 0.6627007722854614, "y": 0.11269974708557129}], "text": "17.7 21.3\n"}
{"page": 8, "bbox": [{"x": 0.7935752272605896, "y": 0.10428931564092636}, {"x": 0.8239143490791321, "y": 0.10470984131097794}, {"x": 0.8239143490791321, "y": 0.11312027275562286}, {"x": 0.7935752272605896, "y": 0.11269974708557129}], "text": "33.2\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.10470984131097794}, {"x": 0.55205237865448, "y": 0.10513035953044891}, {"x": 0.55205237865448, "y": 0.11312027275562286}, {"x": 0.5205234885215759, "y": 0.11269974708557129}], "text": "26.5\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.10302775353193283}, {"x": 0.4818560481071472, "y": 0.10428931564092636}, {"x": 0.4818560481071472, "y": 0.11522287875413895}, {"x": 0.24866151809692383, "y": 0.11396130919456482}], "text": "BM25+BERT (Lee et al., 2019)\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.10513035953044891}, {"x": 0.622248649597168, "y": 0.10513035953044891}, {"x": 0.622248649597168, "y": 0.11312027275562286}, {"x": 0.5907198190689087, "y": 0.11312027275562286}], "text": "47.1\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.10428931564092636}, {"x": 0.20999404788017273, "y": 0.10428931564092636}, {"x": 0.20999404788017273, "y": 0.11480235308408737}, {"x": 0.1641879826784134, "y": 0.11480235308408737}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.11984860897064209}, {"x": 0.55205237865448, "y": 0.11942809075117111}, {"x": 0.5526472330093384, "y": 0.12825904786586761}, {"x": 0.5217132568359375, "y": 0.128679558634758}], "text": "33.3\n"}
{"page": 8, "bbox": [{"x": 0.5901249051094055, "y": 0.12068965286016464}, {"x": 0.622248649597168, "y": 0.12068965286016464}, {"x": 0.622248649597168, "y": 0.128679558634758}, {"x": 0.5901249051094055, "y": 0.128679558634758}], "text": "45.0\n"}
{"page": 8, "bbox": [{"x": 0.662105917930603, "y": 0.12068965286016464}, {"x": 0.7501487135887146, "y": 0.12068965286016464}, {"x": 0.7501487135887146, "y": 0.128679558634758}, {"x": 0.662105917930603, "y": 0.128679558634758}], "text": "36.4 30.1\n"}
{"page": 8, "bbox": [{"x": 0.792385458946228, "y": 0.12068965286016464}, {"x": 0.8245092034339905, "y": 0.12068965286016464}, {"x": 0.8245092034339905, "y": 0.128679558634758}, {"x": 0.792385458946228, "y": 0.128679558634758}], "text": "20.2\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.12026913464069366}, {"x": 0.4306960105895996, "y": 0.12026913464069366}, {"x": 0.4306960105895996, "y": 0.1303616464138031}, {"x": 0.24866151809692383, "y": 0.1303616464138031}], "text": "ORQA (Lee et al., 2019)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.11984860897064209}, {"x": 0.21058893203735352, "y": 0.12026913464069366}, {"x": 0.21058893203735352, "y": 0.13120269775390625}, {"x": 0.1641879826784134, "y": 0.13078217208385468}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.13624894618988037}, {"x": 0.55205237865448, "y": 0.13666947185993195}, {"x": 0.55205237865448, "y": 0.14550042152404785}, {"x": 0.5211184024810791, "y": 0.14507989585399628}], "text": "28.1\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.13708999752998352}, {"x": 0.6216537952423096, "y": 0.1375105082988739}, {"x": 0.6216537952423096, "y": 0.14507989585399628}, {"x": 0.5913146734237671, "y": 0.1446593701839447}], "text": "50.9\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.13666947185993195}, {"x": 0.45449137687683105, "y": 0.13666947185993195}, {"x": 0.45449137687683105, "y": 0.14676198363304138}, {"x": 0.24866151809692383, "y": 0.14676198363304138}], "text": "HardEM (Min et al., 2019a)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.13708999752998352}, {"x": 0.21058893203735352, "y": 0.13708999752998352}, {"x": 0.21058893203735352, "y": 0.1480235457420349}, {"x": 0.1641879826784134, "y": 0.1480235457420349}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.15096719563007355}, {"x": 0.55205237865448, "y": 0.15096719563007355}, {"x": 0.55205237865448, "y": 0.1627417951822281}, {"x": 0.5205234885215759, "y": 0.1627417951822281}], "text": "34.5\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.15306980907917023}, {"x": 0.6234384179115295, "y": 0.15306980907917023}, {"x": 0.6234384179115295, "y": 0.16105970740318298}, {"x": 0.5913146734237671, "y": 0.16105970740318298}], "text": "56.0\n"}
{"page": 8, "bbox": [{"x": 0.662105917930603, "y": 0.15306980907917023}, {"x": 0.6930398344993591, "y": 0.15306980907917023}, {"x": 0.6930398344993591, "y": 0.16105970740318298}, {"x": 0.662105917930603, "y": 0.16105970740318298}], "text": "36.4\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.1518082469701767}, {"x": 0.5038667321205139, "y": 0.15096719563007355}, {"x": 0.5038667321205139, "y": 0.16316232085227966}, {"x": 0.24866151809692383, "y": 0.16400335729122162}], "text": "GraphRetriever (Min et al., 2019b)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.15264928340911865}, {"x": 0.20999404788017273, "y": 0.15264928340911865}, {"x": 0.20999404788017273, "y": 0.16316232085227966}, {"x": 0.1641879826784134, "y": 0.16316232085227966}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.16904962062835693}, {"x": 0.5526472330093384, "y": 0.16904962062835693}, {"x": 0.5526472330093384, "y": 0.17703953385353088}, {"x": 0.5211184024810791, "y": 0.17703953385353088}], "text": "32.6\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.16904962062835693}, {"x": 0.8251041173934937, "y": 0.16904962062835693}, {"x": 0.8251041173934937, "y": 0.17746004462242126}, {"x": 0.7917906045913696, "y": 0.17746004462242126}], "text": "56.5\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.16736753284931183}, {"x": 0.48423558473587036, "y": 0.16820858418941498}, {"x": 0.48423558473587036, "y": 0.17956265807151794}, {"x": 0.24866151809692383, "y": 0.178721621632576}], "text": "PathRetriever (Asai et al., 2020)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.16820858418941498}, {"x": 0.21058893203735352, "y": 0.16862909495830536}, {"x": 0.21058893203735352, "y": 0.17956265807151794}, {"x": 0.1641879826784134, "y": 0.17914213240146637}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.18460892140865326}, {"x": 0.7507436275482178, "y": 0.18418839573860168}, {"x": 0.7507436275482178, "y": 0.19259881973266602}, {"x": 0.6609161496162415, "y": 0.1930193454027176}], "text": "40.2 46.8\n"}
{"page": 8, "bbox": [{"x": 0.5217132568359375, "y": 0.18418839573860168}, {"x": 0.55205237865448, "y": 0.18460892140865326}, {"x": 0.5514574646949768, "y": 0.19343987107276917}, {"x": 0.5211184024810791, "y": 0.1930193454027176}], "text": "39.2\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.18460892140865326}, {"x": 0.4747174382209778, "y": 0.18460892140865326}, {"x": 0.4747174382209778, "y": 0.19512194395065308}, {"x": 0.24866151809692383, "y": 0.19512194395065308}], "text": "REALMWiki (Guu et al., 2020)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.18460892140865326}, {"x": 0.20939916372299194, "y": 0.18460892140865326}, {"x": 0.20939916372299194, "y": 0.19596299529075623}, {"x": 0.1641879826784134, "y": 0.19596299529075623}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.6615110039710999, "y": 0.20100925862789154}, {"x": 0.6918500661849976, "y": 0.20058873295783997}, {"x": 0.6918500661849976, "y": 0.2089991569519043}, {"x": 0.6615110039710999, "y": 0.20941968262195587}], "text": "40.7\n"}
{"page": 8, "bbox": [{"x": 0.7192147374153137, "y": 0.20100925862789154}, {"x": 0.7501487135887146, "y": 0.20100925862789154}, {"x": 0.7501487135887146, "y": 0.20941968262195587}, {"x": 0.7192147374153137, "y": 0.20941968262195587}], "text": "42.9\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.20142976939678192}, {"x": 0.5526472330093384, "y": 0.20142976939678192}, {"x": 0.5526472330093384, "y": 0.20941968262195587}, {"x": 0.5205234885215759, "y": 0.20941968262195587}], "text": "40.4\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.20100925862789154}, {"x": 0.21058893203735352, "y": 0.20100925862789154}, {"x": 0.21058893203735352, "y": 0.21152228116989136}, {"x": 0.1641879826784134, "y": 0.21152228116989136}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.24806663393974304, "y": 0.20100925862789154}, {"x": 0.4782867431640625, "y": 0.20100925862789154}, {"x": 0.4782867431640625, "y": 0.21152228116989136}, {"x": 0.24806663393974304, "y": 0.21152228116989136}], "text": "REALMNews (Guu et al., 2020)\n"}
{"page": 8, "bbox": [{"x": 0.6044021248817444, "y": 0.20605550706386566}, {"x": 0.6091611981391907, "y": 0.20605550706386566}, {"x": 0.6091611981391907, "y": 0.20773759484291077}, {"x": 0.6044021248817444, "y": 0.20773759484291077}], "text": "-\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.22371740639209747}, {"x": 0.6228435635566711, "y": 0.2232968807220459}, {"x": 0.6228435635566711, "y": 0.23128679394721985}, {"x": 0.5913146734237671, "y": 0.23170731961727142}], "text": "52.4\n"}
{"page": 8, "bbox": [{"x": 0.24925640225410461, "y": 0.22287636995315552}, {"x": 0.29506245255470276, "y": 0.2232968807220459}, {"x": 0.29506245255470276, "y": 0.232127845287323}, {"x": 0.24925640225410461, "y": 0.23170731961727142}], "text": "BM25\n"}
{"page": 8, "bbox": [{"x": 0.7929803729057312, "y": 0.2232968807220459}, {"x": 0.8227245807647705, "y": 0.22287636995315552}, {"x": 0.8227245807647705, "y": 0.23170731961727142}, {"x": 0.7929803729057312, "y": 0.232127845287323}], "text": "38.1\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.22371740639209747}, {"x": 0.5538370013237, "y": 0.22371740639209747}, {"x": 0.5538370013237, "y": 0.23170731961727142}, {"x": 0.5211184024810791, "y": 0.23170731961727142}], "text": "32.6\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.22371740639209747}, {"x": 0.7513384819030762, "y": 0.22371740639209747}, {"x": 0.7513384819030762, "y": 0.23170731961727142}, {"x": 0.6609161496162415, "y": 0.23170731961727142}], "text": "29.9 24.9\n"}
{"page": 8, "bbox": [{"x": 0.662105917930603, "y": 0.23717409372329712}, {"x": 0.7513384819030762, "y": 0.23969721794128418}, {"x": 0.7507436275482178, "y": 0.2502102553844452}, {"x": 0.6615110039710999, "y": 0.24768713116645813}], "text": "34.6 25.9\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.24011774361133575}, {"x": 0.28316476941108704, "y": 0.24011774361133575}, {"x": 0.28316476941108704, "y": 0.2481076568365097}, {"x": 0.24866151809692383, "y": 0.2481076568365097}], "text": "DPR\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.24011774361133575}, {"x": 0.5526472330093384, "y": 0.24011774361133575}, {"x": 0.5526472330093384, "y": 0.2481076568365097}, {"x": 0.5205234885215759, "y": 0.2481076568365097}], "text": "41.5\n"}
{"page": 8, "bbox": [{"x": 0.792385458946228, "y": 0.24011774361133575}, {"x": 0.8233194351196289, "y": 0.24011774361133575}, {"x": 0.8233194351196289, "y": 0.2481076568365097}, {"x": 0.792385458946228, "y": 0.2481076568365097}], "text": "29.8\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.24011774361133575}, {"x": 0.6216537952423096, "y": 0.24011774361133575}, {"x": 0.6216537952423096, "y": 0.24852816760540009}, {"x": 0.5907198190689087, "y": 0.24852816760540009}], "text": "56.8\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.2392767071723938}, {"x": 0.21058893203735352, "y": 0.23969721794128418}, {"x": 0.21058893203735352, "y": 0.25105130672454834}, {"x": 0.1641879826784134, "y": 0.25063079595565796}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.7929803729057312, "y": 0.2552565038204193}, {"x": 0.8245092034339905, "y": 0.25483599305152893}, {"x": 0.8245092034339905, "y": 0.26366695761680603}, {"x": 0.7929803729057312, "y": 0.2640874683856964}], "text": "36.7\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.25609755516052246}, {"x": 0.5532421469688416, "y": 0.25609755516052246}, {"x": 0.5532421469688416, "y": 0.2640874683856964}, {"x": 0.5211184024810791, "y": 0.2640874683856964}], "text": "39.0\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.25609755516052246}, {"x": 0.6228435635566711, "y": 0.25609755516052246}, {"x": 0.6228435635566711, "y": 0.2640874683856964}, {"x": 0.5913146734237671, "y": 0.2640874683856964}], "text": "57.0\n"}
{"page": 8, "bbox": [{"x": 0.6615110039710999, "y": 0.2556770443916321}, {"x": 0.7513384819030762, "y": 0.2556770443916321}, {"x": 0.7513384819030762, "y": 0.2645079791545868}, {"x": 0.6615110039710999, "y": 0.2645079791545868}], "text": "35.2 28.0\n"}
{"page": 8, "bbox": [{"x": 0.24925640225410461, "y": 0.25609755516052246}, {"x": 0.3408685326576233, "y": 0.25609755516052246}, {"x": 0.3408685326576233, "y": 0.2645079791545868}, {"x": 0.24925640225410461, "y": 0.2645079791545868}], "text": "BM25+DPR\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.278385192155838}, {"x": 0.28197500109672546, "y": 0.27754414081573486}, {"x": 0.28256988525390625, "y": 0.28679561614990234}, {"x": 0.24925640225410461, "y": 0.2876366674900055}], "text": "DPR\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.278385192155838}, {"x": 0.6216537952423096, "y": 0.278385192155838}, {"x": 0.6216537952423096, "y": 0.28679561614990234}, {"x": 0.5907198190689087, "y": 0.28679561614990234}], "text": "56.8\n"}
{"page": 8, "bbox": [{"x": 0.7192147374153137, "y": 0.2788057327270508}, {"x": 0.7513384819030762, "y": 0.278385192155838}, {"x": 0.7513384819030762, "y": 0.28637510538101196}, {"x": 0.7192147374153137, "y": 0.28679561614990234}], "text": "49.4\n"}
{"page": 8, "bbox": [{"x": 0.5199286341667175, "y": 0.278385192155838}, {"x": 0.55205237865448, "y": 0.278385192155838}, {"x": 0.55205237865448, "y": 0.2872161567211151}, {"x": 0.5199286341667175, "y": 0.2872161567211151}], "text": "41.5\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.2788057327270508}, {"x": 0.6936347484588623, "y": 0.2788057327270508}, {"x": 0.6936347484588623, "y": 0.28679561614990234}, {"x": 0.6609161496162415, "y": 0.28679561614990234}], "text": "42.4\n"}
{"page": 8, "bbox": [{"x": 0.792385458946228, "y": 0.2788057327270508}, {"x": 0.8239143490791321, "y": 0.2788057327270508}, {"x": 0.8239143490791321, "y": 0.2872161567211151}, {"x": 0.792385458946228, "y": 0.2872161567211151}], "text": "24.1\n"}
{"page": 8, "bbox": [{"x": 0.1635930985212326, "y": 0.28637510538101196}, {"x": 0.20404520630836487, "y": 0.2855340540409088}, {"x": 0.20464009046554565, "y": 0.2943650186061859}, {"x": 0.1641879826784134, "y": 0.29520606994628906}], "text": "Multi\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.2943650186061859}, {"x": 0.5514574646949768, "y": 0.29394447803497314}, {"x": 0.5514574646949768, "y": 0.30235493183135986}, {"x": 0.5211184024810791, "y": 0.30277544260025024}], "text": "38.8\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.2947855293750763}, {"x": 0.622248649597168, "y": 0.2947855293750763}, {"x": 0.622248649597168, "y": 0.30277544260025024}, {"x": 0.5907198190689087, "y": 0.30277544260025024}], "text": "57.9\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.29394447803497314}, {"x": 0.7531231641769409, "y": 0.2943650186061859}, {"x": 0.7531231641769409, "y": 0.3036164939403534}, {"x": 0.6609161496162415, "y": 0.3031959533691406}], "text": "41.1 50.6\n"}
{"page": 8, "bbox": [{"x": 0.7929803729057312, "y": 0.2943650186061859}, {"x": 0.8233194351196289, "y": 0.2947855293750763}, {"x": 0.8233194351196289, "y": 0.3031959533691406}, {"x": 0.7929803729057312, "y": 0.30277544260025024}], "text": "35.8\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.2947855293750763}, {"x": 0.34205830097198486, "y": 0.2947855293750763}, {"x": 0.34205830097198486, "y": 0.3031959533691406}, {"x": 0.24866151809692383, "y": 0.3031959533691406}], "text": "BM25+DPR\n"}
{"page": 8, "bbox": [{"x": 0.12016656994819641, "y": 0.3246425688266754}, {"x": 0.8857823014259338, "y": 0.3246425688266754}, {"x": 0.8857823014259338, "y": 0.37804877758026123}, {"x": 0.12016656994819641, "y": 0.37804877758026123}], "text": "Table 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers.\nREALMwiki and REALM News are the same model but pretrained on Wikipedia and CC-News, respectively. Single\nand Multi denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets\n(all except SQUAD). For WQ and TREC in the Multi setting, we fine-tune the reader trained on NQ.\n"}
{"page": 8, "bbox": [{"x": 0.5151695609092712, "y": 0.40790581703186035}, {"x": 0.8863771557807922, "y": 0.40790581703186035}, {"x": 0.8863771557807922, "y": 0.7737594842910767}, {"x": 0.5151695609092712, "y": 0.7737594842910767}], "text": "trained, following Lee et al. (2019). This approach\nobtains a score of 39.8 EM, which suggests that our\nstrategy of training a strong retriever and reader in\nisolation can leverage effectively available supervi-\nsion, while outperforming a comparable joint train-\ning approach with a simpler design (Appendix D).\nOne thing worth noticing is that our reader does\nconsider more passages compared to ORQA, al-\nthough it is not completely clear how much more\ntime it takes for inference. While DPR processes\nup to 100 passages for each question, the reader\nis able to fit all of them into one batch on a sin-\ngle 32GB GPU, thus the latency remains almost\nidentical to the single passage case (around 20ms).\nThe exact impact on throughput is harder to mea-\nsure: ORQA uses 2-3x longer passages compared\nto DPR (288 word pieces compared to our 100\ntokens) and the computational complexity is super-\nlinear in passage length. We also note that we\nfound k 50 to be optimal for NQ, and k\nleads to only marginal loss in exact match accu-\nracy (40.8 vs. 41.5 EM on NQ), which should be\nroughly comparable to ORQA's 5-passage setup.\n"}
{"page": 8, "bbox": [{"x": 0.11897680163383484, "y": 0.40790581703186035}, {"x": 0.4913741946220398, "y": 0.4074852764606476}, {"x": 0.4919690787792206, "y": 0.9083263278007507}, {"x": 0.11957168579101562, "y": 0.9087468385696411}], "text": "see that higher retriever accuracy typically leads to\nbetter final QA results: in all cases except SQUAD,\nanswers extracted from the passages retrieved by\nDPR are more likely to be correct, compared to\nthose from BM25. For large datasets like NQ and\nTriviaQA, models trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\non smaller datasets like WQ and TREC, the multi-\ndataset setting has a clear advantage. Overall, our\nDPR-based models outperform the previous state-\nof-the-art results on four out of the five datasets,\nwith 1% to 12% absolute differences in exact match\naccuracy. It is interesting to contrast our results to\nthose of ORQA (Lee et al., 2019) and also the\nconcurrently developed approach, REALM (Guu\net al., 2020). While both methods include addi-\ntional pretraining tasks and employ an expensive\nend-to-end training regime, DPR manages to out-\nperform them on both NQ and TriviaQA, simply\nby focusing on learning a strong passage retrieval\nmodel using pairs of questions and answers. The\nadditional pretraining tasks are likely more useful\nonly when the target training sets are small. Al-\nthough the results of DPR on WQ and TREC in the\nsingle-dataset setting are less competitive, adding\nmore question-answer pairs helps boost the perfor-\nmance, achieving the new state of the art.\nTo compare our pipeline training approach with\njoint learning, we run an ablation on Natural Ques-\ntions where the retriever and reader are jointly\n"}
{"page": 8, "bbox": [{"x": 0.8441404104232788, "y": 0.714886486530304}, {"x": 0.8828078508377075, "y": 0.714886486530304}, {"x": 0.8828078508377075, "y": 0.7224558591842651}, {"x": 0.8441404104232788, "y": 0.7224558591842651}], "text": "= 10\n"}
{"page": 8, "bbox": [{"x": 0.5157644152641296, "y": 0.7910008430480957}, {"x": 0.527067244052887, "y": 0.7910008430480957}, {"x": 0.527067244052887, "y": 0.7994112968444824}, {"x": 0.5157644152641296, "y": 0.7994112968444824}], "text": "7\n"}
{"page": 8, "bbox": [{"x": 0.5466983914375305, "y": 0.7905803322792053}, {"x": 0.6662700772285461, "y": 0.7905803322792053}, {"x": 0.6662700772285461, "y": 0.8002523183822632}, {"x": 0.5466983914375305, "y": 0.8002523183822632}], "text": "Related Work\n"}
{"page": 8, "bbox": [{"x": 0.5151695609092712, "y": 0.8145500421524048}, {"x": 0.8863771557807922, "y": 0.8149705529212952}, {"x": 0.8863771557807922, "y": 0.9087468385696411}, {"x": 0.5151695609092712, "y": 0.9083263278007507}], "text": "Passage retrieval has been an important compo-\nnent for open-domain QA (Voorhees, 1999). It\nnot only effectively reduces the search space for\nanswer extraction, but also identifies the support\ncontext for users to verify the answer. Strong sparse\nvector space models like TF-IDF or BM25 have\n"}
{"page": 8, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5211184024810791, "y": 0.9238856434822083}, {"x": 0.5211184024810791, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6776\n"}
{"page": 9, "bbox": [{"x": 0.5151695609092712, "y": 0.07527334243059158}, {"x": 0.8869720697402954, "y": 0.07569386065006256}, {"x": 0.8863771557807922, "y": 0.36164844036102295}, {"x": 0.5145746469497681, "y": 0.36122792959213257}], "text": "effective solution that shows stronger empirical per-\nformance, without relying on additional pretraining\nor complex joint training schemes.\nDPR has also been used as an important mod-\nule in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the pre-\nvious iteration to discover new negatives and con-\nstruct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raf-\nfel et al., 2019), achieving good performance on\nopen-domain QA and other knowledge-intensive\ntasks.\n"}
{"page": 9, "bbox": [{"x": 0.5163593292236328, "y": 0.37973088026046753}, {"x": 0.6418798565864563, "y": 0.37973088026046753}, {"x": 0.6418798565864563, "y": 0.3894028663635254}, {"x": 0.5163593292236328, "y": 0.3894028663635254}], "text": "8 Conclusion\n"}
{"page": 9, "bbox": [{"x": 0.11957168579101562, "y": 0.07695542275905609}, {"x": 0.4913741946220398, "y": 0.07737594842910767}, {"x": 0.4901844263076782, "y": 0.9095878601074219}, {"x": 0.11838191747665405, "y": 0.9091673493385315}], "text": "been used as the standard method applied broadly\nto various QA tasks (e.g., Chen et al., 2017; Yang\net al., 2019a,b; Nie et al., 2019; Min et al., 2019a;\nWolfson et al., 2020). Augmenting text-based re-\ntrieval with external structured information, such\nas knowledge graph and Wikipedia hyperlinks, has\nalso been explored recently (Min et al., 2019b; Asai\net al., 2020).\nThe use of dense vector representations for re-\ntrieval has a long history since Latent Semantic\nAnalysis (Deerwester et al., 1990). Using labeled\npairs of queries and documents, discriminatively\ntrained dense encoders have become popular re-\ncently (Yih et al., 2011; Huang et al., 2013; Gillick\net al., 2019), with applications to cross-lingual\ndocument retrieval, ad relevance prediction, Web\nsearch and entity retrieval. Such approaches com-\nplement the sparse vector methods as they can po-\ntentially give high similarity scores to semantically\nrelevant text pairs, even without exact token match-\ning. The dense representation alone, however, is\ntypically inferior to the sparse one. While not the\nfocus of this work, dense representations from pre-\ntrained models, along with cross-attention mecha-\nnisms, have also been shown effective in passage\nor dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\nof the BERT encoders.\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to re-\ntrieve relevant passages iteratively using reformu-\nlated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) pro-\npose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input ques-\ntions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\noutperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages dur-\ning training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n"}
{"page": 9, "bbox": [{"x": 0.5145746469497681, "y": 0.4053826630115509}, {"x": 0.8857823014259338, "y": 0.4049621522426605}, {"x": 0.8863771557807922, "y": 0.611017644405365}, {"x": 0.5151695609092712, "y": 0.6114381551742554}], "text": "In this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the tradi-\ntional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\nour empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional val-\nues. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\nopen-domain question answering benchmarks.\n"}
{"page": 9, "bbox": [{"x": 0.5151695609092712, "y": 0.6257359385490417}, {"x": 0.674003541469574, "y": 0.6257359385490417}, {"x": 0.674003541469574, "y": 0.6375105381011963}, {"x": 0.5151695609092712, "y": 0.6375105381011963}], "text": "Acknowledgments\n"}
{"page": 9, "bbox": [{"x": 0.5145746469497681, "y": 0.6509671807289124}, {"x": 0.8822129964828491, "y": 0.6509671807289124}, {"x": 0.8822129964828491, "y": 0.6787216067314148}, {"x": 0.5145746469497681, "y": 0.6787216067314148}], "text": "We thank the anonymous reviewers for their helpful\ncomments and suggestions.\n"}
{"page": 9, "bbox": [{"x": 0.5163593292236328, "y": 0.7102607488632202}, {"x": 0.6091611981391907, "y": 0.7102607488632202}, {"x": 0.6091611981391907, "y": 0.7195122241973877}, {"x": 0.5163593292236328, "y": 0.7195122241973877}], "text": "References\n"}
{"page": 9, "bbox": [{"x": 0.5157644152641296, "y": 0.7312868237495422}, {"x": 0.8863771557807922, "y": 0.732127845287323}, {"x": 0.8857823014259338, "y": 0.9095878601074219}, {"x": 0.5151695609092712, "y": 0.9087468385696411}], "text": "Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over Wikipedia graph\nfor question answering. In International Conference\non Learning Representations (ICLR).\nPetr Baudiš and Jan Šedivỳ. 2015. Modeling of the\nquestion answering task in the yodaqa system. In In-\nternational Conference of the Cross-Language Eval-\nuation Forum for European Languages, pages 222-\n228. Springer.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\n"}
{"page": 9, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6777\n"}
{"page": 10, "bbox": [{"x": 0.13860797882080078, "y": 0.0786375105381012}, {"x": 0.48958954215049744, "y": 0.07821699231863022}, {"x": 0.48958954215049744, "y": 0.10260723531246185}, {"x": 0.13860797882080078, "y": 0.10302775353193283}], "text": "question-answer pairs. In Empirical Methods in Nat-\nural Language Processing (EMNLP).\n"}
{"page": 10, "bbox": [{"x": 0.5324211716651917, "y": 0.07947855442762375}, {"x": 0.8857823014259338, "y": 0.07821699231863022}, {"x": 0.8857823014259338, "y": 0.11522287875413895}, {"x": 0.5324211716651917, "y": 0.11648444086313248}], "text": "clickthrough data. In ACM International Confer-\nence on Information and Knowledge Management\n(CIKM), pages 2333-2338.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.1160639226436615}, {"x": 0.4901844263076782, "y": 0.11564339697360992}, {"x": 0.4901844263076782, "y": 0.16568544507026672}, {"x": 0.12016656994819641, "y": 0.1661059707403183}], "text": "Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nSäckinger, and Roopak Shah. 1994. Signature verifi-\ncation using a “Siamese” time delay neural network.\nIn NIPS, pages 737–744.\n"}
{"page": 10, "bbox": [{"x": 0.5163593292236328, "y": 0.13120269775390625}, {"x": 0.8851873874664307, "y": 0.13078217208385468}, {"x": 0.8851873874664307, "y": 0.24558451771736145}, {"x": 0.5163593292236328, "y": 0.24600504338741302}], "text": "Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In International Conference\non Learning Representations (ICLR).\nGautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. ArXiv, abs/2007.01282.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.26156434416770935}, {"x": 0.8851873874664307, "y": 0.26156434416770935}, {"x": 0.8851873874664307, "y": 0.29562658071517944}, {"x": 0.5157644152641296, "y": 0.29562658071517944}], "text": "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with GPUs. Arxiv,\nabs/1702.08734.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.17830109596252441}, {"x": 0.4913741946220398, "y": 0.17914213240146637}, {"x": 0.490779310464859, "y": 0.38099244236946106}, {"x": 0.11957168579101562, "y": 0.3801513910293579}], "text": "Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\nProceedings of the 22nd international conference on\nMachine learning, pages 89–96.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Association for Computa-\ntional Linguistics (ACL), pages 1870-1879.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nand Andrew McCallum. 2019. Multi-step retriever-\nreader interaction for scalable open-domain question\nanswering. In International Conference on Learn-\ning Representations (ICLR).\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.3128679692745209}, {"x": 0.8857823014259338, "y": 0.3124474287033081}, {"x": 0.8857823014259338, "y": 0.37594616413116455}, {"x": 0.5157644152641296, "y": 0.3763667047023773}], "text": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Association for Computational Lin-\nguistics (ACL), pages 1601–1611.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.3910849392414093}, {"x": 0.8857823014259338, "y": 0.39150547981262207}, {"x": 0.8857823014259338, "y": 0.453742653131485}, {"x": 0.5157644152641296, "y": 0.4533221125602722}], "text": "Omar Khattab and Matei Zaharia. 2020. ColBERT:\nEfficient and effective passage search via contextu-\nalized late interaction over BERT. In ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval (SIGIR), pages 39-48.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.39360806345939636}, {"x": 0.490779310464859, "y": 0.393187552690506}, {"x": 0.490779310464859, "y": 0.4550042152404785}, {"x": 0.12016656994819641, "y": 0.4554247260093689}], "text": "Scott Deerwester, Susan T Dumais, George W Fur-\nnas, Thomas K Landauer, and Richard Harshman.\n1990. Indexing by latent semantic analysis. Jour-\nnal of the American society for information science,\n41(6):391-407.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.4676198363304138}, {"x": 0.8851873874664307, "y": 0.4680403769016266}, {"x": 0.8851873874664307, "y": 0.5033641457557678}, {"x": 0.5157644152641296, "y": 0.5029436349868774}], "text": "Brian Kulis. 2013. Metric learning: A survey. Foun-\ndations and Trends in Machine Learning, 5(4):287–\n364.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.46888139843940735}, {"x": 0.4901844263076782, "y": 0.46888139843940735}, {"x": 0.4901844263076782, "y": 0.5319596529006958}, {"x": 0.12016656994819641, "y": 0.5319596529006958}], "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL).\n"}
{"page": 10, "bbox": [{"x": 0.1207614541053772, "y": 0.5437342524528503}, {"x": 0.4901844263076782, "y": 0.5449957847595215}, {"x": 0.4901844263076782, "y": 0.581581175327301}, {"x": 0.1207614541053772, "y": 0.5803195834159851}], "text": "David A Ferrucci. 2012. Introduction to \"This is Wat-\nson\". IBM Journal of Research and Development,\n56(3.4):1-1.\n"}
{"page": 10, "bbox": [{"x": 0.5163593292236328, "y": 0.5201850533485413}, {"x": 0.8851873874664307, "y": 0.5206055641174316}, {"x": 0.8851873874664307, "y": 0.6362489461898804}, {"x": 0.5163593292236328, "y": 0.63582843542099}], "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics (TACL).\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.6501261591911316}, {"x": 0.8857823014259338, "y": 0.6509671807289124}, {"x": 0.8857823014259338, "y": 0.7014297842979431}, {"x": 0.5157644152641296, "y": 0.7005887031555176}], "text": "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Association for Com-\nputational Linguistics (ACL), pages 6086-6096.\n"}
{"page": 10, "bbox": [{"x": 0.11957168579101562, "y": 0.593776285648346}, {"x": 0.4901844263076782, "y": 0.5933557748794556}, {"x": 0.4901844263076782, "y": 0.7813288569450378}, {"x": 0.11957168579101562, "y": 0.7817493677139282}], "text": "Daniel Gillick, Sayali Kulkarni, Larry Lansing,\nAlessandro Presta, Jason Baldridge, Eugene Ie, and\nDiego Garcia-Olano. 2019. Learning dense repre-\nsentations for entity retrieval. In Computational Nat-\nural Language Learning (CoNLL).\nRuiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and\nDavid Simcha. 2016. Quantization based fast inner\nproduct search. In Artificial Intelligence and Statis-\ntics, pages 482-490.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training.\nArXiv, abs/2002.08909.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.7157275080680847}, {"x": 0.8857823014259338, "y": 0.7161480188369751}, {"x": 0.8857823014259338, "y": 0.8057190775871277}, {"x": 0.5157644152641296, "y": 0.8052985668182373}], "text": "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Association for Computa-\ntional Linguistics (ACL), pages 7871-7880.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.7964676022529602}, {"x": 0.4901844263076782, "y": 0.7960470914840698}, {"x": 0.4901844263076782, "y": 0.8587048053741455}, {"x": 0.12016656994819641, "y": 0.8591253161430359}], "text": "Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nhsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Effi-\ncient natural language response suggestion for smart\nreply. ArXiv, abs/1705.00652.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.8195962905883789}, {"x": 0.8845925331115723, "y": 0.8195962905883789}, {"x": 0.8845925331115723, "y": 0.9087468385696411}, {"x": 0.5157644152641296, "y": 0.9087468385696411}], "text": "Patrick Lewis, Ethan Perez, Aleksandara Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in\nNeural Information Processing Systems (NeurIPS).\n"}
{"page": 10, "bbox": [{"x": 0.1207614541053772, "y": 0.8721615076065063}, {"x": 0.48958954215049744, "y": 0.8721615076065063}, {"x": 0.48958954215049744, "y": 0.9083263278007507}, {"x": 0.1207614541053772, "y": 0.9083263278007507}], "text": "Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for Web search using\n"}
{"page": 10, "bbox": [{"x": 0.4830458164215088, "y": 0.924726665019989}, {"x": 0.5193337202072144, "y": 0.924726665019989}, {"x": 0.5193337202072144, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6778\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.07821699231863022}, {"x": 0.8857823014259338, "y": 0.07947855442762375}, {"x": 0.8851873874664307, "y": 0.1408746838569641}, {"x": 0.5151695609092712, "y": 0.13961312174797058}], "text": "Anshumali Shrivastava and Ping Li. 2014. Asymmet-\nric LSH (ALSH) for sublinear time maximum inner\nproduct search (MIPS). In Advances in Neural In-\nformation Processing Systems (NIPS), pages 2321-\n2329.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.07905802875757217}, {"x": 0.4913741946220398, "y": 0.0786375105381012}, {"x": 0.4913741946220398, "y": 0.20311185717582703}, {"x": 0.12016656994819641, "y": 0.2035323828458786}], "text": "Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun.\n2018. Denoising distantly supervised open-domain\nquestion answering. In Association for Computa-\ntional Linguistics (ACL), pages 1736-1745.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\nproach for weakly supervised question answering.\nIn Empirical Methods in Natural Language Process-\ning (EMNLP).\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.15391084551811218}, {"x": 0.8857823014259338, "y": 0.15475189685821533}, {"x": 0.8857823014259338, "y": 0.18965516984462738}, {"x": 0.5157644152641296, "y": 0.18881413340568542}], "text": "Ellen M Voorhees. 1999. The TREC-8 question an-\nswering track report. In TREC, volume 99, pages\n77-82.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.2161480188369751}, {"x": 0.4901844263076782, "y": 0.2161480188369751}, {"x": 0.4901844263076782, "y": 0.3393608033657074}, {"x": 0.12016656994819641, "y": 0.3393608033657074}], "text": "Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\ntrieval and reading for open domain question answer-\ning. ArXiv, abs/1911.03868.\nDan Moldovan, Marius Paşca, Sanda Harabagiu, and\nMihai Surdeanu. 2003. Performance issues and er-\nror analysis in an open-domain question answering\nsystem. ACM Transactions on Information Systems\n(TOIS), 21(2):133–154.\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.20479394495487213}, {"x": 0.8857823014259338, "y": 0.20437341928482056}, {"x": 0.8863771557807922, "y": 0.35618165135383606}, {"x": 0.5163593292236328, "y": 0.35660219192504883}], "text": "Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R^3:\nReinforced ranker-reader for open-domain question\nanswering. In Conference on Artificial Intelligence\n(AAAI).\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-\npati, and Bing Xiang. 2019. Multi-passage BERT:\nA globally normalized bert model for open-domain\nquestion answering. In Empirical Methods in Natu-\nral Language Processing (EMNLP).\n"}
{"page": 11, "bbox": [{"x": 0.5163593292236328, "y": 0.3692178428173065}, {"x": 0.8857823014259338, "y": 0.36879730224609375}, {"x": 0.8857823014259338, "y": 0.4318755269050598}, {"x": 0.5163593292236328, "y": 0.4322960376739502}], "text": "Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question under-\nstanding benchmark. Transactions of the Associa-\ntion of Computational Linguistics (TACL).\n"}
{"page": 11, "bbox": [{"x": 0.1207614541053772, "y": 0.3519764542579651}, {"x": 0.4901844263076782, "y": 0.3515559434890747}, {"x": 0.4901844263076782, "y": 0.4629940986633301}, {"x": 0.1207614541053772, "y": 0.46341463923454285}], "text": "Stephen Mussmann and Stefano Ermon. 2016. Learn-\ning and inference via maximum inner product search.\nIn International Conference on Machine Learning\n(ICML), pages 2587-2596.\nYixin Nie, Songhe Wang, and Mohit Bansal. 2019. Re-\nvealing the importance of semantic retrieval for ma-\nchine reading at scale. In Empirical Methods in Nat-\nural Language Processing (EMNLP).\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.44533219933509827}, {"x": 0.8851873874664307, "y": 0.44533219933509827}, {"x": 0.8851873874664307, "y": 0.5058873295783997}, {"x": 0.5157644152641296, "y": 0.5058873295783997}], "text": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020a. Approximate nearest neighbor\nnegative contrastive learning for dense text retrieval.\nArXiv, abs/2007.00808.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.47603029012680054}, {"x": 0.4878048896789551, "y": 0.4751892387866974}, {"x": 0.4878048896789551, "y": 0.4983179271221161}, {"x": 0.12016656994819641, "y": 0.49915894865989685}], "text": "Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with BERT. ArXiv, abs/1901.04085.\n"}
{"page": 11, "bbox": [{"x": 0.1207614541053772, "y": 0.5117745995521545}, {"x": 0.4901844263076782, "y": 0.5117745995521545}, {"x": 0.4901844263076782, "y": 0.5723296999931335}, {"x": 0.1207614541053772, "y": 0.5723296999931335}], "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. ArXiv, abs/1910.10683.\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.5197644829750061}, {"x": 0.8845925331115723, "y": 0.5206055641174316}, {"x": 0.8845925331115723, "y": 0.5693860650062561}, {"x": 0.5157644152641296, "y": 0.5685449838638306}], "text": "Wenhan Xiong, Hankang Wang, and William Yang\nWang. 2020b. Progressively pretrained dense corpus\nindex for open-domain question answering. ArXiv,\nabs/2005.00038.\n"}
{"page": 11, "bbox": [{"x": 0.5151695609092712, "y": 0.5832632184028625}, {"x": 0.8857823014259338, "y": 0.5836837887763977}, {"x": 0.8857823014259338, "y": 0.6467620134353638}, {"x": 0.5151695609092712, "y": 0.6463414430618286}], "text": "Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.\nEnd-to-end open-domain question answering with\nbertserini. In North American Association for Com-\nputational Linguistics (NAACL), pages 72–77.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.5862069129943848}, {"x": 0.48899465799331665, "y": 0.5841042995452881}, {"x": 0.490779310464859, "y": 0.7216147780418396}, {"x": 0.12195122241973877, "y": 0.7237173914909363}], "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQUAD: 100,000+ questions\nfor machine comprehension of text. In Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2383-2392.\nParikshit Ram and Alexander G Gray. 2012. Maximum\ninner-product search using cone trees. In Proceed-\nings of the 18th ACM SIGKDD international con-\nference on Knowledge discovery and data mining,\npages 931-939.\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.6593776345252991}, {"x": 0.8851873874664307, "y": 0.6589571237564087}, {"x": 0.8851873874664307, "y": 0.707317054271698}, {"x": 0.5157644152641296, "y": 0.7077375650405884}], "text": "Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019b. Data augmentation for\nbert fine-tuning in open-domain question answering.\nArXiv, abs/1904.06652.\n"}
{"page": 11, "bbox": [{"x": 0.5151695609092712, "y": 0.7220353484153748}, {"x": 0.8851873874664307, "y": 0.7190916538238525}, {"x": 0.8863771557807922, "y": 0.7821698784828186}, {"x": 0.5163593292236328, "y": 0.785113513469696}], "text": "Wen-tau Yih, Kristina Toutanova, John C Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In Com-\nputational Natural Language Learning (CoNLL),\npages 247-256.\n"}
{"page": 11, "bbox": [{"x": 0.1207614541053772, "y": 0.7354919910430908}, {"x": 0.490779310464859, "y": 0.7350714802742004}, {"x": 0.490779310464859, "y": 0.7720773816108704}, {"x": 0.1207614541053772, "y": 0.7724978923797607}], "text": "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? ArXiv, abs/2002.08910.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.7846930027008057}, {"x": 0.490779310464859, "y": 0.7842724919319153}, {"x": 0.490779310464859, "y": 0.9079058170318604}, {"x": 0.12016656994819641, "y": 0.9083263278007507}], "text": "Stephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333-389.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.\nReal-time open-domain question answering with\ndense-sparse phrase index. In Association for Com-\nputational Linguistics (ACL).\n"}
{"page": 11, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6779\n"}
{"page": 12, "bbox": [{"x": 0.11957168579101562, "y": 0.07569386065006256}, {"x": 0.3242117762565613, "y": 0.07737594842910767}, {"x": 0.3242117762565613, "y": 0.09083263576030731}, {"x": 0.11957168579101562, "y": 0.08915054798126221}], "text": "A Distant Supervision\n"}
{"page": 12, "bbox": [{"x": 0.6151100397109985, "y": 0.08158116042613983}, {"x": 0.8744794726371765, "y": 0.08158116042613983}, {"x": 0.8744794726371765, "y": 0.0929352417588234}, {"x": 0.6151100397109985, "y": 0.0929352417588234}], "text": "Top-1 Top-5 Top-20 Top-100\n"}
{"page": 12, "bbox": [{"x": 0.6210588812828064, "y": 0.10428931564092636}, {"x": 0.7810826897621155, "y": 0.10428931564092636}, {"x": 0.7810826897621155, "y": 0.12910008430480957}, {"x": 0.6210588812828064, "y": 0.12910008430480957}], "text": "44.9 66.8 78.1\n43.9 65.3 77.1\n"}
{"page": 12, "bbox": [{"x": 0.8262938857078552, "y": 0.10470984131097794}, {"x": 0.8578227162361145, "y": 0.10470984131097794}, {"x": 0.8578227162361145, "y": 0.128679558634758}, {"x": 0.8262938857078552, "y": 0.128679558634758}], "text": "85.0\n84.4\n"}
{"page": 12, "bbox": [{"x": 0.5246877074241638, "y": 0.10428931564092636}, {"x": 0.5984532833099365, "y": 0.10513035953044891}, {"x": 0.5978584289550781, "y": 0.13120269775390625}, {"x": 0.5240927934646606, "y": 0.1303616464138031}], "text": "Gold\nDist. Sup.\n"}
{"page": 12, "bbox": [{"x": 0.5157644152641296, "y": 0.14928510785102844}, {"x": 0.8851873874664307, "y": 0.1501261591911316}, {"x": 0.8851873874664307, "y": 0.20479394495487213}, {"x": 0.5157644152641296, "y": 0.20395290851593018}], "text": "Table 5: Retrieval accuracy on the development set of\nNatural Questions, trained on passages that match the\ngold context (Gold) or the top BM25 passage that con-\ntains the answer (Dist. Sup.).\n"}
{"page": 12, "bbox": [{"x": 0.11957168579101562, "y": 0.10555088520050049}, {"x": 0.4913741946220398, "y": 0.10555088520050049}, {"x": 0.4913741946220398, "y": 0.3263246417045593}, {"x": 0.11957168579101562, "y": 0.3263246417045593}], "text": "When training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion and answer pairs, it is thus interesting\nto see when using the passages that contain the\nanswers as positives (i.e., the distant supervision\nsetting), whether there is a significant performance\ndegradation. Using the question and answer to-\ngether as the query, we run Lucene-BM25 and pick\nthe top passage that contains the answer as the pos-\nitive passage. Table 5 shows the performance of\nDPR when trained using the original setting and\nthe distant supervision setting.\n"}
{"page": 12, "bbox": [{"x": 0.5252825617790222, "y": 0.2283431440591812}, {"x": 0.6044021248817444, "y": 0.2287636697292328}, {"x": 0.6044021248817444, "y": 0.23717409372329712}, {"x": 0.5252825617790222, "y": 0.23675356805324554}], "text": "Sim Loss\n"}
{"page": 12, "bbox": [{"x": 0.6781677603721619, "y": 0.22708158195018768}, {"x": 0.8292682766914368, "y": 0.2287636697292328}, {"x": 0.8292682766914368, "y": 0.23885618150234222}, {"x": 0.6781677603721619, "y": 0.23717409372329712}], "text": "Retrieval Accuracy\n"}
{"page": 12, "bbox": [{"x": 0.8262938857078552, "y": 0.26703113317489624}, {"x": 0.8584176301956177, "y": 0.26661059260368347}, {"x": 0.8584176301956177, "y": 0.27544155716896057}, {"x": 0.8262938857078552, "y": 0.27586206793785095}], "text": "85.0\n"}
{"page": 12, "bbox": [{"x": 0.5252825617790222, "y": 0.27544155716896057}, {"x": 0.5490779280662537, "y": 0.27544155716896057}, {"x": 0.5490779280662537, "y": 0.28343144059181213}, {"x": 0.5252825617790222, "y": 0.28343144059181213}], "text": "DP\n"}
{"page": 12, "bbox": [{"x": 0.825698971748352, "y": 0.28301092982292175}, {"x": 0.8572278618812561, "y": 0.2821698784828186}, {"x": 0.8578227162361145, "y": 0.2910008430480957}, {"x": 0.8262938857078552, "y": 0.29184189438819885}], "text": "84.5\n"}
{"page": 12, "bbox": [{"x": 0.6942296028137207, "y": 0.28343144059181213}, {"x": 0.7876263856887817, "y": 0.28343144059181213}, {"x": 0.7876263856887817, "y": 0.29184189438819885}, {"x": 0.6942296028137207, "y": 0.29184189438819885}], "text": "65.0 77.2\n"}
{"page": 12, "bbox": [{"x": 0.5681142210960388, "y": 0.24516400694847107}, {"x": 0.8744794726371765, "y": 0.24390244483947754}, {"x": 0.8750743865966797, "y": 0.3322119414806366}, {"x": 0.5687090754508972, "y": 0.3334735035896301}], "text": "Top-1 Top-5 Top-20 Top-100\nNLL 44.9 66.8 78.1\nTriplet 41.6\nNLL 43.5 64.7\nTriplet 42.2 66.0 78.1\n"}
{"page": 12, "bbox": [{"x": 0.8274836540222168, "y": 0.3052985668182373}, {"x": 0.8572278618812561, "y": 0.3057190775871277}, {"x": 0.8572278618812561, "y": 0.31497055292129517}, {"x": 0.8274836540222168, "y": 0.3145500421524048}], "text": "83.1\n"}
{"page": 12, "bbox": [{"x": 0.7566924691200256, "y": 0.30613961815834045}, {"x": 0.7858417630195618, "y": 0.30613961815834045}, {"x": 0.7858417630195618, "y": 0.3145500421524048}, {"x": 0.7566924691200256, "y": 0.3145500421524048}], "text": "76.1\n"}
{"page": 12, "bbox": [{"x": 0.5246877074241638, "y": 0.31370899081230164}, {"x": 0.545508623123169, "y": 0.31370899081230164}, {"x": 0.545508623123169, "y": 0.32253995537757874}, {"x": 0.5246877074241638, "y": 0.32253995537757874}], "text": "L2\n"}
{"page": 12, "bbox": [{"x": 0.8262938857078552, "y": 0.32211941480636597}, {"x": 0.8572278618812561, "y": 0.3216989040374756}, {"x": 0.8572278618812561, "y": 0.32968881726264954}, {"x": 0.8262938857078552, "y": 0.3301093280315399}], "text": "84.9\n"}
{"page": 12, "bbox": [{"x": 0.12016656994819641, "y": 0.34398654103279114}, {"x": 0.4521118402481079, "y": 0.3406223654747009}, {"x": 0.4527067244052887, "y": 0.37047940492630005}, {"x": 0.1207614541053772, "y": 0.37384358048439026}], "text": "B Alternative Similarity Functions &\nTriplet Loss\n"}
{"page": 12, "bbox": [{"x": 0.5157644152641296, "y": 0.35071489214897156}, {"x": 0.8834027647972107, "y": 0.35113540291786194}, {"x": 0.8834027647972107, "y": 0.3894028663635254}, {"x": 0.5157644152641296, "y": 0.3889823257923126}], "text": "Table 6: Retrieval Top-k accuracy on the development\nset of Natural Questions using different similarity and\nloss functions.\n"}
{"page": 12, "bbox": [{"x": 0.5151695609092712, "y": 0.4213624894618988}, {"x": 0.8851873874664307, "y": 0.4217830002307892}, {"x": 0.8851873874664307, "y": 0.5142977237701416}, {"x": 0.5151695609092712, "y": 0.5138772130012512}], "text": "the correct answer, presumably by matching \"body\nof water\" with semantic neighbors such as sea and\nchannel, even though no lexical overlap exists. The\nsecond example is one where BM25 does better.\nThe salient phrase \"Thoros of Myr” is critical, and\nDPR is unable to capture it.\n"}
{"page": 12, "bbox": [{"x": 0.12016656994819641, "y": 0.38645920157432556}, {"x": 0.49256396293640137, "y": 0.38687974214553833}, {"x": 0.4919690787792206, "y": 0.656854510307312}, {"x": 0.11957168579101562, "y": 0.6564339995384216}], "text": "In addition to dot product (DP) and negative log-\nlikelihood based on softmax (NLL), we also exper-\niment with Euclidean distance (L2) and the triplet\nloss. We negate L2 similarity scores before ap-\nplying softmax and change signs of question-to-\npositive and question-to-negative similarities when\napplying the triplet loss on dot product scores. The\nmargin value of the triplet loss is set to 1. Ta-\nble 6 summarizes the results. All these additional\nexperiments are conducted using the same hyper-\nparameters tuned for the baseline (DP, NLL).\nNote that the retrieval accuracy for our \"baseline\"\nsettings reported in Table 5 (Gold) and Table 6\n(DP, NLL) is slightly better than those reported in\nTable 3. This is due to a better hyper-parameter\nsetting used in these analysis experiments, which\nis documented in our code release.\n"}
{"page": 12, "bbox": [{"x": 0.5157644152641296, "y": 0.5306980609893799}, {"x": 0.817370593547821, "y": 0.5298570394515991}, {"x": 0.817370593547821, "y": 0.556770384311676}, {"x": 0.5157644152641296, "y": 0.5576114654541016}], "text": "D Joint Training of Retriever and\nReader\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.5756938457489014}, {"x": 0.8857823014259338, "y": 0.5756938457489014}, {"x": 0.8857823014259338, "y": 0.910008430480957}, {"x": 0.5139797925949097, "y": 0.910008430480957}], "text": "We fix the passage encoder in our joint-training\nscheme while allowing only the question encoder\nto receive backpropagation signal from the com-\nbined (retriever + reader) loss function. This allows\nus to leverage the HNSW-based FAISS index for\nefficient low-latency retrieving, without reindexing\nthe passages during model updates. Our loss func-\ntion largely follows ORQA's approach, which uses\nlog probabilities of positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages'\nvectors participate in the loss calculation for all\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected\n"}
{"page": 12, "bbox": [{"x": 0.11957168579101562, "y": 0.6761984825134277}, {"x": 0.490779310464859, "y": 0.6761984825134277}, {"x": 0.490779310464859, "y": 0.9087468385696411}, {"x": 0.11957168579101562, "y": 0.9087468385696411}], "text": "C Qualitative Analysis\nAlthough DPR performs better than BM25 in gen-\neral, the retrieved passages of these two retrievers\nactually differ qualitatively. Methods like BM25\nare sensitive to highly selective keywords and\nphrases, but cannot capture lexical variations or se-\nmantic relationships well. In contrast, DPR excels\nat semantic representation, but might lack sufficient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples. In the first example, the top scor-\ning passage from BM25 is irrelevant, even though\nkeywords such as England and Ireland appear mul-\ntiple times. In comparison, DPR is able to return\n"}
{"page": 12, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5193337202072144, "y": 0.9238856434822083}, {"x": 0.5193337202072144, "y": 0.9322960376739502}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6780\n"}
{"page": 13, "bbox": [{"x": 0.13087448477745056, "y": 0.07989907264709473}, {"x": 0.17430101335048676, "y": 0.07947855442762375}, {"x": 0.17430101335048676, "y": 0.085786372423172}, {"x": 0.13087448477745056, "y": 0.08620689809322357}], "text": "Question\n"}
{"page": 13, "bbox": [{"x": 0.2938726842403412, "y": 0.07947855442762375}, {"x": 0.4259369373321533, "y": 0.0786375105381012}, {"x": 0.4259369373321533, "y": 0.08662741631269455}, {"x": 0.2938726842403412, "y": 0.0874684602022171}], "text": "Passage received by BM25\n"}
{"page": 13, "bbox": [{"x": 0.6008328199386597, "y": 0.07989907264709473}, {"x": 0.7293277978897095, "y": 0.07905802875757217}, {"x": 0.7293277978897095, "y": 0.08662741631269455}, {"x": 0.6008328199386597, "y": 0.0874684602022171}], "text": "Passage retrieved by DPR\n"}
{"page": 13, "bbox": [{"x": 0.29506245255470276, "y": 0.09419680386781693}, {"x": 0.3920285403728485, "y": 0.09545836597681046}, {"x": 0.3920285403728485, "y": 0.1034482792019844}, {"x": 0.29506245255470276, "y": 0.10218670964241028}], "text": "Title: British Cycling\n"}
{"page": 13, "bbox": [{"x": 0.6008328199386597, "y": 0.09587889164686203}, {"x": 0.6710291504859924, "y": 0.09629940986633301}, {"x": 0.6710291504859924, "y": 0.10218670964241028}, {"x": 0.6008328199386597, "y": 0.1017661914229393}], "text": "Title: Irish Sea\n"}
{"page": 13, "bbox": [{"x": 0.13027960062026978, "y": 0.09545836597681046}, {"x": 0.27305176854133606, "y": 0.09545836597681046}, {"x": 0.27305176854133606, "y": 0.11269974708557129}, {"x": 0.13027960062026978, "y": 0.11269974708557129}], "text": "What is the body of water\nbetween England and Ireland?\n"}
{"page": 13, "bbox": [{"x": 0.6002379655838013, "y": 0.10555088520050049}, {"x": 0.8875669240951538, "y": 0.10555088520050049}, {"x": 0.8875669240951538, "y": 0.17830109596252441}, {"x": 0.6002379655838013, "y": 0.17830109596252441}], "text": "...Annual traffic between Great Britain and Ireland amounts\nto over 12 million passengers and of traded goods. The Irish\nSea is connected to the North Atlantic at both its northern\nand southern ends. To the north, the connection is through\nthe North Channel between Scotland and Northern Ireland\nand the Malin Sea. The southern end is linked to the Atlantic\nthrough the St George's Channel between Ireland and Pem-\nbrokeshire, and the Celtic Sea....\n"}
{"page": 13, "bbox": [{"x": 0.2938726842403412, "y": 0.10513035953044891}, {"x": 0.5823914408683777, "y": 0.10555088520050049}, {"x": 0.5817965269088745, "y": 0.26198485493659973}, {"x": 0.2932778000831604, "y": 0.26156434416770935}], "text": "... England is not recognised as a region by the UCI, and\nthere is no English cycling team outside the Commonwealth\nGames. For those occasions, British Cycling selects and sup-\nports the England team. Cycling is represented on the Isle\nof Man by the Isle of Man Cycling Association. Cycling in\nNorthern Ireland is organised under Cycling Ulster, part of\nthe all-Ireland governing body Cycling Ireland. Until 2006,\na rival governing body existed, ...\nTitle: No One (Game of Thrones)\n... He may be \"no one,\" but there's still enough of a person\nleft in him to respect, and admire who this girl is and what\nshe's become. Arya finally tells us something that we've kind\nof known all along, that she's not no one, she's Arya Stark\nof Winterfell.\" \"No One\" saw the reintroduction of Richard\nDormer and Paul Kaye, who portrayed Beric Dondarrion and\nThoros of Myr, respectively, in the third season,...\n"}
{"page": 13, "bbox": [{"x": 0.6014277338981628, "y": 0.18671151995658875}, {"x": 0.7114812731742859, "y": 0.1875525712966919}, {"x": 0.7114812731742859, "y": 0.1947014331817627}, {"x": 0.6014277338981628, "y": 0.19386038184165955}], "text": "Title: Pål Sverre Hagen\n"}
{"page": 13, "bbox": [{"x": 0.13027960062026978, "y": 0.18797308206558228}, {"x": 0.26472339034080505, "y": 0.1875525712966919}, {"x": 0.26472339034080505, "y": 0.20311185717582703}, {"x": 0.13027960062026978, "y": 0.2035323828458786}], "text": "Who plays Thoros of Myr in\nGame of Thrones?\n"}
{"page": 13, "bbox": [{"x": 0.6002379655838013, "y": 0.19722455739974976}, {"x": 0.888161838054657, "y": 0.19722455739974976}, {"x": 0.888161838054657, "y": 0.27081581950187683}, {"x": 0.6002379655838013, "y": 0.27081581950187683}], "text": "Pål Sverre Valheim Hagen (born 6 November 1980) is a Nor-\nwegian stage and screen actor. He appeared in the Norwe-\ngian film \"Max Manus\" and played Thor Heyerdahl in the\nOscar-nominated 2012 film \"Kon-Tiki”. Pl Hagen was born\nin Stavanger, Norway, the son of Roar Hagen, a Norwegian\ncartoonist who has long been associated with Norwayś largest\ndaily, \"VG\". He lived in Jtten, a neighborhood in the city of\nStavanger in south-western Norway....\n"}
{"page": 13, "bbox": [{"x": 0.11957168579101562, "y": 0.288898229598999}, {"x": 0.8839976191520691, "y": 0.288898229598999}, {"x": 0.8839976191520691, "y": 0.3128679692745209}, {"x": 0.11957168579101562, "y": 0.3128679692745209}], "text": "Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content\nwords in the question are written in bold.\n"}
{"page": 13, "bbox": [{"x": 0.11957168579101562, "y": 0.34188392758369446}, {"x": 0.4901844263076782, "y": 0.3423044681549072}, {"x": 0.48958954215049744, "y": 0.5151387453079224}, {"x": 0.11897680163383484, "y": 0.514718234539032}], "text": "from the top 5 positive and top 30 negative passages\n(from the set of top 100 passages retrieved from\nthe same question). The question encoder's initial\nstate is taken from a DPR model previously trained\non the NQ dataset. The reader's initial state is a\nBERT-base model. In terms of the end-to-end QA\nresults, our joint-training scheme does not provide\nbetter results compared to the usual retriever/reader\ntraining pipeline, resulting in the same 39.8 exact\nmatch score on NQ dev as in our regular reader\nmodel training.\n"}
{"page": 13, "bbox": [{"x": 0.4836407005786896, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.9327165484428406}, {"x": 0.4836407005786896, "y": 0.9327165484428406}], "text": "6781\n"}
{"page": 1, "bbox": [{"x": 0.07016023993492126, "y": 0.08718244731426239}, {"x": 0.12906019389629364, "y": 0.08718244731426239}, {"x": 0.12906019389629364, "y": 0.10450346767902374}, {"x": 0.07016023993492126, "y": 0.10450346767902374}], "text": "eBook\n"}
{"page": 1, "bbox": [{"x": 0.06756171584129333, "y": 0.1339491903781891}, {"x": 0.9294066429138184, "y": 0.13048498332500458}, {"x": 0.9298397302627563, "y": 0.32621246576309204}, {"x": 0.06799480319023132, "y": 0.32967668771743774}], "text": "A Compact Guide to\nRetrieval Augmented Generation (RAG)\nDefinitions, components and basics for practitioners\n"}
{"page": 1, "bbox": [{"x": 0.7479428052902222, "y": 0.48845264315605164}, {"x": 0.8237332105636597, "y": 0.487297922372818}, {"x": 0.8241662979125977, "y": 0.5542725324630737}, {"x": 0.7483758926391602, "y": 0.5554272532463074}], "text": "((•))\n"}
{"page": 1, "bbox": [{"x": 0.26938068866729736, "y": 0.6200923919677734}, {"x": 0.3373754918575287, "y": 0.6195150017738342}, {"x": 0.3378085792064667, "y": 0.6864895820617676}, {"x": 0.26981377601623535, "y": 0.6870669722557068}], "text": "=\n"}
{"page": 1, "bbox": [{"x": 0.8484192490577698, "y": 0.9249422550201416}, {"x": 0.9610220789909363, "y": 0.9255196452140808}, {"x": 0.9610220789909363, "y": 0.9474595785140991}, {"x": 0.8484192490577698, "y": 0.9468821883201599}], "text": "databricks\n"}
{"page": 2, "bbox": [{"x": 0.9558250308036804, "y": 0.054849885404109955}, {"x": 0.9610220789909363, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.06351039558649063}, {"x": 0.9553919434547424, "y": 0.06293302774429321}], "text": "2\n"}
{"page": 2, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 2, "bbox": [{"x": 0.038977913558483124, "y": 0.16512702405452728}, {"x": 0.13512343168258667, "y": 0.16685912013053894}, {"x": 0.13512343168258667, "y": 0.18533487617969513}, {"x": 0.038977913558483124, "y": 0.18360276520252228}], "text": "Contents\n"}
{"page": 2, "bbox": [{"x": 0.25205716490745544, "y": 0.15473441779613495}, {"x": 0.6769164204597473, "y": 0.15877598524093628}, {"x": 0.6769164204597473, "y": 0.1968822181224823}, {"x": 0.25205716490745544, "y": 0.19284065067768097}], "text": "Introduction: Retrieval Augmented Generation (RAG) With Vector Search.\nLLMs and prompts.\n"}
{"page": 2, "bbox": [{"x": 0.8458207249641418, "y": 0.16108545660972595}, {"x": 0.8527501225471497, "y": 0.16108545660972595}, {"x": 0.8527501225471497, "y": 0.19110853970050812}, {"x": 0.8458207249641418, "y": 0.19110853970050812}], "text": "3\n.5\n"}
{"page": 2, "bbox": [{"x": 0.8475530743598938, "y": 0.20265589654445648}, {"x": 0.8527501225471497, "y": 0.20265589654445648}, {"x": 0.8527501225471497, "y": 0.21016165614128113}, {"x": 0.8475530743598938, "y": 0.21016165614128113}], "text": "6\n"}
{"page": 2, "bbox": [{"x": 0.2680814266204834, "y": 0.20207852125167847}, {"x": 0.4413165748119354, "y": 0.20207852125167847}, {"x": 0.4413165748119354, "y": 0.2124711275100708}, {"x": 0.2680814266204834, "y": 0.2124711275100708}], "text": "Vector Search and embedding models.\n"}
{"page": 2, "bbox": [{"x": 0.8471199870109558, "y": 0.23672054708003998}, {"x": 0.8523170351982117, "y": 0.23672054708003998}, {"x": 0.8523170351982117, "y": 0.24480369687080383}, {"x": 0.8471199870109558, "y": 0.24480369687080383}], "text": "8\n"}
{"page": 2, "bbox": [{"x": 0.25162407755851746, "y": 0.23267897963523865}, {"x": 0.5530532598495483, "y": 0.230946883559227}, {"x": 0.5530532598495483, "y": 0.28637412190437317}, {"x": 0.25162407755851746, "y": 0.288106232881546}], "text": "Asking RAG About Databricks Asset Bundles (DABS).\nAsking an LLM without RAG about DABS.\nUsing RAG to give an LLM access to documentation about DABS.\n"}
{"page": 2, "bbox": [{"x": 0.8479861617088318, "y": 0.25750577449798584}, {"x": 0.8523170351982117, "y": 0.25750577449798584}, {"x": 0.8523170351982117, "y": 0.2650115489959717}, {"x": 0.8479861617088318, "y": 0.2650115489959717}], "text": "8\n"}
{"page": 2, "bbox": [{"x": 0.8440883755683899, "y": 0.27713626623153687}, {"x": 0.8523170351982117, "y": 0.27713626623153687}, {"x": 0.8523170351982117, "y": 0.2840646505355835}, {"x": 0.8440883755683899, "y": 0.2840646505355835}], "text": "10\n"}
{"page": 2, "bbox": [{"x": 0.8436552882194519, "y": 0.31120091676712036}, {"x": 0.8527501225471497, "y": 0.31120091676712036}, {"x": 0.8527501225471497, "y": 0.32043880224227905}, {"x": 0.8436552882194519, "y": 0.32043880224227905}], "text": ".11\n"}
{"page": 2, "bbox": [{"x": 0.25205716490745544, "y": 0.30773672461509705}, {"x": 0.5322650671005249, "y": 0.306581974029541}, {"x": 0.5322650671005249, "y": 0.3412240147590637}, {"x": 0.25205716490745544, "y": 0.34237876534461975}], "text": "Addressing the Shortcomings of LLMs With RAG.\nRAG compared to LLM-only approaches...\n"}
{"page": 2, "bbox": [{"x": 0.8432222008705139, "y": 0.3314087688922882}, {"x": 0.8523170351982117, "y": 0.3314087688922882}, {"x": 0.8523170351982117, "y": 0.33891454339027405}, {"x": 0.8432222008705139, "y": 0.33891454339027405}], "text": ".12\n"}
{"page": 2, "bbox": [{"x": 0.841056764125824, "y": 0.3660508096218109}, {"x": 0.8518839478492737, "y": 0.3660508096218109}, {"x": 0.8518839478492737, "y": 0.3747113049030304}, {"x": 0.841056764125824, "y": 0.3747113049030304}], "text": ".14\n"}
{"page": 2, "bbox": [{"x": 0.8445214629173279, "y": 0.3868360221385956}, {"x": 0.8523170351982117, "y": 0.3868360221385956}, {"x": 0.8523170351982117, "y": 0.3931870758533478}, {"x": 0.8445214629173279, "y": 0.3931870758533478}], "text": "14\n"}
{"page": 2, "bbox": [{"x": 0.2529233396053314, "y": 0.36374133825302124}, {"x": 0.4014725089073181, "y": 0.36374133825302124}, {"x": 0.4014725089073181, "y": 0.4532332420349121}, {"x": 0.2529233396053314, "y": 0.4532332420349121}], "text": "RAG Use Cases.\nQuestion-answering systems.\nCustomer service\nContent generation.\nCode assistance\n"}
{"page": 2, "bbox": [{"x": 0.8445214629173279, "y": 0.4064665138721466}, {"x": 0.8527501225471497, "y": 0.4064665138721466}, {"x": 0.8527501225471497, "y": 0.4133949279785156}, {"x": 0.8445214629173279, "y": 0.4133949279785156}], "text": "14\n"}
{"page": 2, "bbox": [{"x": 0.8445214629173279, "y": 0.4255196452140808}, {"x": 0.8523170351982117, "y": 0.4255196452140808}, {"x": 0.8523170351982117, "y": 0.43244802951812744}, {"x": 0.8445214629173279, "y": 0.43244802951812744}], "text": "14\n"}
{"page": 2, "bbox": [{"x": 0.8445214629173279, "y": 0.44572749733924866}, {"x": 0.8531832098960876, "y": 0.44572749733924866}, {"x": 0.8531832098960876, "y": 0.4526558816432953}, {"x": 0.8445214629173279, "y": 0.4526558816432953}], "text": "14\n"}
{"page": 2, "bbox": [{"x": 0.8432222008705139, "y": 0.47921478748321533}, {"x": 0.8523170351982117, "y": 0.47921478748321533}, {"x": 0.8523170351982117, "y": 0.48672056198120117}, {"x": 0.8432222008705139, "y": 0.48672056198120117}], "text": "15\n"}
{"page": 2, "bbox": [{"x": 0.8440883755683899, "y": 0.5190531015396118}, {"x": 0.8523170351982117, "y": 0.5190531015396118}, {"x": 0.8523170351982117, "y": 0.5259815454483032}, {"x": 0.8440883755683899, "y": 0.5259815454483032}], "text": "19\n"}
{"page": 2, "bbox": [{"x": 0.25249025225639343, "y": 0.47401848435401917}, {"x": 0.6271113157272339, "y": 0.474595844745636}, {"x": 0.6271113157272339, "y": 0.5889145731925964}, {"x": 0.25249025225639343, "y": 0.5883371829986572}], "text": "RAG With Vector Search - Step by Step.\nData preparation: Getting an external information source into a vector database.\nRetrieval: Getting relevant context\nAugmentation: Adding context to the user's prompt.\nGeneration: Producing useful output with an LLM\nEvaluation: Measuring RAG performance\n"}
{"page": 2, "bbox": [{"x": 0.8436552882194519, "y": 0.4971131682395935}, {"x": 0.8527501225471497, "y": 0.4971131682395935}, {"x": 0.8510177731513977, "y": 0.5854503512382507}, {"x": 0.8419229388237, "y": 0.5854503512382507}], "text": "51323\n"}
{"page": 2, "bbox": [{"x": 0.8432222008705139, "y": 0.5583140850067139}, {"x": 0.8527501225471497, "y": 0.5583140850067139}, {"x": 0.8527501225471497, "y": 0.5658198595046997}, {"x": 0.8432222008705139, "y": 0.5658198595046997}], "text": "27\n"}
{"page": 2, "bbox": [{"x": 0.8388912677764893, "y": 0.5779445767402649}, {"x": 0.8527501225471497, "y": 0.5779445767402649}, {"x": 0.8527501225471497, "y": 0.5854503512382507}, {"x": 0.8388912677764893, "y": 0.5854503512382507}], "text": "..30\n"}
{"page": 2, "bbox": [{"x": 0.8436552882194519, "y": 0.6125866174697876}, {"x": 0.8531832098960876, "y": 0.6125866174697876}, {"x": 0.8531832098960876, "y": 0.6206697225570679}, {"x": 0.8436552882194519, "y": 0.6206697225570679}], "text": "31\n"}
{"page": 2, "bbox": [{"x": 0.2529233396053314, "y": 0.6096997857093811}, {"x": 0.658726692199707, "y": 0.6079676747322083}, {"x": 0.658726692199707, "y": 0.642609715461731}, {"x": 0.2529233396053314, "y": 0.6443418264389038}], "text": "Utilizing RAG With Other Modeling and Model Customization Methods.\nPrompt engineering..\n"}
{"page": 2, "bbox": [{"x": 0.8453876376152039, "y": 0.6322171092033386}, {"x": 0.8523170351982117, "y": 0.6322171092033386}, {"x": 0.8523170351982117, "y": 0.6391454935073853}, {"x": 0.8453876376152039, "y": 0.6391454935073853}], "text": "31\n"}
{"page": 2, "bbox": [{"x": 0.8432222008705139, "y": 0.6512702107429504}, {"x": 0.8523170351982117, "y": 0.6512702107429504}, {"x": 0.8523170351982117, "y": 0.6587759852409363}, {"x": 0.8432222008705139, "y": 0.6587759852409363}], "text": "32\n"}
{"page": 2, "bbox": [{"x": 0.2689476013183594, "y": 0.6518476009368896}, {"x": 0.32091814279556274, "y": 0.6518476009368896}, {"x": 0.32091814279556274, "y": 0.6610854268074036}, {"x": 0.2689476013183594, "y": 0.6610854268074036}], "text": "Fine-tuning\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.6709007024765015}, {"x": 0.8527501225471497, "y": 0.6709007024765015}, {"x": 0.8527501225471497, "y": 0.6789838075637817}, {"x": 0.8401905298233032, "y": 0.6789838075637817}], "text": ".33\n"}
{"page": 2, "bbox": [{"x": 0.26938068866729736, "y": 0.6703233122825623}, {"x": 0.3178865313529968, "y": 0.6709007024765015}, {"x": 0.3178865313529968, "y": 0.6812933087348938}, {"x": 0.26938068866729736, "y": 0.6807159185409546}], "text": "Pretraining\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.6911085247993469}, {"x": 0.8527501225471497, "y": 0.6911085247993469}, {"x": 0.8527501225471497, "y": 0.6986142992973328}, {"x": 0.8401905298233032, "y": 0.6986142992973328}], "text": ".33\n"}
{"page": 2, "bbox": [{"x": 0.2689476013183594, "y": 0.6911085247993469}, {"x": 0.2862711250782013, "y": 0.6905311942100525}, {"x": 0.2862711250782013, "y": 0.6986142992973328}, {"x": 0.2689476013183594, "y": 0.699191689491272}], "text": "RAG\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.710739016532898}, {"x": 0.8523170351982117, "y": 0.710739016532898}, {"x": 0.8523170351982117, "y": 0.7182447910308838}, {"x": 0.8401905298233032, "y": 0.7182447910308838}], "text": ".34\n"}
{"page": 2, "bbox": [{"x": 0.2685145139694214, "y": 0.7101616859436035}, {"x": 0.3845820724964142, "y": 0.7101616859436035}, {"x": 0.3845820724964142, "y": 0.7193995118141174}, {"x": 0.2685145139694214, "y": 0.7193995118141174}], "text": "Combinations of methods\n"}
{"page": 2, "bbox": [{"x": 0.2529233396053314, "y": 0.741916835308075}, {"x": 0.36032915115356445, "y": 0.7413395047187805}, {"x": 0.36032915115356445, "y": 0.7546189427375793}, {"x": 0.2529233396053314, "y": 0.7551963329315186}], "text": "RAG on Databricks\n"}
{"page": 2, "bbox": [{"x": 0.841056764125824, "y": 0.7453810572624207}, {"x": 0.8523170351982117, "y": 0.7453810572624207}, {"x": 0.8523170351982117, "y": 0.7540415525436401}, {"x": 0.841056764125824, "y": 0.7540415525436401}], "text": "35\n"}
{"page": 2, "bbox": [{"x": 0.2689476013183594, "y": 0.7644341588020325}, {"x": 0.37462103366851807, "y": 0.7644341588020325}, {"x": 0.37462103366851807, "y": 0.7730947136878967}, {"x": 0.2689476013183594, "y": 0.7730947136878967}], "text": "Lakehouse architecture\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.7650115489959717}, {"x": 0.8523170351982117, "y": 0.7650115489959717}, {"x": 0.8523170351982117, "y": 0.7725173234939575}, {"x": 0.8401905298233032, "y": 0.7725173234939575}], "text": ".35\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.7846420407295227}, {"x": 0.8523170351982117, "y": 0.7846420407295227}, {"x": 0.8523170351982117, "y": 0.7921478152275085}, {"x": 0.8401905298233032, "y": 0.7921478152275085}], "text": ".35\n"}
{"page": 2, "bbox": [{"x": 0.2689476013183594, "y": 0.7834873199462891}, {"x": 0.33174535632133484, "y": 0.7840646505355835}, {"x": 0.33131226897239685, "y": 0.8152424693107605}, {"x": 0.2689476013183594, "y": 0.8146651387214661}], "text": "Vector Search\nModel serving\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.8042725324630737}, {"x": 0.8514508605003357, "y": 0.8042725324630737}, {"x": 0.8514508605003357, "y": 0.8117783069610596}, {"x": 0.8401905298233032, "y": 0.8117783069610596}], "text": ".35\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.8239030241966248}, {"x": 0.8518839478492737, "y": 0.8239030241966248}, {"x": 0.8518839478492737, "y": 0.8314087986946106}, {"x": 0.8401905298233032, "y": 0.8314087986946106}], "text": ".36\n"}
{"page": 2, "bbox": [{"x": 0.2689476013183594, "y": 0.8227482438087463}, {"x": 0.29926374554634094, "y": 0.8239030241966248}, {"x": 0.29926374554634094, "y": 0.8331408500671387}, {"x": 0.2689476013183594, "y": 0.831986129283905}], "text": "MLflow\n"}
{"page": 2, "bbox": [{"x": 0.8401905298233032, "y": 0.8435335159301758}, {"x": 0.8523170351982117, "y": 0.8435335159301758}, {"x": 0.8523170351982117, "y": 0.8510392904281616}, {"x": 0.8401905298233032, "y": 0.8510392904281616}], "text": ".36\n"}
{"page": 2, "bbox": [{"x": 0.26938068866729736, "y": 0.8418014049530029}, {"x": 0.36725854873657227, "y": 0.8429561257362366}, {"x": 0.36725854873657227, "y": 0.8533487319946289}, {"x": 0.26938068866729736, "y": 0.8521940112113953}], "text": "Lakehouse Monitoring\n"}
{"page": 2, "bbox": [{"x": 0.8419229388237, "y": 0.8775981664657593}, {"x": 0.8523170351982117, "y": 0.8775981664657593}, {"x": 0.8523170351982117, "y": 0.8856812715530396}, {"x": 0.8419229388237, "y": 0.8856812715530396}], "text": "37\n"}
{"page": 2, "bbox": [{"x": 0.8423560261726379, "y": 0.8758660554885864}, {"x": 0.8505846858024597, "y": 0.8758660554885864}, {"x": 0.8505846858024597, "y": 0.9243649244308472}, {"x": 0.8423560261726379, "y": 0.9243649244308472}], "text": "333\n"}
{"page": 2, "bbox": [{"x": 0.25205716490745544, "y": 0.8752886652946472}, {"x": 0.36379384994506836, "y": 0.8764434456825256}, {"x": 0.36336076259613037, "y": 0.9266743659973145}, {"x": 0.25162407755851746, "y": 0.9255196452140808}], "text": "Summary.\nGenAl training..\nAdditional resources.\n"}
{"page": 2, "bbox": [{"x": 0.8432222008705139, "y": 0.8978059887886047}, {"x": 0.8531832098960876, "y": 0.8978059887886047}, {"x": 0.8531832098960876, "y": 0.9053117632865906}, {"x": 0.8432222008705139, "y": 0.9053117632865906}], "text": "37\n"}
{"page": 2, "bbox": [{"x": 0.8432222008705139, "y": 0.9174364805221558}, {"x": 0.8531832098960876, "y": 0.9174364805221558}, {"x": 0.8531832098960876, "y": 0.9249422550201416}, {"x": 0.8432222008705139, "y": 0.9249422550201416}], "text": "37\n"}
{"page": 2, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 3, "bbox": [{"x": 0.9558250308036804, "y": 0.054849885404109955}, {"x": 0.9605889916419983, "y": 0.054849885404109955}, {"x": 0.9605889916419983, "y": 0.062355659902095795}, {"x": 0.9558250308036804, "y": 0.062355659902095795}], "text": "3\n"}
{"page": 3, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 3, "bbox": [{"x": 0.038977913558483124, "y": 0.16454964876174927}, {"x": 0.16803811490535736, "y": 0.16454964876174927}, {"x": 0.16803811490535736, "y": 0.1841801404953003}, {"x": 0.038977913558483124, "y": 0.1841801404953003}], "text": "Introduction\n"}
{"page": 3, "bbox": [{"x": 0.2533564269542694, "y": 0.1622401773929596}, {"x": 0.8614118695259094, "y": 0.1622401773929596}, {"x": 0.8614118695259094, "y": 0.18879908323287964}, {"x": 0.2533564269542694, "y": 0.18879908323287964}], "text": "Retrieval Augmented Generation (RAG) With Vector Search\n"}
{"page": 3, "bbox": [{"x": 0.25205716490745544, "y": 0.22401846945285797}, {"x": 0.8397574424743652, "y": 0.22401846945285797}, {"x": 0.8397574424743652, "y": 0.29214781522750854}, {"x": 0.25205716490745544, "y": 0.29214781522750854}], "text": "Retrieval augmented generation (RAG) is the process of combining a user's prompt with relevant external\ninformation to form a new, expanded prompt for a large language model (LLM) such as GPT-4 or Llama 2. The\nexpanded prompt enables the LLM to provide more relevant, timely and accurate responses.\n"}
{"page": 3, "bbox": [{"x": 0.25205716490745544, "y": 0.3129330277442932}, {"x": 0.8614118695259094, "y": 0.31639721989631653}, {"x": 0.8609787821769714, "y": 0.41108545660972595}, {"x": 0.25162407755851746, "y": 0.40762123465538025}], "text": "LLMs offer powerful language understanding and generation capabilities, but they aren't reliable information\nsources and they lack access to proprietary information or any other information they weren't trained on. They\nare also prone to so-called \"hallucinations\" — fabricating answers instead of acknowledging that they don't know\nthe correct answer.\n"}
{"page": 3, "bbox": [{"x": 0.283672571182251, "y": 0.48672056198120117}, {"x": 0.36812472343444824, "y": 0.4901847541332245}, {"x": 0.36769163608551025, "y": 0.5121247172355652}, {"x": 0.283239483833313, "y": 0.5086604952812195}], "text": "LLM Only\n"}
{"page": 3, "bbox": [{"x": 0.6275444030761719, "y": 0.5987297892570496}, {"x": 0.7011693120002747, "y": 0.5987297892570496}, {"x": 0.7011693120002747, "y": 0.6125866174697876}, {"x": 0.6275444030761719, "y": 0.6125866174697876}], "text": "Generates\n"}
{"page": 3, "bbox": [{"x": 0.4166305661201477, "y": 0.5987297892570496}, {"x": 0.4681680500507355, "y": 0.5993071794509888}, {"x": 0.4681680500507355, "y": 0.6154734492301941}, {"x": 0.4166305661201477, "y": 0.6148960590362549}], "text": "Prompt\n"}
{"page": 3, "bbox": [{"x": 0.3152880072593689, "y": 0.6189376711845398}, {"x": 0.3473365008831024, "y": 0.6195150017738342}, {"x": 0.3473365008831024, "y": 0.6322171092033386}, {"x": 0.3152880072593689, "y": 0.6316397190093994}], "text": "User\n"}
{"page": 3, "bbox": [{"x": 0.5396275520324707, "y": 0.6195150017738342}, {"x": 0.566478967666626, "y": 0.6195150017738342}, {"x": 0.566478967666626, "y": 0.6322171092033386}, {"x": 0.5396275520324707, "y": 0.6322171092033386}], "text": "LLM\n"}
{"page": 3, "bbox": [{"x": 0.7496751546859741, "y": 0.6195150017738342}, {"x": 0.8007795810699463, "y": 0.6195150017738342}, {"x": 0.8007795810699463, "y": 0.6356812715530396}, {"x": 0.7496751546859741, "y": 0.6356812715530396}], "text": "Output\n"}
{"page": 3, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 4, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 4, "bbox": [{"x": 0.9558250308036804, "y": 0.05542725324630737}, {"x": 0.9614551663398743, "y": 0.05600461736321449}, {"x": 0.9610220789909363, "y": 0.06351039558649063}, {"x": 0.9553919434547424, "y": 0.06293302774429321}], "text": "4\n"}
{"page": 4, "bbox": [{"x": 0.25205716490745544, "y": 0.15819860994815826}, {"x": 0.8514508605003357, "y": 0.15993072092533112}, {"x": 0.8510177731513977, "y": 0.3054272532463074}, {"x": 0.25162407755851746, "y": 0.3036951422691345}], "text": "Using RAG with an LLM helps to address some of these issues. Giving the LLM all the information it needs to\nanswer a question enables it to provide answers about topics it was not trained on and reduces the likelihood\nof hallucinations. For example, an LLM trained on public data cannot answer any questions about a company's\ninternal memos or project documents. It's likely to hallucinate if asked about such documents. A RAG\napplication can supply all or parts of those documents to the LLM, giving it the context it needs to give correct\nand relevant answers.\n"}
{"page": 4, "bbox": [{"x": 0.27760934829711914, "y": 0.37990760803222656}, {"x": 0.501082718372345, "y": 0.3775981664657593}, {"x": 0.501082718372345, "y": 0.39260968565940857}, {"x": 0.27760934829711914, "y": 0.39491915702819824}], "text": "RAG Application With Vector Database\n"}
{"page": 4, "bbox": [{"x": 0.4084019064903259, "y": 0.45785218477249146}, {"x": 0.45127761363983154, "y": 0.45785218477249146}, {"x": 0.45127761363983154, "y": 0.48267897963523865}, {"x": 0.4084019064903259, "y": 0.48267897963523865}], "text": "Vector\nDatabase\n"}
{"page": 4, "bbox": [{"x": 0.5682113766670227, "y": 0.45785218477249146}, {"x": 0.6015591025352478, "y": 0.45842957496643066}, {"x": 0.6011260151863098, "y": 0.4844110906124115}, {"x": 0.5677782297134399, "y": 0.4838337302207947}], "text": "User\nPrompt\n"}
{"page": 4, "bbox": [{"x": 0.48809006810188293, "y": 0.474595844745636}, {"x": 0.526634931564331, "y": 0.474595844745636}, {"x": 0.526634931564331, "y": 0.49884527921676636}, {"x": 0.48809006810188293, "y": 0.49884527921676636}], "text": "Relevant\nSources\n"}
{"page": 4, "bbox": [{"x": 0.718059778213501, "y": 0.4878752827644348}, {"x": 0.7652663588523865, "y": 0.4878752827644348}, {"x": 0.7652663588523865, "y": 0.4965358078479767}, {"x": 0.718059778213501, "y": 0.4965358078479767}], "text": "Generates\n"}
{"page": 4, "bbox": [{"x": 0.3438718020915985, "y": 0.49191686511039734}, {"x": 0.3772195875644684, "y": 0.49191686511039734}, {"x": 0.3772195875644684, "y": 0.5023094415664673}, {"x": 0.3438718020915985, "y": 0.5023094415664673}], "text": "Prompt\n"}
{"page": 4, "bbox": [{"x": 0.41489821672439575, "y": 0.5046189427375793}, {"x": 0.4447813034057617, "y": 0.5046189427375793}, {"x": 0.4447813034057617, "y": 0.511547327041626}, {"x": 0.41489821672439575, "y": 0.511547327041626}], "text": "Source 1\n"}
{"page": 4, "bbox": [{"x": 0.5530532598495483, "y": 0.5046189427375793}, {"x": 0.6167172193527222, "y": 0.5046189427375793}, {"x": 0.6167172193527222, "y": 0.5121247172355652}, {"x": 0.5530532598495483, "y": 0.5121247172355652}], "text": "Relevant Source 1\n"}
{"page": 4, "bbox": [{"x": 0.29233434796333313, "y": 0.5051963329315186}, {"x": 0.31312257051467896, "y": 0.505773663520813}, {"x": 0.31312257051467896, "y": 0.513856828212738}, {"x": 0.29233434796333313, "y": 0.5132794380187988}], "text": "User\n"}
{"page": 4, "bbox": [{"x": 0.6730186343193054, "y": 0.5046189427375793}, {"x": 0.689909040927887, "y": 0.5051963329315186}, {"x": 0.689909040927887, "y": 0.5144341588020325}, {"x": 0.6730186343193054, "y": 0.513856828212738}], "text": "LLM\n"}
{"page": 4, "bbox": [{"x": 0.7882199883460999, "y": 0.5051963329315186}, {"x": 0.8211346864700317, "y": 0.5063510537147522}, {"x": 0.8211346864700317, "y": 0.517320990562439}, {"x": 0.7882199883460999, "y": 0.5161662697792053}], "text": "Output\n"}
{"page": 4, "bbox": [{"x": 0.4135989546775818, "y": 0.5375288724899292}, {"x": 0.44651365280151367, "y": 0.5375288724899292}, {"x": 0.44651365280151367, "y": 0.5444572567939758}, {"x": 0.4135989546775818, "y": 0.5444572567939758}], "text": "Source 2\n"}
{"page": 4, "bbox": [{"x": 0.5521870851516724, "y": 0.53695148229599}, {"x": 0.6175833940505981, "y": 0.5363741517066956}, {"x": 0.6175833940505981, "y": 0.545034646987915}, {"x": 0.5521870851516724, "y": 0.5456120371818542}], "text": "Relevant Source 2\n"}
{"page": 4, "bbox": [{"x": 0.4131658673286438, "y": 0.5837182402610779}, {"x": 0.4460805654525757, "y": 0.5831408500671387}, {"x": 0.4460805654525757, "y": 0.5912240147590637}, {"x": 0.4131658673286438, "y": 0.5918014049530029}], "text": "Source N\n"}
{"page": 4, "bbox": [{"x": 0.25205716490745544, "y": 0.7020785212516785}, {"x": 0.8527501225471497, "y": 0.699191689491272}, {"x": 0.8531832098960876, "y": 0.7915704250335693}, {"x": 0.25249025225639343, "y": 0.7944572567939758}], "text": "RAG can work with a variety of data sources, including text, podcasts, videos, live search results and structured\ndatabases. In this document, we look at RAG over stored unstructured data such as PDFs, scraped web pages\nor code. This guide focuses on a RAG approach in which data is retrieved from vector databases using a\nprocess known as Vector Search.\n"}
{"page": 4, "bbox": [{"x": 0.25162407755851746, "y": 0.8192840814590454}, {"x": 0.8432222008705139, "y": 0.8192840814590454}, {"x": 0.8432222008705139, "y": 0.8342956304550171}, {"x": 0.25162407755851746, "y": 0.8342956304550171}], "text": "We'll start with some brief background information on LLMs and prompts, which form the foundation for RAG.\n"}
{"page": 4, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 5, "bbox": [{"x": 0.9553919434547424, "y": 0.05427251756191254}, {"x": 0.9601559042930603, "y": 0.05427251756191254}, {"x": 0.9601559042930603, "y": 0.061778292059898376}, {"x": 0.9553919434547424, "y": 0.061778292059898376}], "text": "LO\n"}
{"page": 5, "bbox": [{"x": 0.9553919434547424, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.06293302774429321}, {"x": 0.9553919434547424, "y": 0.06293302774429321}], "text": "5\n"}
{"page": 5, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 5, "bbox": [{"x": 0.2533564269542694, "y": 0.16050808131694794}, {"x": 0.4101342558860779, "y": 0.16454964876174927}, {"x": 0.4097011685371399, "y": 0.18533487617969513}, {"x": 0.2529233396053314, "y": 0.1812933087348938}], "text": "LLMs and prompts\n"}
{"page": 5, "bbox": [{"x": 0.25205716490745544, "y": 0.2003464251756668}, {"x": 0.8267648220062256, "y": 0.2003464251756668}, {"x": 0.8267648220062256, "y": 0.3187066912651062}, {"x": 0.25205716490745544, "y": 0.3187066912651062}], "text": "LLMs are a relatively new class of machine learning algorithms that can interpret, manipulate and generate\ntext-based content. They're trained on massive text datasets from diverse sources, including books, text\nscraped from the internet and code repositories. During the training process, the model learns statistical\nrelationships between words and phrases, enabling it to generate new text using the context of text it has\nalready seen or generated.\n"}
{"page": 5, "bbox": [{"x": 0.25205716490745544, "y": 0.3435334861278534}, {"x": 0.8449545502662659, "y": 0.3435334861278534}, {"x": 0.8449545502662659, "y": 0.5802540183067322}, {"x": 0.25205716490745544, "y": 0.5802540183067322}], "text": "LLMs are typically used via \"prompting.” A prompt is text that a user provides to an LLM and that the LLM\nresponds to. Prompts can take many different forms. Some models are trained to complete text, so prompts\nto these models take the form of incomplete statements like \"Jack and Jill went up the hill to...,\" which the\nmodel then continues. Other models expect questions or instructions as prompts. These models can answer\nquestions such as, \"What happened to Jack after Jack and Jill went up the hill?\" RAG applications that enable\nusers to ask questions about text generally use instruction-following and question-answering LLMs.\nLLMs can typically handle prompts of at least several paragraphs in length. This is important for RAG. In RAG,\nthe user's question or instruction is combined with some information retrieved from an external data source,\nforming the new, augmented prompt.\n"}
{"page": 5, "bbox": [{"x": 0.25249025225639343, "y": 0.6039261221885681}, {"x": 0.6132524609565735, "y": 0.6039261221885681}, {"x": 0.6132524609565735, "y": 0.6160507798194885}, {"x": 0.25249025225639343, "y": 0.6160507798194885}], "text": "Next, let's discuss where the external information is retrieved from.\n"}
{"page": 5, "bbox": [{"x": 0.4088349938392639, "y": 0.7667436599731445}, {"x": 0.9813772439956665, "y": 0.7667436599731445}, {"x": 0.9813772439956665, "y": 0.9064664840698242}, {"x": 0.4088349938392639, "y": 0.9064664840698242}], "text": "﹀=====\n"}
{"page": 5, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.9312933087348938}, {"x": 0.14335210621356964, "y": 0.9486142992973328}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 6, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 6, "bbox": [{"x": 0.9553919434547424, "y": 0.05542725324630737}, {"x": 0.9610220789909363, "y": 0.05542725324630737}, {"x": 0.9610220789909363, "y": 0.06351039558649063}, {"x": 0.9553919434547424, "y": 0.06351039558649063}], "text": "6\n"}
{"page": 6, "bbox": [{"x": 0.25162407755851746, "y": 0.1628175526857376}, {"x": 0.8536162972450256, "y": 0.1622401773929596}, {"x": 0.8536162972450256, "y": 0.40819862484931946}, {"x": 0.25162407755851746, "y": 0.4087759852409363}], "text": "Vector Search and embedding models\nAn effective RAG application must be able to find information relevant to the user's prompt and supply it to the\nLLM. Selecting the most relevant texts from potentially millions of documents can be a significant challenge. To\naddress this challenge, we use a technique called Vector Search to identify text relevant to the user's prompt.\nIn a RAG system with Vector Search, a special type of language model called an \"embedding model\" translates\neach text we want to search into a numeric vector, a series of numbers that encapsulates the text's meaning.\nThe same model also converts the user's query to a comparable vector. This process makes it possible\nto mathematically compare the user's query to the text and identify those that are the most similar and the\nmost relevant.\n"}
{"page": 6, "bbox": [{"x": 0.25119099020957947, "y": 0.43533486127853394}, {"x": 0.8492854237556458, "y": 0.43533486127853394}, {"x": 0.8492854237556458, "y": 0.6183602809906006}, {"x": 0.25119099020957947, "y": 0.6183602809906006}], "text": "Crucially, these vectors represent the meanings of the text from which they are generated. Thus, when we\nsearch for relevant vectors, we are searching on the basis of meaning. This approach enables us to retrieve the\ntext most relevant to the user's query and pass it to the LLM along with the user's original query.\nWe've talked a lot about embeddings encoding the meanings of texts. It's important to note that this is the\nmeaning according to the model. The models are trained to capture meanings and relationships that humans\ncare about, but the meaning captured by the embedding model may not be exactly what we want. This is one\nreason why it's essential to test and evaluate every component of a RAG application.\n"}
{"page": 6, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 7, "bbox": [{"x": 0.9558250308036804, "y": 0.054849885404109955}, {"x": 0.9610220789909363, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.06351039558649063}, {"x": 0.9553919434547424, "y": 0.06293302774429321}], "text": "7\n"}
{"page": 7, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 7, "bbox": [{"x": 0.4932871460914612, "y": 0.18937644362449646}, {"x": 0.5725422501564026, "y": 0.18879908323287964}, {"x": 0.5725422501564026, "y": 0.21189376711845398}, {"x": 0.4932871460914612, "y": 0.2124711275100708}], "text": "Embedding Vectors\nof Source Texts\n"}
{"page": 7, "bbox": [{"x": 0.28843656182289124, "y": 0.20265589654445648}, {"x": 0.33954092860221863, "y": 0.20207852125167847}, {"x": 0.33954092860221863, "y": 0.21073903143405914}, {"x": 0.28843656182289124, "y": 0.21131639182567596}], "text": "Source Texts\n"}
{"page": 7, "bbox": [{"x": 0.657860517501831, "y": 0.2159353345632553}, {"x": 0.6916413903236389, "y": 0.2159353345632553}, {"x": 0.6916413903236389, "y": 0.22344110906124115}, {"x": 0.657860517501831, "y": 0.22344110906124115}], "text": "Relevant\n"}
{"page": 7, "bbox": [{"x": 0.7721957564353943, "y": 0.22921478748321533}, {"x": 0.8297964334487915, "y": 0.22921478748321533}, {"x": 0.8297964334487915, "y": 0.237297922372818}, {"x": 0.7721957564353943, "y": 0.237297922372818}], "text": "Relevant Texts\n"}
{"page": 7, "bbox": [{"x": 0.6349068880081177, "y": 0.22921478748321533}, {"x": 0.7141619920730591, "y": 0.2280600517988205}, {"x": 0.7141619920730591, "y": 0.23845265805721283}, {"x": 0.6349068880081177, "y": 0.23960739374160767}], "text": "Embedding Vectors\n"}
{"page": 7, "bbox": [{"x": 0.3044607937335968, "y": 0.24191686511039734}, {"x": 0.3235166668891907, "y": 0.24249422550201416}, {"x": 0.3235166668891907, "y": 0.25}, {"x": 0.3044607937335968, "y": 0.24942263960838318}], "text": "Text 1\n"}
{"page": 7, "bbox": [{"x": 0.49112170934677124, "y": 0.24307160079479218}, {"x": 0.5751407742500305, "y": 0.24307160079479218}, {"x": 0.5751407742500305, "y": 0.2505773603916168}, {"x": 0.49112170934677124, "y": 0.2505773603916168}], "text": "[.121, .892, ..., -.044]\n"}
{"page": 7, "bbox": [{"x": 0.7899523377418518, "y": 0.2690531313419342}, {"x": 0.811173677444458, "y": 0.2690531313419342}, {"x": 0.811173677444458, "y": 0.27598151564598083}, {"x": 0.7899523377418518, "y": 0.27598151564598083}], "text": "Text 2\n"}
{"page": 7, "bbox": [{"x": 0.6310091018676758, "y": 0.2690531313419342}, {"x": 0.718492865562439, "y": 0.26674365997314453}, {"x": 0.718492865562439, "y": 0.27655890583992004}, {"x": 0.6310091018676758, "y": 0.27886834740638733}], "text": "[-.990, .002, ..., -.265]\n"}
{"page": 7, "bbox": [{"x": 0.3035946190357208, "y": 0.2748267948627472}, {"x": 0.32438284158706665, "y": 0.2748267948627472}, {"x": 0.32438284158706665, "y": 0.2817552089691162}, {"x": 0.3035946190357208, "y": 0.2817552089691162}], "text": "Text 2\n"}
{"page": 7, "bbox": [{"x": 0.4893893599510193, "y": 0.27424943447113037}, {"x": 0.5764400362968445, "y": 0.2748267948627472}, {"x": 0.5764400362968445, "y": 0.28290992975234985}, {"x": 0.4893893599510193, "y": 0.28233256936073303}], "text": "[.990, .002, ..., -.265]\n"}
{"page": 7, "bbox": [{"x": 0.3941099941730499, "y": 0.2806004583835602}, {"x": 0.44045040011405945, "y": 0.27944573760032654}, {"x": 0.44088348746299744, "y": 0.30196306109428406}, {"x": 0.3945430815219879, "y": 0.3031177818775177}], "text": "Embedding\nModel\n"}
{"page": 7, "bbox": [{"x": 0.7899523377418518, "y": 0.3002309501171112}, {"x": 0.811606764793396, "y": 0.300808310508728}, {"x": 0.811606764793396, "y": 0.3088914453983307}, {"x": 0.7899523377418518, "y": 0.30831408500671387}], "text": "Text 3\n"}
{"page": 7, "bbox": [{"x": 0.6310091018676758, "y": 0.2996535897254944}, {"x": 0.718059778213501, "y": 0.300808310508728}, {"x": 0.718059778213501, "y": 0.3094688355922699}, {"x": 0.6310091018676758, "y": 0.30831408500671387}], "text": "[.544, .818, ..., -.110]\n"}
{"page": 7, "bbox": [{"x": 0.4893893599510193, "y": 0.3071593642234802}, {"x": 0.5768731236457825, "y": 0.30773672461509705}, {"x": 0.5768731236457825, "y": 0.3158198595046997}, {"x": 0.4893893599510193, "y": 0.3152424991130829}], "text": "[.544, .818, ..., .110]\n"}
{"page": 7, "bbox": [{"x": 0.3035946190357208, "y": 0.30831408500671387}, {"x": 0.32438284158706665, "y": 0.30831408500671387}, {"x": 0.32438284158706665, "y": 0.3152424991130829}, {"x": 0.3035946190357208, "y": 0.3152424991130829}], "text": "Text 3\n"}
{"page": 7, "bbox": [{"x": 0.297964483499527, "y": 0.3533487319946289}, {"x": 0.3304460942745209, "y": 0.3539260923862457}, {"x": 0.3304460942745209, "y": 0.36143186688423157}, {"x": 0.297964483499527, "y": 0.36085450649261475}], "text": "Source N\n"}
{"page": 7, "bbox": [{"x": 0.49112170934677124, "y": 0.3527713716030121}, {"x": 0.5747076869010925, "y": 0.35450345277786255}, {"x": 0.5747076869010925, "y": 0.36374133825302124}, {"x": 0.49112170934677124, "y": 0.3620092272758484}], "text": "[..347, 782, ..., .604]\n"}
{"page": 7, "bbox": [{"x": 0.6089215874671936, "y": 0.3562355637550354}, {"x": 0.6388046741485596, "y": 0.3568129241466522}, {"x": 0.6383715867996216, "y": 0.3758660554885864}, {"x": 0.6084885001182556, "y": 0.3752886950969696}], "text": "Similarity\nSearch\n"}
{"page": 7, "bbox": [{"x": 0.4932871460914612, "y": 0.38856813311576843}, {"x": 0.5729753375053406, "y": 0.3897228538990021}, {"x": 0.5725422501564026, "y": 0.4162817597389221}, {"x": 0.4928540587425232, "y": 0.4151270091533661}], "text": "Embedding Vectors\nof User Query\n"}
{"page": 7, "bbox": [{"x": 0.8072758913040161, "y": 0.40127021074295044}, {"x": 0.8345603942871094, "y": 0.40127021074295044}, {"x": 0.8345603942871094, "y": 0.4093533456325531}, {"x": 0.8072758913040161, "y": 0.4093533456325531}], "text": "To LLM\n"}
{"page": 7, "bbox": [{"x": 0.4893893599510193, "y": 0.43937644362449646}, {"x": 0.5773062109947205, "y": 0.4399538040161133}, {"x": 0.5773062109947205, "y": 0.44861432909965515}, {"x": 0.4893893599510193, "y": 0.44803693890571594}], "text": "[..121, .007,..., -.184]\n"}
{"page": 7, "bbox": [{"x": 0.2936336100101471, "y": 0.43879908323287964}, {"x": 0.33477696776390076, "y": 0.4405311644077301}, {"x": 0.33434388041496277, "y": 0.4497690498828888}, {"x": 0.2932005226612091, "y": 0.44803693890571594}], "text": "User Query\n"}
{"page": 7, "bbox": [{"x": 0.25205716490745544, "y": 0.5479214787483215}, {"x": 0.406669557094574, "y": 0.5479214787483215}, {"x": 0.406669557094574, "y": 0.5617783069610596}, {"x": 0.25205716490745544, "y": 0.5617783069610596}], "text": "VECTOR DATABASES\n"}
{"page": 7, "bbox": [{"x": 0.25205716490745544, "y": 0.5860277414321899}, {"x": 0.8345603942871094, "y": 0.5860277414321899}, {"x": 0.8345603942871094, "y": 0.7043879628181458}, {"x": 0.25205716490745544, "y": 0.7043879628181458}], "text": "The vectors generated by embedding models are often stored in a specialized vector database. Vector\ndatabases are optimized for storing and retrieving vector data efficiently. Like traditional databases, vector\ndatabases can be used to manage permissions, metadata and data integrity, ensuring secure and organized\naccess to information. They also tend to include update mechanisms so newly added texts are indexed and\nready to use quickly.\n"}
{"page": 7, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 8, "bbox": [{"x": 0.9553919434547424, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9553919434547424, "y": 0.06293302774429321}], "text": "8\n"}
{"page": 8, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 8, "bbox": [{"x": 0.25162407755851746, "y": 0.15877598524093628}, {"x": 0.841056764125824, "y": 0.15877598524093628}, {"x": 0.841056764125824, "y": 0.2505773603916168}, {"x": 0.25162407755851746, "y": 0.2505773603916168}], "text": "Suppose we want to ask some questions about Databricks Asset Bundles (DABS), a new Databricks feature\nthat enables an infrastructure as code (laC) approach to managing Databricks projects. We'll use the Llama 2\nLLM, which doesn't have access to up-to-date information about DAB because it was trained before the DAB\nPublic Preview was released.\n"}
{"page": 8, "bbox": [{"x": 0.038544826209545135, "y": 0.16454964876174927}, {"x": 0.18666088581085205, "y": 0.16454964876174927}, {"x": 0.18666088581085205, "y": 0.3285219371318817}, {"x": 0.038544826209545135, "y": 0.3285219371318817}], "text": "Asking\nRAG About\nDatabricks\nAsset Bundles\n(DABS)\n"}
{"page": 8, "bbox": [{"x": 0.25205716490745544, "y": 0.2777136266231537}, {"x": 0.7358163595199585, "y": 0.2777136266231537}, {"x": 0.7358163595199585, "y": 0.29157042503356934}, {"x": 0.25205716490745544, "y": 0.29157042503356934}], "text": "All the following results are actual results generated by an LLM or by the vector database.\n"}
{"page": 8, "bbox": [{"x": 0.25162407755851746, "y": 0.3539260923862457}, {"x": 0.8172369003295898, "y": 0.3527713716030121}, {"x": 0.8172369003295898, "y": 0.4624711275100708}, {"x": 0.25162407755851746, "y": 0.46362587809562683}], "text": "Asking an LLM without RAG about DABS\nLet's see what happens if we ask the Llama 2-70B-chat LLM a couple questions about Databricks Asset\nBundles without providing it with any documentation. In this case, the LLM is being used alone, not in the\ncontext of a RAG application.\n"}
{"page": 8, "bbox": [{"x": 0.2689476013183594, "y": 0.5265588760375977}, {"x": 0.27327847480773926, "y": 0.5265588760375977}, {"x": 0.27327847480773926, "y": 0.5329099297523499}, {"x": 0.2689476013183594, "y": 0.5329099297523499}], "text": "1\n"}
{"page": 8, "bbox": [{"x": 0.29839757084846497, "y": 0.5259815454483032}, {"x": 0.44088348746299744, "y": 0.5259815454483032}, {"x": 0.44088348746299744, "y": 0.5334873199462891}, {"x": 0.29839757084846497, "y": 0.5334873199462891}], "text": "User: What are databricks DABS?\n"}
{"page": 8, "bbox": [{"x": 0.2685145139694214, "y": 0.5415704250335693}, {"x": 0.27371156215667725, "y": 0.5415704250335693}, {"x": 0.27371156215667725, "y": 0.5490761995315552}, {"x": 0.2685145139694214, "y": 0.5490761995315552}], "text": "2\n"}
{"page": 8, "bbox": [{"x": 0.2689476013183594, "y": 0.5571593642234802}, {"x": 0.27327847480773926, "y": 0.5571593642234802}, {"x": 0.27327847480773926, "y": 0.5640877485275269}, {"x": 0.2689476013183594, "y": 0.5640877485275269}], "text": "3\n"}
{"page": 8, "bbox": [{"x": 0.29839757084846497, "y": 0.5404157042503357}, {"x": 0.8362927436828613, "y": 0.5404157042503357}, {"x": 0.8362927436828613, "y": 0.5981523990631104}, {"x": 0.29839757084846497, "y": 0.5981523990631104}], "text": "Bot: Databricks DABS (Databricks Automation Building blocks) are a set of pre-built, reusable components that enable\nusers to automate and streamline their data engineering workflows on the Databricks platform. DABS provide a way to\nencapsulate and reuse common data engineering patterns, such as data ingestion, transformation, and loading, across\ndifferent use cases and projects.\n"}
{"page": 8, "bbox": [{"x": 0.2685145139694214, "y": 0.5739030241966248}, {"x": 0.27371156215667725, "y": 0.5739030241966248}, {"x": 0.27371156215667725, "y": 0.5802540183067322}, {"x": 0.2685145139694214, "y": 0.5802540183067322}], "text": "4\n"}
{"page": 8, "bbox": [{"x": 0.2689476013183594, "y": 0.5889145731925964}, {"x": 0.27371156215667725, "y": 0.5889145731925964}, {"x": 0.27371156215667725, "y": 0.5958429574966431}, {"x": 0.2689476013183594, "y": 0.5958429574966431}], "text": "5\n"}
{"page": 8, "bbox": [{"x": 0.2689476013183594, "y": 0.6177828907966614}, {"x": 0.27371156215667725, "y": 0.6177828907966614}, {"x": 0.27371156215667725, "y": 0.6247113347053528}, {"x": 0.2689476013183594, "y": 0.6247113347053528}], "text": "6\n"}
{"page": 8, "bbox": [{"x": 0.2685145139694214, "y": 0.6327944397926331}, {"x": 0.27327847480773926, "y": 0.6327944397926331}, {"x": 0.27327847480773926, "y": 0.6397228837013245}, {"x": 0.2685145139694214, "y": 0.6397228837013245}], "text": "7\n"}
{"page": 8, "bbox": [{"x": 0.29883065819740295, "y": 0.6166281700134277}, {"x": 0.8393243551254272, "y": 0.6166281700134277}, {"x": 0.8393243551254272, "y": 0.6576212644577026}, {"x": 0.29883065819740295, "y": 0.6576212644577026}], "text": "DABS are designed to be modular and can be combined to create more complex workflows. They provide a way to define\nrepeatable data engineering tasks that can be shared across different teams and projects, improving productivity and\nreducing the risk of errors.\n"}
{"page": 8, "bbox": [{"x": 0.2689476013183594, "y": 0.648383378982544}, {"x": 0.27327847480773926, "y": 0.648383378982544}, {"x": 0.27327847480773926, "y": 0.6547344326972961}, {"x": 0.2689476013183594, "y": 0.6547344326972961}], "text": "8\n"}
{"page": 8, "bbox": [{"x": 0.25249025225639343, "y": 0.7136258482933044}, {"x": 0.8211346864700317, "y": 0.7136258482933044}, {"x": 0.8211346864700317, "y": 0.7806004881858826}, {"x": 0.25249025225639343, "y": 0.7806004881858826}], "text": "In this example, the model just guessed, stating incorrectly that DAB stands for \"Databricks Automation\nBuilding blocks\" and expanding on what those might be. Notably, it never indicated that it didn't know the\nanswer or that it was making things up. This could represent a real issue in a customer support setting.\n"}
{"page": 8, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 9, "bbox": [{"x": 0.03811173513531685, "y": 0.05369514971971512}, {"x": 0.4699003994464874, "y": 0.05369514971971512}, {"x": 0.4699003994464874, "y": 0.06524249166250229}, {"x": 0.03811173513531685, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 9, "bbox": [{"x": 0.9549588561058044, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.06351039558649063}, {"x": 0.9549588561058044, "y": 0.06351039558649063}], "text": "9\n"}
{"page": 9, "bbox": [{"x": 0.25249025225639343, "y": 0.1593533456325531}, {"x": 0.3945430815219879, "y": 0.16050808131694794}, {"x": 0.3945430815219879, "y": 0.1749422699213028}, {"x": 0.25249025225639343, "y": 0.17378753423690796}], "text": "Let's try another question.\n"}
{"page": 9, "bbox": [{"x": 0.2689476013183594, "y": 0.22863741219043732}, {"x": 0.27371156215667725, "y": 0.22863741219043732}, {"x": 0.27371156215667725, "y": 0.23556582629680634}, {"x": 0.2689476013183594, "y": 0.23556582629680634}], "text": "1\n"}
{"page": 9, "bbox": [{"x": 0.29839757084846497, "y": 0.22748267650604248}, {"x": 0.7267215251922607, "y": 0.22748267650604248}, {"x": 0.7267215251922607, "y": 0.237297922372818}, {"x": 0.29839757084846497, "y": 0.237297922372818}], "text": "User: What version of the databricks CLI do I need in order to use Databricks Asset Bundles?\n"}
{"page": 9, "bbox": [{"x": 0.2689476013183594, "y": 0.2563510537147522}, {"x": 0.27371156215667725, "y": 0.2563510537147522}, {"x": 0.27371156215667725, "y": 0.26327943801879883}, {"x": 0.2689476013183594, "y": 0.26327943801879883}], "text": "2\n"}
{"page": 9, "bbox": [{"x": 0.29883065819740295, "y": 0.2563510537147522}, {"x": 0.8189692497253418, "y": 0.2563510537147522}, {"x": 0.8189692497253418, "y": 0.28002309799194336}, {"x": 0.29883065819740295, "y": 0.28002309799194336}], "text": "Bot: To use Databricks Asset Bundles, you need to have a Databricks CLI version that is greater than or equal to\n1.10.0.\n"}
{"page": 9, "bbox": [{"x": 0.2689476013183594, "y": 0.2725173234939575}, {"x": 0.27327847480773926, "y": 0.2725173234939575}, {"x": 0.27327847480773926, "y": 0.27944573760032654}, {"x": 0.2689476013183594, "y": 0.27944573760032654}], "text": "3\n"}
{"page": 9, "bbox": [{"x": 0.2689476013183594, "y": 0.30138567090034485}, {"x": 0.27371156215667725, "y": 0.30138567090034485}, {"x": 0.27371156215667725, "y": 0.30773672461509705}, {"x": 0.2689476013183594, "y": 0.30773672461509705}], "text": "4\n"}
{"page": 9, "bbox": [{"x": 0.297964483499527, "y": 0.2961893677711487}, {"x": 0.811173677444458, "y": 0.3002309501171112}, {"x": 0.811173677444458, "y": 0.33025404810905457}, {"x": 0.297964483499527, "y": 0.32621246576309204}], "text": "You can check your current version of the Databricks CLI by running the command `databricks --version in your\nterminal or command prompt.\n"}
{"page": 9, "bbox": [{"x": 0.2685145139694214, "y": 0.31639721989631653}, {"x": 0.27327847480773926, "y": 0.31639721989631653}, {"x": 0.27327847480773926, "y": 0.3227482736110687}, {"x": 0.2685145139694214, "y": 0.3227482736110687}], "text": "5\n"}
{"page": 9, "bbox": [{"x": 0.2689476013183594, "y": 0.34584295749664307}, {"x": 0.27371156215667725, "y": 0.34584295749664307}, {"x": 0.27371156215667725, "y": 0.3527713716030121}, {"x": 0.2689476013183594, "y": 0.3527713716030121}], "text": "6\n"}
{"page": 9, "bbox": [{"x": 0.2680814266204834, "y": 0.3441108465194702}, {"x": 0.27284538745880127, "y": 0.3441108465194702}, {"x": 0.27284538745880127, "y": 0.38279446959495544}, {"x": 0.2680814266204834, "y": 0.38279446959495544}], "text": "818\n"}
{"page": 9, "bbox": [{"x": 0.29883065819740295, "y": 0.34468820691108704}, {"x": 0.8297964334487915, "y": 0.34237876534461975}, {"x": 0.8297964334487915, "y": 0.38337182998657227}, {"x": 0.29883065819740295, "y": 0.38568130135536194}], "text": "If you have an older version of the Databricks CLI installed, you can upgrade to the latest version by running the\ncommand pip install --upgrade databricks-cli (for Python 3) or `pip install --upgrade databricks-cli<=1.10.0 (for\nPython 2).\n"}
{"page": 9, "bbox": [{"x": 0.2689476013183594, "y": 0.36085450649261475}, {"x": 0.27371156215667725, "y": 0.36085450649261475}, {"x": 0.27371156215667725, "y": 0.36778292059898376}, {"x": 0.2689476013183594, "y": 0.36778292059898376}], "text": "7\n"}
{"page": 9, "bbox": [{"x": 0.2685145139694214, "y": 0.4058891534805298}, {"x": 0.27327847480773926, "y": 0.4058891534805298}, {"x": 0.27327847480773926, "y": 0.4122401773929596}, {"x": 0.2685145139694214, "y": 0.4122401773929596}], "text": "9\n"}
{"page": 9, "bbox": [{"x": 0.29839757084846497, "y": 0.4024249315261841}, {"x": 0.8310956954956055, "y": 0.40473440289497375}, {"x": 0.8310956954956055, "y": 0.43244802951812744}, {"x": 0.29839757084846497, "y": 0.43013855814933777}], "text": "Alternatively, you can download the latest version of the Databricks CLI from the Databricks website and install it\nmanually.\n"}
{"page": 9, "bbox": [{"x": 0.26678216457366943, "y": 0.4214780628681183}, {"x": 0.2750108242034912, "y": 0.4214780628681183}, {"x": 0.2750108242034912, "y": 0.4278290867805481}, {"x": 0.26678216457366943, "y": 0.4278290867805481}], "text": "10\n"}
{"page": 9, "bbox": [{"x": 0.26678216457366943, "y": 0.4497690498828888}, {"x": 0.2750108242034912, "y": 0.4497690498828888}, {"x": 0.2750108242034912, "y": 0.456120103597641}, {"x": 0.26678216457366943, "y": 0.456120103597641}], "text": "11\n"}
{"page": 9, "bbox": [{"x": 0.297964483499527, "y": 0.44861432909965515}, {"x": 0.8297964334487915, "y": 0.44572749733924866}, {"x": 0.8297964334487915, "y": 0.47113165259361267}, {"x": 0.297964483499527, "y": 0.47401848435401917}], "text": "Once you have installed the required version of the Databricks CLI, you can use the databricks assets command to\nmanage Databricks Asset Bundles.\n"}
{"page": 9, "bbox": [{"x": 0.26678216457366943, "y": 0.4647805988788605}, {"x": 0.2754439115524292, "y": 0.4647805988788605}, {"x": 0.2754439115524292, "y": 0.4717090129852295}, {"x": 0.26678216457366943, "y": 0.4717090129852295}], "text": "12\n"}
{"page": 9, "bbox": [{"x": 0.25249025225639343, "y": 0.5415704250335693}, {"x": 0.8354265689849854, "y": 0.5415704250335693}, {"x": 0.8354265689849854, "y": 0.5802540183067322}, {"x": 0.25249025225639343, "y": 0.5802540183067322}], "text": "Databricks CLI version 1.10.0 doesn't exist and isn't the correct answer. Furthermore, there is no databricks\nassets command. In both cases, the model hallucinated an answer.\n"}
{"page": 9, "bbox": [{"x": 0.058466870337724686, "y": 0.929561197757721}, {"x": 0.14378519356250763, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.0580337792634964, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 10, "bbox": [{"x": 0.9519272446632385, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.06293302774429321}, {"x": 0.9519272446632385, "y": 0.06293302774429321}], "text": "10\n"}
{"page": 10, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 10, "bbox": [{"x": 0.25162407755851746, "y": 0.1622401773929596}, {"x": 0.8137722015380859, "y": 0.1622401773929596}, {"x": 0.8137722015380859, "y": 0.24133948981761932}, {"x": 0.25162407755851746, "y": 0.24133948981761932}], "text": "Using RAG to give an LLM access to documentation about DABS\nWhat happens if we set up a RAG system that can search for relevant segments of the Databricks Asset\nBundles docs and provide them to the model?\n"}
{"page": 10, "bbox": [{"x": 0.25162407755851746, "y": 0.2655889093875885}, {"x": 0.8137722015380859, "y": 0.2655889093875885}, {"x": 0.8137722015380859, "y": 0.306581974029541}, {"x": 0.25162407755851746, "y": 0.306581974029541}], "text": "We set up a RAG application by extracting text from documentation about DABs, loading it into a vector\ndatabase and setting up a retrieval system to pass relevant information to the LLM.\n"}
{"page": 10, "bbox": [{"x": 0.25205716490745544, "y": 0.3308314085006714}, {"x": 0.8254655599594116, "y": 0.3308314085006714}, {"x": 0.8254655599594116, "y": 0.3943417966365814}, {"x": 0.25205716490745544, "y": 0.3943417966365814}], "text": "Now when we ask our question, the RAG application queries the vector database and retrieves the most\nrelevant results, which it sends along with our question to the LLM. Let's ask the same questions as before\nand see how the results differ.\n"}
{"page": 10, "bbox": [{"x": 0.25249025225639343, "y": 0.42263278365135193}, {"x": 0.38631442189216614, "y": 0.42263278365135193}, {"x": 0.38631442189216614, "y": 0.4341801404953003}, {"x": 0.25249025225639343, "y": 0.4341801404953003}], "text": "Here are the new results.\n"}
{"page": 10, "bbox": [{"x": 0.29839757084846497, "y": 0.49538105726242065}, {"x": 0.44045040011405945, "y": 0.49480369687080383}, {"x": 0.44045040011405945, "y": 0.5034642219543457}, {"x": 0.29839757084846497, "y": 0.5040415525436401}], "text": "User: What are databricks DABS?\n"}
{"page": 10, "bbox": [{"x": 0.2689476013183594, "y": 0.4965358078479767}, {"x": 0.27327847480773926, "y": 0.4965358078479767}, {"x": 0.27327847480773926, "y": 0.5028868317604065}, {"x": 0.2689476013183594, "y": 0.5028868317604065}], "text": "1\n"}
{"page": 10, "bbox": [{"x": 0.2685145139694214, "y": 0.5242494344711304}, {"x": 0.27284538745880127, "y": 0.5242494344711304}, {"x": 0.27284538745880127, "y": 0.6085450053215027}, {"x": 0.2685145139694214, "y": 0.6085450053215027}], "text": "234569\n"}
{"page": 10, "bbox": [{"x": 0.29839757084846497, "y": 0.5236720442771912}, {"x": 0.8423560261726379, "y": 0.5236720442771912}, {"x": 0.8423560261726379, "y": 0.6120092272758484}, {"x": 0.29839757084846497, "y": 0.6120092272758484}], "text": "Bot: According to the provided context, Databricks Asset Bundles (DABS) are a tool for streamlining the development\nof complex data, analytics, and machine learning (ML) projects on the Databricks platform. They provide a way to\nmanage complex projects during active development by offering CI/CD capabilities to software development workflows\nin a single, concise, and declarative YAML syntax. DABS allow users to automate project tests, deployments, and\nconfiguration management, which can reduce errors and promote software best practices across an organization through\ntemplated projects.\n"}
{"page": 10, "bbox": [{"x": 0.2689476013183594, "y": 0.6893764138221741}, {"x": 0.27371156215667725, "y": 0.6893764138221741}, {"x": 0.27371156215667725, "y": 0.6957274675369263}, {"x": 0.2689476013183594, "y": 0.6957274675369263}], "text": "1\n"}
{"page": 10, "bbox": [{"x": 0.29839757084846497, "y": 0.6887990832328796}, {"x": 0.7262884378433228, "y": 0.6887990832328796}, {"x": 0.7262884378433228, "y": 0.6974595785140991}, {"x": 0.29839757084846497, "y": 0.6974595785140991}], "text": "User: What version of the databricks CLI do I need in order to use Databricks Asset Bundles?\n"}
{"page": 10, "bbox": [{"x": 0.2676483392715454, "y": 0.7165126800537109}, {"x": 0.2724123001098633, "y": 0.7165126800537109}, {"x": 0.27284538745880127, "y": 0.8019630312919617}, {"x": 0.2680814266204834, "y": 0.8019630312919617}], "text": "234569\n"}
{"page": 10, "bbox": [{"x": 0.29839757084846497, "y": 0.7170900702476501}, {"x": 0.8367258310317993, "y": 0.7170900702476501}, {"x": 0.8367258310317993, "y": 0.8036951422691345}, {"x": 0.29839757084846497, "y": 0.8036951422691345}], "text": "Bot: Databricks CLI version 0.205 or higher is required for Databricks Asset Bundles. You can confirm the version by\nrunning the command 'databricks --version' in your terminal. Note that version 0.205.2 or higher is required. If you\nhaven't installed the Databricks CLI yet, you can install it by following the instructions in the Install or update\nthe Databricks CLI documentation. Additionally, make sure that your remote Databricks workspaces are configured\ncorrectly, specifically that the workspace files feature is enabled, which is the case by default for Databricks\nRuntime version 11.2 or later.\n"}
{"page": 10, "bbox": [{"x": 0.25249025225639343, "y": 0.8556581735610962}, {"x": 0.8488523364067078, "y": 0.8556581735610962}, {"x": 0.8488523364067078, "y": 0.8700923919677734}, {"x": 0.25249025225639343, "y": 0.8700923919677734}], "text": "In both cases, the model generated correct answers because it was given access to the information it needed.\n"}
{"page": 10, "bbox": [{"x": 0.25205716490745544, "y": 0.888568103313446}, {"x": 0.8323949575424194, "y": 0.8943418264389038}, {"x": 0.8319618701934814, "y": 0.9393764138221741}, {"x": 0.25162407755851746, "y": 0.9336027503013611}], "text": "Without RAG, the best the LLM could do was guess or admit it didn't know. With RAG, the LLM provided the\ncorrect answers.\n"}
{"page": 10, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 11, "bbox": [{"x": 0.9553919434547424, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.062355659902095795}, {"x": 0.9553919434547424, "y": 0.062355659902095795}], "text": "11\n"}
{"page": 11, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 11, "bbox": [{"x": 0.25249025225639343, "y": 0.15993072092533112}, {"x": 0.8393243551254272, "y": 0.15993072092533112}, {"x": 0.8393243551254272, "y": 0.22690531611442566}, {"x": 0.25249025225639343, "y": 0.22690531611442566}], "text": "LLMs are capable of giving compelling and coherent answers to user prompts. They're often able to detect\nnuance, identify context and give the appearance of reasoning when answering. And because they're trained\non vast amounts of data, they have access to enormous amounts of knowledge.\n"}
{"page": 11, "bbox": [{"x": 0.038544826209545135, "y": 0.16454964876174927}, {"x": 0.21134690940380096, "y": 0.16454964876174927}, {"x": 0.21134690940380096, "y": 0.2540415823459625}, {"x": 0.038544826209545135, "y": 0.2540415823459625}], "text": "Addressing the\nShortcomings of\nLLMs With RAG\n"}
{"page": 11, "bbox": [{"x": 0.25162407755851746, "y": 0.2505773603916168}, {"x": 0.8419229388237, "y": 0.25115472078323364}, {"x": 0.8419229388237, "y": 0.7095842957496643}, {"x": 0.25162407755851746, "y": 0.7090069055557251}], "text": "LLMs, however, are not reliable as knowledge sources. LLMs often respond with made-up answers, or\nhallucinations, rather than acknowledging that they don't know the correct answer. Furthermore, LLMs are\nfundamentally limited by their training data. An LLM alone doesn't know anything that happened after it's\ntraining is completed, and it lacks access to proprietary information, such as company documents, that\nweren't publicly available during training.\nWe can try to mitigate these problems by explicitly giving the LLM the information it needs to address the\nuser's prompt. This can be as simple as copying and pasting a couple of pages of reference documents and\nsending them, along with a question, to ChatGPT or another LLM. In so doing, we have augmented the original\nprompt (the question) with the information needed to answer the question accurately. The additional step\nof building a retrieval system, such as a vector database, allows us to automate this process and ensure\nthat the model has the most relevant information without requiring the user to seek it out and add it to\nthe prompt manually.\nWhile implementing RAG with Vector Search involves the extra steps of data processing and managing the\ngenerated vectors (often with a vector database), it can help to address the limitations of using LLMs alone.\nRAG improves on LLM-only approaches by providing additional, specific context that the LLM can use when\nformulating an answer. RAG also has benefits compared to retrieval-only systems, as the LLM can process\ntexts from multiple sources into a readable output tailored to the user's prompt.\n"}
{"page": 11, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 12, "bbox": [{"x": 0.9527934193611145, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9527934193611145, "y": 0.06293302774429321}], "text": "12\n"}
{"page": 12, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 12, "bbox": [{"x": 0.25249025225639343, "y": 0.16339491307735443}, {"x": 0.59462970495224, "y": 0.16339491307735443}, {"x": 0.59462970495224, "y": 0.21478059887886047}, {"x": 0.25249025225639343, "y": 0.21478059887886047}], "text": "RAG compared to LLM-only approaches\nLet's explore some of the benefits of RAG in more detail.\n"}
{"page": 12, "bbox": [{"x": 0.5703768134117126, "y": 0.2598152458667755}, {"x": 0.6929406523704529, "y": 0.26154735684394836}, {"x": 0.6929406523704529, "y": 0.281177818775177}, {"x": 0.5703768134117126, "y": 0.27944573760032654}], "text": "RAG Application\n"}
{"page": 12, "bbox": [{"x": 0.6132524609565735, "y": 0.3181293308734894}, {"x": 0.6496318578720093, "y": 0.3181293308734894}, {"x": 0.6496318578720093, "y": 0.3314087688922882}, {"x": 0.6132524609565735, "y": 0.3314087688922882}], "text": "Query\n"}
{"page": 12, "bbox": [{"x": 0.5868341326713562, "y": 0.3660508096218109}, {"x": 0.6760502457618713, "y": 0.3660508096218109}, {"x": 0.6760502457618713, "y": 0.37644341588020325}, {"x": 0.5868341326713562, "y": 0.37644341588020325}], "text": "Vector Database\n"}
{"page": 12, "bbox": [{"x": 0.3378085792064667, "y": 0.3810623586177826}, {"x": 0.40710264444351196, "y": 0.3839491903781891}, {"x": 0.406669557094574, "y": 0.40184757113456726}, {"x": 0.3373754918575287, "y": 0.39896073937416077}], "text": "LLM Only\n"}
{"page": 12, "bbox": [{"x": 0.5725422501564026, "y": 0.40819862484931946}, {"x": 0.689909040927887, "y": 0.4093533456325531}, {"x": 0.689909040927887, "y": 0.42263278365135193}, {"x": 0.5725422501564026, "y": 0.4214780628681183}], "text": "Checks Permissions\n"}
{"page": 12, "bbox": [{"x": 0.49501949548721313, "y": 0.41743648052215576}, {"x": 0.5495885610580444, "y": 0.41801387071609497}, {"x": 0.5495885610580444, "y": 0.44803693890571594}, {"x": 0.49501949548721313, "y": 0.4474595785140991}], "text": "Combined\nwith\n"}
{"page": 12, "bbox": [{"x": 0.35339975357055664, "y": 0.4399538040161133}, {"x": 0.38977912068367004, "y": 0.4411085546016693}, {"x": 0.38977912068367004, "y": 0.45438799262046814}, {"x": 0.35339975357055664, "y": 0.4532332420349121}], "text": "Query\n"}
{"page": 12, "bbox": [{"x": 0.5734084248542786, "y": 0.45785218477249146}, {"x": 0.688609778881073, "y": 0.45727482438087463}, {"x": 0.688609778881073, "y": 0.48903003334999084}, {"x": 0.5734084248542786, "y": 0.48960739374160767}], "text": "Searches Permitted\nData Sources\n"}
{"page": 12, "bbox": [{"x": 0.716327428817749, "y": 0.4624711275100708}, {"x": 0.8349934816360474, "y": 0.4624711275100708}, {"x": 0.8349934816360474, "y": 0.4907621145248413}, {"x": 0.716327428817749, "y": 0.4907621145248413}], "text": "Regularly Updated\nFrom Outside Sources\n"}
{"page": 12, "bbox": [{"x": 0.36032915115356445, "y": 0.5352193713188171}, {"x": 0.38241663575172424, "y": 0.5352193713188171}, {"x": 0.38241663575172424, "y": 0.5461893677711487}, {"x": 0.36032915115356445, "y": 0.5461893677711487}], "text": "LLM\n"}
{"page": 12, "bbox": [{"x": 0.5838025212287903, "y": 0.5554272532463074}, {"x": 0.6790818572044373, "y": 0.5554272532463074}, {"x": 0.6790818572044373, "y": 0.5669745802879333}, {"x": 0.5838025212287903, "y": 0.5669745802879333}], "text": "Relevant Results\n"}
{"page": 12, "bbox": [{"x": 0.5868341326713562, "y": 0.6039261221885681}, {"x": 0.6855781674385071, "y": 0.6039261221885681}, {"x": 0.6855781674385071, "y": 0.6166281700134277}, {"x": 0.5868341326713562, "y": 0.6166281700134277}], "text": "Augmented Query\n"}
{"page": 12, "bbox": [{"x": 0.34993502497673035, "y": 0.6293302774429321}, {"x": 0.39324381947517395, "y": 0.6310623288154602}, {"x": 0.39281073212623596, "y": 0.6454965472221375}, {"x": 0.34950193762779236, "y": 0.6437644362449646}], "text": "Output\n"}
{"page": 12, "bbox": [{"x": 0.6080554127693176, "y": 0.636836051940918}, {"x": 0.6539627313613892, "y": 0.6362586617469788}, {"x": 0.6539627313613892, "y": 0.6501154899597168}, {"x": 0.6080554127693176, "y": 0.6506928205490112}], "text": "Query +\n"}
{"page": 12, "bbox": [{"x": 0.566045880317688, "y": 0.6564664840698242}, {"x": 0.6968384385108948, "y": 0.6570438742637634}, {"x": 0.6968384385108948, "y": 0.6882216930389404}, {"x": 0.566045880317688, "y": 0.687644362449646}], "text": "Relevant Results\nFrom Vector Database\n"}
{"page": 12, "bbox": [{"x": 0.6201819181442261, "y": 0.7367205619812012}, {"x": 0.6422693729400635, "y": 0.7367205619812012}, {"x": 0.6422693729400635, "y": 0.7476905584335327}, {"x": 0.6201819181442261, "y": 0.7476905584335327}], "text": "LLM\n"}
{"page": 12, "bbox": [{"x": 0.6102208495140076, "y": 0.8071593642234802}, {"x": 0.6530965566635132, "y": 0.8077366948127747}, {"x": 0.6530965566635132, "y": 0.8215935230255127}, {"x": 0.6102208495140076, "y": 0.8210161924362183}], "text": "Output\n"}
{"page": 12, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 13, "bbox": [{"x": 0.9527934193611145, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9527934193611145, "y": 0.06293302774429321}], "text": "13\n"}
{"page": 13, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 13, "bbox": [{"x": 0.27371156215667725, "y": 0.1622401773929596}, {"x": 0.8531832098960876, "y": 0.16166281700134277}, {"x": 0.8531832098960876, "y": 0.6651270389556885}, {"x": 0.27371156215667725, "y": 0.6657043695449829}], "text": "■ RAG applications can incorporate proprietary data. Most LLMs are trained on publicly available data.\nThey don't have access to a company's internal documents or communications. RAG allows you to\nsupply proprietary or domain-specific information such as internal memos, emails or design documents\nto the model.\n■ RAG applications can access up-to-date information. LLMs are generally trained at a particular point\nin time and then released for use. Older models lack updated information about the state of the world or\na particular field or business. For example, if a new version of a software product is released after an LLM\nwas trained, it won't be able to provide assistance specific to the new version. RAG provides a way to\nsupply the model with up-to-date information.\n■ RAG can enhance the accuracy of LLM responses. LLMs alone can respond with incorrect or\nfabricated information (hallucinations). They aren't consistent or reliable information sources. An\neffective RAG system can retrieve relevant and correct references and supply them to the model,\npotentially reducing the occurrence of hallucinations. Outputs can include citations of original sources,\nallowing for human verification.\n■ RAG enables fine-grained data access control. LLMs alone cannot reliably provide different\nresponses to different users based on security or permission considerations. RAG applications,\non the other hand, can be designed to retrieve only documents that a user has permission to access.\nThis can enable LLMs to securely reference confidential or personal data based on the access\ncredentials of the system's user.\n"}
{"page": 13, "bbox": [{"x": 0.058466870337724686, "y": 0.9307159185409546}, {"x": 0.14378519356250763, "y": 0.9312933087348938}, {"x": 0.14378519356250763, "y": 0.9486142992973328}, {"x": 0.058466870337724686, "y": 0.9480369687080383}], "text": "databricks\n"}
{"page": 14, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 14, "bbox": [{"x": 0.9523603320121765, "y": 0.05600461736321449}, {"x": 0.9601559042930603, "y": 0.05600461736321449}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9523603320121765, "y": 0.06293302774429321}], "text": "14\n"}
{"page": 14, "bbox": [{"x": 0.03941100090742111, "y": 0.16512702405452728}, {"x": 0.195322647690773, "y": 0.16512702405452728}, {"x": 0.195322647690773, "y": 0.18475750088691711}, {"x": 0.03941100090742111, "y": 0.18475750088691711}], "text": "RAG Use Cases\n"}
{"page": 14, "bbox": [{"x": 0.25205716490745544, "y": 0.15993072092533112}, {"x": 0.8518839478492737, "y": 0.15531177818775177}, {"x": 0.8523170351982117, "y": 0.22344110906124115}, {"x": 0.25249025225639343, "y": 0.2280600517988205}], "text": "RAG equips LLMs with context-specific information that LLMs alone either don't possess or may not be able to\ngenerate reliably when needed. This enables several different applications that would be difficult or impossible\nusing only LLMs.\n"}
{"page": 14, "bbox": [{"x": 0.2529233396053314, "y": 0.262702077627182}, {"x": 0.5028150677680969, "y": 0.2650115489959717}, {"x": 0.5028150677680969, "y": 0.28579676151275635}, {"x": 0.2529233396053314, "y": 0.2834872901439667}], "text": "Question-answering systems\n"}
{"page": 14, "bbox": [{"x": 0.25205716490745544, "y": 0.30196306109428406}, {"x": 0.8436552882194519, "y": 0.30196306109428406}, {"x": 0.8436552882194519, "y": 0.39549654722213745}, {"x": 0.25205716490745544, "y": 0.39549654722213745}], "text": "RAG becomes invaluable in use cases where the aim is to \"talk to documents,\" such as querying HR policies\nor accessing real-time financial reports. Using RAG, information can be dynamically retrieved and presented\nin a conversational manner to an end user. For instance, a large e-commerce company uses Databricks for an\ninternal RAG application, enabling their HR team to query hundreds of employee policy documents.\n"}
{"page": 14, "bbox": [{"x": 0.25249025225639343, "y": 0.43244802951812744}, {"x": 0.40363794565200806, "y": 0.43244802951812744}, {"x": 0.40363794565200806, "y": 0.44861432909965515}, {"x": 0.25249025225639343, "y": 0.44861432909965515}], "text": "Customer service\n"}
{"page": 14, "bbox": [{"x": 0.25205716490745544, "y": 0.46997690200805664}, {"x": 0.8449545502662659, "y": 0.46997690200805664}, {"x": 0.8449545502662659, "y": 0.5629330277442932}, {"x": 0.25205716490745544, "y": 0.5629330277442932}], "text": "RAG systems can streamline the customer service process by providing support personnel with personalized\nand more informed responses to customer queries. This can enhance customer experience, reduce response\ntimes and increase resolution efficiency. We see this kind of “internal copilot” RAG application across many\ncustomers seeking to improve the efficiency and effectiveness of internal workers.\n"}
{"page": 14, "bbox": [{"x": 0.25249025225639343, "y": 0.5993071794509888}, {"x": 0.41879600286483765, "y": 0.6010392904281616}, {"x": 0.41879600286483765, "y": 0.6218245029449463}, {"x": 0.25249025225639343, "y": 0.6200923919677734}], "text": "Content generation\n"}
{"page": 14, "bbox": [{"x": 0.25205716490745544, "y": 0.6379907727241516}, {"x": 0.8501515984535217, "y": 0.6379907727241516}, {"x": 0.8501515984535217, "y": 0.7309468984603882}, {"x": 0.25205716490745544, "y": 0.7309468984603882}], "text": "In content creation scenarios, RAG can be used to draft communications, like sales emails, by integrating the\nmost recent data and relevant context. This can ensure that customer outreach is both personalized and\nreflects the latest information. One Databricks customer is leveraging RAG to draft email responses to inbound\nsales emails, incorporating external product and customer information into responses.\n"}
{"page": 14, "bbox": [{"x": 0.25249025225639343, "y": 0.7678983807563782}, {"x": 0.39367690682411194, "y": 0.7690531015396118}, {"x": 0.39367690682411194, "y": 0.7863741517066956}, {"x": 0.25249025225639343, "y": 0.7852193713188171}], "text": "Code assistance\n"}
{"page": 14, "bbox": [{"x": 0.25249025225639343, "y": 0.8060046434402466}, {"x": 0.7960156202316284, "y": 0.8060046434402466}, {"x": 0.7960156202316284, "y": 0.8724018335342407}, {"x": 0.25249025225639343, "y": 0.8724018335342407}], "text": "RAG can enhance code completion and code Q&A systems by intelligently searching and retrieving\ninformation from code bases, documentation and external libraries. This can result in improved code\ngeneration and more relevant responses compared to LLM-only code assistants.\n"}
{"page": 14, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 15, "bbox": [{"x": 0.9523603320121765, "y": 0.05542725324630737}, {"x": 0.9592897295951843, "y": 0.05542725324630737}, {"x": 0.9592897295951843, "y": 0.062355659902095795}, {"x": 0.9523603320121765, "y": 0.062355659902095795}], "text": "15\n"}
{"page": 15, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 15, "bbox": [{"x": 0.038544826209545135, "y": 0.16339491307735443}, {"x": 0.2135123461484909, "y": 0.16454964876174927}, {"x": 0.21307925879955292, "y": 0.22401846945285797}, {"x": 0.03811173513531685, "y": 0.22286374866962433}], "text": "RAG With Vector\nSearch - - Step\n"}
{"page": 15, "bbox": [{"x": 0.25205716490745544, "y": 0.15993072092533112}, {"x": 0.8536162972450256, "y": 0.15993072092533112}, {"x": 0.8536162972450256, "y": 0.27944573760032654}, {"x": 0.25205716490745544, "y": 0.27944573760032654}], "text": "RAG with Vector Search involves retrieving information using a vector database, augmenting the user's\nprompt with that information and generating a response based on the user's prompt and information retrieved\nusing an LLM. In this section, we'll review each of these steps, focusing on the processes represented in a\nreasonably standard RAG system. Be aware that there are many different approaches to these steps, including\nsome advanced techniques that may increase performance but add complexity.\n"}
{"page": 15, "bbox": [{"x": 0.03941100090742111, "y": 0.2344110906124115}, {"x": 0.11823300272226334, "y": 0.23556582629680634}, {"x": 0.11779991537332535, "y": 0.2598152458667755}, {"x": 0.03941100090742111, "y": 0.2586604952812195}], "text": "by Step\n"}
{"page": 15, "bbox": [{"x": 0.25249025225639343, "y": 0.31697461009025574}, {"x": 0.7743611931800842, "y": 0.3129330277442932}, {"x": 0.7743611931800842, "y": 0.3620092272758484}, {"x": 0.2529233396053314, "y": 0.3660508096218109}], "text": "Data preparation: Getting an external information source into\na vector database\n"}
{"page": 15, "bbox": [{"x": 0.6959722638130188, "y": 0.4503464102745056}, {"x": 0.7968817949295044, "y": 0.4503464102745056}, {"x": 0.7968817949295044, "y": 0.4624711275100708}, {"x": 0.6959722638130188, "y": 0.4624711275100708}], "text": "Vector Database\n"}
{"page": 15, "bbox": [{"x": 0.45864009857177734, "y": 0.5086604952812195}, {"x": 0.4932871460914612, "y": 0.5086604952812195}, {"x": 0.4932871460914612, "y": 0.5184757709503174}, {"x": 0.45864009857177734, "y": 0.5184757709503174}], "text": "Chunk\n"}
{"page": 15, "bbox": [{"x": 0.7284538745880127, "y": 0.5092378854751587}, {"x": 0.7652663588523865, "y": 0.5092378854751587}, {"x": 0.7652663588523865, "y": 0.519630491733551}, {"x": 0.7284538745880127, "y": 0.519630491733551}], "text": "Vector\n"}
{"page": 15, "bbox": [{"x": 0.45864009857177734, "y": 0.5571593642234802}, {"x": 0.4932871460914612, "y": 0.5571593642234802}, {"x": 0.4932871460914612, "y": 0.5669745802879333}, {"x": 0.45864009857177734, "y": 0.5669745802879333}], "text": "Chunk\n"}
{"page": 15, "bbox": [{"x": 0.3074924349784851, "y": 0.556581974029541}, {"x": 0.37072324752807617, "y": 0.556581974029541}, {"x": 0.37072324752807617, "y": 0.568129301071167}, {"x": 0.3074924349784851, "y": 0.568129301071167}], "text": "Document\n"}
{"page": 15, "bbox": [{"x": 0.5859679579734802, "y": 0.5479214787483215}, {"x": 0.6483325958251953, "y": 0.5479214787483215}, {"x": 0.6483325958251953, "y": 0.5779445767402649}, {"x": 0.5859679579734802, "y": 0.5779445767402649}], "text": "Embedding\nModel\n"}
{"page": 15, "bbox": [{"x": 0.7284538745880127, "y": 0.5571593642234802}, {"x": 0.7656994462013245, "y": 0.5583140850067139}, {"x": 0.7656994462013245, "y": 0.5687066912651062}, {"x": 0.7284538745880127, "y": 0.5675519704818726}], "text": "Vector\n"}
{"page": 15, "bbox": [{"x": 0.45864009857177734, "y": 0.6050808429718018}, {"x": 0.49372023344039917, "y": 0.6056581735610962}, {"x": 0.49372023344039917, "y": 0.6166281700134277}, {"x": 0.45864009857177734, "y": 0.6160507798194885}], "text": "Chunk\n"}
{"page": 15, "bbox": [{"x": 0.7284538745880127, "y": 0.6056581735610962}, {"x": 0.7652663588523865, "y": 0.6062355637550354}, {"x": 0.7652663588523865, "y": 0.6166281700134277}, {"x": 0.7284538745880127, "y": 0.6160507798194885}], "text": "Vector\n"}
{"page": 15, "bbox": [{"x": 0.25249025225639343, "y": 0.7344110608100891}, {"x": 0.8475530743598938, "y": 0.7355658411979675}, {"x": 0.8475530743598938, "y": 0.8025404214859009}, {"x": 0.25249025225639343, "y": 0.8013857007026672}], "text": "Before we can perform RAG with Vector Search, we need to get data - in this case, unstructured text data\ninto a vector database. There are many approaches to doing this, and it's essential to try different methods to\ndetermine which is the most effective for your use case.\n"}
{"page": 15, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 16, "bbox": [{"x": 0.9519272446632385, "y": 0.054849885404109955}, {"x": 0.9597228169441223, "y": 0.054849885404109955}, {"x": 0.9597228169441223, "y": 0.06293302774429321}, {"x": 0.9519272446632385, "y": 0.06293302774429321}], "text": "16\n"}
{"page": 16, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 16, "bbox": [{"x": 0.25249025225639343, "y": 0.15993072092533112}, {"x": 0.8505846858024597, "y": 0.15993072092533112}, {"x": 0.8505846858024597, "y": 0.22690531611442566}, {"x": 0.25249025225639343, "y": 0.22690531611442566}], "text": "Data preparation generally isn't a one-time task, because a vector database should be regularly updated to\nprovide up-to-date and high-quality information. This is one of the key benefits of RAG ― we can continuously\nupdate the vector database without needing to update the LLM weights over time.\n"}
{"page": 16, "bbox": [{"x": 0.25162407755851746, "y": 0.25115472078323364}, {"x": 0.6058899760246277, "y": 0.25115472078323364}, {"x": 0.6058899760246277, "y": 0.2661662697792053}, {"x": 0.25162407755851746, "y": 0.2661662697792053}], "text": "A few core steps for preparing data for RAG include the following:\n"}
{"page": 16, "bbox": [{"x": 0.25249025225639343, "y": 0.29503464698791504}, {"x": 0.5075790286064148, "y": 0.29503464698791504}, {"x": 0.5075790286064148, "y": 0.3088914453983307}, {"x": 0.25249025225639343, "y": 0.3088914453983307}], "text": "PARSING THE INPUT DOCUMENTS\n"}
{"page": 16, "bbox": [{"x": 0.25249025225639343, "y": 0.33256351947784424}, {"x": 0.8458207249641418, "y": 0.33314087986946106}, {"x": 0.8458207249641418, "y": 0.4284064769744873}, {"x": 0.25249025225639343, "y": 0.4278290867805481}], "text": "The raw documents may not be in a format amenable to processing for RAG with Vector Search. Images may\nneed to be converted to text; tables or images might require further processing and there may be extraneous\ntext, such as page headers or page numbers, to clean up or remove. It is often necessary to parse the raw\ninput documents and get them into a format - usually text - that will work with the rest of the RAG pipeline.\n"}
{"page": 16, "bbox": [{"x": 0.25249025225639343, "y": 0.4555427134037018}, {"x": 0.5456907749176025, "y": 0.4555427134037018}, {"x": 0.5456907749176025, "y": 0.4693995416164398}, {"x": 0.25249025225639343, "y": 0.4693995416164398}], "text": "SPLITTING DOCUMENTS INTO CHUNKS\n"}
{"page": 16, "bbox": [{"x": 0.25162407755851746, "y": 0.4936489462852478}, {"x": 0.8518839478492737, "y": 0.4936489462852478}, {"x": 0.8518839478492737, "y": 0.5606235861778259}, {"x": 0.25162407755851746, "y": 0.5606235861778259}], "text": "You typically don't want to retrieve entire books, web pages or articles in a RAG application. Instead, split the\ndocuments into smaller chunks so you can send more specific results to the LLM for context. \"Documents\" is a\ngeneral term for referring to source texts, but you can think of documents as any kind of text.\n"}
{"page": 16, "bbox": [{"x": 0.25205716490745544, "y": 0.5848729610443115}, {"x": 0.8536162972450256, "y": 0.5848729610443115}, {"x": 0.8536162972450256, "y": 0.6506928205490112}, {"x": 0.25205716490745544, "y": 0.6506928205490112}], "text": "Chunk size can affect the output quality of a RAG application. If the chunks are too small, they may not include\nenough context to address the user's query. If the chunks are too large, the LLM may fail to pull out the relevant\ndetails, focusing instead on other details included in the chunk.\n"}
{"page": 16, "bbox": [{"x": 0.25205716490745544, "y": 0.6760969758033752}, {"x": 0.8475530743598938, "y": 0.6760969758033752}, {"x": 0.8475530743598938, "y": 0.7170900702476501}, {"x": 0.25205716490745544, "y": 0.7170900702476501}], "text": "There's no one-size-fits-all solution to choosing the best chunk size. It depends on the source documents, the\nLLM and the RAG application's goals. It's important to try different chunk sizes.\n"}
{"page": 16, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 17, "bbox": [{"x": 0.9527934193611145, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9597228169441223, "y": 0.06351039558649063}, {"x": 0.9523603320121765, "y": 0.06293302774429321}], "text": "17\n"}
{"page": 17, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 17, "bbox": [{"x": 0.2529233396053314, "y": 0.16108545660972595}, {"x": 0.49112170934677124, "y": 0.16108545660972595}, {"x": 0.49112170934677124, "y": 0.1749422699213028}, {"x": 0.2529233396053314, "y": 0.1749422699213028}], "text": "EMBEDDING THE TEXT CHUNKS\n"}
{"page": 17, "bbox": [{"x": 0.25205716490745544, "y": 0.19919168949127197}, {"x": 0.8345603942871094, "y": 0.19919168949127197}, {"x": 0.8345603942871094, "y": 0.23960739374160767}, {"x": 0.25205716490745544, "y": 0.23960739374160767}], "text": "After splitting the source documents into manageable chunks, use an embedding model to convert each of\nthose chunks into a high-dimensional numerical vector.\n"}
{"page": 17, "bbox": [{"x": 0.25162407755851746, "y": 0.26443418860435486}, {"x": 0.8375920057296753, "y": 0.26443418860435486}, {"x": 0.8375920057296753, "y": 0.3568129241466522}, {"x": 0.25162407755851746, "y": 0.3568129241466522}], "text": "An embedding model is a special kind of language model that uses its knowledge of language to generate a\nnumeric vector or a series of numbers, called an embedding, from a text. Embeddings encode the nuanced\nand context-specific meaning of each text in numeric form. A good embedding model will know that \"raining\ncats and dogs\" is a phrase about the weather, not a phrase about pets.\n"}
{"page": 17, "bbox": [{"x": 0.25205716490745544, "y": 0.3816397190093994}, {"x": 0.8440883755683899, "y": 0.3816397190093994}, {"x": 0.8440883755683899, "y": 0.5}, {"x": 0.25205716490745544, "y": 0.5}], "text": "The true power of embeddings for RAG is that they can be mathematically compared to each other. We can\nmeasure how \"similar\" two embeddings are, which in this context, equates to how closely the meanings of\ntheir original texts are related. This will be especially useful later in the RAG process when we embed a user's\nprompt, compare it to the embedded texts in the vector database and identify those we think are most likely\nto help the LLM provide a useful answer.\n"}
{"page": 17, "bbox": [{"x": 0.25205716490745544, "y": 0.5288683772087097}, {"x": 0.8479861617088318, "y": 0.5300230979919434}, {"x": 0.8475530743598938, "y": 0.831986129283905}, {"x": 0.25162407755851746, "y": 0.8308314085006714}], "text": "STORING AND INDEXING THE EMBEDDINGS\nEmbeddings are stored in a specialized kind of database known as a vector database, which is designed to\nefficiently store and search for vector data like embeddings. A vector database is a type of vector store\nthese terms are often used interchangeably — though \"vector store\" can refer to any type of vector storage\nsolution, not just to databases. Vector databases often incorporate update mechanisms so newly added\nchunks can be searched and retrieved immediately. While such databases are not strictly necessary for RAG\nor Vector Search, they often meaningfully improve RAG performance and reliability.\nHaving a huge number of text chunks can result in slower retrieval speeds. A common approach to maintain\nperformance is to index the embeddings with a vector index. A vector index is a mechanism, often part of a\nvector database, that uses various algorithms to organize and map vector embeddings in a way that optimizes\nsearch efficiency.\n"}
{"page": 17, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 18, "bbox": [{"x": 0.9523603320121765, "y": 0.05542725324630737}, {"x": 0.9592897295951843, "y": 0.05542725324630737}, {"x": 0.9592897295951843, "y": 0.062355659902095795}, {"x": 0.9523603320121765, "y": 0.062355659902095795}], "text": "18\n"}
{"page": 18, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 18, "bbox": [{"x": 0.25249025225639343, "y": 0.16108545660972595}, {"x": 0.43005630373954773, "y": 0.16108545660972595}, {"x": 0.43005630373954773, "y": 0.1749422699213028}, {"x": 0.25249025225639343, "y": 0.1749422699213028}], "text": "RECORDING METADATA\n"}
{"page": 18, "bbox": [{"x": 0.25205716490745544, "y": 0.19630484282970428}, {"x": 0.8341273069381714, "y": 0.19803695380687714}, {"x": 0.8341273069381714, "y": 0.293879896402359}, {"x": 0.25205716490745544, "y": 0.29214781522750854}], "text": "Capturing metadata along with text chunks allows us to filter results based on metadata (if applicable)\nand provide detailed references along with the results. A RAG application with metadata could, for instance,\nprovide specific URLs or page numbers for the sources retrieved, or it could allow users to explicitly filter\nby date or source.\n"}
{"page": 18, "bbox": [{"x": 0.25249025225639343, "y": 0.3210161626338959}, {"x": 0.8618449568748474, "y": 0.3210161626338959}, {"x": 0.8618449568748474, "y": 0.4122401773929596}, {"x": 0.25249025225639343, "y": 0.4122401773929596}], "text": "EXAMPLE: PREPROCESSING AND EMBEDDING\nIn the example above, we started with two documents about Databricks Asset Bundles. To use them for RAG, we:\n1. Split each document into chunks. For example, one of those chunks is:\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.4722863733768463}, {"x": 0.27327847480773926, "y": 0.4722863733768463}, {"x": 0.27327847480773926, "y": 0.4786374270915985}, {"x": 0.2689476013183594, "y": 0.4786374270915985}], "text": "1\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.487297922372818}, {"x": 0.27371156215667725, "y": 0.487297922372818}, {"x": 0.27371156215667725, "y": 0.494226336479187}, {"x": 0.2689476013183594, "y": 0.494226336479187}], "text": "2\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.5028868317604065}, {"x": 0.27371156215667725, "y": 0.5028868317604065}, {"x": 0.27371156215667725, "y": 0.5098152160644531}, {"x": 0.2689476013183594, "y": 0.5098152160644531}], "text": "3\n"}
{"page": 18, "bbox": [{"x": 0.2685145139694214, "y": 0.5190531015396118}, {"x": 0.27371156215667725, "y": 0.5190531015396118}, {"x": 0.27371156215667725, "y": 0.525404155254364}, {"x": 0.2685145139694214, "y": 0.525404155254364}], "text": "4\n"}
{"page": 18, "bbox": [{"x": 0.297964483499527, "y": 0.47055426239967346}, {"x": 0.8406236171722412, "y": 0.47055426239967346}, {"x": 0.8406236171722412, "y": 0.5906466245651245}, {"x": 0.297964483499527, "y": 0.5906466245651245}], "text": "Databricks Assets Bundles are an infrastructure-as-code (IaC) approach to managing your Databricks projects. Use them\nwhen you want to manage complex projects where multiple contributors and automation are essential, and continuous\nintegration and deployment (CI/CD) are a requirement. Since bundles are defined and managed through YAML templates\nand files you create and maintain alongside source code, they map well to scenarios where IaC is an appropriate\napproach.\\n\\nSome ideal scenarios for bundles include:\\n\\nDevelop data, analytics, and ML projects in a team-\nbased environment. Bundles can help you organize and manage various source files efficiently. This ensures smooth\ncollaboration and streamlined processes.\\n\\nIterate on ML problems faster. Manage ML pipeline resources (such as\ntraining and batch inference jobs) by using ML projects that follow production best practices from the beginning.'\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.5340646505355835}, {"x": 0.27327847480773926, "y": 0.5340646505355835}, {"x": 0.27327847480773926, "y": 0.5409930944442749}, {"x": 0.2689476013183594, "y": 0.5409930944442749}], "text": "5\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.5502309203147888}, {"x": 0.27414464950561523, "y": 0.5502309203147888}, {"x": 0.27414464950561523, "y": 0.5571593642234802}, {"x": 0.2689476013183594, "y": 0.5571593642234802}], "text": "6\n"}
{"page": 18, "bbox": [{"x": 0.2685145139694214, "y": 0.5652424693107605}, {"x": 0.27327847480773926, "y": 0.5652424693107605}, {"x": 0.27327847480773926, "y": 0.5721709132194519}, {"x": 0.2685145139694214, "y": 0.5721709132194519}], "text": "7\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.5808314085006714}, {"x": 0.27327847480773926, "y": 0.5808314085006714}, {"x": 0.27327847480773926, "y": 0.587759792804718}, {"x": 0.2689476013183594, "y": 0.587759792804718}], "text": "8\n"}
{"page": 18, "bbox": [{"x": 0.27371156215667725, "y": 0.6304849982261658}, {"x": 0.8588133454322815, "y": 0.6316397190093994}, {"x": 0.8588133454322815, "y": 0.6968821883201599}, {"x": 0.27371156215667725, "y": 0.6957274675369263}], "text": "2. Embed the chunks. We use a general-purpose embedding model called bge-large-en to turn each\nchunk into a 1024-dimension numeric vector, which is basically a list of 1024 numbers. The chunk above is\ntranslated to:\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.7546189427375793}, {"x": 0.27327847480773926, "y": 0.7546189427375793}, {"x": 0.27327847480773926, "y": 0.7609699964523315}, {"x": 0.2689476013183594, "y": 0.7609699964523315}], "text": "1\n"}
{"page": 18, "bbox": [{"x": 0.2685145139694214, "y": 0.7690531015396118}, {"x": 0.2724123001098633, "y": 0.7690531015396118}, {"x": 0.2724123001098633, "y": 0.792725145816803}, {"x": 0.2685145139694214, "y": 0.792725145816803}], "text": "23\n"}
{"page": 18, "bbox": [{"x": 0.29926374554634094, "y": 0.7534642219543457}, {"x": 0.8345603942871094, "y": 0.7534642219543457}, {"x": 0.8345603942871094, "y": 0.8250577449798584}, {"x": 0.29926374554634094, "y": 0.8250577449798584}], "text": "[0.0209503173828125, 0.0172576904296875, -0.003314971923828125, -0.0025310516357421875, 0.00670623779296875,\n-0.00506591796875, 0.0005450248718261719, -0.049896240234375, 0.00630950927734375, 0.0003032684326171875,\n-0.001049041748046875, -0.0084991455078125, 0.031585693359375, -0.0621337890625, -0.009765625, 0.017669677734375,\n-0.045623779296875, 0.0022907257080078125, -0.0736083984375, 0.0286102294921875, 0.01532745361328125, 0.0298919677734375,\n-0.09027099609375, -0.0207977294921875, -0.048736572265625, 0.07818603515625, 0.0648193359375,\n"}
{"page": 18, "bbox": [{"x": 0.2685145139694214, "y": 0.800808310508728}, {"x": 0.27414464950561523, "y": 0.800808310508728}, {"x": 0.27414464950561523, "y": 0.8071593642234802}, {"x": 0.2685145139694214, "y": 0.8071593642234802}], "text": "4\n"}
{"page": 18, "bbox": [{"x": 0.2685145139694214, "y": 0.8163972496986389}, {"x": 0.27327847480773926, "y": 0.8163972496986389}, {"x": 0.27327847480773926, "y": 0.8233256340026855}, {"x": 0.2685145139694214, "y": 0.8233256340026855}], "text": "5\n"}
{"page": 18, "bbox": [{"x": 0.2689476013183594, "y": 0.8481523990631104}, {"x": 0.27371156215667725, "y": 0.8481523990631104}, {"x": 0.27371156215667725, "y": 0.8550808429718018}, {"x": 0.2689476013183594, "y": 0.8550808429718018}], "text": "6\n"}
{"page": 18, "bbox": [{"x": 0.2685145139694214, "y": 0.8793302774429321}, {"x": 0.27371156215667725, "y": 0.8793302774429321}, {"x": 0.27371156215667725, "y": 0.8862586617469788}, {"x": 0.2685145139694214, "y": 0.8862586617469788}], "text": "7\n"}
{"page": 18, "bbox": [{"x": 0.29926374554634094, "y": 0.8787528872489929}, {"x": 0.7475097179412842, "y": 0.8787528872489929}, {"x": 0.7475097179412842, "y": 0.888568103313446}, {"x": 0.29926374554634094, "y": 0.888568103313446}], "text": "-0.01076507568359375, 0.0123443603515625, -0.016693115234375, 0.0243377685546875, 0.0244903564453125]\n"}
{"page": 18, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 19, "bbox": [{"x": 0.9519272446632385, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9519272446632385, "y": 0.06293302774429321}], "text": "19\n"}
{"page": 19, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 19, "bbox": [{"x": 0.2745777368545532, "y": 0.15993072092533112}, {"x": 0.8427891135215759, "y": 0.15993072092533112}, {"x": 0.8427891135215759, "y": 0.2523094713687897}, {"x": 0.2745777368545532, "y": 0.2523094713687897}], "text": "3. Once both documents are split into chunks and embedded, we use Databricks Vector Search to store\nand index the embeddings. We also record some metadata the document title and the document\ndate - along with each text chunk and embedding. We'll come back to these vectors and show how\nthey are used to retrieve relevant information in the next section, which focuses on retrieval.\n"}
{"page": 19, "bbox": [{"x": 0.25249025225639343, "y": 0.27713626623153687}, {"x": 0.811173677444458, "y": 0.27713626623153687}, {"x": 0.811173677444458, "y": 0.3158198595046997}, {"x": 0.25249025225639343, "y": 0.3158198595046997}], "text": "At this phase, the data has been preprocessed and can now be queried. The next step is to retrieve the\nrelevant information from the vector database.\n"}
{"page": 19, "bbox": [{"x": 0.2529233396053314, "y": 0.3556582033634186}, {"x": 0.5465569496154785, "y": 0.3556582033634186}, {"x": 0.5465569496154785, "y": 0.37644341588020325}, {"x": 0.2529233396053314, "y": 0.37644341588020325}], "text": "Retrieval: Getting relevant context\n"}
{"page": 19, "bbox": [{"x": 0.592897355556488, "y": 0.45727482438087463}, {"x": 0.6669554114341736, "y": 0.4566974639892578}, {"x": 0.6669554114341736, "y": 0.46709007024765015}, {"x": 0.592897355556488, "y": 0.46766743063926697}], "text": "Vector Database\n"}
{"page": 19, "bbox": [{"x": 0.6167172193527222, "y": 0.4971131682395935}, {"x": 0.6435686349868774, "y": 0.4971131682395935}, {"x": 0.6435686349868774, "y": 0.5051963329315186}, {"x": 0.6167172193527222, "y": 0.5051963329315186}], "text": "Vector\n"}
{"page": 19, "bbox": [{"x": 0.5391944646835327, "y": 0.5028868317604065}, {"x": 0.5812039971351624, "y": 0.5023094415664673}, {"x": 0.5812039971351624, "y": 0.5271362662315369}, {"x": 0.5391944646835327, "y": 0.5277135968208313}], "text": "Similarity\nSearch\n"}
{"page": 19, "bbox": [{"x": 0.4772628843784332, "y": 0.5248267650604248}, {"x": 0.5253356695175171, "y": 0.5248267650604248}, {"x": 0.5253356695175171, "y": 0.550808310508728}, {"x": 0.4772628843784332, "y": 0.550808310508728}], "text": "Embedded\nQuery\n"}
{"page": 19, "bbox": [{"x": 0.3767865002155304, "y": 0.5259815454483032}, {"x": 0.4235599935054779, "y": 0.5265588760375977}, {"x": 0.4235599935054779, "y": 0.5496535897254944}, {"x": 0.3767865002155304, "y": 0.5490761995315552}], "text": "Embedding\nModel\n"}
{"page": 19, "bbox": [{"x": 0.28497186303138733, "y": 0.5323325395584106}, {"x": 0.31268948316574097, "y": 0.5329099297523499}, {"x": 0.31268948316574097, "y": 0.5438799262046814}, {"x": 0.28497186303138733, "y": 0.5433025360107422}], "text": "Query\n"}
{"page": 19, "bbox": [{"x": 0.7440450191497803, "y": 0.5248267650604248}, {"x": 0.8068428039550781, "y": 0.525404155254364}, {"x": 0.8068428039550781, "y": 0.5519630312919617}, {"x": 0.7440450191497803, "y": 0.5513857007026672}], "text": "Most Relevant\nVector(s)\n"}
{"page": 19, "bbox": [{"x": 0.6167172193527222, "y": 0.5346420407295227}, {"x": 0.6444348096847534, "y": 0.5346420407295227}, {"x": 0.6444348096847534, "y": 0.5421478152275085}, {"x": 0.6167172193527222, "y": 0.5421478152275085}], "text": "Vector\n"}
{"page": 19, "bbox": [{"x": 0.6167172193527222, "y": 0.5715935230255127}, {"x": 0.6444348096847534, "y": 0.5715935230255127}, {"x": 0.6444348096847534, "y": 0.5790992975234985}, {"x": 0.6167172193527222, "y": 0.5790992975234985}], "text": "Vector\n"}
{"page": 19, "bbox": [{"x": 0.25205716490745544, "y": 0.7147806286811829}, {"x": 0.8401905298233032, "y": 0.7147806286811829}, {"x": 0.8401905298233032, "y": 0.8337182402610779}, {"x": 0.25205716490745544, "y": 0.8337182402610779}], "text": "After preprocessing our original documents, we have a vector database storing the text chunks, embeddings\nand metadata. With this in place, we can get to the first step in RAG: retrieval. In the retrieval step, the user\nprovides a prompt, often a question, to the RAG application. The RAG application uses the prompt to query\nthe database and identify the most relevant results, which can be used to augment the original prompt (the\nnext step).\n"}
{"page": 19, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 20, "bbox": [{"x": 0.9497618079185486, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9497618079185486, "y": 0.06293302774429321}], "text": "20\n"}
{"page": 20, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 20, "bbox": [{"x": 0.25205716490745544, "y": 0.16108545660972595}, {"x": 0.5162407755851746, "y": 0.15993072092533112}, {"x": 0.5162407755851746, "y": 0.17551963031291962}, {"x": 0.25205716490745544, "y": 0.17667436599731445}], "text": "QUERYING THE VECTOR DATABASE\n"}
{"page": 20, "bbox": [{"x": 0.25162407755851746, "y": 0.19919168949127197}, {"x": 0.8388912677764893, "y": 0.19861431419849396}, {"x": 0.8388912677764893, "y": 0.28868359327316284}, {"x": 0.25162407755851746, "y": 0.28926098346710205}], "text": "We can't directly match a user's input, which is usually plain text, with the records in our vector database.\nSo, first we need to use the same embedding model that was used to embed the original text chunks to also\nembed the user's query. Once we have the embedded query, we can search the vector database to find the\nmost similar records.\n"}
{"page": 20, "bbox": [{"x": 0.7674317955970764, "y": 0.3475750684738159}, {"x": 0.7739281058311462, "y": 0.3475750684738159}, {"x": 0.7739281058311462, "y": 0.3504619002342224}, {"x": 0.7674317955970764, "y": 0.3504619002342224}], "text": "-\n"}
{"page": 20, "bbox": [{"x": 0.25205716490745544, "y": 0.31639721989631653}, {"x": 0.8462538123130798, "y": 0.31639721989631653}, {"x": 0.8462538123130798, "y": 0.38337182998657227}, {"x": 0.25205716490745544, "y": 0.38337182998657227}], "text": "If the database contains only a small number of records, searching might involve calculating a similarity score\nfor each record. For larger databases, we use vector indexes and specialized search algorithms — many of\nwhich use approximations to improve efficiency — to speed up the process.\n"}
{"page": 20, "bbox": [{"x": 0.25205716490745544, "y": 0.40762123465538025}, {"x": 0.8475530743598938, "y": 0.40762123465538025}, {"x": 0.8475530743598938, "y": 0.5}, {"x": 0.25205716490745544, "y": 0.5}], "text": "Once the vector database has identified the most relevant results, the texts of those results can be combined\nwith the user's prompt and sent to the LLM to generate the final response. Note that the embeddings are\nnot \"translated back\" to text. Instead, the text chunks are stored with the embeddings or linked to them via\ndatabase keys, so the chunks can simply be retrieved and sent to the next step in the RAG process.\n"}
{"page": 20, "bbox": [{"x": 0.25205716490745544, "y": 0.5236720442771912}, {"x": 0.8475530743598938, "y": 0.5248267650604248}, {"x": 0.8475530743598938, "y": 0.6183602809906006}, {"x": 0.25205716490745544, "y": 0.6172055602073669}], "text": "We should decide how many results our RAG system should retrieve. This, like chunk size, is worth testing and\ncan have a significant impact on the quality of the results. Retrieving too few records may mean missing some\nrelevant information, while too many results may dilute the relevant information and make it more likely for the\nLLM to give irrelevant answers.\n"}
{"page": 20, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 21, "bbox": [{"x": 0.9523603320121765, "y": 0.051385682076215744}, {"x": 0.9597228169441223, "y": 0.051385682076215744}, {"x": 0.9597228169441223, "y": 0.06697459518909454}, {"x": 0.9523603320121765, "y": 0.06697459518909454}], "text": "21\n"}
{"page": 21, "bbox": [{"x": 0.9527934193611145, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.06293302774429321}, {"x": 0.9527934193611145, "y": 0.06293302774429321}], "text": "21\n"}
{"page": 21, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 21, "bbox": [{"x": 0.25249025225639343, "y": 0.16166281700134277}, {"x": 0.4278908669948578, "y": 0.16166281700134277}, {"x": 0.4278908669948578, "y": 0.1749422699213028}, {"x": 0.25249025225639343, "y": 0.1749422699213028}], "text": "IMPROVING RETRIEVAL\n"}
{"page": 21, "bbox": [{"x": 0.25249025225639343, "y": 0.1968822181224823}, {"x": 0.8497185111045837, "y": 0.19919168949127197}, {"x": 0.8488523364067078, "y": 0.6154734492301941}, {"x": 0.25162407755851746, "y": 0.613163948059082}], "text": "The approach described above is often quite effective, but there are many more advanced techniques for\nimproving retrieval, including:\n■ Hybrid search: This method blends traditional keyword search with Vector Search, which can improve\nretrieval accuracy\n■ Reranking: An additional model can be used to reorder the records initially returned by the similarity\nsearch, ensuring the most relevant results are prioritized\n▪ Summarized text comparison: Some RAG applications don't compare the user's prompt directly\nto raw text embeddings. Instead, they use embeddings of summarized texts for a more efficient\nmatching process.\n■ Contextual chunk retrieval: It's often beneficial to include chunks adjacent to the most relevant ones\n(e.g., the paragraphs preceding and following a retrieved chunk). This approach provides more complete\ncontext, which might aid the LLM in generating a useful response.\n■ Prompt refinement: Some RAG applications employ a language model to refine the user's original\nprompt, crafting a new query that better captures the user's intent for more effective searching in the\nvector database\n"}
{"page": 21, "bbox": [{"x": 0.25205716490745544, "y": 0.6322171092033386}, {"x": 0.8484192490577698, "y": 0.6322171092033386}, {"x": 0.8484192490577698, "y": 0.7297921180725098}, {"x": 0.25205716490745544, "y": 0.7297921180725098}], "text": "■ Domain-specific tuning: Utilizing embedding models that are fine-tuned for specific tasks or domains\ncan enhance the accuracy and relevance of the retrieved information\nThese approaches are worth trying if testing reveals that the retrieval component of a RAG application is often\nfailing to return the most relevant records from the vector database.\n"}
{"page": 21, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 22, "bbox": [{"x": 0.9592897295951843, "y": 0.04849884659051895}, {"x": 0.9592897295951843, "y": 0.06697459518909454}, {"x": 0.9506279826164246, "y": 0.06697459518909454}, {"x": 0.9506279826164246, "y": 0.04849884659051895}], "text": "22\n"}
{"page": 22, "bbox": [{"x": 0.9506279826164246, "y": 0.054849885404109955}, {"x": 0.9610220789909363, "y": 0.054849885404109955}, {"x": 0.9610220789909363, "y": 0.06293302774429321}, {"x": 0.9506279826164246, "y": 0.06293302774429321}], "text": "22\n"}
{"page": 22, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 22, "bbox": [{"x": 0.25249025225639343, "y": 0.16108545660972595}, {"x": 0.5721091628074646, "y": 0.16108545660972595}, {"x": 0.5721091628074646, "y": 0.1749422699213028}, {"x": 0.25249025225639343, "y": 0.1749422699213028}], "text": "EXAMPLE: RETRIEVING THE INFORMATION\n"}
{"page": 22, "bbox": [{"x": 0.25205716490745544, "y": 0.19919168949127197}, {"x": 0.8258986473083496, "y": 0.19919168949127197}, {"x": 0.8258986473083496, "y": 0.23960739374160767}, {"x": 0.25205716490745544, "y": 0.23960739374160767}], "text": "All the information we need about Databricks Asset Bundles is now available in the vector database. In the\nretrieval phase, we have to get that information out. To do so, we:\n"}
{"page": 22, "bbox": [{"x": 0.27414464950561523, "y": 0.26443418860435486}, {"x": 0.8401905298233032, "y": 0.26443418860435486}, {"x": 0.8401905298233032, "y": 0.3308314085006714}, {"x": 0.27414464950561523, "y": 0.3308314085006714}], "text": "1. Embed the prompt. We use the same embedding model we used to embed the original document\nchunks to embed the query, and once again the result is a 1024-dimensional vector. So if we start with\nthe prompt \"What are Databricks Asset Bundles?,\" we end up with the embedding:\n"}
{"page": 22, "bbox": [{"x": 0.2689476013183594, "y": 0.3874133825302124}, {"x": 0.27327847480773926, "y": 0.3874133825302124}, {"x": 0.27327847480773926, "y": 0.3937644362449646}, {"x": 0.2689476013183594, "y": 0.3937644362449646}], "text": "1\n"}
{"page": 22, "bbox": [{"x": 0.2685145139694214, "y": 0.4024249315261841}, {"x": 0.27371156215667725, "y": 0.4024249315261841}, {"x": 0.27371156215667725, "y": 0.4099307060241699}, {"x": 0.2685145139694214, "y": 0.4099307060241699}], "text": "2\n"}
{"page": 22, "bbox": [{"x": 0.2689476013183594, "y": 0.41801387071609497}, {"x": 0.27327847480773926, "y": 0.41801387071609497}, {"x": 0.27327847480773926, "y": 0.4243648946285248}, {"x": 0.2689476013183594, "y": 0.4243648946285248}], "text": "3\n"}
{"page": 22, "bbox": [{"x": 0.29839757084846497, "y": 0.38625866174697876}, {"x": 0.8349934816360474, "y": 0.38625866174697876}, {"x": 0.8349934816360474, "y": 0.45785218477249146}, {"x": 0.29839757084846497, "y": 0.45785218477249146}], "text": "[0.006649017333984375, 0.029144287109375, 0.0001398324966430664, 0.00481414794921875, -0.006526947021484375,\n-0.00818634033203125, 0.029571533203125, -0.031982421875, 0.01082611083984375, 0.0025653839111328125, -0.013031005859375,\n0.01155853271484375, 0.054931640625, -0.049224853515625, -8.767843246459961e-05, 0.03131103515625, -0.022613525390625,\n0.0148162841796875, -0.052520751953125, 0.003780364990234375, 0.0279998779296875, 0.018585205078125, -0.081787109375,\n-0.030731201171875, -0.0236053466796875, 0.0357666015625, 0.03387451171875, 0.0335693359375,\n"}
{"page": 22, "bbox": [{"x": 0.2685145139694214, "y": 0.4341801404953003}, {"x": 0.27371156215667725, "y": 0.4341801404953003}, {"x": 0.27371156215667725, "y": 0.4405311644077301}, {"x": 0.2685145139694214, "y": 0.4405311644077301}], "text": "4\n"}
{"page": 22, "bbox": [{"x": 0.2685145139694214, "y": 0.449191689491272}, {"x": 0.27327847480773926, "y": 0.449191689491272}, {"x": 0.27327847480773926, "y": 0.4566974639892578}, {"x": 0.2685145139694214, "y": 0.4566974639892578}], "text": "5\n"}
{"page": 22, "bbox": [{"x": 0.2689476013183594, "y": 0.4809468686580658}, {"x": 0.27371156215667725, "y": 0.4809468686580658}, {"x": 0.27371156215667725, "y": 0.4878752827644348}, {"x": 0.2689476013183594, "y": 0.4878752827644348}], "text": "6\n"}
{"page": 22, "bbox": [{"x": 0.2685145139694214, "y": 0.5121247172355652}, {"x": 0.27371156215667725, "y": 0.5121247172355652}, {"x": 0.27371156215667725, "y": 0.5190531015396118}, {"x": 0.2685145139694214, "y": 0.5190531015396118}], "text": "7\n"}
{"page": 22, "bbox": [{"x": 0.29839757084846497, "y": 0.5109699964523315}, {"x": 0.8393243551254272, "y": 0.5109699964523315}, {"x": 0.8393243551254272, "y": 0.5207852125167847}, {"x": 0.29839757084846497, "y": 0.5207852125167847}], "text": "0.0260467529296875, -0.0013675689697265625, 0.032318115234375, -0.002666473388671875, 0.0269012451171875, 0.0616455078125]\n"}
{"page": 22, "bbox": [{"x": 0.27371156215667725, "y": 0.562355637550354}, {"x": 0.8471199870109558, "y": 0.562355637550354}, {"x": 0.8471199870109558, "y": 0.6530023217201233}, {"x": 0.27371156215667725, "y": 0.6530023217201233}], "text": "2. Use the embedding to search the vector database. We use the built-in similarity_search method\nof Databricks Vector Search to query the vector database with the embedded prompt. We specify that\nwe want it to return the stored text and that we want the two most relevant results. From this,\nthe database returns:\n"}
{"page": 22, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 23, "bbox": [{"x": 0.9506279826164246, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.06351039558649063}, {"x": 0.9506279826164246, "y": 0.06351039558649063}], "text": "23\n"}
{"page": 23, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 23, "bbox": [{"x": 0.2676483392715454, "y": 0.18879908323287964}, {"x": 0.2724123001098633, "y": 0.18879908323287964}, {"x": 0.27284538745880127, "y": 0.2280600517988205}, {"x": 0.2680814266204834, "y": 0.2280600517988205}], "text": "123\n"}
{"page": 23, "bbox": [{"x": 0.2685145139694214, "y": 0.23787528276443481}, {"x": 0.27371156215667725, "y": 0.23787528276443481}, {"x": 0.27371156215667725, "y": 0.24422632157802582}, {"x": 0.2685145139694214, "y": 0.24422632157802582}], "text": "4\n"}
{"page": 23, "bbox": [{"x": 0.29883065819740295, "y": 0.18937644362449646}, {"x": 0.8440883755683899, "y": 0.18937644362449646}, {"x": 0.8440883755683899, "y": 0.3094688355922699}, {"x": 0.29883065819740295, "y": 0.3094688355922699}], "text": "['What are Databricks Asset Bundles?\\nJanuary 08, 2024\\n\\nIn this article you will learn the basics of using\nDatabricks Asset Bundles, a new tool for streamlining the development of complex data, analytics, and ML projects for\nthe Databricks platform. Bundles make it easy to manage complex projects during active development by providing CI/CD\ncapabilities to your software development workflow in a single concise and declarative YAML syntax. By using bundles\nto automate your project's tests, deployments, and configuration management you can reduce errors while promoting\nsoftware best practices across your organization as templated projects.\\n\\nPreview\\n\\nThis feature is in Public\nPreview.\\n\\nBundles provide a way to include metadata alongside your project's source files to specify information\nincluding: \\n\\nRequired cloud infrastructure and workspace configurations.\\n\\nUnit and integration tests.',\n"}
{"page": 23, "bbox": [{"x": 0.2685145139694214, "y": 0.25173211097717285}, {"x": 0.2724123001098633, "y": 0.25173211097717285}, {"x": 0.2724123001098633, "y": 0.3060046136379242}, {"x": 0.2685145139694214, "y": 0.3060046136379242}], "text": "5698∞\n"}
{"page": 23, "bbox": [{"x": 0.2689476013183594, "y": 0.2840646505355835}, {"x": 0.27327847480773926, "y": 0.2840646505355835}, {"x": 0.27327847480773926, "y": 0.2904157042503357}, {"x": 0.2689476013183594, "y": 0.2904157042503357}], "text": "7\n"}
{"page": 23, "bbox": [{"x": 0.2685145139694214, "y": 0.3285219371318817}, {"x": 0.27371156215667725, "y": 0.3285219371318817}, {"x": 0.27371156215667725, "y": 0.33602771162986755}, {"x": 0.2685145139694214, "y": 0.33602771162986755}], "text": "9\n"}
{"page": 23, "bbox": [{"x": 0.26678216457366943, "y": 0.34468820691108704}, {"x": 0.2745777368545532, "y": 0.34468820691108704}, {"x": 0.2745777368545532, "y": 0.35103926062583923}, {"x": 0.26678216457366943, "y": 0.35103926062583923}], "text": "10\n"}
{"page": 23, "bbox": [{"x": 0.26678216457366943, "y": 0.3602771461009979}, {"x": 0.2750108242034912, "y": 0.3602771461009979}, {"x": 0.2750108242034912, "y": 0.36662817001342773}, {"x": 0.26678216457366943, "y": 0.36662817001342773}], "text": "11\n"}
{"page": 23, "bbox": [{"x": 0.26678216457366943, "y": 0.3758660554885864}, {"x": 0.2750108242034912, "y": 0.3758660554885864}, {"x": 0.2750108242034912, "y": 0.38221707940101624}, {"x": 0.26678216457366943, "y": 0.38221707940101624}], "text": "12\n"}
{"page": 23, "bbox": [{"x": 0.29839757084846497, "y": 0.32736721634864807}, {"x": 0.8432222008705139, "y": 0.32736721634864807}, {"x": 0.8432222008705139, "y": 0.4474595785140991}, {"x": 0.29839757084846497, "y": 0.4474595785140991}], "text": "'Databricks Assets Bundles are an infrastructure-as-code (IaC) approach to managing your Databricks projects.\nUse them when you want to manage complex projects where multiple contributors and automation are essential, and\ncontinuous integration and deployment (CI/CD) are a requirement. Since bundles are defined and managed through\nYAML templates and files you create and maintain alongside source code, they map well to scenarios where Iac is an\nappropriate approach.\\n\\nSome ideal scenarios for bundles include:\\n\\nDevelop data, analytics, and ML projects in\na team-based environment. Bundles can help you organize and manage various source files efficiently. This ensures\nsmooth collaboration and streamlined processes.\\n\\nIterate on ML problems faster. Manage ML pipeline resources (such\nas training and batch inference jobs) by using ML projects that follow production best practices from the beginning.']\n"}
{"page": 23, "bbox": [{"x": 0.2672152519226074, "y": 0.3914549648761749}, {"x": 0.2745777368545532, "y": 0.3914549648761749}, {"x": 0.2745777368545532, "y": 0.3978060185909271}, {"x": 0.2672152519226074, "y": 0.3978060185909271}], "text": "13\n"}
{"page": 23, "bbox": [{"x": 0.26678216457366943, "y": 0.4070438742637634}, {"x": 0.2750108242034912, "y": 0.4070438742637634}, {"x": 0.2750108242034912, "y": 0.4128175377845764}, {"x": 0.26678216457366943, "y": 0.4128175377845764}], "text": "14\n"}
{"page": 23, "bbox": [{"x": 0.2672152519226074, "y": 0.42263278365135193}, {"x": 0.2745777368545532, "y": 0.42263278365135193}, {"x": 0.2745777368545532, "y": 0.4289838373661041}, {"x": 0.2672152519226074, "y": 0.4289838373661041}], "text": "15\n"}
{"page": 23, "bbox": [{"x": 0.2672152519226074, "y": 0.4376443326473236}, {"x": 0.2750108242034912, "y": 0.4376443326473236}, {"x": 0.2750108242034912, "y": 0.4445727467536926}, {"x": 0.2672152519226074, "y": 0.4445727467536926}], "text": "16\n"}
{"page": 23, "bbox": [{"x": 0.25249025225639343, "y": 0.49249422550201416}, {"x": 0.7726288437843323, "y": 0.49249422550201416}, {"x": 0.7726288437843323, "y": 0.5075057744979858}, {"x": 0.25249025225639343, "y": 0.5075057744979858}], "text": "Now what do we do with this information? This is where the augmentation part of RAG comes in.\n"}
{"page": 23, "bbox": [{"x": 0.25249025225639343, "y": 0.5444572567939758}, {"x": 0.6925075650215149, "y": 0.5444572567939758}, {"x": 0.6925075650215149, "y": 0.5652424693107605}, {"x": 0.25249025225639343, "y": 0.5652424693107605}], "text": "Augmentation: Adding context to the user's prompt\n"}
{"page": 23, "bbox": [{"x": 0.5084452033042908, "y": 0.6414549946784973}, {"x": 0.593330442905426, "y": 0.6431870460510254}, {"x": 0.593330442905426, "y": 0.6570438742637634}, {"x": 0.5084452033042908, "y": 0.6553117632865906}], "text": "RAG Application\n"}
{"page": 23, "bbox": [{"x": 0.7466435432434082, "y": 0.6766743659973145}, {"x": 0.7895192503929138, "y": 0.6772517561912537}, {"x": 0.7895192503929138, "y": 0.687644362449646}, {"x": 0.7466435432434082, "y": 0.6870669722557068}], "text": "Response\n"}
{"page": 23, "bbox": [{"x": 0.7496751546859741, "y": 0.6922633051872253}, {"x": 0.7869207262992859, "y": 0.6928406357765198}, {"x": 0.7869207262992859, "y": 0.6997690796852112}, {"x": 0.7496751546859741, "y": 0.699191689491272}], "text": "to user\n"}
{"page": 23, "bbox": [{"x": 0.30316153168678284, "y": 0.6905311942100525}, {"x": 0.3577306270599365, "y": 0.693418025970459}, {"x": 0.35729753971099854, "y": 0.7032332420349121}, {"x": 0.30272844433784485, "y": 0.7003464102745056}], "text": "User query\n"}
{"page": 23, "bbox": [{"x": 0.6370723247528076, "y": 0.693418025970459}, {"x": 0.7046340703964233, "y": 0.6928406357765198}, {"x": 0.7046340703964233, "y": 0.7176674604415894}, {"x": 0.6370723247528076, "y": 0.7182447910308838}], "text": "Generate\nanswer w/ LLM\n"}
{"page": 23, "bbox": [{"x": 0.39627543091773987, "y": 0.693418025970459}, {"x": 0.4564746618270874, "y": 0.693418025970459}, {"x": 0.4564746618270874, "y": 0.7176674604415894}, {"x": 0.39627543091773987, "y": 0.7176674604415894}], "text": "Retrieve\nrelevant data\n"}
{"page": 23, "bbox": [{"x": 0.5136422514915466, "y": 0.693418025970459}, {"x": 0.5868341326713562, "y": 0.6916859149932861}, {"x": 0.5872672200202942, "y": 0.7182447910308838}, {"x": 0.5140753388404846, "y": 0.7199769020080566}], "text": "Augment\nprompt w/data\n"}
{"page": 23, "bbox": [{"x": 0.27067995071411133, "y": 0.6986142992973328}, {"x": 0.3001299202442169, "y": 0.6980369687080383}, {"x": 0.3001299202442169, "y": 0.7182447910308838}, {"x": 0.27067995071411133, "y": 0.718822181224823}], "text": "283\n"}
{"page": 23, "bbox": [{"x": 0.4634040594100952, "y": 0.7817552089691162}, {"x": 0.4963187575340271, "y": 0.7817552089691162}, {"x": 0.4963187575340271, "y": 0.7904157042503357}, {"x": 0.4634040594100952, "y": 0.7904157042503357}], "text": "relevant\n"}
{"page": 23, "bbox": [{"x": 0.4629709720611572, "y": 0.7961893677711487}, {"x": 0.4824599325656891, "y": 0.7961893677711487}, {"x": 0.4824599325656891, "y": 0.8042725324630737}, {"x": 0.4629709720611572, "y": 0.8042725324630737}], "text": "data\n"}
{"page": 23, "bbox": [{"x": 0.3001299202442169, "y": 0.8088914752006531}, {"x": 0.3958423435688019, "y": 0.8100461959838867}, {"x": 0.3954092562198639, "y": 0.857390284538269}, {"x": 0.29969683289527893, "y": 0.8562355637550354}], "text": "Voyager: An Open-Ended\nEmbodied Agent\nwith Large\nLanguage Models\n"}
{"page": 23, "bbox": [{"x": 0.5881333947181702, "y": 0.8342956304550171}, {"x": 0.6184495687484741, "y": 0.8342956304550171}, {"x": 0.6184495687484741, "y": 0.8446882367134094}, {"x": 0.5881333947181702, "y": 0.8446882367134094}], "text": "loading\n"}
{"page": 23, "bbox": [{"x": 0.40320485830307007, "y": 0.8487297892570496}, {"x": 0.4460805654525757, "y": 0.8487297892570496}, {"x": 0.4460805654525757, "y": 0.857390284538269}, {"x": 0.40320485830307007, "y": 0.857390284538269}], "text": "Relevance\n"}
{"page": 23, "bbox": [{"x": 0.5400606393814087, "y": 0.8585450053215027}, {"x": 0.5625811815261841, "y": 0.8591223955154419}, {"x": 0.5625811815261841, "y": 0.8677828907966614}, {"x": 0.5400606393814087, "y": 0.8672055602073669}], "text": "Index\n"}
{"page": 23, "bbox": [{"x": 0.3941099941730499, "y": 0.8625866174697876}, {"x": 0.45517539978027344, "y": 0.8625866174697876}, {"x": 0.45517539978027344, "y": 0.8718245029449463}, {"x": 0.3941099941730499, "y": 0.8718245029449463}], "text": "Determination\n"}
{"page": 23, "bbox": [{"x": 0.6184495687484741, "y": 0.8273671865463257}, {"x": 0.7540060877799988, "y": 0.8267898559570312}, {"x": 0.7544391751289368, "y": 0.9099307060241699}, {"x": 0.6188826560974121, "y": 0.9105080962181091}], "text": "000\ndocuments, APIs, Wikipedia, etc.\n"}
{"page": 23, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 24, "bbox": [{"x": 0.9506279826164246, "y": 0.04849884659051895}, {"x": 0.9592897295951843, "y": 0.04849884659051895}, {"x": 0.9592897295951843, "y": 0.0687066987156868}, {"x": 0.9506279826164246, "y": 0.0687066987156868}], "text": "24\n"}
{"page": 24, "bbox": [{"x": 0.9506279826164246, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9506279826164246, "y": 0.06293302774429321}], "text": "24\n"}
{"page": 24, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 24, "bbox": [{"x": 0.25249025225639343, "y": 0.16050808131694794}, {"x": 0.8362927436828613, "y": 0.16050808131694794}, {"x": 0.8362927436828613, "y": 0.20150116086006165}, {"x": 0.25249025225639343, "y": 0.20150116086006165}], "text": "The retrieval component of a RAG system queries the vector database with the user's prompt. It returns a\nselection of relevant texts and, in some cases, metadata. The texts are used to augment the original prompt.\n"}
{"page": 24, "bbox": [{"x": 0.25205716490745544, "y": 0.230946883559227}, {"x": 0.7072325944900513, "y": 0.230946883559227}, {"x": 0.7072325944900513, "y": 0.24422632157802582}, {"x": 0.25205716490745544, "y": 0.24422632157802582}], "text": "AUGMENTING THE PROMPT WITH THE RETRIEVED CONTEXT\n"}
{"page": 24, "bbox": [{"x": 0.25205716490745544, "y": 0.268475741147995}, {"x": 0.8544824719429016, "y": 0.268475741147995}, {"x": 0.8544824719429016, "y": 0.35796767473220825}, {"x": 0.25205716490745544, "y": 0.35796767473220825}], "text": "In its simplest form, \"augmentation\" means combining the user's original prompt with the retrieved texts.\nThis equips the model with both the prompt and the context needed to address the prompt. In practice, the\nstructure of the new prompt that combines the retrieved texts and the user's prompt can impact the quality of\nthe final result.\n"}
{"page": 24, "bbox": [{"x": 0.25249025225639343, "y": 0.38568130135536194}, {"x": 0.7258553504943848, "y": 0.38568130135536194}, {"x": 0.7258553504943848, "y": 0.4006928503513336}, {"x": 0.25249025225639343, "y": 0.4006928503513336}], "text": "For example, the final prompt usually includes an instruction on how to use the context:\n"}
{"page": 24, "bbox": [{"x": 0.2689476013183594, "y": 0.4693995416164398}, {"x": 0.27327847480773926, "y": 0.4693995416164398}, {"x": 0.27327847480773926, "y": 0.47575056552886963}, {"x": 0.2689476013183594, "y": 0.47575056552886963}], "text": "1\n"}
{"page": 24, "bbox": [{"x": 0.2689476013183594, "y": 0.4976905286312103}, {"x": 0.27371156215667725, "y": 0.4976905286312103}, {"x": 0.27371156215667725, "y": 0.5046189427375793}, {"x": 0.2689476013183594, "y": 0.5046189427375793}], "text": "2\n"}
{"page": 24, "bbox": [{"x": 0.29883065819740295, "y": 0.4971131682395935}, {"x": 0.6106539368629456, "y": 0.4971131682395935}, {"x": 0.6106539368629456, "y": 0.5075057744979858}, {"x": 0.29883065819740295, "y": 0.5075057744979858}], "text": "Based on the following context, answer the user's question. Context:\n"}
{"page": 24, "bbox": [{"x": 0.2689476013183594, "y": 0.5259815454483032}, {"x": 0.27327847480773926, "y": 0.5259815454483032}, {"x": 0.27327847480773926, "y": 0.5323325395584106}, {"x": 0.2689476013183594, "y": 0.5323325395584106}], "text": "3\n"}
{"page": 24, "bbox": [{"x": 0.29969683289527893, "y": 0.5248267650604248}, {"x": 0.33910784125328064, "y": 0.5242494344711304}, {"x": 0.33910784125328064, "y": 0.5346420407295227}, {"x": 0.29969683289527893, "y": 0.5352193713188171}], "text": "{context}\n"}
{"page": 24, "bbox": [{"x": 0.2689476013183594, "y": 0.5837182402610779}, {"x": 0.27371156215667725, "y": 0.5837182402610779}, {"x": 0.27371156215667725, "y": 0.5894919037818909}, {"x": 0.2689476013183594, "y": 0.5894919037818909}], "text": "4\n"}
{"page": 24, "bbox": [{"x": 0.29883065819740295, "y": 0.581986129283905}, {"x": 0.3624945878982544, "y": 0.5831408500671387}, {"x": 0.3624945878982544, "y": 0.5929561257362366}, {"x": 0.29883065819740295, "y": 0.5918014049530029}], "text": "User question:\n"}
{"page": 24, "bbox": [{"x": 0.2689476013183594, "y": 0.6120092272758484}, {"x": 0.27371156215667725, "y": 0.6120092272758484}, {"x": 0.27371156215667725, "y": 0.6183602809906006}, {"x": 0.2689476013183594, "y": 0.6183602809906006}], "text": "5\n"}
{"page": 24, "bbox": [{"x": 0.29926374554634094, "y": 0.6091223955154419}, {"x": 0.36725854873657227, "y": 0.6096997857093811}, {"x": 0.36725854873657227, "y": 0.6212471127510071}, {"x": 0.29926374554634094, "y": 0.6206697225570679}], "text": "{user question}\n"}
{"page": 24, "bbox": [{"x": 0.2689476013183594, "y": 0.6691685914993286}, {"x": 0.27371156215667725, "y": 0.6691685914993286}, {"x": 0.27371156215667725, "y": 0.6760969758033752}, {"x": 0.2689476013183594, "y": 0.6760969758033752}], "text": "6\n"}
{"page": 24, "bbox": [{"x": 0.25249025225639343, "y": 0.7321016192436218}, {"x": 0.8518839478492737, "y": 0.7321016192436218}, {"x": 0.8518839478492737, "y": 0.7984988689422607}, {"x": 0.25249025225639343, "y": 0.7984988689422607}], "text": "Depending on the model, putting the context first might be more or less effective than putting the user's\nquestion first. The phrasing might also be consequential. For example, you might want to phrase the instruction\nto emphasize that the model should generate its answer using only the retrieved context.\n"}
{"page": 24, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 25, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 25, "bbox": [{"x": 0.9506279826164246, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.06351039558649063}, {"x": 0.9506279826164246, "y": 0.06351039558649063}], "text": "25\n"}
{"page": 25, "bbox": [{"x": 0.25205716490745544, "y": 0.16108545660972595}, {"x": 0.7540060877799988, "y": 0.16108545660972595}, {"x": 0.7540060877799988, "y": 0.1749422699213028}, {"x": 0.25205716490745544, "y": 0.1749422699213028}], "text": "CONTEXT WINDOW: HOW MUCH CONTEXT SHOULD WE PROVIDE?\n"}
{"page": 25, "bbox": [{"x": 0.25205716490745544, "y": 0.19745957851409912}, {"x": 0.8544824719429016, "y": 0.1968822181224823}, {"x": 0.8544824719429016, "y": 0.43591225147247314}, {"x": 0.25205716490745544, "y": 0.43648961186408997}], "text": "LLMs are limited by a \"context window,\" or the amount of text they can process to generate a response.\nDesigning a RAG system involves ensuring that all retrieved texts and the user's prompt fit within this window.\nOverloading the system with too many texts might lead to errors or lost context.\nSome LLMs boast longer context windows, capable of handling texts as lengthy as short books. But this doesn't\nmean adding more texts to the user's prompt will always be beneficial. LLMs sometimes struggle to pay equal\nattention to all parts of a lengthy context. They typically focus more effectively on the beginning and end,\npotentially overlooking the middle content. This is known as the \"lost in the middle\" phenomenon. Hence,\neven with longer context windows, careful selection and arrangement of texts are crucial for augmenting\nprompts effectively.\n"}
{"page": 25, "bbox": [{"x": 0.25249025225639343, "y": 0.46420323848724365}, {"x": 0.5244694948196411, "y": 0.46420323848724365}, {"x": 0.5244694948196411, "y": 0.4780600368976593}, {"x": 0.25249025225639343, "y": 0.4780600368976593}], "text": "EXAMPLE: PROMPT AUGMENTATION\n"}
{"page": 25, "bbox": [{"x": 0.25205716490745544, "y": 0.5028868317604065}, {"x": 0.8501515984535217, "y": 0.5028868317604065}, {"x": 0.8501515984535217, "y": 0.5404157042503357}, {"x": 0.25205716490745544, "y": 0.5404157042503357}], "text": "Our question for the RAG application was \"What are Databricks Asset Bundles?\" and we retrieved the following\ntwo chunks for context:\n"}
{"page": 25, "bbox": [{"x": 0.2689476013183594, "y": 0.6062355637550354}, {"x": 0.27327847480773926, "y": 0.6062355637550354}, {"x": 0.27327847480773926, "y": 0.6125866174697876}, {"x": 0.2689476013183594, "y": 0.6125866174697876}], "text": "1\n"}
{"page": 25, "bbox": [{"x": 0.2689476013183594, "y": 0.6212471127510071}, {"x": 0.27327847480773926, "y": 0.6212471127510071}, {"x": 0.27327847480773926, "y": 0.6281754970550537}, {"x": 0.2689476013183594, "y": 0.6281754970550537}], "text": "2\n"}
{"page": 25, "bbox": [{"x": 0.2689476013183594, "y": 0.636836051940918}, {"x": 0.27284538745880127, "y": 0.636836051940918}, {"x": 0.27284538745880127, "y": 0.6431870460510254}, {"x": 0.2689476013183594, "y": 0.6431870460510254}], "text": "3\n"}
{"page": 25, "bbox": [{"x": 0.2685145139694214, "y": 0.6530023217201233}, {"x": 0.27371156215667725, "y": 0.6530023217201233}, {"x": 0.27371156215667725, "y": 0.6593533754348755}, {"x": 0.2685145139694214, "y": 0.6593533754348755}], "text": "4\n"}
{"page": 25, "bbox": [{"x": 0.29839757084846497, "y": 0.6050808429718018}, {"x": 0.8436552882194519, "y": 0.6050808429718018}, {"x": 0.8436552882194519, "y": 0.724595844745636}, {"x": 0.29839757084846497, "y": 0.724595844745636}], "text": "['What are Databricks Asset Bundles?\\nJanuary 08, 2024\\n\\nIn this article you will learn the basics of using\nDatabricks Asset Bundles, a new tool for streamlining the development of complex data, analytics, and ML projects for\nthe Databricks platform. Bundles make it easy to manage complex projects during active development by providing CI/CD\ncapabilities to your software development workflow in a single concise and declarative YAML syntax. By using bundles\nto automate your project's tests, deployments, and configuration management you can reduce errors while promoting\nsoftware best practices across your organization as templated projects.\\n\\nPreview\\n\\nThis feature is in Public\nPreview.\\n\\nBundles provide a way to include metadata alongside your project's source files to specify information\nincluding:\\n\\nRequired cloud infrastructure and workspace configurations.\\n\\nUnit and integration tests.',\n"}
{"page": 25, "bbox": [{"x": 0.2689476013183594, "y": 0.6685912013053894}, {"x": 0.27371156215667725, "y": 0.6685912013053894}, {"x": 0.27371156215667725, "y": 0.6755196452140808}, {"x": 0.2689476013183594, "y": 0.6755196452140808}], "text": "5\n"}
{"page": 25, "bbox": [{"x": 0.2689476013183594, "y": 0.6836027503013611}, {"x": 0.27327847480773926, "y": 0.6836027503013611}, {"x": 0.27327847480773926, "y": 0.6905311942100525}, {"x": 0.2689476013183594, "y": 0.6905311942100525}], "text": "6\n"}
{"page": 25, "bbox": [{"x": 0.2689476013183594, "y": 0.6986142992973328}, {"x": 0.27371156215667725, "y": 0.6986142992973328}, {"x": 0.27371156215667725, "y": 0.7055427432060242}, {"x": 0.2689476013183594, "y": 0.7055427432060242}], "text": "7\n"}
{"page": 25, "bbox": [{"x": 0.2689476013183594, "y": 0.7153579592704773}, {"x": 0.27371156215667725, "y": 0.7153579592704773}, {"x": 0.27371156215667725, "y": 0.7217090129852295}, {"x": 0.2689476013183594, "y": 0.7217090129852295}], "text": "8\n"}
{"page": 25, "bbox": [{"x": 0.2685145139694214, "y": 0.744226336479187}, {"x": 0.27327847480773926, "y": 0.744226336479187}, {"x": 0.27327847480773926, "y": 0.7511547207832336}, {"x": 0.2685145139694214, "y": 0.7511547207832336}], "text": "9\n"}
{"page": 25, "bbox": [{"x": 0.26678216457366943, "y": 0.7598152160644531}, {"x": 0.2758769989013672, "y": 0.7598152160644531}, {"x": 0.2758769989013672, "y": 0.7661662697792053}, {"x": 0.26678216457366943, "y": 0.7661662697792053}], "text": "10\n"}
{"page": 25, "bbox": [{"x": 0.26678216457366943, "y": 0.775404155254364}, {"x": 0.2750108242034912, "y": 0.775404155254364}, {"x": 0.2750108242034912, "y": 0.7817552089691162}, {"x": 0.26678216457366943, "y": 0.7817552089691162}], "text": "11\n"}
{"page": 25, "bbox": [{"x": 0.26678216457366943, "y": 0.7909930944442749}, {"x": 0.2750108242034912, "y": 0.7909930944442749}, {"x": 0.2750108242034912, "y": 0.7973440885543823}, {"x": 0.26678216457366943, "y": 0.7973440885543823}], "text": "12\n"}
{"page": 25, "bbox": [{"x": 0.29839757084846497, "y": 0.7424942255020142}, {"x": 0.8436552882194519, "y": 0.7424942255020142}, {"x": 0.8436552882194519, "y": 0.8625866174697876}, {"x": 0.29839757084846497, "y": 0.8625866174697876}], "text": "'Databricks Assets Bundles are an infrastructure-as-code (IaC) approach to managing your Databricks projects.\nUse them when you want to manage complex projects where multiple contributors and automation are essential, and\ncontinuous integration and deployment (CI/CD) are a requirement. Since bundles are defined and managed through\nYAML templates and files you create and maintain alongside source code, they map well to scenarios where Iac is an\nappropriate approach.\\n\\nSome ideal scenarios for bundles include:\\n\\nDevelop data, analytics, and ML projects in\na team-based environment. Bundles can help you organize and manage various source files efficiently. This ensures\nsmooth collaboration and streamlined processes.\\n\\nIterate on ML problems faster. Manage ML pipeline resources (such\nas training and batch inference jobs) by using ML projects that follow production best practices from the beginning.']\n"}
{"page": 25, "bbox": [{"x": 0.2672152519226074, "y": 0.806581974029541}, {"x": 0.2754439115524292, "y": 0.806581974029541}, {"x": 0.2754439115524292, "y": 0.8129330277442932}, {"x": 0.2672152519226074, "y": 0.8129330277442932}], "text": "13\n"}
{"page": 25, "bbox": [{"x": 0.26678216457366943, "y": 0.8221709132194519}, {"x": 0.2750108242034912, "y": 0.8221709132194519}, {"x": 0.2750108242034912, "y": 0.8285219669342041}, {"x": 0.26678216457366943, "y": 0.8285219669342041}], "text": "14\n"}
{"page": 25, "bbox": [{"x": 0.26678216457366943, "y": 0.837759792804718}, {"x": 0.2745777368545532, "y": 0.837759792804718}, {"x": 0.2745777368545532, "y": 0.8441108465194702}, {"x": 0.26678216457366943, "y": 0.8441108465194702}], "text": "15\n"}
{"page": 25, "bbox": [{"x": 0.2672152519226074, "y": 0.8533487319946289}, {"x": 0.2745777368545532, "y": 0.8533487319946289}, {"x": 0.2745777368545532, "y": 0.8596997857093811}, {"x": 0.2672152519226074, "y": 0.8596997857093811}], "text": "16\n"}
{"page": 25, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 26, "bbox": [{"x": 0.9501948952674866, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.054849885404109955}, {"x": 0.9601559042930603, "y": 0.06351039558649063}, {"x": 0.9501948952674866, "y": 0.06351039558649063}], "text": "26\n"}
{"page": 26, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 26, "bbox": [{"x": 0.25162407755851746, "y": 0.15993072092533112}, {"x": 0.8077089786529541, "y": 0.15993072092533112}, {"x": 0.8077089786529541, "y": 0.1749422699213028}, {"x": 0.25162407755851746, "y": 0.1749422699213028}], "text": "We use these and some more general instructions to construct the final prompt we send to the model:\n"}
{"page": 26, "bbox": [{"x": 0.2689476013183594, "y": 0.2315242439508438}, {"x": 0.27327847480773926, "y": 0.2315242439508438}, {"x": 0.27327847480773926, "y": 0.23787528276443481}, {"x": 0.2689476013183594, "y": 0.23787528276443481}], "text": "1\n"}
{"page": 26, "bbox": [{"x": 0.2689476013183594, "y": 0.2465357929468155}, {"x": 0.27371156215667725, "y": 0.2465357929468155}, {"x": 0.27371156215667725, "y": 0.2534641921520233}, {"x": 0.2689476013183594, "y": 0.2534641921520233}], "text": "2\n"}
{"page": 26, "bbox": [{"x": 0.297964483499527, "y": 0.2280600517988205}, {"x": 0.841489851474762, "y": 0.230946883559227}, {"x": 0.841489851474762, "y": 0.27309468388557434}, {"x": 0.297964483499527, "y": 0.27020785212516785}], "text": "You are a helpful assistant. Answer the user's question. If context is provided, you must answer based only on the\ncontext. If no context is provided, answer based on your knowledge. If you don't know the answer, say you don't know.\nBe concise.\n"}
{"page": 26, "bbox": [{"x": 0.2689476013183594, "y": 0.2621247172355652}, {"x": 0.27327847480773926, "y": 0.2621247172355652}, {"x": 0.27327847480773926, "y": 0.2690531313419342}, {"x": 0.2689476013183594, "y": 0.2690531313419342}], "text": "3\n"}
{"page": 26, "bbox": [{"x": 0.2689476013183594, "y": 0.31986144185066223}, {"x": 0.27371156215667725, "y": 0.31986144185066223}, {"x": 0.27371156215667725, "y": 0.3256351053714752}, {"x": 0.2689476013183594, "y": 0.3256351053714752}], "text": "4\n"}
{"page": 26, "bbox": [{"x": 0.297964483499527, "y": 0.3187066912651062}, {"x": 0.5734084248542786, "y": 0.3187066912651062}, {"x": 0.5734084248542786, "y": 0.32909929752349854}, {"x": 0.297964483499527, "y": 0.32909929752349854}], "text": "Answer the question based on the provided context. Context:\n"}
{"page": 26, "bbox": [{"x": 0.26634907722473145, "y": 0.3752886950969696}, {"x": 0.2745777368545532, "y": 0.3752886950969696}, {"x": 0.2745777368545532, "y": 0.49249422550201416}, {"x": 0.26634907722473145, "y": 0.49249422550201416}], "text": "5698 9212\n"}
{"page": 26, "bbox": [{"x": 0.297531396150589, "y": 0.3758660554885864}, {"x": 0.8393243551254272, "y": 0.3758660554885864}, {"x": 0.8393243551254272, "y": 0.4959584176540375}, {"x": 0.297531396150589, "y": 0.4959584176540375}], "text": "What are Databricks Asset Bundles?\\nJanuary 08, 2024\\n\\nIn this article you will learn the basics of using Databricks\nAsset Bundles, a new tool for streamlining the development of complex data, analytics, and ML projects for the\nDatabricks platform. Bundles make it easy to manage complex projects during active development by providing CI/CD\ncapabilities to your software development workflow in a single concise and declarative YAML syntax. By using bundles\nto automate your project's tests, deployments, and configuration management you can reduce errors while promoting\nsoftware best practices across your organization as templated projects.\\n\\nPreview\\n\\nThis feature is in Public\nPreview.\\n\\nBundles provide a way to include metadata alongside your project's source files to specify information\nincluding: \\n\\nRequired cloud infrastructure and workspace configurations.\\n\\nUnit and integration tests.\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.4555427134037018}, {"x": 0.2745777368545532, "y": 0.4555427134037018}, {"x": 0.2745777368545532, "y": 0.461893767118454}, {"x": 0.26678216457366943, "y": 0.461893767118454}], "text": "10\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.47113165259361267}, {"x": 0.2750108242034912, "y": 0.47113165259361267}, {"x": 0.2750108242034912, "y": 0.4774826765060425}, {"x": 0.26678216457366943, "y": 0.4774826765060425}], "text": "11\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.5433025360107422}, {"x": 0.2750108242034912, "y": 0.5433025360107422}, {"x": 0.2750108242034912, "y": 0.5502309203147888}, {"x": 0.26678216457366943, "y": 0.5502309203147888}], "text": "13\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.5594688057899475}, {"x": 0.2750108242034912, "y": 0.5594688057899475}, {"x": 0.2750108242034912, "y": 0.5658198595046997}, {"x": 0.26678216457366943, "y": 0.5658198595046997}], "text": "14\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.5750577449798584}, {"x": 0.2754439115524292, "y": 0.5750577449798584}, {"x": 0.2754439115524292, "y": 0.5814087986946106}, {"x": 0.26678216457366943, "y": 0.5814087986946106}], "text": "15\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.5900692939758301}, {"x": 0.2750108242034912, "y": 0.5900692939758301}, {"x": 0.2750108242034912, "y": 0.5969976782798767}, {"x": 0.26678216457366943, "y": 0.5969976782798767}], "text": "16\n"}
{"page": 26, "bbox": [{"x": 0.297964483499527, "y": 0.542725145816803}, {"x": 0.8406236171722412, "y": 0.542725145816803}, {"x": 0.8406236171722412, "y": 0.662240207195282}, {"x": 0.297964483499527, "y": 0.662240207195282}], "text": "Databricks Assets Bundles are an infrastructure-as-code (IaC) approach to managing your Databricks projects. Use them\nwhen you want to manage complex projects where multiple contributors and automation are essential, and continuous\nintegration and deployment (CI/CD) are a requirement. Since bundles are defined and managed through YAML templates\nand files you create and maintain alongside source code, they map well to scenarios where IaC is an appropriate\napproach.\\n\\nSome ideal scenarios for bundles include:\\n\\nDevelop data, analytics, and ML projects in a team-\nbased environment. Bundles can help you organize and manage various source files efficiently. This ensures smooth\ncollaboration and streamlined processes.\\n\\nIterate on ML problems faster. Manage ML pipeline resources (such as\ntraining and batch inference jobs) by using ML projects that follow production best practices from the beginning.\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.6062355637550354}, {"x": 0.2745777368545532, "y": 0.6062355637550354}, {"x": 0.2745777368545532, "y": 0.6125866174697876}, {"x": 0.26678216457366943, "y": 0.6125866174697876}], "text": "17\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.6218245029449463}, {"x": 0.2750108242034912, "y": 0.6218245029449463}, {"x": 0.2750108242034912, "y": 0.6281754970550537}, {"x": 0.26678216457366943, "y": 0.6281754970550537}], "text": "18\n"}
{"page": 26, "bbox": [{"x": 0.26678216457366943, "y": 0.636836051940918}, {"x": 0.2745777368545532, "y": 0.636836051940918}, {"x": 0.2745777368545532, "y": 0.6437644362449646}, {"x": 0.26678216457366943, "y": 0.6437644362449646}], "text": "19\n"}
{"page": 26, "bbox": [{"x": 0.26634907722473145, "y": 0.6530023217201233}, {"x": 0.2750108242034912, "y": 0.6530023217201233}, {"x": 0.2750108242034912, "y": 0.6599307060241699}, {"x": 0.26634907722473145, "y": 0.6599307060241699}], "text": "20\n"}
{"page": 26, "bbox": [{"x": 0.25205716490745544, "y": 0.7084295749664307}, {"x": 0.8497185111045837, "y": 0.7084295749664307}, {"x": 0.8497185111045837, "y": 0.7494226098060608}, {"x": 0.25205716490745544, "y": 0.7494226098060608}], "text": "We've now augmented the original prompt with the context needed to address it and with instructions on how\nto use the context. The last step is to pass this along to the LLM.\n"}
{"page": 26, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 27, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 27, "bbox": [{"x": 0.9510610699653625, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05600461736321449}, {"x": 0.9605889916419983, "y": 0.06408776342868805}, {"x": 0.9510610699653625, "y": 0.06351039558649063}], "text": "27\n"}
{"page": 27, "bbox": [{"x": 0.25249025225639343, "y": 0.1628175526857376}, {"x": 0.6656561493873596, "y": 0.1628175526857376}, {"x": 0.6656561493873596, "y": 0.18360276520252228}, {"x": 0.25249025225639343, "y": 0.18360276520252228}], "text": "Generation: Producing useful output with an LLM\n"}
{"page": 27, "bbox": [{"x": 0.5084452033042908, "y": 0.26443418860435486}, {"x": 0.593763530254364, "y": 0.2661662697792053}, {"x": 0.593763530254364, "y": 0.27944573760032654}, {"x": 0.5084452033042908, "y": 0.2777136266231537}], "text": "RAG Application\n"}
{"page": 27, "bbox": [{"x": 0.7462104558944702, "y": 0.2990761995315552}, {"x": 0.7895192503929138, "y": 0.2996535897254944}, {"x": 0.7895192503929138, "y": 0.3100461959838867}, {"x": 0.7462104558944702, "y": 0.3094688355922699}], "text": "Response\n"}
{"page": 27, "bbox": [{"x": 0.7496751546859741, "y": 0.31351038813591003}, {"x": 0.7864876389503479, "y": 0.3152424991130829}, {"x": 0.7864876389503479, "y": 0.32332563400268555}, {"x": 0.7496751546859741, "y": 0.3215935230255127}], "text": "to user\n"}
{"page": 27, "bbox": [{"x": 0.30316153168678284, "y": 0.31120091676712036}, {"x": 0.3581637144088745, "y": 0.3152424991130829}, {"x": 0.3577306270599365, "y": 0.32621246576309204}, {"x": 0.30272844433784485, "y": 0.3221709132194519}], "text": "User query\n"}
{"page": 27, "bbox": [{"x": 0.39627543091773987, "y": 0.3152424991130829}, {"x": 0.4573408365249634, "y": 0.31466513872146606}, {"x": 0.4573408365249634, "y": 0.33949190378189087}, {"x": 0.39627543091773987, "y": 0.3400692939758301}], "text": "Retrieve\nrelevant data\n"}
{"page": 27, "bbox": [{"x": 0.6318752765655518, "y": 0.3158198595046997}, {"x": 0.7093980312347412, "y": 0.3152424991130829}, {"x": 0.7093980312347412, "y": 0.3406466543674469}, {"x": 0.6318752765655518, "y": 0.3412240147590637}], "text": "Generate\nanswer with LLM\n"}
{"page": 27, "bbox": [{"x": 0.5084452033042908, "y": 0.31639721989631653}, {"x": 0.5907319188117981, "y": 0.31408774852752686}, {"x": 0.5911650061607361, "y": 0.3412240147590637}, {"x": 0.5088782906532288, "y": 0.3435334861278534}], "text": "Augment\nprompt with data\n"}
{"page": 27, "bbox": [{"x": 0.43655261397361755, "y": 0.430715948343277}, {"x": 0.6630576252937317, "y": 0.42956119775772095}, {"x": 0.6630576252937317, "y": 0.4445727467536926}, {"x": 0.43655261397361755, "y": 0.44572749733924866}], "text": "Augmentation (Context Integration) Process\n"}
{"page": 27, "bbox": [{"x": 0.4010394215583801, "y": 0.4780600368976593}, {"x": 0.46167171001434326, "y": 0.48267897963523865}, {"x": 0.4612386226654053, "y": 0.494226336479187}, {"x": 0.40060633420944214, "y": 0.48960739374160767}], "text": "User query\n"}
{"page": 27, "bbox": [{"x": 0.4776959717273712, "y": 0.4809468686580658}, {"x": 0.4898224472999573, "y": 0.4809468686580658}, {"x": 0.4898224472999573, "y": 0.4936489462852478}, {"x": 0.4776959717273712, "y": 0.4936489462852478}], "text": "+\n"}
{"page": 27, "bbox": [{"x": 0.5378952026367188, "y": 0.47575056552886963}, {"x": 0.5859679579734802, "y": 0.47575056552886963}, {"x": 0.5859679579734802, "y": 0.5005773901939392}, {"x": 0.5378952026367188, "y": 0.5005773901939392}], "text": "relevant\ndocuments\n"}
{"page": 27, "bbox": [{"x": 0.6773495078086853, "y": 0.4821016192436218}, {"x": 0.7029016613960266, "y": 0.48267897963523865}, {"x": 0.7029016613960266, "y": 0.49538105726242065}, {"x": 0.6773495078086853, "y": 0.49480369687080383}], "text": "LLM\n"}
{"page": 27, "bbox": [{"x": 0.5075790286064148, "y": 0.5011547207832336}, {"x": 0.5075790286064148, "y": 0.4786374270915985}, {"x": 0.5240364074707031, "y": 0.4786374270915985}, {"x": 0.5240364074707031, "y": 0.5011547207832336}], "text": "וויוי\n"}
{"page": 27, "bbox": [{"x": 0.25205716490745544, "y": 0.5975750684738159}, {"x": 0.8207015991210938, "y": 0.5975750684738159}, {"x": 0.8207015991210938, "y": 0.6639722585678101}, {"x": 0.25205716490745544, "y": 0.6639722585678101}], "text": "After the retrieval and augmentation steps, we have a prompt and a set of retrieved texts, formatted with\ninstructions on how to use the texts to answer the prompt. In the generation step of RAG, we send the\naugmented prompt to an LLM, and the LLM responds with an answer.\n"}
{"page": 27, "bbox": [{"x": 0.25205716490745544, "y": 0.6887990832328796}, {"x": 0.7916846871376038, "y": 0.6887990832328796}, {"x": 0.7916846871376038, "y": 0.7292147874832153}, {"x": 0.25205716490745544, "y": 0.7292147874832153}], "text": "At this stage, many approaches can be used to customize the final output and adjust the end user's\nexperience with the RAG system.\n"}
{"page": 27, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 28, "bbox": [{"x": 0.9506279826164246, "y": 0.05542725324630737}, {"x": 0.9597228169441223, "y": 0.05542725324630737}, {"x": 0.9597228169441223, "y": 0.06293302774429321}, {"x": 0.9506279826164246, "y": 0.06293302774429321}], "text": "28\n"}
{"page": 28, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 28, "bbox": [{"x": 0.25249025225639343, "y": 0.16166281700134277}, {"x": 0.42529234290122986, "y": 0.16166281700134277}, {"x": 0.42529234290122986, "y": 0.17436489462852478}, {"x": 0.25249025225639343, "y": 0.17436489462852478}], "text": "PROMPT ENGINEERING\n"}
{"page": 28, "bbox": [{"x": 0.25162407755851746, "y": 0.19919168949127197}, {"x": 0.8401905298233032, "y": 0.19919168949127197}, {"x": 0.8401905298233032, "y": 0.3187066912651062}, {"x": 0.25162407755851746, "y": 0.3187066912651062}], "text": "As noted in the previous section, we can include instructions on how the model should use the retrieved\ncontext (e.g., \"answer only based on the provided context\"). We can provide similar instructions to the LLM\nto guide its output tone and structure. We might, for example, specify that the answers should be very polite\nor should be phrased in a short and direct manner. We could also instruct the model to refuse to answer\nquestions about a competitor's products.\n"}
{"page": 28, "bbox": [{"x": 0.25249025225639343, "y": 0.3469976782798767}, {"x": 0.5617150068283081, "y": 0.3469976782798767}, {"x": 0.5617150068283081, "y": 0.36085450649261475}, {"x": 0.25249025225639343, "y": 0.36085450649261475}], "text": "PREPROCESSING AND POSTPROCESSING\n"}
{"page": 28, "bbox": [{"x": 0.25162407755851746, "y": 0.38568130135536194}, {"x": 0.8436552882194519, "y": 0.38568130135536194}, {"x": 0.8436552882194519, "y": 0.5046189427375793}, {"x": 0.25162407755851746, "y": 0.5046189427375793}], "text": "Some LLMs are quite good at following instructions, but none are completely reliable. Some programmatic\npreprocessing or postprocessing can be used to structure the output in a certain way or to guarantee that\ncertain types of responses are or are not generated. For example, we could append a list of links to the most\nrelevant context sources at the end of each response, or we could automatically respond with a polite refusal\nto answer if specific words or phrases are included in the user's prompt.\n"}
{"page": 28, "bbox": [{"x": 0.25249025225639343, "y": 0.5334873199462891}, {"x": 0.4447813034057617, "y": 0.5334873199462891}, {"x": 0.4447813034057617, "y": 0.5473440885543823}, {"x": 0.25249025225639343, "y": 0.5473440885543823}], "text": "CONVERSATION HISTORY\n"}
{"page": 28, "bbox": [{"x": 0.25205716490745544, "y": 0.5715935230255127}, {"x": 0.8401905298233032, "y": 0.5715935230255127}, {"x": 0.8401905298233032, "y": 0.6905311942100525}, {"x": 0.25205716490745544, "y": 0.6905311942100525}], "text": "Another important consideration is whether the RAG system should have a memory. In other words, can it\nengage in multi-turn conversations in which it remembers earlier questions and responses? A conversational\nAl system can give users the opportunity to refine their questions and ask follow-ups, but it adds some\ncomplexity. Different models and frameworks use different approaches to storing conversation histories, and\nincluding conversation histories may require more careful management of context length.\n"}
{"page": 28, "bbox": [{"x": 0.4088349938392639, "y": 0.7569283843040466}, {"x": 0.9809441566467285, "y": 0.763856828212738}, {"x": 0.9800779819488525, "y": 0.9133949279785156}, {"x": 0.40796881914138794, "y": 0.9064664840698242}], "text": "====སྐྱུ=\n"}
{"page": 28, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 29, "bbox": [{"x": 0.9588566422462463, "y": 0.04849884659051895}, {"x": 0.9588566422462463, "y": 0.0687066987156868}, {"x": 0.9514941573143005, "y": 0.0687066987156868}, {"x": 0.9514941573143005, "y": 0.04849884659051895}], "text": "29\n"}
{"page": 29, "bbox": [{"x": 0.9497618079185486, "y": 0.054849885404109955}, {"x": 0.9597228169441223, "y": 0.054849885404109955}, {"x": 0.9597228169441223, "y": 0.06293302774429321}, {"x": 0.9497618079185486, "y": 0.06293302774429321}], "text": "29\n"}
{"page": 29, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 29, "bbox": [{"x": 0.25249025225639343, "y": 0.16166281700134277}, {"x": 0.4313555657863617, "y": 0.16166281700134277}, {"x": 0.4313555657863617, "y": 0.1749422699213028}, {"x": 0.25249025225639343, "y": 0.1749422699213028}], "text": "EXAMPLE: RAG OUTPUT\n"}
{"page": 29, "bbox": [{"x": 0.25205716490745544, "y": 0.1997690498828888}, {"x": 0.7063664197921753, "y": 0.1997690498828888}, {"x": 0.7063664197921753, "y": 0.21420323848724365}, {"x": 0.25205716490745544, "y": 0.21420323848724365}], "text": "After we generate our augmented prompt, it is sent to the LLM. This is the response:\n"}
{"page": 29, "bbox": [{"x": 0.26938068866729736, "y": 0.27020785212516785}, {"x": 0.27371156215667725, "y": 0.27020785212516785}, {"x": 0.27371156215667725, "y": 0.27655890583992004}, {"x": 0.26938068866729736, "y": 0.27655890583992004}], "text": "1\n"}
{"page": 29, "bbox": [{"x": 0.29839757084846497, "y": 0.2690531313419342}, {"x": 0.48332610726356506, "y": 0.2690531313419342}, {"x": 0.48332610726356506, "y": 0.2777136266231537}, {"x": 0.29839757084846497, "y": 0.2777136266231537}], "text": "User: What are Databricks Asset Bundles?\n"}
{"page": 29, "bbox": [{"x": 0.2685145139694214, "y": 0.32621246576309204}, {"x": 0.27284538745880127, "y": 0.32621246576309204}, {"x": 0.27327847480773926, "y": 0.3966512680053711}, {"x": 0.2689476013183594, "y": 0.3966512680053711}], "text": "23456\n"}
{"page": 29, "bbox": [{"x": 0.29839757084846497, "y": 0.32621246576309204}, {"x": 0.8427891135215759, "y": 0.32621246576309204}, {"x": 0.8427891135215759, "y": 0.430715948343277}, {"x": 0.29839757084846497, "y": 0.430715948343277}], "text": "Bot: Databricks Asset Bundles are a way to manage complex data, analytics, and machine learning projects on the\nDatabricks platform using an infrastructure-as-code (IaC) approach. They provide a way to include metadata alongside\nproject source files to specify information such as required cloud infrastructure and workspace configurations, unit\nand integration tests, and CI/CD capabilities. Bundles are defined and managed through YAML templates and files,\nmaking it easier to manage complex projects with multiple contributors and automation, and promote software best\npractices across an organization. They are particularly useful in scenarios such as team-based development, iterating\non machine learning problems, and managing ML pipeline resources.\n"}
{"page": 29, "bbox": [{"x": 0.2689476013183594, "y": 0.40531179308891296}, {"x": 0.27327847480773926, "y": 0.40531179308891296}, {"x": 0.27327847480773926, "y": 0.4122401773929596}, {"x": 0.2689476013183594, "y": 0.4122401773929596}], "text": "7\n"}
{"page": 29, "bbox": [{"x": 0.2689476013183594, "y": 0.4214780628681183}, {"x": 0.27284538745880127, "y": 0.4214780628681183}, {"x": 0.27284538745880127, "y": 0.4278290867805481}, {"x": 0.2689476013183594, "y": 0.4278290867805481}], "text": "8\n"}
{"page": 29, "bbox": [{"x": 0.25205716490745544, "y": 0.47690531611442566}, {"x": 0.8458207249641418, "y": 0.47690531611442566}, {"x": 0.8458207249641418, "y": 0.5178983807563782}, {"x": 0.25205716490745544, "y": 0.5178983807563782}], "text": "One way to enhance the generation step of RAG is to add multi-turn conversation ability. This allows us to ask\na follow-up question that refers back to the original question and answer:\n"}
{"page": 29, "bbox": [{"x": 0.2689476013183594, "y": 0.5790992975234985}, {"x": 0.27327847480773926, "y": 0.5790992975234985}, {"x": 0.27327847480773926, "y": 0.5854503512382507}, {"x": 0.2689476013183594, "y": 0.5854503512382507}], "text": "1\n"}
{"page": 29, "bbox": [{"x": 0.29839757084846497, "y": 0.5779445767402649}, {"x": 0.5833694338798523, "y": 0.5779445767402649}, {"x": 0.5833694338798523, "y": 0.5871824622154236}, {"x": 0.29839757084846497, "y": 0.5871824622154236}], "text": "User: What version of the CLI do I need in order to use them?\n"}
{"page": 29, "bbox": [{"x": 0.2680814266204834, "y": 0.6062355637550354}, {"x": 0.27284538745880127, "y": 0.6062355637550354}, {"x": 0.27327847480773926, "y": 0.6605080962181091}, {"x": 0.2685145139694214, "y": 0.6605080962181091}], "text": "23450\n"}
{"page": 29, "bbox": [{"x": 0.29839757084846497, "y": 0.6062355637550354}, {"x": 0.8358596563339233, "y": 0.6056581735610962}, {"x": 0.8358596563339233, "y": 0.6633949279785156}, {"x": 0.29839757084846497, "y": 0.6639722585678101}], "text": "Bot: According to the provided context, you need Databricks CLI version 0.205.2 or higher to use Databricks Asset\nBundles. You can check the version of the Databricks CLI you are currently using by running the command `databricks\n--version in your terminal. If you don't have the required version, you can install or update the Databricks CLI\nfollowing the instructions provided in the article.\n"}
{"page": 29, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 30, "bbox": [{"x": 0.9497618079185486, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9497618079185486, "y": 0.06293302774429321}], "text": "30\n"}
{"page": 30, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 30, "bbox": [{"x": 0.25249025225639343, "y": 0.16108545660972595}, {"x": 0.8536162972450256, "y": 0.16339491307735443}, {"x": 0.8536162972450256, "y": 0.243648961186409}, {"x": 0.25249025225639343, "y": 0.24133948981761932}], "text": "Evaluation: Measuring RAG performance\nBecause a RAG application has many adjustable variables that may affect the retrieval or generation quality, it's\nimportant to have ways to measure its performance.\n"}
{"page": 30, "bbox": [{"x": 0.25249025225639343, "y": 0.2655889093875885}, {"x": 0.8453876376152039, "y": 0.2655889093875885}, {"x": 0.8453876376152039, "y": 0.3585450351238251}, {"x": 0.25249025225639343, "y": 0.3585450351238251}], "text": "RAG evaluation is an area of active research and experimentation. It's one of the most challenging parts of\nsetting up a RAG application, and there's no universal solution. As we've discussed, RAG comprises multiple\nsteps, and it's often helpful to evaluate these steps separately. A great LLM won't be able to compensate for a\npoor retrieval pipeline, and the best retrieval system can't overcome the limitations of a weak model.\n"}
{"page": 30, "bbox": [{"x": 0.25205716490745544, "y": 0.38279446959495544}, {"x": 0.8362927436828613, "y": 0.38279446959495544}, {"x": 0.8362927436828613, "y": 0.4751732051372528}, {"x": 0.25205716490745544, "y": 0.4751732051372528}], "text": "At its core, RAG evaluations involve generating prompts, identifying the relevant records that should be\nretrieved to address each prompt and generating good answers to those questions. Running the evaluations\nmeans passing each evaluation prompt to the RAG application and comparing the desired retrievals and\nresponses to the actual retrievals and responses.\n"}
{"page": 30, "bbox": [{"x": 0.25249025225639343, "y": 0.5}, {"x": 0.8254655599594116, "y": 0.5}, {"x": 0.8254655599594116, "y": 0.5669745802879333}, {"x": 0.25249025225639343, "y": 0.5669745802879333}], "text": "RAG evaluations often rely on other LLMs to judge response quality. For example, RAG responses are often\nevaluated on their “faithfulness\" to the provided context. A judge LLM examines the context and the end\nresponse from a RAG application and provides a rating on how true the response is to the context.\n"}
{"page": 30, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 31, "bbox": [{"x": 0.9532265067100525, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9532265067100525, "y": 0.06293302774429321}], "text": "31\n"}
{"page": 31, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 31, "bbox": [{"x": 0.03811173513531685, "y": 0.1628175526857376}, {"x": 0.16803811490535736, "y": 0.16166281700134277}, {"x": 0.16847120225429535, "y": 0.2188221663236618}, {"x": 0.038544826209545135, "y": 0.21997690200805664}], "text": "Utilizing RAG\nWith Other\n"}
{"page": 31, "bbox": [{"x": 0.25205716490745544, "y": 0.15993072092533112}, {"x": 0.8505846858024597, "y": 0.15993072092533112}, {"x": 0.8505846858024597, "y": 0.22690531611442566}, {"x": 0.25205716490745544, "y": 0.22690531611442566}], "text": "At this point, you should have a good idea of what RAG is and how it compares to using an LLM alone. However,\nRAG isn't the only approach to customizing LLMs or equipping them with new information. In this section, we\nexplain how RAG fits into the broader context of LLM customization approaches.\n"}
{"page": 31, "bbox": [{"x": 0.038977913558483124, "y": 0.23498845100402832}, {"x": 0.18882633745670319, "y": 0.23498845100402832}, {"x": 0.18882633745670319, "y": 0.3591223955154419}, {"x": 0.038977913558483124, "y": 0.3591223955154419}], "text": "Modeling\nand Model\nCustomization\nMethods\n"}
{"page": 31, "bbox": [{"x": 0.25162407755851746, "y": 0.25115472078323364}, {"x": 0.8505846858024597, "y": 0.24769052863121033}, {"x": 0.8510177731513977, "y": 0.36662817001342773}, {"x": 0.25205716490745544, "y": 0.37009239196777344}], "text": "All of the following approaches involve tradeoffs between cost, complexity and expressive power. \"Cost\" refers\nsimply to the financial cost of setting up and using a given model or system. \"Complexity\" means the intricacy\nor technical difficulty, which may be reflected in the time, effort and expertise required. And \"expressiveness\"\nrefers to the model's or system's ability to generate diverse, meaningful and useful responses tailored to your\nspecific needs.\n"}
{"page": 31, "bbox": [{"x": 0.25249025225639343, "y": 0.3943417966365814}, {"x": 0.8375920057296753, "y": 0.3943417966365814}, {"x": 0.8375920057296753, "y": 0.4347575008869171}, {"x": 0.25249025225639343, "y": 0.4347575008869171}], "text": "These methods are not mutually exclusive and should be used in combination to maximize task- or domain-\nspecific performance.\n"}
{"page": 31, "bbox": [{"x": 0.2529233396053314, "y": 0.47286373376846313}, {"x": 0.42139455676078796, "y": 0.47286373376846313}, {"x": 0.42139455676078796, "y": 0.4936489462852478}, {"x": 0.2529233396053314, "y": 0.4936489462852478}], "text": "Prompt engineering\n"}
{"page": 31, "bbox": [{"x": 0.25205716490745544, "y": 0.5103926062583923}, {"x": 0.8310956954956055, "y": 0.5103926062583923}, {"x": 0.8310956954956055, "y": 0.5767898559570312}, {"x": 0.25205716490745544, "y": 0.5767898559570312}], "text": "Prompt engineering is the process of designing prompts or prompt templates that guide a model's outputs\ntoward a desired result. It's typically the least complex approach and entails the lowest up-front costs\nbecause it doesn't involve changing the model's weights or working with any external data systems.\n"}
{"page": 31, "bbox": [{"x": 0.25205716490745544, "y": 0.601616621017456}, {"x": 0.8475530743598938, "y": 0.601616621017456}, {"x": 0.8475530743598938, "y": 0.6939953565597534}, {"x": 0.25205716490745544, "y": 0.6939953565597534}], "text": "The cost associated with prompt engineering will vary. Large and highly capable models are often required in\norder to understand and follow complex prompts. These models often entail higher serving costs or per-token\ncosts than smaller, less-capable models. That said, prompt engineering doesn't come with the high up-front\ncosts of training a model or setting up the infrastructure for a production RAG system.\n"}
{"page": 31, "bbox": [{"x": 0.25249025225639343, "y": 0.718822181224823}, {"x": 0.8176699876785278, "y": 0.718822181224823}, {"x": 0.8176699876785278, "y": 0.7852193713188171}, {"x": 0.25249025225639343, "y": 0.7852193713188171}], "text": "The expressiveness obtainable via prompt engineering is fundamentally limited by the underlying model.\nPrompt engineering offers a good alternative to RAG in cases where there's no need for proprietary\nor recent knowledge.\n"}
{"page": 31, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 32, "bbox": [{"x": 0.9510610699653625, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9510610699653625, "y": 0.06293302774429321}], "text": "32\n"}
{"page": 32, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 32, "bbox": [{"x": 0.2533564269542694, "y": 0.1628175526857376}, {"x": 0.3512342870235443, "y": 0.16339491307735443}, {"x": 0.3512342870235443, "y": 0.18187066912651062}, {"x": 0.2533564269542694, "y": 0.1812933087348938}], "text": "Fine-tuning\n"}
{"page": 32, "bbox": [{"x": 0.25162407755851746, "y": 0.1997690498828888}, {"x": 0.8479861617088318, "y": 0.19861431419849396}, {"x": 0.8484192490577698, "y": 0.46073901653289795}, {"x": 0.25205716490745544, "y": 0.461893767118454}], "text": "Fine-tuning is the process of adapting a pretrained generative model to a new domain or task by training all or\nsome of the model's weights (or, in the case of adapter methods, new weights) on new data. The primary goal\nof fine-tuning is to enhance the model's expressiveness and accuracy in handling domain-specific queries\nor tasks. For example, a language model might be fine-tuned to follow instructions based on a large dataset\nof instruction and response data. Or a model could be fine-tuned on a collection of medical texts in order to\nbetter understand medical jargon.\nThe cost and complexity of this process can vary greatly, depending on factors such as the size of the model,\nthe quantity and specificity of the training data and the nature of the task. Fine-tuning can sometimes be\nused to reduce costs. A smaller model fine-tuned on a specific task can replace a larger and more expensive\ngeneralist model.\n"}
{"page": 32, "bbox": [{"x": 0.25249025225639343, "y": 0.49191686511039734}, {"x": 0.4270246922969818, "y": 0.49191686511039734}, {"x": 0.4270246922969818, "y": 0.505773663520813}, {"x": 0.25249025225639343, "y": 0.505773663520813}], "text": "FINE-TUNING AND RAG\n"}
{"page": 32, "bbox": [{"x": 0.25162407755851746, "y": 0.5300230979919434}, {"x": 0.841489851474762, "y": 0.5300230979919434}, {"x": 0.841489851474762, "y": 0.6224018335342407}, {"x": 0.25162407755851746, "y": 0.6224018335342407}], "text": "While RAG excels in enhancing a model's responses with additional, relevant information, it doesn't\nfundamentally change the model's behavior or linguistic style. Any limitations or quirks of the base model will\nstill be present in a RAG system, while fine-tuning can durably change the model's behavior in ways that are\nless constrained by the base model's behavior.\n"}
{"page": 32, "bbox": [{"x": 0.25205716490745544, "y": 0.6472286581993103}, {"x": 0.8492854237556458, "y": 0.6460738778114319}, {"x": 0.8492854237556458, "y": 0.710739016532898}, {"x": 0.25205716490745544, "y": 0.7118937373161316}], "text": "On the other hand, fine-tuning doesn't include a straightforward mechanism for rapidly updating the model\nwith new information, and it may not be as reliable as RAG for generating relevant responses, even if the model\nwas fine-tuned on relevant data.\n"}
{"page": 32, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 33, "bbox": [{"x": 0.9506279826164246, "y": 0.06697459518909454}, {"x": 0.9506279826164246, "y": 0.0467667430639267}, {"x": 0.9597228169441223, "y": 0.0467667430639267}, {"x": 0.9597228169441223, "y": 0.06697459518909454}], "text": "33\n"}
{"page": 33, "bbox": [{"x": 0.9506279826164246, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06293302774429321}, {"x": 0.9506279826164246, "y": 0.06293302774429321}], "text": "33\n"}
{"page": 33, "bbox": [{"x": 0.038544826209545135, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038544826209545135, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 33, "bbox": [{"x": 0.2529233396053314, "y": 0.1628175526857376}, {"x": 0.34603723883628845, "y": 0.16339491307735443}, {"x": 0.34603723883628845, "y": 0.18244802951812744}, {"x": 0.2529233396053314, "y": 0.18187066912651062}], "text": "Pretraining\n"}
{"page": 33, "bbox": [{"x": 0.25249025225639343, "y": 0.2003464251756668}, {"x": 0.8254655599594116, "y": 0.2003464251756668}, {"x": 0.8254655599594116, "y": 0.2407621294260025}, {"x": 0.25249025225639343, "y": 0.2407621294260025}], "text": "Pretraining is the process of training an LLM from scratch. This is the highest-cost and highest-complexity\napproach, but it offers the greatest potential control over the model's expressiveness.\n"}
{"page": 33, "bbox": [{"x": 0.25119099020957947, "y": 0.2655889093875885}, {"x": 0.8514508605003357, "y": 0.262702077627182}, {"x": 0.8518839478492737, "y": 0.4463048577308655}, {"x": 0.25162407755851746, "y": 0.449191689491272}], "text": "Pretraining a model gives you control over all the data that goes into it. This might mean including proprietary\ndata not available to off-the-shelf models or excluding data from sources not deemed trustworthy, reliable or\nlegally acceptable in a given business context. For example, you may decide not to include Reddit data when\npretraining a model to give legal or financial advice.\nConsider pretraining when it is essential to understand and control all the data a model is trained on or when\nyou need a domain-specific model that meets certain evaluation or performance requirements not available in\nexisting models.\n"}
{"page": 33, "bbox": [{"x": 0.25249025225639343, "y": 0.487297922372818}, {"x": 0.2867042124271393, "y": 0.487297922372818}, {"x": 0.2867042124271393, "y": 0.5028868317604065}, {"x": 0.25249025225639343, "y": 0.5028868317604065}], "text": "RAG\n"}
{"page": 33, "bbox": [{"x": 0.25249025225639343, "y": 0.5242494344711304}, {"x": 0.8258986473083496, "y": 0.5242494344711304}, {"x": 0.8258986473083496, "y": 0.5658198595046997}, {"x": 0.25249025225639343, "y": 0.5658198595046997}], "text": "RAG is more complex than prompt engineering alone. It requires setting up a retrieval system (i.e., a vector\ndatabase) and integrating the retrieved context with the prompt.\n"}
{"page": 33, "bbox": [{"x": 0.2529233396053314, "y": 0.5900692939758301}, {"x": 0.6310091018676758, "y": 0.5900692939758301}, {"x": 0.6310091018676758, "y": 0.6045034527778625}, {"x": 0.2529233396053314, "y": 0.6045034527778625}], "text": "If access to external information is the goal, RAG offers many benefits:\n"}
{"page": 33, "bbox": [{"x": 0.27414464950561523, "y": 0.6287528872489929}, {"x": 0.6561281681060791, "y": 0.6287528872489929}, {"x": 0.6561281681060791, "y": 0.6437644362449646}, {"x": 0.27414464950561523, "y": 0.6437644362449646}], "text": "■ Ability to add and remove data sources without changing the model\n"}
{"page": 33, "bbox": [{"x": 0.27371156215667725, "y": 0.6605080962181091}, {"x": 0.6349068880081177, "y": 0.6605080962181091}, {"x": 0.6349068880081177, "y": 0.6755196452140808}, {"x": 0.27371156215667725, "y": 0.6755196452140808}], "text": "■ Control over who has permission to access certain data sources\n"}
{"page": 33, "bbox": [{"x": 0.25249025225639343, "y": 0.6928406357765198}, {"x": 0.8510177731513977, "y": 0.6928406357765198}, {"x": 0.8510177731513977, "y": 0.8175519704818726}, {"x": 0.25249025225639343, "y": 0.8175519704818726}], "text": "Flexibility to compare different LLMs without needing to train them on new data\nThe cost and complexity of a RAG system depends on the choice of model and on the scale and structure of\nthe retrieval system. As with any database, a vector database that guarantees low-latency retrieval over a vast\nnumber of records will be more costly than a higher-latency, smaller-scale system. But a RAG system alone will\nnot entail the up-front cost and complexity of pretraining or fine-tuning a model.\n"}
{"page": 33, "bbox": [{"x": 0.25249025225639343, "y": 0.8418014049530029}, {"x": 0.8185361623764038, "y": 0.8418014049530029}, {"x": 0.8185361623764038, "y": 0.8822171092033386}, {"x": 0.25249025225639343, "y": 0.8822171092033386}], "text": "The expressiveness of a RAG system, though limited by the choice of LLM, can still be quite high given its\naccess to contextually relevant external data sources.\n"}
{"page": 33, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 34, "bbox": [{"x": 0.9501948952674866, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.05542725324630737}, {"x": 0.9605889916419983, "y": 0.062355659902095795}, {"x": 0.9501948952674866, "y": 0.062355659902095795}], "text": "34\n"}
{"page": 34, "bbox": [{"x": 0.03811173513531685, "y": 0.05369514971971512}, {"x": 0.4699003994464874, "y": 0.05369514971971512}, {"x": 0.4699003994464874, "y": 0.06524249166250229}, {"x": 0.03811173513531685, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 34, "bbox": [{"x": 0.25205716490745544, "y": 0.1622401773929596}, {"x": 0.4746643602848053, "y": 0.1622401773929596}, {"x": 0.4746643602848053, "y": 0.17956119775772095}, {"x": 0.25205716490745544, "y": 0.17956119775772095}], "text": "Combinations of methods\n"}
{"page": 34, "bbox": [{"x": 0.25249025225639343, "y": 0.2003464251756668}, {"x": 0.841489851474762, "y": 0.2003464251756668}, {"x": 0.841489851474762, "y": 0.24133948981761932}, {"x": 0.25249025225639343, "y": 0.24133948981761932}], "text": "These methods can and often should be used together. RAG and prompt engineering are already inseparable\n― merging the user's prompt with the external data sources is a form of prompt engineering.\n"}
{"page": 34, "bbox": [{"x": 0.25205716490745544, "y": 0.26327943801879883}, {"x": 0.8202685117721558, "y": 0.2655889093875885}, {"x": 0.8202685117721558, "y": 0.3342956006526947}, {"x": 0.25205716490745544, "y": 0.33198612928390503}], "text": "Using a custom pretrained or fine-tuned model in a RAG system can improve the RAG system by offering\nfine-grained control over the model's data, the response tone and structure and the aptitude with\ndomain-specific language.\n"}
{"page": 34, "bbox": [{"x": 0.25249025225639343, "y": 0.3568129241466522}, {"x": 0.8371589183807373, "y": 0.3539260923862457}, {"x": 0.8371589183807373, "y": 0.42090070247650146}, {"x": 0.25249025225639343, "y": 0.42378753423690796}], "text": "In general, it's good practice to start with less costly, less complex methods and evaluate their performance.\nMoving to more complex methods or combinations of methods is a good option when the simpler methods\nprove inadequate for their tasks.\n"}
{"page": 34, "bbox": [{"x": 0.058466870337724686, "y": 0.929561197757721}, {"x": 0.14378519356250763, "y": 0.9312933087348938}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.0580337792634964, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 35, "bbox": [{"x": 0.9497618079185486, "y": 0.06697459518909454}, {"x": 0.9497618079185486, "y": 0.04618937522172928}, {"x": 0.9597228169441223, "y": 0.04618937522172928}, {"x": 0.9597228169441223, "y": 0.06697459518909454}], "text": "55\n"}
{"page": 35, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 35, "bbox": [{"x": 0.9501948952674866, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.05542725324630737}, {"x": 0.9601559042930603, "y": 0.06351039558649063}, {"x": 0.9501948952674866, "y": 0.06351039558649063}], "text": "35\n"}
{"page": 35, "bbox": [{"x": 0.0398440882563591, "y": 0.16397228837013245}, {"x": 0.1524469405412674, "y": 0.1657043844461441}, {"x": 0.1520138531923294, "y": 0.21997690200805664}, {"x": 0.03941100090742111, "y": 0.21824480593204498}], "text": "RAG on\nDatabricks\n"}
{"page": 35, "bbox": [{"x": 0.25205716490745544, "y": 0.15993072092533112}, {"x": 0.8349934816360474, "y": 0.15993072092533112}, {"x": 0.8349934816360474, "y": 0.2540415823459625}, {"x": 0.25205716490745544, "y": 0.2540415823459625}], "text": "While it's possible to set up a quick RAG demo on your laptop in a few minutes, a production-ready RAG\nsystem requires careful orchestration of several different components in a reliable, scalable and secure\nmanner. Databricks offers an end-to-end RAG solution combining data management and governance with a\nvector database, model serving and other tools for managing and monitoring Al processes.\n"}
{"page": 35, "bbox": [{"x": 0.25249025225639343, "y": 0.2904157042503357}, {"x": 0.45474231243133545, "y": 0.2904157042503357}, {"x": 0.45474231243133545, "y": 0.3071593642234802}, {"x": 0.25249025225639343, "y": 0.3071593642234802}], "text": "Lakehouse architecture\n"}
{"page": 35, "bbox": [{"x": 0.25205716490745544, "y": 0.3279445767402649}, {"x": 0.8540493845939636, "y": 0.3279445767402649}, {"x": 0.8540493845939636, "y": 0.42090070247650146}, {"x": 0.25205716490745544, "y": 0.42090070247650146}], "text": "RAG applications in Databricks are built on lakehouse architecture. The lakehouse centralizes the management\nof structured data, unstructured data and Al assets under a common governance scheme, Unity Catalog.\nOrganizations can build cloud-agnostic RAG systems on proprietary data with sophisticated security, lineage\ntracking and monitoring.\n"}
{"page": 35, "bbox": [{"x": 0.25162407755851746, "y": 0.45842957496643066}, {"x": 0.3728886842727661, "y": 0.45842957496643066}, {"x": 0.3728886842727661, "y": 0.474595844745636}, {"x": 0.25162407755851746, "y": 0.474595844745636}], "text": "Vector Search\n"}
{"page": 35, "bbox": [{"x": 0.25249025225639343, "y": 0.4959584176540375}, {"x": 0.8254655599594116, "y": 0.4959584176540375}, {"x": 0.8254655599594116, "y": 0.562355637550354}, {"x": 0.25249025225639343, "y": 0.562355637550354}], "text": "Databricks Vector Search enables you to create an auto-updating vector database from Delta tables,\nmanaged via Databricks Unity Catalog and searchable using a simple API. Databricks Vector Search scales\nautomatically to handle different numbers of documents and queries.\n"}
{"page": 35, "bbox": [{"x": 0.2529233396053314, "y": 0.5981523990631104}, {"x": 0.37202250957489014, "y": 0.6021940112113953}, {"x": 0.37158942222595215, "y": 0.6218245029449463}, {"x": 0.25249025225639343, "y": 0.6177828907966614}], "text": "Model serving\n"}
{"page": 35, "bbox": [{"x": 0.25162407755851746, "y": 0.6379907727241516}, {"x": 0.8536162972450256, "y": 0.6379907727241516}, {"x": 0.8536162972450256, "y": 0.730369508266449}, {"x": 0.25162407755851746, "y": 0.730369508266449}], "text": "Databricks Model Serving provides a number of different ways to host and use LLMs and embedding models\nfor RAG. Databricks Model Serving supports custom models via MLflow; governance of external models such as\nSaaS models from OpenAI, Anthropic or Google; and state-of-the-art open source models from the Databricks\nFoundation Model APIs, which offer both pay-per-token and provisioned throughput offerings.\n"}
{"page": 35, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 36, "bbox": [{"x": 0.9501948952674866, "y": 0.054849885404109955}, {"x": 0.9605889916419983, "y": 0.054849885404109955}, {"x": 0.9605889916419983, "y": 0.06293302774429321}, {"x": 0.9501948952674866, "y": 0.06293302774429321}], "text": "36\n"}
{"page": 36, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 36, "bbox": [{"x": 0.25249025225639343, "y": 0.16339491307735443}, {"x": 0.310524046421051, "y": 0.16397228837013245}, {"x": 0.310524046421051, "y": 0.17956119775772095}, {"x": 0.25249025225639343, "y": 0.17898383736610413}], "text": "MLflow\n"}
{"page": 36, "bbox": [{"x": 0.25205716490745544, "y": 0.2003464251756668}, {"x": 0.8224339485168457, "y": 0.19745957851409912}, {"x": 0.8224339485168457, "y": 0.26385679841041565}, {"x": 0.25205716490745544, "y": 0.26674365997314453}], "text": "MLflow is an open source end-to-end model lifecycle management platform. It includes a variety of tools\nuseful for implementing and improving RAG systems, including an evaluation framework and a prompt\nengineering UI.\n"}
{"page": 36, "bbox": [{"x": 0.2533564269542694, "y": 0.30196306109428406}, {"x": 0.4413165748119354, "y": 0.3054272532463074}, {"x": 0.44088348746299744, "y": 0.32621246576309204}, {"x": 0.2529233396053314, "y": 0.3227482736110687}], "text": "Lakehouse Monitoring\n"}
{"page": 36, "bbox": [{"x": 0.25249025225639343, "y": 0.34237876534461975}, {"x": 0.8310956954956055, "y": 0.34237876534461975}, {"x": 0.8310956954956055, "y": 0.4093533456325531}, {"x": 0.25249025225639343, "y": 0.4093533456325531}], "text": "Databricks Lakehouse Monitoring provides a centralized monitoring solution for both model and data\nmonitoring. It allows you to keep track of various statistical properties for all your data sources as well as to\nmonitor the performance of your served models.\n"}
{"page": 36, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 37, "bbox": [{"x": 0.9506279826164246, "y": 0.06466512382030487}, {"x": 0.9506279826164246, "y": 0.0467667430639267}, {"x": 0.9592897295951843, "y": 0.0467667430639267}, {"x": 0.9592897295951843, "y": 0.06466512382030487}], "text": "31\n"}
{"page": 37, "bbox": [{"x": 0.9510610699653625, "y": 0.054849885404109955}, {"x": 0.9597228169441223, "y": 0.054849885404109955}, {"x": 0.9597228169441223, "y": 0.062355659902095795}, {"x": 0.9510610699653625, "y": 0.062355659902095795}], "text": "37\n"}
{"page": 37, "bbox": [{"x": 0.038977913558483124, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.05369514971971512}, {"x": 0.4703334867954254, "y": 0.06524249166250229}, {"x": 0.038977913558483124, "y": 0.06524249166250229}], "text": "EBOOK: A COMPACT GUIDE TO RETRIEVAL AUGMENTED GENERATION (RAG)\n"}
{"page": 37, "bbox": [{"x": 0.038544826209545135, "y": 0.16454964876174927}, {"x": 0.13945430517196655, "y": 0.16454964876174927}, {"x": 0.13945430517196655, "y": 0.18822170794010162}, {"x": 0.038544826209545135, "y": 0.18822170794010162}], "text": "Summary\n"}
{"page": 37, "bbox": [{"x": 0.25205716490745544, "y": 0.15877598524093628}, {"x": 0.8484192490577698, "y": 0.15762124955654144}, {"x": 0.8484192490577698, "y": 0.3117782771587372}, {"x": 0.25205716490745544, "y": 0.3129330277442932}], "text": "Whether you're looking to disrupt traditional industries, enhance creative endeavors or solve complex\nproblems in novel ways, the potential applications of generative Al are limited only by your imagination and\nwillingness to experiment. Remember, every significant advancement in this field began with a simple idea and\nthe courage to explore it further.\nFor those seeking more knowledge or who are simply curious about the latest developments in the realm of\ngenerative Al, we've provided some resources on training, demos and product information.\n"}
{"page": 37, "bbox": [{"x": 0.25249025225639343, "y": 0.3406466543674469}, {"x": 0.37462103366851807, "y": 0.3429561257362366}, {"x": 0.3741879463195801, "y": 0.3625866174697876}, {"x": 0.25205716490745544, "y": 0.3602771461009979}], "text": "GenAl training\n"}
{"page": 37, "bbox": [{"x": 0.2745777368545532, "y": 0.37990760803222656}, {"x": 0.8388912677764893, "y": 0.3752886950969696}, {"x": 0.8388912677764893, "y": 0.4162817597389221}, {"x": 0.2745777368545532, "y": 0.42090070247650146}], "text": "■ Generative Al engineer learning pathway: Take self-paced, on-demand and instructor-led courses on\ngenerative Al\n"}
{"page": 37, "bbox": [{"x": 0.27371156215667725, "y": 0.4347575008869171}, {"x": 0.8388912677764893, "y": 0.4376443326473236}, {"x": 0.8384581804275513, "y": 0.5155889391899109}, {"x": 0.27327847480773926, "y": 0.5127021074295044}], "text": "■ Free LLM course (edX): An in-depth course to learn GenAl and LLMs inside and out\n■ GenAl webinar: Learn how to take control of your GenAl app performance, privacy and cost, and drive\nvalue with generative Al\n"}
{"page": 37, "bbox": [{"x": 0.25249025225639343, "y": 0.5334873199462891}, {"x": 0.42962321639060974, "y": 0.5363741517066956}, {"x": 0.42962321639060974, "y": 0.5536951422691345}, {"x": 0.25249025225639343, "y": 0.5513857007026672}], "text": "Additional resources\n"}
{"page": 37, "bbox": [{"x": 0.27414464950561523, "y": 0.5727482438087463}, {"x": 0.8492854237556458, "y": 0.5727482438087463}, {"x": 0.8492854237556458, "y": 0.6108545064926147}, {"x": 0.27414464950561523, "y": 0.6108545064926147}], "text": "■ The Big Book of MLOps: A deep dive into the architectures and technologies behind MLOps - including\nLLMs and GenAl\n"}
{"page": 37, "bbox": [{"x": 0.27414464950561523, "y": 0.6310623288154602}, {"x": 0.7029016613960266, "y": 0.6310623288154602}, {"x": 0.7029016613960266, "y": 0.6454965472221375}, {"x": 0.27414464950561523, "y": 0.6454965472221375}], "text": "■ Mosaic Al: Product page covering the features of Mosaic Al within Databricks\n"}
{"page": 37, "bbox": [{"x": 0.27371156215667725, "y": 0.6628175377845764}, {"x": 0.809874415397644, "y": 0.6628175377845764}, {"x": 0.809874415397644, "y": 0.6778290867805481}, {"x": 0.27371156215667725, "y": 0.6778290867805481}], "text": "■ The Big Book of Generative Al: Best practices for building production-quality GenAl applications\n"}
{"page": 37, "bbox": [{"x": 0.058466870337724686, "y": 0.9301385879516602}, {"x": 0.14335210621356964, "y": 0.931870698928833}, {"x": 0.14335210621356964, "y": 0.949191689491272}, {"x": 0.058466870337724686, "y": 0.9474595785140991}], "text": "databricks\n"}
{"page": 38, "bbox": [{"x": 0.06972715258598328, "y": 0.0842956155538559}, {"x": 0.5448246002197266, "y": 0.0842956155538559}, {"x": 0.5448246002197266, "y": 0.18822170794010162}, {"x": 0.06972715258598328, "y": 0.18822170794010162}], "text": "Build Production-Quality GenAl Applications - See How\nCreate high-quality generative Al applications and ensure your output is accurate,\ngoverned and safe. See why over 10,000 organizations worldwide rely on Databricks for\nall their workloads from BI to Al - test-drive the full Databricks Platform free for 14 days.\n"}
{"page": 38, "bbox": [{"x": 0.08791684359312057, "y": 0.23498845100402832}, {"x": 0.17886531352996826, "y": 0.2344110906124115}, {"x": 0.17886531352996826, "y": 0.2465357929468155}, {"x": 0.08791684359312057, "y": 0.2471131682395935}], "text": "Try Databricks free\n"}
{"page": 38, "bbox": [{"x": 0.2390645295381546, "y": 0.23556582629680634}, {"x": 0.5041143298149109, "y": 0.23556582629680634}, {"x": 0.5041143298149109, "y": 0.2471131682395935}, {"x": 0.2390645295381546, "y": 0.2471131682395935}], "text": "Take Generative Al Fundamentals On-Demand Training\n"}
{"page": 38, "bbox": [{"x": 0.06972715258598328, "y": 0.3002309501171112}, {"x": 0.20268514752388, "y": 0.3002309501171112}, {"x": 0.20268514752388, "y": 0.31466513872146606}, {"x": 0.06972715258598328, "y": 0.31466513872146606}], "text": "About Databricks\n"}
{"page": 38, "bbox": [{"x": 0.07016023993492126, "y": 0.34237876534461975}, {"x": 0.528367280960083, "y": 0.34237876534461975}, {"x": 0.528367280960083, "y": 0.4849884510040283}, {"x": 0.07016023993492126, "y": 0.4849884510040283}], "text": "Databricks is the data and Al company. More than 10,000 organizations worldwide\nincluding Comcast, Condé Nast, Grammarly and over 50% of the Fortune 500 - rely\non the Databricks Data Intelligence Platform to unify and democratize data, analytics\nand Al. Databricks is headquartered in San Francisco, with offices around the globe,\nand was founded by the original creators of Lakehouse, Apache Spark™M, Delta Lake\nand MLflow. To learn more, follow Databricks on LinkedIn, X and Facebook.\n"}
{"page": 38, "bbox": [{"x": 0.8423560261726379, "y": 0.53695148229599}, {"x": 0.8930273056030273, "y": 0.53695148229599}, {"x": 0.8930273056030273, "y": 0.5790992975234985}, {"x": 0.8423560261726379, "y": 0.5790992975234985}], "text": "((•))\n"}
{"page": 38, "bbox": [{"x": 0.0939800813794136, "y": 0.9105080962181091}, {"x": 0.20788219571113586, "y": 0.9105080962181091}, {"x": 0.20788219571113586, "y": 0.9324480295181274}, {"x": 0.0939800813794136, "y": 0.9324480295181274}], "text": "databricks\n"}
{"page": 38, "bbox": [{"x": 0.07059332728385925, "y": 0.9555427432060242}, {"x": 0.7024685740470886, "y": 0.9555427432060242}, {"x": 0.7024685740470886, "y": 0.9659353494644165}, {"x": 0.07059332728385925, "y": 0.9659353494644165}], "text": "© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policy | Terms of Use\n"}
{"page": 1, "bbox": [{"x": 0.16716240346431732, "y": 0.09167367219924927}, {"x": 0.8316478133201599, "y": 0.09167367219924927}, {"x": 0.8316478133201599, "y": 0.10597140341997147}, {"x": 0.16716240346431732, "y": 0.10597140341997147}], "text": "Searching for Best Practices in Retrieval-Augmented Generation\n"}
{"page": 1, "bbox": [{"x": 0.1814396232366562, "y": 0.12363330274820328}, {"x": 0.8179655075073242, "y": 0.1232127845287323}, {"x": 0.8179655075073242, "y": 0.23633305728435516}, {"x": 0.1814396232366562, "y": 0.23675356805324554}], "text": "Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu,\nZhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li,\nQi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng*, Xuanjing Huang\nSchool of Computer Science, Fudan University, Shanghai, China\nShanghai Key Laboratory of Intelligent Information Processing\n{xiaohuawang22}@m. fudan.edu.cn\n{zhengxq, xjhuang}@fudan.edu.cn\n"}
{"page": 1, "bbox": [{"x": 0.26472339034080505, "y": 0.26492851972579956}, {"x": 0.3402736485004425, "y": 0.26492851972579956}, {"x": 0.3402736485004425, "y": 0.27417999505996704}, {"x": 0.26472339034080505, "y": 0.27417999505996704}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.1463414579629898, "y": 0.288898229598999}, {"x": 0.4610351026058197, "y": 0.288898229598999}, {"x": 0.4604402184486389, "y": 0.6253153681755066}, {"x": 0.14574657380580902, "y": 0.6253153681755066}], "text": "Retrieval-augmented generation (RAG) tech-\nniques have proven to be effective in integrat-\ning up-to-date information, mitigating halluci-\nnations, and enhancing response quality, par-\nticularly in specialized domains. While many\nRAG approaches have been proposed to en-\nhance large language models through query-\ndependent retrievals, these approaches still suf-\nfer from their complex implementation and pro-\nlonged response times. Typically, a RAG work-\nflow involves multiple processing steps, each of\nwhich can be executed in various ways. Here,\nwe investigate existing RAG approaches and\ntheir potential combinations to identify opti-\nmal RAG practices. Through extensive experi-\nments, we suggest several strategies for deploy-\ning RAG that balance both performance and ef-\nficiency. Moreover, we demonstrate that multi-\nmodal retrieval techniques can significantly en-\nhance question-answering capabilities about vi-\nsual inputs and accelerate the generation of mul-\ntimodal content using a \"retrieval as generation\"\nstrategy. Code and resources are available at\nhttps://github.com/FudanDNN-NLP/RAG.\n"}
{"page": 1, "bbox": [{"x": 0.5127900242805481, "y": 0.26534903049468994}, {"x": 0.8845925331115723, "y": 0.26534903049468994}, {"x": 0.8851873874664307, "y": 0.9201009273529053}, {"x": 0.5133848786354065, "y": 0.9201009273529053}], "text": "Many RAG approaches have been proposed to\nenhance large language models (LLMs) through\nquery-dependent retrievals (Cai et al., 2022; Gao\net al., 2023; Li et al., 2022). A typical RAG\nworkflow usually contains multiple intervening pro-\ncessing steps: query classification (determining\nwhether retrieval is necessary for a given input\nquery), retrieval (efficiently obtaining relevant doc-\numents for the query), reranking (refining the order\nof retrieved documents based on their relevance to\nthe query), repacking (organizing the retrieved doc-\numents into a structured one for better generation),\nsummarization (extracting key information for re-\nsponse generation from the repacked document and\neliminating redundancies) modules. Implementing\nRAG also requires decisions on the ways to prop-\nerly split documents into chunks, the types of em-\nbeddings to use for semantically representing these\nchunks, the choice of vector databases to efficiently\nstore feature representations, and the methods for\neffectively fine-tuning LLMs (see Figure 1).\nWhat adds complexity and challenge is the vari-\nability in implementing each processing step. For\nexample, in retrieving relevant documents for an in-\nput query, various methods can be employed. One\napproach involves rewriting the query first and us-\ning the rewritten queries for retrieval (Ma et al.,\n2023a). Alternatively, pseudo-responses to the\nquery can be generated first, and the similarity be-\ntween these pseudo-responses and the backend doc-\numents can be compared for retrieval (Gao et al.,\n2022). Another option is to directly employ em-\nbedding models, typically trained in a contrastive\nmanner using positive and negative query-response\npairs (Wang et al., 2022; Xiao et al., 2023). The\ntechniques chosen for each step and their combi-\nnations significantly impact both the effectiveness\nand efficiency of RAG systems. To the best of our\nknowledge, there has been no systematic effort to\npursue the optimal implementation of RAG, partic-\nularly for the entire RAG workflow.\n"}
{"page": 1, "bbox": [{"x": 0.11957168579101562, "y": 0.6387720704078674}, {"x": 0.2575847804546356, "y": 0.6387720704078674}, {"x": 0.2575847804546356, "y": 0.6484440565109253}, {"x": 0.11957168579101562, "y": 0.6484440565109253}], "text": "1 Introduction\n"}
{"page": 1, "bbox": [{"x": 0.11838191747665405, "y": 0.6614802479743958}, {"x": 0.48958954215049744, "y": 0.6619007587432861}, {"x": 0.48899465799331665, "y": 0.9011774659156799}, {"x": 0.11778703331947327, "y": 0.9007569551467896}], "text": "Generative large language models are prone to pro-\nducing outdated information or fabricating facts,\nalthough they were aligned with human preferences\nby reinforcement learning (Ouyang et al., 2022) or\nlightweight alternatives (Liu et al., 2023; Rafailov\net al., 2023; Yuan et al., 2023; Zhao et al., 2023b).\nRetrieval-augmented generation (RAG) techniques\naddress these issues by combining the strengths\nof pretraining and retrieval-based models, thereby\nproviding a robust framework for enhancing model\nperformance (Gao et al., 2023). Furthermore, RAG\nenables rapid deployment of applications for spe-\ncific organizations and domains without necessi-\ntating updates to the model parameters, as long as\nquery-related documents are provided.\n"}
{"page": 1, "bbox": [{"x": 0.13920284807682037, "y": 0.9104289412498474}, {"x": 0.2837596535682678, "y": 0.9095878601074219}, {"x": 0.2837596535682678, "y": 0.9196804165840149}, {"x": 0.13920284807682037, "y": 0.9205214381217957}], "text": "*Corresponding Author.\n"}
{"page": 1, "bbox": [{"x": 0.48007139563560486, "y": 0.931034505367279}, {"x": 0.5234979391098022, "y": 0.931034505367279}, {"x": 0.5234979391098022, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17716\n"}
{"page": 1, "bbox": [{"x": 0.15645448863506317, "y": 0.9486963748931885}, {"x": 0.8429506421089172, "y": 0.9486963748931885}, {"x": 0.8429506421089172, "y": 0.9709840416908264}, {"x": 0.15645448863506317, "y": 0.9709840416908264}], "text": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 17716–17736\nNovember 12-16, 2024 2024 Association for Computational Linguistics\n"}
{"page": 2, "bbox": [{"x": 0.11719214916229248, "y": 0.0874684602022171}, {"x": 0.48958954215049744, "y": 0.0874684602022171}, {"x": 0.48899465799331665, "y": 0.4533221125602722}, {"x": 0.1165972650051117, "y": 0.4533221125602722}], "text": "In this study, we aim to identify the best practices\nfor RAG through extensive experimentation. Given\nthe infeasibility of testing all possible combinations\nof these methods, we adopt a three-step approach to\nidentify optimal RAG practices. First, we compare\nrepresentative methods for each RAG step (or mod-\nule) and select up to three of the best-performing\nmethods. Next, we evaluate the impact of each\nmethod on the overall RAG performance by testing\none method at a time for an individual step, while\nkeeping the other RAG modules unchanged. This\nallows us to determine the most effective method\nfor each step based on its contribution and interac-\ntion with other modules during response generation.\nOnce the best method is chosen for a module, it\nis used in subsequent experiments. Finally, we\nempirically explore a few promising combinations\nsuitable for different application scenarios where\nefficiency might be prioritized over performance,\nor vice versa. Based on these findings, we suggest\nseveral strategies for deploying RAG that balance\nboth performance and efficiency.\nThe contributions of this study are three-fold:\n"}
{"page": 2, "bbox": [{"x": 0.14098750054836273, "y": 0.464255690574646}, {"x": 0.1463414579629898, "y": 0.464255690574646}, {"x": 0.1463414579629898, "y": 0.4676198363304138}, {"x": 0.14098750054836273, "y": 0.4676198363304138}], "text": "•\n"}
{"page": 2, "bbox": [{"x": 0.5116002559661865, "y": 0.08788898587226868}, {"x": 0.8857823014259338, "y": 0.08788898587226868}, {"x": 0.8851873874664307, "y": 0.9205214381217957}, {"x": 0.5110053420066833, "y": 0.9205214381217957}], "text": "nal knowledge bases, providing accurate, real-time,\ndomain-specific context to LLMs (Gao et al., 2023).\nPrevious works have optimized the RAG pipeline\nthrough query and retrieval transformations, en-\nhancing retriever performance, and fine-tuning both\nthe retriever and generator. These optimizations\nimprove the interaction between input queries, re-\ntrieval mechanisms, and generation processes, en-\nsuring the accuracy and relevance of responses.\n2.1 Query and Retrieval Transformation\nEffective retrieval requires queries accurate, clear,\nand detailed. Even when converted into em-\nbeddings, semantic differences between queries\nand relevant documents can persist. Previous\nworks have explored methods to enhance query\ninformation through query transformation, thereby\nimproving retrieval performance. For instance,\nQuery2Doc (Wang et al., 2023a) and HyDE (Gao\net al., 2022) generate pseudo-documents from orig-\ninal queries to enhance retrieval, while TOC (Kim\net al., 2023) decomposes queries into subqueries,\naggregating the retrieved content for final results.\nOther studies have focused on transforming re-\ntrieval source documents. LlamaIndex (Liu, 2022)\nprovides an interface to generate pseudo-queries for\nretrieval documents, improving matching with real\nqueries. Some works employ contrastive learning\nto bring query and document embeddings closer in\nsemantic space (Li et al., 2023; Xiao et al., 2023;\nZhang et al., 2023a). Post-processing retrieved doc-\numents is another method to enhance generator out-\nput, with techniques like hierarchical prompt sum-\nmarization (Jiang et al., 2023a) and using abstrac-\ntive and extractive compressors (Xu et al., 2023)\nto reduce context length and remove redundancy\n(Wang et al., 2023c).\n2.2 Retriever Enhancement Strategy\nDocument chunking and embedding methods sig-\nnificantly impact retrieval performance. Common\nchunking strategies divide documents into chunks,\nbut determining optimal chunk length can be chal-\nlenging. Small chunks may fragment sentences,\nwhile large chunks might include irrelevant con-\ntext. LlamaIndex (Liu, 2022) optimizes the chunk-\ning method like Small2Big and sliding window.\nRetrieved chunks can be irrelevant and numbers\ncan be large, so reranking is necessary to filter\nirrelevant documents. A common reranking ap-\nproach employs deep language models such as\nBERT (Nogueira et al., 2019), T5 (Nogueira et al.,\n"}
{"page": 2, "bbox": [{"x": 0.14039261639118195, "y": 0.4608915150165558}, {"x": 0.490779310464859, "y": 0.46173253655433655}, {"x": 0.48958954215049744, "y": 0.7144659161567688}, {"x": 0.13920284807682037, "y": 0.713624894618988}], "text": "Through extensive experimentation, we thor-\noughly investigated existing RAG approaches\nand their combinations to identify and recom-\nmend optimal RAG practices.\n• We introduce a comprehensive framework of\nevaluation metrics and corresponding datasets\nto comprehensively assess the performance of\nretrieval-augmented generation models, cover-\ning general, specialized (or domain-specific),\nand RAG-related capabilities.\n• We demonstrate that the integration of multi-\nmodal retrieval techniques can substantially\nimprove question-answering capabilities on\nvisual inputs and speed up the generation of\nmultimodal content through a strategy of \"re-\ntrieval as generation\".\n"}
{"page": 2, "bbox": [{"x": 0.11719214916229248, "y": 0.7388561964035034}, {"x": 0.26888757944107056, "y": 0.7388561964035034}, {"x": 0.26888757944107056, "y": 0.7485281825065613}, {"x": 0.11719214916229248, "y": 0.7485281825065613}], "text": "2 Related Work\n"}
{"page": 2, "bbox": [{"x": 0.11778703331947327, "y": 0.7649285197257996}, {"x": 0.48899465799331665, "y": 0.7645080089569092}, {"x": 0.48958954215049744, "y": 0.920941948890686}, {"x": 0.11838191747665405, "y": 0.9213625192642212}], "text": "Ensuring the accuracy of responses generated by\nLarge Language Models (LLMs) such as Chat-\nGPT (OpenAI, 2023) and LLaMA (Touvron et al.,\n2023a) is essential. However, simply enlarg-\ning model size does not fundamentally address\nthe issue of hallucinations (Wang et al., 2023b;\nZhang et al., 2023c), especially in knowledge-\nintensive tasks and specialized domains. Retrieval-\naugmented generation (RAG) addresses these chal-\nlenges by retrieving relevant documents from exter-\n"}
{"page": 2, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.939444899559021}, {"x": 0.48007139563560486, "y": 0.939444899559021}], "text": "17717\n"}
{"page": 3, "bbox": [{"x": 0.2623438537120819, "y": 0.09545836597681046}, {"x": 0.3242117762565613, "y": 0.09545836597681046}, {"x": 0.3242117762565613, "y": 0.1017661914229393}, {"x": 0.2623438537120819, "y": 0.1017661914229393}], "text": "Evaluation\n"}
{"page": 3, "bbox": [{"x": 0.41701367497444153, "y": 0.10386879742145538}, {"x": 0.5806067585945129, "y": 0.1017661914229393}, {"x": 0.5806067585945129, "y": 0.11227922886610031}, {"x": 0.41701367497444153, "y": 0.1143818348646164}], "text": "Large Language Model\n"}
{"page": 3, "bbox": [{"x": 0.24033313989639282, "y": 0.11227922886610031}, {"x": 0.34681737422943115, "y": 0.11227922886610031}, {"x": 0.34681737422943115, "y": 0.13624894618988037}, {"x": 0.24033313989639282, "y": 0.13624894618988037}], "text": "⚫ General Performance\n⚫ Specific Domains\n. Retrieval Capability\n"}
{"page": 3, "bbox": [{"x": 0.6781677603721619, "y": 0.12952060997486115}, {"x": 0.7703747749328613, "y": 0.1303616464138031}, {"x": 0.7703747749328613, "y": 0.13708999752998352}, {"x": 0.6781677603721619, "y": 0.13624894618988037}], "text": "Retrieval Source\n"}
{"page": 3, "bbox": [{"x": 0.6603212356567383, "y": 0.14507989585399628}, {"x": 0.7204045057296753, "y": 0.14550042152404785}, {"x": 0.7204045057296753, "y": 0.1534903347492218}, {"x": 0.6603212356567383, "y": 0.15306980907917023}], "text": "Chunking\n"}
{"page": 3, "bbox": [{"x": 0.4312908947467804, "y": 0.1463414579629898}, {"x": 0.5508626103401184, "y": 0.1446593701839447}, {"x": 0.5508626103401184, "y": 0.15264928340911865}, {"x": 0.4312908947467804, "y": 0.15433137118816376}], "text": "Query Classification\n"}
{"page": 3, "bbox": [{"x": 0.2623438537120819, "y": 0.15054668486118317}, {"x": 0.3182629346847534, "y": 0.15054668486118317}, {"x": 0.3182629346847534, "y": 0.15685449540615082}, {"x": 0.2623438537120819, "y": 0.15685449540615082}], "text": "Fine-tune\n"}
{"page": 3, "bbox": [{"x": 0.63712078332901, "y": 0.16148023307323456}, {"x": 0.7233789563179016, "y": 0.16105970740318298}, {"x": 0.7233789563179016, "y": 0.19259881973266602}, {"x": 0.63712078332901, "y": 0.1930193454027176}], "text": "Chunking Size\n• Small2big\nSliding Windows\n⚫ Chunk Metadata\n"}
{"page": 3, "bbox": [{"x": 0.2498512864112854, "y": 0.16862909495830536}, {"x": 0.28673407435417175, "y": 0.16862909495830536}, {"x": 0.28673407435417175, "y": 0.1909167319536209}, {"x": 0.2498512864112854, "y": 0.1909167319536209}], "text": "Disturb\nRandom\nNormal\n"}
{"page": 3, "bbox": [{"x": 0.46281975507736206, "y": 0.18334734439849854}, {"x": 0.5157644152641296, "y": 0.18334734439849854}, {"x": 0.5157644152641296, "y": 0.18965516984462738}, {"x": 0.46281975507736206, "y": 0.18965516984462738}], "text": "Retrieval\n"}
{"page": 3, "bbox": [{"x": 0.43723973631858826, "y": 0.19722455739974976}, {"x": 0.5139797925949097, "y": 0.1984861195087433}, {"x": 0.5139797925949097, "y": 0.2056349813938141}, {"x": 0.43723973631858826, "y": 0.20437341928482056}], "text": "• Original Query\n"}
{"page": 3, "bbox": [{"x": 0.24390244483947754, "y": 0.2052144706249237}, {"x": 0.33372992277145386, "y": 0.2052144706249237}, {"x": 0.33372992277145386, "y": 0.21194280683994293}, {"x": 0.24390244483947754, "y": 0.21194280683994293}], "text": "Summarization\n"}
{"page": 3, "bbox": [{"x": 0.43664485216140747, "y": 0.20647603273391724}, {"x": 0.5443188548088074, "y": 0.20647603273391724}, {"x": 0.5443188548088074, "y": 0.24684609472751617}, {"x": 0.43664485216140747, "y": 0.24684609472751617}], "text": "⚫ BM25\n• Contriever\n⚫ LLM-Embedder\n• Query Rewriting\n• Query Decomposition\n"}
{"page": 3, "bbox": [{"x": 0.6377156376838684, "y": 0.20857863128185272}, {"x": 0.7352766394615173, "y": 0.2089991569519043}, {"x": 0.7352766394615173, "y": 0.2552565038204193}, {"x": 0.6377156376838684, "y": 0.25483599305152893}], "text": "Embedding\n⚫ LLM-Embedder\n⚫ intfloat/e5\n⚫ BAAI/bge\n⚫ Jina-embeddings-v2\n"}
{"page": 3, "bbox": [{"x": 0.43723973631858826, "y": 0.2481076568365097}, {"x": 0.474122554063797, "y": 0.24768713116645813}, {"x": 0.474122554063797, "y": 0.25441548228263855}, {"x": 0.43723973631858826, "y": 0.25483599305152893}], "text": "⚫ HYDE\n"}
{"page": 3, "bbox": [{"x": 0.23973825573921204, "y": 0.2211942821741104}, {"x": 0.3486020267009735, "y": 0.2211942821741104}, {"x": 0.3486020267009735, "y": 0.288898229598999}, {"x": 0.23973825573921204, "y": 0.288898229598999}], "text": "• Extractive\n• Recomp\n⚫ BM25\n• Contriever\n⚫ Abstractive\n⚫ LongLLMlingua\n• SelectiveContext\n• Recomp\n"}
{"page": 3, "bbox": [{"x": 0.6377156376838684, "y": 0.257359117269516}, {"x": 0.6627007722854614, "y": 0.25651809573173523}, {"x": 0.6632956862449646, "y": 0.26198485493659973}, {"x": 0.6383105516433716, "y": 0.2628259062767029}], "text": "⚫ Gte\n"}
{"page": 3, "bbox": [{"x": 0.43664485216140747, "y": 0.2556770443916321}, {"x": 0.545508623123169, "y": 0.25483599305152893}, {"x": 0.545508623123169, "y": 0.2712363302707672}, {"x": 0.43723973631858826, "y": 0.27207738161087036}], "text": "⚫ Hybrid Search\n⚫ HyDE+Hybrid Search\n"}
{"page": 3, "bbox": [{"x": 0.6377156376838684, "y": 0.26534903049468994}, {"x": 0.7269482612609863, "y": 0.26534903049468994}, {"x": 0.7269482612609863, "y": 0.27207738161087036}, {"x": 0.6377156376838684, "y": 0.27207738161087036}], "text": "⚫ all-mpnet-base-v2\n"}
{"page": 3, "bbox": [{"x": 0.6436644792556763, "y": 0.288898229598999}, {"x": 0.7382510304450989, "y": 0.288898229598999}, {"x": 0.7382510304450989, "y": 0.29562658071517944}, {"x": 0.6436644792556763, "y": 0.29562658071517944}], "text": "Vector Database\n"}
{"page": 3, "bbox": [{"x": 0.6383105516433716, "y": 0.30445751547813416}, {"x": 0.6799523830413818, "y": 0.3048780560493469}, {"x": 0.6799523830413818, "y": 0.3103448152542114}, {"x": 0.6383105516433716, "y": 0.30992430448532104}], "text": "⚫ Milvus\n"}
{"page": 3, "bbox": [{"x": 0.25996431708335876, "y": 0.30445751547813416}, {"x": 0.3236168920993805, "y": 0.30445751547813416}, {"x": 0.3236168920993805, "y": 0.3128679692745209}, {"x": 0.25996431708335876, "y": 0.3128679692745209}], "text": "Repacking\n"}
{"page": 3, "bbox": [{"x": 0.43723973631858826, "y": 0.2910008430480957}, {"x": 0.5300416350364685, "y": 0.2905803322792053}, {"x": 0.5306365489959717, "y": 0.33641716837882996}, {"x": 0.43783462047576904, "y": 0.33683767914772034}], "text": "Reranking\n⚫ DLM-based\n⚫ monoT5\n⚫ monoBERT\n• RankLLAMA\n"}
{"page": 3, "bbox": [{"x": 0.647233784198761, "y": 0.31328848004341125}, {"x": 0.6686496138572693, "y": 0.31328848004341125}, {"x": 0.6686496138572693, "y": 0.3183347284793854}, {"x": 0.647233784198761, "y": 0.3183347284793854}], "text": "Faiss\n"}
{"page": 3, "bbox": [{"x": 0.6383105516433716, "y": 0.3216989040374756}, {"x": 0.6882807612419128, "y": 0.32211941480636597}, {"x": 0.6882807612419128, "y": 0.3271656930446625}, {"x": 0.6383105516433716, "y": 0.3267451524734497}], "text": "• Weaviate\n"}
{"page": 3, "bbox": [{"x": 0.2498512864112854, "y": 0.32211941480636597}, {"x": 0.28732895851135254, "y": 0.32211941480636597}, {"x": 0.28732895851135254, "y": 0.3359966278076172}, {"x": 0.2498512864112854, "y": 0.3359966278076172}], "text": "Sides\nForward\n"}
{"page": 3, "bbox": [{"x": 0.6383105516433716, "y": 0.3301093280315399}, {"x": 0.6799523830413818, "y": 0.3305298686027527}, {"x": 0.6799523830413818, "y": 0.3359966278076172}, {"x": 0.6383105516433716, "y": 0.3355761170387268}], "text": "• Qdrant\n"}
{"page": 3, "bbox": [{"x": 0.6383105516433716, "y": 0.33851975202560425}, {"x": 0.6835216879844666, "y": 0.33851975202560425}, {"x": 0.6835216879844666, "y": 0.34356603026390076}, {"x": 0.6383105516433716, "y": 0.34356603026390076}], "text": "• Chroma\n"}
{"page": 3, "bbox": [{"x": 0.2498512864112854, "y": 0.3393608033657074}, {"x": 0.28613919019699097, "y": 0.3393608033657074}, {"x": 0.28613919019699097, "y": 0.3444070518016815}, {"x": 0.2498512864112854, "y": 0.3444070518016815}], "text": "Reverse\n"}
{"page": 3, "bbox": [{"x": 0.43723973631858826, "y": 0.3406223654747009}, {"x": 0.47650209069252014, "y": 0.34020185470581055}, {"x": 0.47650209069252014, "y": 0.3460891544818878}, {"x": 0.43723973631858826, "y": 0.3465096652507782}], "text": "⚫ TILDE\n"}
{"page": 3, "bbox": [{"x": 0.11838191747665405, "y": 0.35786375403404236}, {"x": 0.881618082523346, "y": 0.35828426480293274}, {"x": 0.881618082523346, "y": 0.5126156210899353}, {"x": 0.11838191747665405, "y": 0.5121951103210449}], "text": "Figure 1: Retrieval-augmented generation workflow. This study investigates the contribution of each component and\nprovides insights into optimal RAG practices through extensive experimentation. The optional methods considered\nfor each component are indicated in bold fonts, while the methods underlined indicate the default choice for\nindividual modules. The methods indicated in blue font denote the best-performing selections identified empirically.\n2020), or LLAMA (Ma et al., 2023b), which re-\nquires slow inference steps during reranking but\ngrants better performance. TILDE (Zhuang and\nZuccon, 2021a,b) achieves efficiency by precom-\nputing and storing the likelihood of query terms,\nranking documents based on their sum.\n"}
{"page": 3, "bbox": [{"x": 0.5139797925949097, "y": 0.4213624894618988}, {"x": 0.8839976191520691, "y": 0.4209419786930084}, {"x": 0.8839976191520691, "y": 0.46509671211242676}, {"x": 0.5139797925949097, "y": 0.4655172526836395}], "text": "challenging. In this paper, we focus on best prac-\ntices for applying RAG methods, advancing the\nunderstanding and application of RAG in LLMs.\n"}
{"page": 3, "bbox": [{"x": 0.5139797925949097, "y": 0.48107653856277466}, {"x": 0.6763830780982971, "y": 0.48107653856277466}, {"x": 0.6763830780982971, "y": 0.4907485246658325}, {"x": 0.5139797925949097, "y": 0.4907485246658325}], "text": "3 RAG Workflow\n"}
{"page": 3, "bbox": [{"x": 0.5121951103210449, "y": 0.5058873295783997}, {"x": 0.8839976191520691, "y": 0.5058873295783997}, {"x": 0.8834027647972107, "y": 0.9213625192642212}, {"x": 0.5116002559661865, "y": 0.9213625192642212}], "text": "In this section, we detail the components of the\nRAG workflow. For each module, we review com-\nmonly used approaches and select the default and\nalternative methods for our final pipeline. Section\n4 will discuss best practices. Figure 1 presents the\nworkflow and methods for each module. Detailed\nexperimental setups, including datasets, hyperpa-\nrameters, and results are provided in Appendix A.\n3.1 Query Classification\nNot all queries require to be retrieval-augmented\ndue to the inherent capabilities of LLMs. While\nRAG can enhance information accuracy and re-\nduce hallucinations, frequent retrieval costs longer\nresponse time. Therefore, we begin by classifying\nqueries to determine retrieval necessity. Queries\nrequiring retrieval proceed through the RAG mod-\nules; others are handled directly by LLMs.\nRetrieval is generally recommended when\nknowledge beyond the model's parameters is\nneeded. However, the need for retrieval varies by\ntask. For instance, an LLM trained up to 2023 can\nhandle a translation request for \"Sora was devel-\noped by OpenAI\" without retrieval. Conversely,\nan introduction request for the same topic would\nrequire retrieval to provide relevant information.\n"}
{"page": 3, "bbox": [{"x": 0.11778703331947327, "y": 0.5298570394515991}, {"x": 0.48899465799331665, "y": 0.5298570394515991}, {"x": 0.48899465799331665, "y": 0.920941948890686}, {"x": 0.11778703331947327, "y": 0.920941948890686}], "text": "2.3 Retriever and Generator Fine-tuning\nFine-tuning within the RAG framework is crucial\nfor optimizing both retrievers and generators. Some\nresearch focuses on fine-tuning the generator to bet-\nter utilize retriever context (Liu et al., 2024b; Luo\net al., 2023; Zhang et al., 2024b), ensuring faith-\nful and robust generated content. Others fine-tune\nthe retriever to learn to retrieve beneficial passages\nfor the generator (Izacard et al., 2022; Shi et al.,\n2023; Zhang et al., 2024a). Holistic approaches\ntreat RAG as an integrated system, fine-tuning both\nretriever and generator together to enhance overall\nperformance (Guu et al., 2020; Lin et al., 2023;\nZamani and Bendersky, 2024), despite increased\ncomplexity and integration challenges.\nSeveral surveys have extensively discussed cur-\nrent RAG systems, covering aspects like text gener-\nation (Cai et al., 2022; Li et al., 2022), integration\nwith LLMs (Gao et al., 2023; Huang and Huang,\n2024), multimodal (Zhao et al., 2023a), and AI-\ngenerated content (Zhao et al., 2024). While these\nsurveys provide comprehensive overviews of ex-\nisting RAG methodologies, selecting the appropri-\nate algorithm for practical implementation remains\n"}
{"page": 3, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5229030251502991, "y": 0.9314550161361694}, {"x": 0.5229030251502991, "y": 0.939444899559021}, {"x": 0.48007139563560486, "y": 0.939444899559021}], "text": "17718\n"}
{"page": 4, "bbox": [{"x": 0.1165972650051117, "y": 0.085786372423172}, {"x": 0.48958954215049744, "y": 0.085786372423172}, {"x": 0.48958954215049744, "y": 0.35492008924484253}, {"x": 0.1165972650051117, "y": 0.35492008924484253}], "text": "To address this issue, we propose classifying\ntasks by type to determine if a query needs retrieval.\nWe categorize 15 tasks based on whether they pro-\nvide sufficient information, with specific tasks and\nexamples illustrated in Figure 2. For tasks entirely\nbased on user-given information, we denote as \"suf-\nficient\", which need not retrieval; otherwise, we\ndenote as \"insufficient”, where retrieval may be\nnecessary. We created a dataset consisting of 111K\nsamples covering 15 different types of tasks, with\n64K samples labeled as \"retrieval required\" and\n47K samples as \"no retrieval required\". A classifier\nwas trained to automate this decision-making pro-\ncess. Specific experimental results are presented\nin Appendix A.1. Section 4 explores the impact\nof query classification on the workflow, comparing\nscenarios with and without classification.\n"}
{"page": 4, "bbox": [{"x": 0.11897680163383484, "y": 0.37426409125328064}, {"x": 0.23914337158203125, "y": 0.3751051425933838}, {"x": 0.23914337158203125, "y": 0.3856181800365448}, {"x": 0.11897680163383484, "y": 0.38477712869644165}], "text": "3.2 Chunking\n"}
{"page": 4, "bbox": [{"x": 0.11838191747665405, "y": 0.3982338011264801}, {"x": 0.48899465799331665, "y": 0.3982338011264801}, {"x": 0.48899465799331665, "y": 0.4730866253376007}, {"x": 0.11838191747665405, "y": 0.4730866253376007}], "text": "Chunking documents into smaller segments is cru-\ncial for enhancing retrieval precision and avoiding\nlength issues in LLMs. This process can be ap-\nplied at various levels of granularity, such as token,\nsentence, and semantic levels.\n"}
{"page": 4, "bbox": [{"x": 0.5133848786354065, "y": 0.08242220431566238}, {"x": 0.8857823014259338, "y": 0.08284272253513336}, {"x": 0.8839976191520691, "y": 0.9205214381217957}, {"x": 0.5116002559661865, "y": 0.9201009273529053}], "text": "Embedding Model Choosing the right embed-\nding model is crucial for effective semantic match-\ning of queries and chunk blocks. Based on the\nevaluation module of FlagEmbedding¹, we select\nthe LLM-Embedder (Zhang et al., 2023a) for its\nbalance of performance and size.\nA detailed study on metadata inclusion will\nbe addressed in future work. Further discussion\non chunk size influence, advanced chunking tech-\nniques, and comparative experiments on different\nembedding models are presented in Appendix A.2.\n3.3 Vector Databases\nVector databases store embedding vectors with\ntheir metadata, enabling efficient retrieval of docu-\nments relevant to queries through various indexing\nand approximate nearest neighbor (ANN) methods.\nTo select an appropriate vector database for our\nresearch, we evaluated several options based on\nfour key criteria: multiple index types, billion-scale\nvector support, hybrid search, and cloud-native ca-\npabilities. These criteria were chosen for their\nimpact on flexibility, scalability, and ease of de-\nployment in modern, cloud-based infrastructures.\nMultiple index types provide the flexibility to opti-\nmize searches based on different data characteris-\ntics and use cases. Billion-scale vector support is\ncrucial for handling large datasets in LLM applica-\ntions. Hybrid search combines vector search with\ntraditional keyword search, enhancing retrieval ac-\ncuracy. Finally, cloud-native capabilities ensure\nseamless integration, scalability, and management\nin cloud environments. Table 6 presents a detailed\ncomparison of five open-source vector databases:\nWeaviate, Faiss, Chroma, Qdrant, and Milvus.\nOur evaluation indicates that Milvus stands out\nas the most comprehensive solution among the\ndatabases evaluated, meeting all the essential crite-\nria and outperforming other open-source options.\n3.4 Retrieval Methods\nGiven a user query, the retrieval module selects the\ntop-k relevant documents from a pre-built corpus\nbased on the similarity between the query and the\ndocuments. The generation model then uses these\ndocuments to formulate an appropriate response\nto the query. However, original queries often un-\nderperform due to poor expression and lack of se-\nmantic information (Gao et al., 2023), negatively\nimpacting the retrieval process. To address these\nissues, we evaluated three query transformation\nhttps://github.com/FlagOpen/FlagEmbedding\n"}
{"page": 4, "bbox": [{"x": 0.11838191747665405, "y": 0.4924306273460388}, {"x": 0.4901844263076782, "y": 0.4932716488838196}, {"x": 0.48899465799331665, "y": 0.9188393354415894}, {"x": 0.11719214916229248, "y": 0.918418824672699}], "text": "• Token-level Chunking is straightforward but\nmay split sentences, affecting retrieval quality.\n• Semantic-level Chunking uses LLMs to deter-\nmine breakpoints, context-preserving but time-\nconsuming.\n• Sentence-level Chunking balances preserving\ntext semantics with simplicity and efficiency.\nIn this study, we use sentence-level chunking, bal-\nancing simplicity and semantic preservation. We\nexamine chunking from four dimensions:\nChunk Size Chunk size significantly impacts\nperformance. Larger chunks provide more context,\nenhancing comprehension but increasing process\ntime. Smaller chunks improve retrieval recall and\nreduce time but may lack sufficient context.\nChunking Techniques Advanced techniques\nsuch as small-to-big and sliding window improve\nretrieval quality by organizing chunk block relation-\nships. Small-sized blocks are used to match queries,\nand larger blocks that include the small ones along\nwith contextual information are returned.\nMetadata Addition Enhancing chunk blocks\nwith metadata like titles, keywords, and hypo-\nthetical questions can improve retrieval, provide\nmore ways to post-process retrieved texts, and help\nLLMs better understand retrieved information.\n"}
{"page": 4, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.939444899559021}, {"x": 0.48007139563560486, "y": 0.939444899559021}], "text": "17719\n"}
{"page": 5, "bbox": [{"x": 0.11897680163383484, "y": 0.08788898587226868}, {"x": 0.48542535305023193, "y": 0.08704794198274612}, {"x": 0.48542535305023193, "y": 0.09798149764537811}, {"x": 0.11897680163383484, "y": 0.09882254153490067}], "text": "methods using the LLM-Embedder recommended\n"}
{"page": 5, "bbox": [{"x": 0.11957168579101562, "y": 0.12573590874671936}, {"x": 0.1249256432056427, "y": 0.12573590874671936}, {"x": 0.1249256432056427, "y": 0.12952060997486115}, {"x": 0.11957168579101562, "y": 0.12952060997486115}], "text": "•\n"}
{"page": 5, "bbox": [{"x": 0.11897680163383484, "y": 0.10386879742145538}, {"x": 0.4878048896789551, "y": 0.1034482792019844}, {"x": 0.4878048896789551, "y": 0.19890664517879486}, {"x": 0.11897680163383484, "y": 0.19932717084884644}], "text": "in Section 3.2 as the query and document encoder:\nQuery Rewriting: Query rewriting refines\nqueries to better match relevant documents.\nInspired by the Rewrite-Retrieve-Read frame-\nwork (Ma et al., 2023a), we prompt an LLM\nto rewrite queries to enhance performance.\n"}
{"page": 5, "bbox": [{"x": 0.11957168579101562, "y": 0.20647603273391724}, {"x": 0.1249256432056427, "y": 0.20647603273391724}, {"x": 0.1249256432056427, "y": 0.20984020829200745}, {"x": 0.11957168579101562, "y": 0.20984020829200745}], "text": "•\n"}
{"page": 5, "bbox": [{"x": 0.5133848786354065, "y": 0.08788898587226868}, {"x": 0.8845925331115723, "y": 0.08830950409173965}, {"x": 0.8839976191520691, "y": 0.3322119414806366}, {"x": 0.5127900242805481, "y": 0.3317914307117462}], "text": "fication, and TILDE Reranking, which focuses\non query likelihoods. These approaches prioritize\nperformance and efficiency, respectively.\n• DLM Reranking: Rerankers utilizing deep\nlanguage models (DLMs) (Ma et al., 2023b;\nNogueira et al., 2020, 2019) are a representa-\ntive method, generally providing the best perfor-\nmance, albeit with reduced efficiency. Models\nare fine-tuned to predict the target tokens \"true\"\nor \"false\" based on the relevancy of the user\nquery and candidate document. The model is\nfine-tuned with the query and document concate-\nnated as input, labeled accordingly. At inference,\ndocuments are then ranked by the probability of\nthe \"true\" token for each query.\n"}
{"page": 5, "bbox": [{"x": 0.5145746469497681, "y": 0.33851975202560425}, {"x": 0.5205234885215759, "y": 0.33851975202560425}, {"x": 0.5205234885215759, "y": 0.34188392758369446}, {"x": 0.5145746469497681, "y": 0.34188392758369446}], "text": "•\n"}
{"page": 5, "bbox": [{"x": 0.5282570123672485, "y": 0.3351556062698364}, {"x": 0.8845925331115723, "y": 0.3355761170387268}, {"x": 0.8839976191520691, "y": 0.5727502107620239}, {"x": 0.5276620984077454, "y": 0.5723296999931335}], "text": "TILDE Reranking: Conventional query likeli-\nhood models (Santos et al., 2020; Zhuang et al.,\n2021) calculate conditional probabilities of query\nterms based on the likelihoods of its preceding\ntokens, but lack efficiency. TILDE (Zhuang and\nZuccon, 2021a,b) instead independently consid-\ners each query term and predicts the probabilities\nof tokens across the entire vocabulary. With the\ncandidate documents preprocessed at indexing,\nrapid reranking can be done by summing the\npre-calculated log probabilities corresponding to\nthe query tokens for each document. TILDEv2\nfurther enhances efficiency and greatly reduces\nindex size by indexing only document-present to-\nkens, using NCE loss, and document expansion.\n"}
{"page": 5, "bbox": [{"x": 0.11719214916229248, "y": 0.20311185717582703}, {"x": 0.48958954215049744, "y": 0.20311185717582703}, {"x": 0.48958954215049744, "y": 0.920941948890686}, {"x": 0.11719214916229248, "y": 0.920941948890686}], "text": "• Query Decomposition: This approach involves\nretrieving documents based on sub-questions de-\nrived from the original query, which is more com-\nplex to comprehend and handle.\n• Pseudo-documents Generation: This approach\ngenerates a hypothetical document based on the\nuser query and uses the embedding of hypotheti-\ncal answers to retrieve similar documents. One\nnotable implement is HyDE (Gao et al., 2022),\nRecent studies, such as Sawarkar et al. (2024),\nindicate that combining lexical-based search with\nvector search significantly enhances performance.\nIn this study, we use BM25 for sparse retrieval and\nContriever(Izacard et al., 2021), an unsupervised\ncontrastive encoder, for dense retrieval, serving as\ntwo robust baselines based on Thakur et al. (2021).\nWe evaluated the performance of different search\nmethods on the TREC DL 2019 and 2020 pas-\nsage ranking datasets. The results presented in\nTable 7 show that supervised methods significantly\noutperformed unsupervised methods. Combining\nwith HyDE and hybrid search, LLM-Embedder\nachieves the highest scores. However, query rewrit-\ning and query decomposition did not enhance re-\ntrieval performance as effectively. Considering the\nbest performance and tolerated latency, we recom-\nmend Hybrid Search with HyDE as the default\nretrieval method. Taking efficiency into consider-\nation, Hybrid Search combines sparse retrieval\n(BM25) and dense retrieval (Original embedding)\nand achieves notable performance with relatively\nlow latency. Additional implementation details and\nexperiments on the HyDE and hyperparameters of\nhybrid search are presented in Appendix A.3.\n3.5 Reranking Methods\nAfter initial retrieval, a reranking phase is em-\nployed to further enhance the relevancy of the re-\ntrieved documents, ensuring that the most pertinent\ninformation appears on top. By leveraging more\nprecise methods, documents are reordered more\neffectively, increasing the similarity between the\nquery and the top-ranked documents.\nWe consider two approaches in our reranking\nmodule: DLM Reranking, which utilizes classi-\n"}
{"page": 5, "bbox": [{"x": 0.5127900242805481, "y": 0.5916736721992493}, {"x": 0.8839976191520691, "y": 0.5916736721992493}, {"x": 0.8839976191520691, "y": 0.9205214381217957}, {"x": 0.5127900242805481, "y": 0.9205214381217957}], "text": "Our experiments were conducted on the MS\nMARCO Passage ranking dataset (Bajaj et al.,\n2016). We followed and made modifications to the\nimplementation provided by PyGaggle (Nogueira\net al., 2020) and TILDE, using the models monoT5,\nmonoBERT, RankLLaMA and TILDEv2. Rerank-\ning results are shown in Table 10. We recommend\nmonoT5 as a comprehensive method balancing\nperformance and efficiency. RankLLAMA is suit-\nable for achieving the best performance, while\nTILDEV2 is ideal for the quickest experience on a\nfixed collection. Details on the experimental setup\nand results are presented in Appendix A.4.\n3.6 Document Repacking\nThe performance of subsequent processes, such\nas LLM response generation, may be affected by\nthe order documents are provided. To address this\nissue, we incorporate a compact repacking mod-\nule into the workflow after reranking, featuring\nthree repacking methods: \"forward”, “reverse\"\n"}
{"page": 5, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17720\n"}
{"page": 6, "bbox": [{"x": 0.11719214916229248, "y": 0.08788898587226868}, {"x": 0.48899465799331665, "y": 0.08788898587226868}, {"x": 0.48899465799331665, "y": 0.27754414081573486}, {"x": 0.11719214916229248, "y": 0.27754414081573486}], "text": "and \"sides\". \"Forward” repacks documents by de-\nscending the relevancy scores from the reranking\nphase, whereas \"reverse\" arranges them in ascend-\ning order. Inspired by (Liu et al., 2024a), which\nconcluded that optimal performance is achieved\nwhen relevant information is placed at the head or\ntail of the input, we also include a \"sides\" option.\nAs the repacking method utilized primarily\naffects subsequent modules, we select the best\nrepacking method in Section 4 by testing it in com-\nbination with other modules. Here, we choose\n\"sides\" as the default repacking method.\n"}
{"page": 6, "bbox": [{"x": 0.5133848786354065, "y": 0.08830950409173965}, {"x": 0.8828078508377075, "y": 0.08830950409173965}, {"x": 0.8828078508377075, "y": 0.28132885694503784}, {"x": 0.5133848786354065, "y": 0.28132885694503784}], "text": "exploration. We aim to investigate the impact of\nfine-tuning, particularly the influence of relevant or\nirrelevant contexts on the generator's performance.\nFormally, we denote x as the query fed into the\nRAG system, and D as the contexts for this input.\nThe fine-tuning loss of the generator is the negative\nlog-likelihood of the ground-truth output y.\nTo explore the impact of fine-tuning, especially\nrelevant and irrelevant contexts, we define dgold as\na context relevant to the query, and drandom as a\nrandomly retrieved context. We train the model by\nvarying the composition of D as follows:\n"}
{"page": 6, "bbox": [{"x": 0.11897680163383484, "y": 0.2893187403678894}, {"x": 0.2813801169395447, "y": 0.2893187403678894}, {"x": 0.2813801169395447, "y": 0.2981497049331665}, {"x": 0.11897680163383484, "y": 0.2981497049331665}], "text": "3.7 Summarization\n"}
{"page": 6, "bbox": [{"x": 0.5145746469497681, "y": 0.30235493183135986}, {"x": 0.5193337202072144, "y": 0.30235493183135986}, {"x": 0.5193337202072144, "y": 0.3052985668182373}, {"x": 0.5145746469497681, "y": 0.3052985668182373}], "text": "•\n"}
{"page": 6, "bbox": [{"x": 0.5163593292236328, "y": 0.29730865359306335}, {"x": 0.8839976191520691, "y": 0.2985702157020569}, {"x": 0.8834027647972107, "y": 0.3629100024700165}, {"x": 0.5157644152641296, "y": 0.36164844036102295}], "text": "Dg: The augmented context consists of query-\nrelevant documents, denoted as Dg = {dgold}.\n• Dr: The context contains one randomly sampled\ndocument, denoted as Dr\n"}
{"page": 6, "bbox": [{"x": 0.5145746469497681, "y": 0.338940292596817}, {"x": 0.5193337202072144, "y": 0.338940292596817}, {"x": 0.5193337202072144, "y": 0.34188392758369446}, {"x": 0.5145746469497681, "y": 0.34188392758369446}], "text": ".\n"}
{"page": 6, "bbox": [{"x": 0.7400357127189636, "y": 0.3519764542579651}, {"x": 0.8233194351196289, "y": 0.3519764542579651}, {"x": 0.8233194351196289, "y": 0.36375105381011963}, {"x": 0.7400357127189636, "y": 0.36375105381011963}], "text": "{drandom}.\n"}
{"page": 6, "bbox": [{"x": 0.7209994196891785, "y": 0.35618165135383606}, {"x": 0.7317073345184326, "y": 0.35618165135383606}, {"x": 0.7317073345184326, "y": 0.35996636748313904}, {"x": 0.7209994196891785, "y": 0.35996636748313904}], "text": "=\n"}
{"page": 6, "bbox": [{"x": 0.5145746469497681, "y": 0.3763667047023773}, {"x": 0.5199286341667175, "y": 0.3763667047023773}, {"x": 0.5199286341667175, "y": 0.37973088026046753}, {"x": 0.5145746469497681, "y": 0.37973088026046753}], "text": "•\n"}
{"page": 6, "bbox": [{"x": 0.5163593292236328, "y": 0.37258198857307434}, {"x": 0.8839976191520691, "y": 0.37216147780418396}, {"x": 0.8839976191520691, "y": 0.4714045524597168}, {"x": 0.5163593292236328, "y": 0.4718250632286072}], "text": "• Dgr: The augmented context comprises a rel-\nevant document and a randomly-selected one,\ndenoted as Dgr = {dgold, drandom}.\n• Dgg: The augmented context consists of two\ncopies of a query-relevant document, denoted as\nDgg = {dgold, dgold}.\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.30950379371643066}, {"x": 0.48899465799331665, "y": 0.30950379371643066}, {"x": 0.48899465799331665, "y": 0.5475189089775085}, {"x": 0.11838191747665405, "y": 0.5475189089775085}], "text": "Retrieval results may contain redundant or unnec-\nessary information, potentially preventing LLMs\nfrom generating accurate responses. Additionally,\nlong prompts can slow down the inference pro-\ncess. Therefore, efficient methods to summarize re-\ntrieved documents are crucial in the RAG pipeline.\nSummarization tasks can be extractive or ab-\nstractive. Extractive methods segment text into\nsentences, then score and rank them based on im-\nportance. Abstractive compressors synthesize in-\nformation from multiple documents to rephrase and\ngenerate a cohesive summary. These tasks can be\nquery-based or non-query-based. In this paper, as\nRAG retrieves information relevant to queries, we\nfocus exclusively on query-based methods.\n"}
{"page": 6, "bbox": [{"x": 0.5145746469497681, "y": 0.42977291345596313}, {"x": 0.5193337202072144, "y": 0.42977291345596313}, {"x": 0.5193337202072144, "y": 0.43313708901405334}, {"x": 0.5145746469497681, "y": 0.43313708901405334}], "text": "•\n"}
{"page": 6, "bbox": [{"x": 0.11957168579101562, "y": 0.5597140192985535}, {"x": 0.1249256432056427, "y": 0.5597140192985535}, {"x": 0.1249256432056427, "y": 0.5630782246589661}, {"x": 0.11957168579101562, "y": 0.5630782246589661}], "text": "•\n"}
{"page": 6, "bbox": [{"x": 0.11897680163383484, "y": 0.6404541730880737}, {"x": 0.12433075904846191, "y": 0.6404541730880737}, {"x": 0.12433075904846191, "y": 0.6438183188438416}, {"x": 0.11897680163383484, "y": 0.6438183188438416}], "text": "•\n"}
{"page": 6, "bbox": [{"x": 0.5133848786354065, "y": 0.4911690354347229}, {"x": 0.8839976191520691, "y": 0.4911690354347229}, {"x": 0.8839976191520691, "y": 0.920941948890686}, {"x": 0.5133848786354065, "y": 0.920941948890686}], "text": "We denote the base LM generator not fine-tuned\nas M, and the model fine-tuned under the cor-\nresponding D as Mg, Mr, Mgr, Mgg. We fine-\ntuned our model on several QA and reading com-\nprehension datasets. Ground-truth coverage is used\nas our evaluation metric since QA task answers\nare relatively short. Specifically, we adopted a\nmore lenient approach to the Exact Match (EM)\nscore, which evaluates the performance based on\nthe presence of the gold response in the model's\noutput. We select Llama-2-7B (Touvron et al.,\n2023b) as the base model. Similar to training,\nwe evaluate all trained models on validation sets\nwith Dg, Dr, Dgr, and Dø, where Do indicates\ninference without retrieval. Figure 3 presents our\nmain results. Models trained with a mix of rele-\nvant and random documents (Mgr) perform best\nwhen provided with either gold or mixed contexts.\nThis suggests that mixing relevant and random\ncontexts during training can enhance the gener-\nator's robustness to irrelevant information while\nensuring effective utilization of relevant contexts.\nTherefore, we identify the practice of augment-\ning with a few relevant and randomly-selected\ndocuments during training as the best approach.\nDetailed dataset information, hyperparameters and\nexperimental results can be found in Appendix A.6.\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.5555088520050049}, {"x": 0.48899465799331665, "y": 0.5559293627738953}, {"x": 0.48839977383613586, "y": 0.8637510538101196}, {"x": 0.11778703331947327, "y": 0.8633305430412292}], "text": "Recomp: Recomp (Xu et al., 2023) has extrac-\ntive and abstractive compressors. The extractive\ncompressor selects useful sentences, while the\nabstractive compressor synthesizes information\nfrom multiple documents.\nLongLLMLingua:\nLongLLMLingua (Jiang\net al., 2023b) improves LLMLingua by focusing\non key information related to the query.\nWe evaluate these methods on three benchmark\ndatasets: NQ, TriviaQA, and HotpotQA. Compara-\ntive results of different summarization methods are\nshown in Table 11. We recommend Recomp for its\noutstanding performance. LongLLMLingua does\nnot perform well but demonstrates better general-\nization capabilities as it was not trained on these\nexperimental datasets. Therefore, we consider it\nas an alternative method. Additional implemen-\ntation details and discussions on non-query-based\nmethods are provided in Appendix A.5.\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.8738435506820679}, {"x": 0.14158238470554352, "y": 0.8742640614509583}, {"x": 0.14158238470554352, "y": 0.8839361071586609}, {"x": 0.11838191747665405, "y": 0.8839361071586609}], "text": "3.8\n"}
{"page": 6, "bbox": [{"x": 0.1600237935781479, "y": 0.8738435506820679}, {"x": 0.33789411187171936, "y": 0.8755256533622742}, {"x": 0.33789411187171936, "y": 0.8860386610031128}, {"x": 0.1600237935781479, "y": 0.8843566179275513}], "text": "Generator Fine-tuning\n"}
{"page": 6, "bbox": [{"x": 0.11897680163383484, "y": 0.8910849690437317}, {"x": 0.48899465799331665, "y": 0.8919259905815125}, {"x": 0.48899465799331665, "y": 0.9217830300331116}, {"x": 0.11897680163383484, "y": 0.920941948890686}], "text": "In this section, we focus on fine-tuning the gener-\nator while leaving retriever fine-tuning for future\n"}
{"page": 6, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5229030251502991, "y": 0.9314550161361694}, {"x": 0.5229030251502991, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17721\n"}
{"page": 7, "bbox": [{"x": 0.5145746469497681, "y": 0.25231286883354187}, {"x": 0.5199286341667175, "y": 0.25231286883354187}, {"x": 0.5199286341667175, "y": 0.2556770443916321}, {"x": 0.5145746469497681, "y": 0.2556770443916321}], "text": "•\n"}
{"page": 7, "bbox": [{"x": 0.1165972650051117, "y": 0.085786372423172}, {"x": 0.4901844263076782, "y": 0.085786372423172}, {"x": 0.48958954215049744, "y": 0.8877207636833191}, {"x": 0.11600238084793091, "y": 0.8877207636833191}], "text": "4 Searching for Best RAG Practices\nIn the following section, we investigate the opti-\nmal practices for implementing RAG. To begin\nwith, we used the default practice identified in Sec-\ntion 3 for each module. Following the workflow\ndepicted in Figure 1, we sequentially optimized\nindividual modules and selected the most effective\noption among alternatives. This iterative process\ncontinued until we determined the best method\nfor implementing the final summarization module.\nBased on Section 3.8, we used the Llama2-7B-Chat\nmodel fine-tuned where each query was augmented\nby a few random-selected and relevant documents\nas the generator. We used Milvus to build a vector\ndatabase that includes 10 million text of English\nWikipedia and 4 million text of medical data. We\nalso investigated the impact of removing the Query\nClassification, Reranking, and Summarization mod-\nules to assess their contributions.\n4.1 Comprehensive Evaluation\nWe conducted extensive experiments across vari-\nous NLP tasks and datasets to assess the perfor-\nmance of RAG systems. Specifically: (I) Com-\nmonsense Reasoning; (II) Fact Checking; (III)\nOpen-Domain QA; (IV) MultiHop QA; (V) Med-\nical QA. For further details on the tasks and their\ncorresponding datasets, please refer to Appendix\nA.7. Furthermore, we evaluated the RAG capa-\nbilities on subsets extracted from these datasets,\nemploying the metrics recommended in RAGAS\n(Shahul et al., 2023), including Faithfulness, Con-\ntext Relevancy, Answer Relevancy, and Answer\nCorrectness. Additionally, we measured Retrieval\nSimilarity by computing the cosine similarity be-\ntween retrieved documents and gold documents.\nWe used accuracy as the evaluation metric for\nthe tasks of Commonsense Reasoning, Fact Check-\ning, and Medical QA. For Open-Domain QA and\nMultihop QA, we employed token-level F1 score\nand Exact Match (EM) score. The final RAG score\nwas calculated by averaging the aforementioned\nfive RAG capabilities. Consistently, the same cor-\npus constructed in Section 3 was used for all tasks.\nWe followed Trivedi et al. (2022) and sub-sampled\nup to 500 examples from each dataset.\n4.2 Results and Analysis\nBased on the experimental results presented in Ta-\nble 1, the following key insights emerge:\n"}
{"page": 7, "bbox": [{"x": 0.5151695609092712, "y": 0.08662741631269455}, {"x": 0.8845925331115723, "y": 0.08620689809322357}, {"x": 0.8863771557807922, "y": 0.9205214381217957}, {"x": 0.5169541835784912, "y": 0.920941948890686}], "text": "ing to an average improvement in the overall\nscore from 0.428 to 0.443 and a reduction in la-\ntency time from 16.41 to 11.58 seconds per query.\nThe query classification method distinguishes be-\ntween queries that require retrieval operations\nand those that do not, based on the completeness\nof information within the queries. This selective\nretrieval strategy avoids unnecessary operations,\nsignificantly enhancing both performance and\nresponse time.\nRetrieval Module: The combination of dense re-\ntrieval and the classical BM25 algorithm demon-\nstrates superior performance due to their com-\nplementary strengths. While dense retrieval ex-\ncels at identifying semantic relationships (e.g.,\nlinking terms like \"bad guy\" and \"villain\"), it\nstruggles with rare terminologies and out-of-\nvocabulary (OOV) words. BM25, however, is\nadept at matching specific terms, compensating\nfor these weaknesses. This hybrid approach bal-\nances the strengths of both methods, enhancing\nretrieval robustness. Moreover, the use of gen-\nerated pseudo-documents minimizes semantic\nmismatches between the query and relevant doc-\numents. While the \"Hybrid with HyDE\" method\nachieved the highest RAG score of 0.58, it came\nat a computational cost of 11.71 seconds per\nquery. In practice, the \"Hybrid\" or \"Original\"\nmethods are recommended, as they maintain\ncomparable performance with reduced latency.\nReranking Module: Reranking is critical to\nmaintaining high-quality results, as demonstrated\nby a performance drop in its absence. Among\nDLM-based rerankers, monoT5 significantly out-\nperformed monoBERT and RankLLAMA. This\nsuperiority can be attributed to monoT5's larger\nparameter set and more extensive training data, as\nwell as its encoder-decoder architecture, which\nprovides enhanced natural language understand-\ning compared to the decoder-only LLAMA model.\nMonoT5's effectiveness in boosting the relevance\nof retrieved documents affirms the necessity of\nreranking in improving the quality of generated\nresponses.\n• Repacking Module: The Reverse configuration\nexhibited superior performance, achieving an\nRAG score of 0.560. This highlights the impor-\ntance of positioning more relevant context closer\nto the query to yield optimal results.\nSummarization Module: The Recomp extrac-\ntive summarization method demonstrated supe-\nrior performance over LongLLMLingua, an ab-\n"}
{"page": 7, "bbox": [{"x": 0.5145746469497681, "y": 0.5740118026733398}, {"x": 0.5199286341667175, "y": 0.5740118026733398}, {"x": 0.5199286341667175, "y": 0.5773759484291077}, {"x": 0.5145746469497681, "y": 0.5773759484291077}], "text": "•\n"}
{"page": 7, "bbox": [{"x": 0.5145746469497681, "y": 0.7994112968444824}, {"x": 0.5199286341667175, "y": 0.7994112968444824}, {"x": 0.5199286341667175, "y": 0.8027754426002502}, {"x": 0.5145746469497681, "y": 0.8027754426002502}], "text": "•\n"}
{"page": 7, "bbox": [{"x": 0.5145746469497681, "y": 0.8797308802604675}, {"x": 0.5193337202072144, "y": 0.8797308802604675}, {"x": 0.5193337202072144, "y": 0.8830950260162354}, {"x": 0.5145746469497681, "y": 0.8830950260162354}], "text": "•\n"}
{"page": 7, "bbox": [{"x": 0.11957168579101562, "y": 0.8957107067108154}, {"x": 0.12433075904846191, "y": 0.8957107067108154}, {"x": 0.12433075904846191, "y": 0.8986543416976929}, {"x": 0.11957168579101562, "y": 0.8986543416976929}], "text": "•\n"}
{"page": 7, "bbox": [{"x": 0.1332540214061737, "y": 0.8910849690437317}, {"x": 0.4878048896789551, "y": 0.8910849690437317}, {"x": 0.4878048896789551, "y": 0.9205214381217957}, {"x": 0.1332540214061737, "y": 0.9205214381217957}], "text": "Query Classification Module: This module is\ncrucial for both effectiveness and efficiency, lead-\n"}
{"page": 7, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17722\n"}
{"page": 8, "bbox": [{"x": 0.6692444682121277, "y": 0.08999159187078476}, {"x": 0.7394407987594604, "y": 0.08957106620073318}, {"x": 0.7394407987594604, "y": 0.09756097197532654}, {"x": 0.6692444682121277, "y": 0.09798149764537811}], "text": "Med RAG\n"}
{"page": 8, "bbox": [{"x": 0.24687686562538147, "y": 0.09041211009025574}, {"x": 0.41998809576034546, "y": 0.09041211009025574}, {"x": 0.41998809576034546, "y": 0.09840201586484909}, {"x": 0.24687686562538147, "y": 0.09840201586484909}], "text": "Commonsense Fact Check\n"}
{"page": 8, "bbox": [{"x": 0.4604402184486389, "y": 0.08999159187078476}, {"x": 0.5050565004348755, "y": 0.09041211009025574}, {"x": 0.5050565004348755, "y": 0.09924305975437164}, {"x": 0.4604402184486389, "y": 0.09882254153490067}], "text": "ODQA\n"}
{"page": 8, "bbox": [{"x": 0.5734681487083435, "y": 0.09083263576030731}, {"x": 0.6335514783859253, "y": 0.09083263576030731}, {"x": 0.6335514783859253, "y": 0.1000841036438942}, {"x": 0.5734681487083435, "y": 0.1000841036438942}], "text": "Multihop\n"}
{"page": 8, "bbox": [{"x": 0.7989292144775391, "y": 0.09083263576030731}, {"x": 0.8268887400627136, "y": 0.09125315397977829}, {"x": 0.8268887400627136, "y": 0.1000841036438942}, {"x": 0.7989292144775391, "y": 0.09966358542442322}], "text": "Avg.\n"}
{"page": 8, "bbox": [{"x": 0.12195122241973877, "y": 0.09714045375585556}, {"x": 0.1725163608789444, "y": 0.09714045375585556}, {"x": 0.1725163608789444, "y": 0.10428931564092636}, {"x": 0.12195122241973877, "y": 0.10428931564092636}], "text": "Method\n"}
{"page": 8, "bbox": [{"x": 0.2813801169395447, "y": 0.11017661541700363}, {"x": 0.3057703673839569, "y": 0.11101765930652618}, {"x": 0.3051754832267761, "y": 0.11774600297212601}, {"x": 0.2807852327823639, "y": 0.11690495908260345}], "text": "Acc\n"}
{"page": 8, "bbox": [{"x": 0.3706127405166626, "y": 0.11101765930652618}, {"x": 0.39500296115875244, "y": 0.11143818497657776}, {"x": 0.39500296115875244, "y": 0.11774600297212601}, {"x": 0.3706127405166626, "y": 0.11732548475265503}], "text": "Acc\n"}
{"page": 8, "bbox": [{"x": 0.6270077228546143, "y": 0.1105971410870552}, {"x": 0.8774539232254028, "y": 0.1105971410870552}, {"x": 0.8774539232254028, "y": 0.11942809075117111}, {"x": 0.6270077228546143, "y": 0.11942809075117111}], "text": "Score Acc Score Score F1 Latency\n"}
{"page": 8, "bbox": [{"x": 0.4009518027305603, "y": 0.10975609719753265}, {"x": 0.6103509664535522, "y": 0.10975609719753265}, {"x": 0.6103509664535522, "y": 0.13540790975093842}, {"x": 0.4009518027305603, "y": 0.13540790975093842}], "text": "EM F1 Score EM F1\nwithout retrieval\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.14129520952701569}, {"x": 0.1856038123369217, "y": 0.1408746838569641}, {"x": 0.1856038123369217, "y": 0.1480235457420349}, {"x": 0.12254610657691956, "y": 0.1484440714120865}], "text": "+ baseline\n"}
{"page": 8, "bbox": [{"x": 0.36466389894485474, "y": 0.1408746838569641}, {"x": 0.39916715025901794, "y": 0.1408746838569641}, {"x": 0.39916715025901794, "y": 0.1484440714120865}, {"x": 0.36466389894485474, "y": 0.1484440714120865}], "text": "0.560\n"}
{"page": 8, "bbox": [{"x": 0.748958945274353, "y": 0.14129520952701569}, {"x": 0.8215348124504089, "y": 0.14129520952701569}, {"x": 0.8215348124504089, "y": 0.1480235457420349}, {"x": 0.748958945274353, "y": 0.1480235457420349}], "text": "0.351 0.292\n"}
{"page": 8, "bbox": [{"x": 0.8411659598350525, "y": 0.1408746838569641}, {"x": 0.8655562400817871, "y": 0.1408746838569641}, {"x": 0.8655562400817871, "y": 0.1484440714120865}, {"x": 0.8411659598350525, "y": 0.1484440714120865}], "text": "1.27\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.14129520952701569}, {"x": 0.3099345564842224, "y": 0.14129520952701569}, {"x": 0.3099345564842224, "y": 0.1484440714120865}, {"x": 0.27602618932724, "y": 0.1484440714120865}], "text": "0.537\n"}
{"page": 8, "bbox": [{"x": 0.24628198146820068, "y": 0.14129520952701569}, {"x": 0.7013682126998901, "y": 0.14129520952701569}, {"x": 0.7013682126998901, "y": 0.17031118273735046}, {"x": 0.24628198146820068, "y": 0.17031118273735046}], "text": "0.373 0.413 0.428 0.167 0.173 0.182 0.360\nclassification module, Hybrid with HyDE, monoT5, sides, Recomp\n"}
{"page": 8, "bbox": [{"x": 0.12195122241973877, "y": 0.17703953385353088}, {"x": 0.22665080428123474, "y": 0.17703953385353088}, {"x": 0.22665080428123474, "y": 0.18418839573860168}, {"x": 0.12195122241973877, "y": 0.18418839573860168}], "text": "w/o classification\n"}
{"page": 8, "bbox": [{"x": 0.3652587831020355, "y": 0.1766190081834793}, {"x": 0.8697203993797302, "y": 0.17703953385353088}, {"x": 0.8697203993797302, "y": 0.19764508306980133}, {"x": 0.3652587831020355, "y": 0.19722455739974976}], "text": "0.505 0.391 0.450 0.478 0.212 0.255 0.254 0.528 0.540 0.422 0.353 16.58\n0.595 0.393 0.450 0.479 0.207 0.257 0.254 0.460 0.580 0.443 0.353 11.71\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.17703953385353088}, {"x": 0.311124324798584, "y": 0.17746004462242126}, {"x": 0.3105294406414032, "y": 0.19764508306980133}, {"x": 0.2754313051700592, "y": 0.19722455739974976}], "text": "0.719\n0.727\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.18965516984462738}, {"x": 0.2159428894519806, "y": 0.18965516984462738}, {"x": 0.2159428894519806, "y": 0.19722455739974976}, {"x": 0.12254610657691956, "y": 0.19722455739974976}], "text": "+ classification\n"}
{"page": 8, "bbox": [{"x": 0.25639501214027405, "y": 0.2089991569519043}, {"x": 0.645449161529541, "y": 0.2089991569519043}, {"x": 0.645449161529541, "y": 0.21909166872501373}, {"x": 0.25639501214027405, "y": 0.21909166872501373}], "text": "with classification, retrieval module, monoT5, sides, Recomp\n"}
{"page": 8, "bbox": [{"x": 0.3652587831020355, "y": 0.2232968807220459}, {"x": 0.39916715025901794, "y": 0.22287636995315552}, {"x": 0.39916715025901794, "y": 0.23002523183822632}, {"x": 0.3652587831020355, "y": 0.2304457575082779}], "text": "0.595\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.22371740639209747}, {"x": 0.30874478816986084, "y": 0.22371740639209747}, {"x": 0.30874478816986084, "y": 0.23002523183822632}, {"x": 0.27602618932724, "y": 0.23002523183822632}], "text": "0.718\n"}
{"page": 8, "bbox": [{"x": 0.8358120322227478, "y": 0.22287636995315552}, {"x": 0.8679357767105103, "y": 0.22287636995315552}, {"x": 0.8679357767105103, "y": 0.23086626827716827}, {"x": 0.8358120322227478, "y": 0.23086626827716827}], "text": "11.58\n"}
{"page": 8, "bbox": [{"x": 0.12195122241973877, "y": 0.22371740639209747}, {"x": 0.1725163608789444, "y": 0.22371740639209747}, {"x": 0.1725163608789444, "y": 0.232127845287323}, {"x": 0.12195122241973877, "y": 0.232127845287323}], "text": "+ HYDE\n"}
{"page": 8, "bbox": [{"x": 0.8405711054801941, "y": 0.2359125316143036}, {"x": 0.8655562400817871, "y": 0.23549200594425201}, {"x": 0.8655562400817871, "y": 0.2430613934993744}, {"x": 0.8405711054801941, "y": 0.24348191916942596}], "text": "1.44\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.23675356805324554}, {"x": 0.3093396723270416, "y": 0.23675356805324554}, {"x": 0.3093396723270416, "y": 0.2430613934993744}, {"x": 0.27602618932724, "y": 0.2430613934993744}], "text": "0.721\n"}
{"page": 8, "bbox": [{"x": 0.3652587831020355, "y": 0.23633305728435516}, {"x": 0.39857226610183716, "y": 0.23633305728435516}, {"x": 0.39857226610183716, "y": 0.24348191916942596}, {"x": 0.3652587831020355, "y": 0.24348191916942596}], "text": "0.585\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.23633305728435516}, {"x": 0.1850089281797409, "y": 0.23633305728435516}, {"x": 0.1850089281797409, "y": 0.2447434812784195}, {"x": 0.12254610657691956, "y": 0.2447434812784195}], "text": "+ Original\n"}
{"page": 8, "bbox": [{"x": 0.42474716901779175, "y": 0.22287636995315552}, {"x": 0.8233194351196289, "y": 0.22287636995315552}, {"x": 0.8233194351196289, "y": 0.2699747681617737}, {"x": 0.42474716901779175, "y": 0.2699747681617737}], "text": "0.320 0.373 0.380 0.170 0.213 0.222 0.400 0.545 0.398 0.293\n0.300 0.350 0.363 0.153 0.197 0.206 0.390 0.486 0.383 0.273\n0.347 0.397 0.418 0.190 0.240 0.233 0.750 0.498 0.429 0.318\n0.393 0.450 0.479 0.207 0.257 0.254 0.460 0.580 0.443 0.353\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.24936921894550323}, {"x": 0.30874478816986084, "y": 0.24894869327545166}, {"x": 0.30874478816986084, "y": 0.25609755516052246}, {"x": 0.27602618932724, "y": 0.25651809573173523}], "text": "0.718\n"}
{"page": 8, "bbox": [{"x": 0.8411659598350525, "y": 0.24936921894550323}, {"x": 0.8655562400817871, "y": 0.24894869327545166}, {"x": 0.8655562400817871, "y": 0.25609755516052246}, {"x": 0.8411659598350525, "y": 0.25651809573173523}], "text": "1.45\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.24894869327545166}, {"x": 0.17727543413639069, "y": 0.24936921894550323}, {"x": 0.17727543413639069, "y": 0.2590412199497223}, {"x": 0.12254610657691956, "y": 0.2586206793785095}], "text": "+ Hybrid\n"}
{"page": 8, "bbox": [{"x": 0.3652587831020355, "y": 0.24894869327545166}, {"x": 0.39916715025901794, "y": 0.24852816760540009}, {"x": 0.39916715025901794, "y": 0.26913371682167053}, {"x": 0.3652587831020355, "y": 0.2695542573928833}], "text": "0.595\n0.595\n"}
{"page": 8, "bbox": [{"x": 0.8375966548919678, "y": 0.26198485493659973}, {"x": 0.8697203993797302, "y": 0.26198485493659973}, {"x": 0.8697203993797302, "y": 0.2699747681617737}, {"x": 0.8375966548919678, "y": 0.2699747681617737}], "text": "11.71\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.2624053955078125}, {"x": 0.3099345564842224, "y": 0.2624053955078125}, {"x": 0.3099345564842224, "y": 0.2695542573928833}, {"x": 0.27602618932724, "y": 0.2695542573928833}], "text": "0.727\n"}
{"page": 8, "bbox": [{"x": 0.12195122241973877, "y": 0.2624053955078125}, {"x": 0.2373587191104889, "y": 0.2624053955078125}, {"x": 0.2373587191104889, "y": 0.2712363302707672}, {"x": 0.12195122241973877, "y": 0.2712363302707672}], "text": "+ Hybrid + HyDE\n"}
{"page": 8, "bbox": [{"x": 0.22248661518096924, "y": 0.28343144059181213}, {"x": 0.6799523830413818, "y": 0.28343144059181213}, {"x": 0.6799523830413818, "y": 0.2931034564971924}, {"x": 0.22248661518096924, "y": 0.2931034564971924}], "text": "with classification, Hybrid with HyDE, reranking module, sides, Recomp\n"}
{"page": 8, "bbox": [{"x": 0.8370018005371094, "y": 0.2985702157020569}, {"x": 0.8703153133392334, "y": 0.2985702157020569}, {"x": 0.8703153133392334, "y": 0.30656012892723083}, {"x": 0.8370018005371094, "y": 0.30656012892723083}], "text": "10.31\n"}
{"page": 8, "bbox": [{"x": 0.3652587831020355, "y": 0.2985702157020569}, {"x": 0.3973824977874756, "y": 0.29899075627326965}, {"x": 0.3967876136302948, "y": 0.30656012892723083}, {"x": 0.3652587831020355, "y": 0.30613961815834045}], "text": "0.591\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.29899075627326965}, {"x": 0.3099345564842224, "y": 0.29899075627326965}, {"x": 0.3099345564842224, "y": 0.30613961815834045}, {"x": 0.27602618932724, "y": 0.30613961815834045}], "text": "0.720\n"}
{"page": 8, "bbox": [{"x": 0.12195122241973877, "y": 0.2977291941642761}, {"x": 0.206424742937088, "y": 0.29941126704216003}, {"x": 0.20582985877990723, "y": 0.30782169103622437}, {"x": 0.12135633826255798, "y": 0.30613961815834045}], "text": "w/o reranking\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.3120269179344177}, {"x": 0.3099345564842224, "y": 0.31160637736320496}, {"x": 0.3099345564842224, "y": 0.31875526905059814}, {"x": 0.27602618932724, "y": 0.3191757798194885}], "text": "0.727\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.3124474287033081}, {"x": 0.18798334896564484, "y": 0.3111858665943146}, {"x": 0.18798334896564484, "y": 0.31875526905059814}, {"x": 0.12254610657691956, "y": 0.3200168311595917}], "text": "+ monoT5\n"}
{"page": 8, "bbox": [{"x": 0.12314099073410034, "y": 0.3246425688266754}, {"x": 0.20523497462272644, "y": 0.32422202825546265}, {"x": 0.20523497462272644, "y": 0.3317914307117462}, {"x": 0.12314099073410034, "y": 0.3322119414806366}], "text": "+ monoBERT\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.3254835903644562}, {"x": 0.3099345564842224, "y": 0.3254835903644562}, {"x": 0.3099345564842224, "y": 0.3322119414806366}, {"x": 0.27602618932724, "y": 0.3322119414806366}], "text": "0.723\n"}
{"page": 8, "bbox": [{"x": 0.3652587831020355, "y": 0.31160637736320496}, {"x": 0.39916715025901794, "y": 0.31160637736320496}, {"x": 0.39916715025901794, "y": 0.35786375403404236}, {"x": 0.3652587831020355, "y": 0.35786375403404236}], "text": "0.595\n0.593\n0.597\n0.588\n"}
{"page": 8, "bbox": [{"x": 0.2111838161945343, "y": 0.2985702157020569}, {"x": 0.8703153133392334, "y": 0.2985702157020569}, {"x": 0.8703153133392334, "y": 0.37973088026046753}, {"x": 0.2111838161945343, "y": 0.37973088026046753}], "text": "0.365 0.429 0.435 0.211 0.260 0.253 0.512 0.530 0.430 0.334\n0.393 0.450 0.479 0.207 0.257 0.253 0.460 0.580 0.443 0.353 11.71\n0.383 0.443 0.463 0.217 0.259 0.253 0.482 0.551 0.438 0.351 11.65\n0.382 0.443 0.459 0.197 0.240 0.237 0.454 0.558 0.431 0.342 13.51\n0.394 0.456 0.473 0.209 0.255 0.249 0.486 0.536 0.440 0.355 11.26\nwith classification, Hybrid with HyDE, monoT5, repacking module, Recomp\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.3376787304878235}, {"x": 0.2153480052947998, "y": 0.3372581899166107}, {"x": 0.2153480052947998, "y": 0.3448275923728943}, {"x": 0.12254610657691956, "y": 0.34524810314178467}], "text": "+ RankLLaMA\n"}
{"page": 8, "bbox": [{"x": 0.2754313051700592, "y": 0.33851975202560425}, {"x": 0.3099345564842224, "y": 0.33851975202560425}, {"x": 0.3099345564842224, "y": 0.35828426480293274}, {"x": 0.2754313051700592, "y": 0.35828426480293274}], "text": "0.723\n0.725\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.35071489214897156}, {"x": 0.19214753806591034, "y": 0.3502943515777588}, {"x": 0.19214753806591034, "y": 0.35828426480293274}, {"x": 0.12254610657691956, "y": 0.3587048053741455}], "text": "+ TILDEV2\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.3856181800365448}, {"x": 0.1635930985212326, "y": 0.38519763946533203}, {"x": 0.1635930985212326, "y": 0.39234650135040283}, {"x": 0.12254610657691956, "y": 0.3927670419216156}], "text": "+ sides\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.3860386908054352}, {"x": 0.3099345564842224, "y": 0.3860386908054352}, {"x": 0.3099345564842224, "y": 0.4053826630115509}, {"x": 0.27602618932724, "y": 0.4053826630115509}], "text": "0.727\n0.722\n"}
{"page": 8, "bbox": [{"x": 0.36466389894485474, "y": 0.3856181800365448}, {"x": 0.39916715025901794, "y": 0.38519763946533203}, {"x": 0.39976203441619873, "y": 0.418418824672699}, {"x": 0.3652587831020355, "y": 0.41883936524391174}], "text": "0.595\n0.599\n0.592\n"}
{"page": 8, "bbox": [{"x": 0.42474716901779175, "y": 0.38519763946533203}, {"x": 0.8703153133392334, "y": 0.38519763946533203}, {"x": 0.8703153133392334, "y": 0.41883936524391174}, {"x": 0.42474716901779175, "y": 0.41883936524391174}], "text": "0.393 0.450 0.479 0.207 0.257 0.253 0.460 0.580 0.443 0.353 11.71\n0.379 0.437 0.458 0.215 0.260 0.254 0.472 0.542 0.437 0.349 11.68\n0.387 0.445 0.473 0.219 0.263 0.260 0.532 0.560 0.446 0.354 11.70\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.39907485246658325}, {"x": 0.18203450739383698, "y": 0.39907485246658325}, {"x": 0.18203450739383698, "y": 0.40580320358276367}, {"x": 0.12254610657691956, "y": 0.40580320358276367}], "text": "+ forward\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.41169050335884094}, {"x": 0.3099345564842224, "y": 0.41169050335884094}, {"x": 0.3099345564842224, "y": 0.41883936524391174}, {"x": 0.27602618932724, "y": 0.41883936524391174}], "text": "0.728\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.41337257623672485}, {"x": 0.18024985492229462, "y": 0.41337257623672485}, {"x": 0.18024985492229462, "y": 0.4192598760128021}, {"x": 0.12254610657691956, "y": 0.4192598760128021}], "text": "+ reverse\n"}
{"page": 8, "bbox": [{"x": 0.12195122241973877, "y": 0.43103447556495667}, {"x": 0.6942296028137207, "y": 0.43103447556495667}, {"x": 0.6942296028137207, "y": 0.45248109102249146}, {"x": 0.12195122241973877, "y": 0.45248109102249146}], "text": "with classification, Hybrid with HyDE, monoT5, reverse, summarization module\nw/o summarization\n"}
{"page": 8, "bbox": [{"x": 0.27602618932724, "y": 0.44533219933509827}, {"x": 0.3099345564842224, "y": 0.44533219933509827}, {"x": 0.3099345564842224, "y": 0.47939443588256836}, {"x": 0.27602618932724, "y": 0.47939443588256836}], "text": "0.729\n0.728\n0.713\n"}
{"page": 8, "bbox": [{"x": 0.3652587831020355, "y": 0.44533219933509827}, {"x": 0.39916715025901794, "y": 0.44533219933509827}, {"x": 0.39916715025901794, "y": 0.47939443588256836}, {"x": 0.3652587831020355, "y": 0.47939443588256836}], "text": "0.591\n0.592\n0.581\n"}
{"page": 8, "bbox": [{"x": 0.42474716901779175, "y": 0.44533219933509827}, {"x": 0.8709101676940918, "y": 0.44533219933509827}, {"x": 0.8709101676940918, "y": 0.47939443588256836}, {"x": 0.42474716901779175, "y": 0.47939443588256836}], "text": "0.402 0.457 0.468 0.205 0.252 0.245 0.528 0.533 0.441 0.355 10.97\n0.387 0.445 0.473 0.219 0.263 0.260 0.532 0.560 0.446 0.354 11.70\n0.362 0.423 0.432 0.199 0.245 0.245 0.530 0.539 0.426 0.334 16.17\n"}
{"page": 8, "bbox": [{"x": 0.12254610657691956, "y": 0.45794785022735596}, {"x": 0.18738846480846405, "y": 0.4587889015674591}, {"x": 0.18738846480846405, "y": 0.4680403769016266}, {"x": 0.12254610657691956, "y": 0.46719932556152344}], "text": "+ Recomp\n"}
{"page": 8, "bbox": [{"x": 0.12195122241973877, "y": 0.4718250632286072}, {"x": 0.24033313989639282, "y": 0.4714045524597168}, {"x": 0.24033313989639282, "y": 0.48107653856277466}, {"x": 0.12195122241973877, "y": 0.48149704933166504}], "text": "+ LongLLMLingua\n"}
{"page": 8, "bbox": [{"x": 0.11778703331947327, "y": 0.4962153136730194}, {"x": 0.8822129964828491, "y": 0.49537426233291626}, {"x": 0.8822129964828491, "y": 0.5643397569656372}, {"x": 0.11778703331947327, "y": 0.5651808381080627}], "text": "Table 1: Results of the search for optimal RAG practices. Modules enclosed in a boxed module are under\ninvestigation to determine the best method. The underlined method represents the selected implementation. For\nthe two QA tasks, ODQA and MultiHop, we use GPT to score them simultaneously. The \"Avg\" (average score) is\ncalculated based on the Acc, EM, and RAG scores for all tasks, while the average latency is measured in seconds\nper query. The best scores are highlighted in bold.\n"}
{"page": 8, "bbox": [{"x": 0.5127900242805481, "y": 0.6047098636627197}, {"x": 0.6347412467002869, "y": 0.6047098636627197}, {"x": 0.6347412467002869, "y": 0.6143818497657776}, {"x": 0.5127900242805481, "y": 0.6143818497657776}], "text": "5 Discussion\n"}
{"page": 8, "bbox": [{"x": 0.13265913724899292, "y": 0.5723296999931335}, {"x": 0.8697203993797302, "y": 0.5731707215309143}, {"x": 0.8691255450248718, "y": 0.7796467542648315}, {"x": 0.13206425309181213, "y": 0.7788057327270508}], "text": "stractive summarization method. Our experi- ing high-quality responses across different tasks.\nments revealed that LongLLMLingua occasion-\nally distorts semantics and produces incoherent\ncontent due to its rewriting approach. Recomp,\non the other hand, preserves the integrity of the\noriginal content, making it better suited for RAG\napplications. Although comparable results can\nbe achieved with lower latency by removing the\nsummarization module, Recomp remains the pre-\nferred choice for scenarios where addressing the\ngenerator's maximum length constraint is crucial.\nIn time-sensitive applications, removing summa-\nrization could effectively reduce response time.\n"}
{"page": 8, "bbox": [{"x": 0.5121951103210449, "y": 0.6248948574066162}, {"x": 0.8839976191520691, "y": 0.6253153681755066}, {"x": 0.8834027647972107, "y": 0.920941948890686}, {"x": 0.5116002559661865, "y": 0.9205214381217957}], "text": "5.1 Best Practices for Implementing RAG\nAccording to our experimental findings, we suggest\ntwo distinct recipes or practices for implementing\nRAG systems, each customized to address specific\nrequirements: one focusing on maximizing perfor-\nmance, and the other on striking a balance between\nefficiency and efficacy.\nBest Performance Practice: To achieve the high-\nest performance, it is recommended to incorporate\nquery classification module, use the “Hybrid with\nHYDE\" method for retrieval, employ monoT5 for\nreranking, opt for Reverse for repacking, and lever-\nage Recomp for summarization. This configuration\nyielded the highest average score of 0.483, albeit\nwith a computationally-intensive process.\nBalanced Efficiency Practice: To achieve a bal-\nance between performance and efficiency, it is rec-\nommended to incorporate the query classification\n"}
{"page": 8, "bbox": [{"x": 0.11778703331947327, "y": 0.7956265807151794}, {"x": 0.48899465799331665, "y": 0.7960470914840698}, {"x": 0.48899465799331665, "y": 0.9213625192642212}, {"x": 0.11778703331947327, "y": 0.920941948890686}], "text": "The experimental results demonstrate that each\nmodule contributes uniquely to the overall perfor-\nmance of the RAG system. The query classifica-\ntion module enhances accuracy and reduces latency,\nwhile the retrieval and reranking modules signif-\nicantly improve the system's ability to handle di-\nverse queries. The repacking and summarization\nmodules further refine the system's output, ensur-\n"}
{"page": 8, "bbox": [{"x": 0.48007139563560486, "y": 0.931034505367279}, {"x": 0.5240927934646606, "y": 0.931034505367279}, {"x": 0.5240927934646606, "y": 0.939444899559021}, {"x": 0.48007139563560486, "y": 0.939444899559021}], "text": "17723\n"}
{"page": 9, "bbox": [{"x": 0.5145746469497681, "y": 0.09125315397977829}, {"x": 0.5205234885215759, "y": 0.09125315397977829}, {"x": 0.5205234885215759, "y": 0.0946173220872879}, {"x": 0.5145746469497681, "y": 0.0946173220872879}], "text": "•\n"}
{"page": 9, "bbox": [{"x": 0.1165972650051117, "y": 0.08662741631269455}, {"x": 0.48958954215049744, "y": 0.08662741631269455}, {"x": 0.48958954215049744, "y": 0.38856181502342224}, {"x": 0.1165972650051117, "y": 0.38856181502342224}], "text": "module, implement the Hybrid method for retrieval,\nuse TILDEV2 for reranking, opt for Reverse for\nrepacking, and employ Recomp for summarization.\nGiven that the retrieval module accounts for the\nmajority of processing time in the system, transi-\ntioning to the Hybrid method while keeping other\nmodules unchanged can substantially reduce la-\ntency while preserving a comparable performance.\n5.2 Generalization of Best Practices\nWhile the above best practices demonstrate strong\nperformance in our experiments, we acknowledge\nthat they may not be universally optimal across\nall tasks and contexts. Therefore, we emphasize\nthe importance of the comprehensive evaluation\nframework, which assesses system performance\nacross general, domain-specific, and task-specific\ncapabilities, and the three-step strategy to identify\nthe most effective practices:\n"}
{"page": 9, "bbox": [{"x": 0.5139797925949097, "y": 0.08704794198274612}, {"x": 0.8851873874664307, "y": 0.0874684602022171}, {"x": 0.8834027647972107, "y": 0.6337258219718933}, {"x": 0.5121951103210449, "y": 0.6333053112030029}], "text": "Efficiency: Retrieval methods are typically more\nefficient, especially when the answer already\nexists in stored materials. Conversely, genera-\ntion methods may require more computational\nresources to produce new content, particularly\nfor images or lengthy texts.\nMaintainability: Generation models often ne-\ncessitate careful fine-tuning to tailor them for new\napplications. In contrast, retrieval-based methods\ncan be improved to address new demands by sim-\nply enlarging the size and enhancing the quality\nof retrieval sources.\nWe adopted the experimental setup from (Koh\net al., 2024). Specifically, we used the PartiPrompts\ndataset to prompt the stable diffusion model to gen-\nerate images and to retrieve images from the CC3M\ndataset. We then use the openai/clip-vit-large-\npatch 142 to compute CLIP Similarity between the\nprompts and both types of images (PRO2GEN and\nPRO2RET) and compute consumed time in both\nmethods. The figure 5 represents Groundedness of\nthe \"retrieval as generation\" strategy, as the genera-\ntion model is uncontrollable and may lack relevant\nknowledge. As demonstrated in Table 15, the \"re-\ntrieval as generation” strategy greatly reduces time\nconsumption while maintaining the quality of the\nimages and we can improve the performance of\nretrieval by expanding the search sources which\ndemonstrates the Efficiency and Maintainability\nof this strategy.\nFurthermore, we plan to broaden the application\nof this strategy to include other modalities, such\nas video and speech, while also exploring efficient\nand effective cross-modal retrieval techniques.\n"}
{"page": 9, "bbox": [{"x": 0.11957168579101562, "y": 0.4049621522426605}, {"x": 0.12433075904846191, "y": 0.4049621522426605}, {"x": 0.12433075904846191, "y": 0.40832632780075073}, {"x": 0.11957168579101562, "y": 0.40832632780075073}], "text": "•\n"}
{"page": 9, "bbox": [{"x": 0.11957168579101562, "y": 0.4693019390106201}, {"x": 0.1249256432056427, "y": 0.4693019390106201}, {"x": 0.1249256432056427, "y": 0.4726661145687103}, {"x": 0.11957168579101562, "y": 0.4726661145687103}], "text": "•\n"}
{"page": 9, "bbox": [{"x": 0.11838191747665405, "y": 0.40117746591567993}, {"x": 0.48958954215049744, "y": 0.40117746591567993}, {"x": 0.48958954215049744, "y": 0.575273334980011}, {"x": 0.11838191747665405, "y": 0.575273334980011}], "text": "Empirical Comparison of Candidate Imple-\nmentations: For each module, we compare mul-\ntiple candidate methods to determine the best-\nperforming options.\nModule Integration: After selecting the optimal\nmethod for each module, we evaluate how they\ninteract when integrated into the full workflow.\n• Evaluation of Module Combinations: Finally,\nwe assess the performance of different module\ncombinations to identify opportunities for im-\nproving system efficiency and effectiveness.\n"}
{"page": 9, "bbox": [{"x": 0.11897680163383484, "y": 0.5878889560699463}, {"x": 0.1421772688627243, "y": 0.5878889560699463}, {"x": 0.1421772688627243, "y": 0.5975610017776489}, {"x": 0.11897680163383484, "y": 0.5975610017776489}], "text": "5.3\n"}
{"page": 9, "bbox": [{"x": 0.1600237935781479, "y": 0.5878889560699463}, {"x": 0.33372992277145386, "y": 0.5878889560699463}, {"x": 0.33372992277145386, "y": 0.5975610017776489}, {"x": 0.1600237935781479, "y": 0.5975610017776489}], "text": "Multimodal Extension\n"}
{"page": 9, "bbox": [{"x": 0.5145746469497681, "y": 0.6488645672798157}, {"x": 0.6406900882720947, "y": 0.6488645672798157}, {"x": 0.6406900882720947, "y": 0.6585366129875183}, {"x": 0.5145746469497681, "y": 0.6585366129875183}], "text": "6 Conclusion\n"}
{"page": 9, "bbox": [{"x": 0.1165972650051117, "y": 0.6072329878807068}, {"x": 0.48839977383613586, "y": 0.6072329878807068}, {"x": 0.48839977383613586, "y": 0.918418824672699}, {"x": 0.1165972650051117, "y": 0.918418824672699}], "text": "We have extended RAG to multimodal applications.\nSpecifically, we have incorporated text2image and\nimage2text retrieval capabilities into the system\nwith a substantial collection of paired image and\ntextual descriptions as a retrieval source. As de-\npicted in Figure 4, the text2image capability speeds\nup the image generation process when a user query\naligns well with the textual descriptions of stored\nimages (i.e., \"retrieval as generation\" strategy),\nwhile the image2text functionality comes into play\nwhen a user provides an image and engages in con-\nversation about the input image. These multimodal\nRAG capabilities offer the following advantages:\n⚫ Groundedness: Retrieval methods provide in-\nformation from verified multimodal materials,\nthereby ensuring authenticity and specificity. In\ncontrast, on-the-fly generation relies on models\nto generate new content, which can occasionally\nresult in factual errors or inaccuracies.\n"}
{"page": 9, "bbox": [{"x": 0.5127900242805481, "y": 0.675357460975647}, {"x": 0.8845925331115723, "y": 0.675357460975647}, {"x": 0.8845925331115723, "y": 0.9205214381217957}, {"x": 0.5127900242805481, "y": 0.9205214381217957}], "text": "In this study, we aim to identify optimal practices\nfor implementing retrieval-augmented generation\nin order to improve the quality and reliability of\ncontent produced by large language models. We\nsystematically assessed a range of potential solu-\ntions for each module within the RAG framework\nand recommended the most effective approach for\neach module. Furthermore, we introduced a com-\nprehensive evaluation benchmark for RAG systems\nand conducted extensive experiments to determine\nthe best practices among various alternatives. Our\nfindings not only contribute to a deeper understand-\ning of retrieval-augmented generation systems but\nalso establish a foundation for future research.\n2https://huggingface.co/openai/clip-vit-large-patch14\n"}
{"page": 9, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.939444899559021}, {"x": 0.48007139563560486, "y": 0.939444899559021}], "text": "17724\n"}
{"page": 10, "bbox": [{"x": 0.11897680163383484, "y": 0.0874684602022171}, {"x": 0.21832242608070374, "y": 0.0874684602022171}, {"x": 0.21832242608070374, "y": 0.09714045375585556}, {"x": 0.11897680163383484, "y": 0.09714045375585556}], "text": "Limitations\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.08873002231121063}, {"x": 0.8839976191520691, "y": 0.08830950409173965}, {"x": 0.8839976191520691, "y": 0.13708999752998352}, {"x": 0.5139797925949097, "y": 0.1375105082988739}], "text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz,\nDaniel Fernando Campos, and Ellen M. Voorhees.\n2020. Overview of the trec 2019 deep learning track.\nArXiv, abs/2003.07820.\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.1555929332971573}, {"x": 0.8839976191520691, "y": 0.1555929332971573}, {"x": 0.8839976191520691, "y": 0.2035323828458786}, {"x": 0.5133848786354065, "y": 0.2035323828458786}], "text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz,\nDaniel Fernando Campos, and Ellen M. Voorhees.\n2021. Overview of the trec 2020 deep learning track.\nArXiv, abs/2102.07662.\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.2211942821741104}, {"x": 0.8834027647972107, "y": 0.22035323083400726}, {"x": 0.8834027647972107, "y": 0.25777965784072876}, {"x": 0.5133848786354065, "y": 0.2586206793785095}], "text": "Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022. Precise zero-shot dense retrieval without rele-\nvance labels. arXiv preprint arXiv:2212.10496.\n"}
{"page": 10, "bbox": [{"x": 0.11719214916229248, "y": 0.11227922886610031}, {"x": 0.48958954215049744, "y": 0.11269974708557129}, {"x": 0.48899465799331665, "y": 0.4301934540271759}, {"x": 0.1165972650051117, "y": 0.42977291345596313}], "text": "We have evaluated the impact of various methods\nfor fine-tuning LLM generators. Previous stud-\nies have demonstrated the feasibility of training\nboth the retriever and generator jointly. We would\nlike to explore this possibility in the future. In\nthis study, we embraced the principle of modular\ndesign to simplify the search for optimal RAG im-\nplementations, thereby reducing complexity. Due\nto the daunting costs associated with constructing\nvector databases and conducting experiments, our\nevaluation was limited to investigating the effec-\ntiveness and influence of representative chunking\ntechniques within the chunking module. It would\nbe intriguing to further explore the impact of differ-\nent chunking techniques on the entire RAG systems.\nWhile we have discussed the application of RAG\nin the domain of NLP and extended its scope to\nimage generation, an enticing avenue for future ex-\nploration would involve expanding this research to\nother modalities such as speech and video.\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.2750210165977478}, {"x": 0.8822129964828491, "y": 0.2750210165977478}, {"x": 0.8822129964828491, "y": 0.3351556062698364}, {"x": 0.5139797925949097, "y": 0.3351556062698364}], "text": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.35407906770706177}, {"x": 0.8834027647972107, "y": 0.35407906770706177}, {"x": 0.8834027647972107, "y": 0.4301934540271759}, {"x": 0.5133848786354065, "y": 0.4301934540271759}], "text": "Michael Günther, Jackmin Ong, Isabelle Mohr, Alaed-\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba\nSturua, Bo Wang, et al. 2023. Jina embeddings 2:\n8192-token general-purpose text embeddings for long\ndocuments. arXiv preprint arXiv:2310.19923.\n"}
{"page": 10, "bbox": [{"x": 0.11897680163383484, "y": 0.46131202578544617}, {"x": 0.2117787003517151, "y": 0.46131202578544617}, {"x": 0.2117787003517151, "y": 0.47056350111961365}, {"x": 0.11897680163383484, "y": 0.47056350111961365}], "text": "References\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.44701430201530457}, {"x": 0.8828078508377075, "y": 0.4465937614440918}, {"x": 0.8828078508377075, "y": 0.49369218945503235}, {"x": 0.5139797925949097, "y": 0.49411270022392273}], "text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv,\nabs/2002.08909.\n"}
{"page": 10, "bbox": [{"x": 0.11838191747665405, "y": 0.48275861144065857}, {"x": 0.4878048896789551, "y": 0.4823381006717682}, {"x": 0.4878048896789551, "y": 0.5319596529006958}, {"x": 0.11838191747665405, "y": 0.5323801636695862}], "text": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\narXiv preprint arXiv:2310.11511.\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.5126156210899353}, {"x": 0.8834027647972107, "y": 0.5117745995521545}, {"x": 0.8839976191520691, "y": 0.5748528242111206}, {"x": 0.5139797925949097, "y": 0.5756938457489014}], "text": "Dan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2020. Measuring massive multitask language\nunderstanding. Cornell University - arXiv, Cornell\nUniversity arXiv.\n"}
{"page": 10, "bbox": [{"x": 0.6061868071556091, "y": 0.5693860650062561}, {"x": 0.6103509664535522, "y": 0.5693860650062561}, {"x": 0.6103509664535522, "y": 0.5706475973129272}, {"x": 0.6061868071556091, "y": 0.5706475973129272}], "text": "-\n"}
{"page": 10, "bbox": [{"x": 0.11897680163383484, "y": 0.5445752739906311}, {"x": 0.48839977383613586, "y": 0.5445752739906311}, {"x": 0.48839977383613586, "y": 0.6181665062904358}, {"x": 0.11897680163383484, "y": 0.6181665062904358}], "text": "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. Ms marco: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.5916736721992493}, {"x": 0.8834027647972107, "y": 0.5920941829681396}, {"x": 0.8834027647972107, "y": 0.6404541730880737}, {"x": 0.5133848786354065, "y": 0.6400336623191833}], "text": "Xanh Ho, A. Nguyen, Saku Sugawara, and Akiko\nAizawa. 2020. Constructing a multi-hop qa dataset\nfor comprehensive evaluation of reasoning steps.\nArXiv, abs/2011.01060.\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.6572750210762024}, {"x": 0.881618082523346, "y": 0.6576955318450928}, {"x": 0.881618082523346, "y": 0.7085786461830139}, {"x": 0.5133848786354065, "y": 0.7081581354141235}], "text": "J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. ArXiv, abs/2106.09685.\n"}
{"page": 10, "bbox": [{"x": 0.11778703331947327, "y": 0.6312026977539062}, {"x": 0.48839977383613586, "y": 0.6312026977539062}, {"x": 0.48839977383613586, "y": 0.8439865708351135}, {"x": 0.11778703331947327, "y": 0.8439865708351135}], "text": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. Empirical Methods in Natural\nLanguage Processing, Empirical Methods in Natural\nLanguage Processing.\nDeng Cai, Yan Wang, Lemao Liu, and Shuming Shi.\n2022. Recent advances in retrieval-augmented text\ngeneration. In Proceedings of the 45th international\nACM SIGIR conference on research and development\nin information retrieval, pages 3417-3419.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. Arxiv,\nabs/1803.05457.\n"}
{"page": 10, "bbox": [{"x": 0.5145746469497681, "y": 0.7249789834022522}, {"x": 0.8828078508377075, "y": 0.7241379022598267}, {"x": 0.8828078508377075, "y": 0.7619848847389221}, {"x": 0.5145746469497681, "y": 0.7628259062767029}], "text": "Yizheng Huang and Jimmy Huang. 2024. A survey\non retrieval-augmented text generation for large lan-\nguage models. arXiv preprint arXiv:2404.10981.\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.7783852219581604}, {"x": 0.8834027647972107, "y": 0.7779646515846252}, {"x": 0.8834027647972107, "y": 0.8406223654747009}, {"x": 0.5139797925949097, "y": 0.8410428762435913}], "text": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. arXiv\npreprint arXiv:2112.09118.\n"}
{"page": 10, "bbox": [{"x": 0.11838191747665405, "y": 0.8570227026939392}, {"x": 0.48839977383613586, "y": 0.8574432134628296}, {"x": 0.48839977383613586, "y": 0.9188393354415894}, {"x": 0.11838191747665405, "y": 0.918418824672699}], "text": "Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world's first truly open instruction-\ntuned llm.\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.8574432134628296}, {"x": 0.8834027647972107, "y": 0.8570227026939392}, {"x": 0.8834027647972107, "y": 0.9192599058151245}, {"x": 0.5139797925949097, "y": 0.9196804165840149}], "text": "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane A. Yu,\nArmand Joulin, Sebastian Riedel, and Edouard Grave.\n2022. Few-shot learning with retrieval augmented\nlanguage models. ArXiv, abs/2208.03299.\n"}
{"page": 10, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17725\n"}
{"page": 11, "bbox": [{"x": 0.5145746469497681, "y": 0.08830950409173965}, {"x": 0.8822129964828491, "y": 0.08704794198274612}, {"x": 0.8822129964828491, "y": 0.1248948723077774}, {"x": 0.5145746469497681, "y": 0.12615643441677094}], "text": "Stephanie Lin, Jacob Hilton, and Owain Evans. 2021b.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958.\n"}
{"page": 11, "bbox": [{"x": 0.11897680163383484, "y": 0.08873002231121063}, {"x": 0.4878048896789551, "y": 0.08830950409173965}, {"x": 0.4878048896789551, "y": 0.2140454202890396}, {"x": 0.11897680163383484, "y": 0.21446593105793}], "text": "Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing\nYang, and Lili Qiu. 2023a. Llmlingua: Compressing\nprompts for accelerated inference of large language\nmodels. arXiv preprint arXiv:2310.05736.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng\nLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b.\nLongllmlingua: Accelerating and enhancing llms\nin long context scenarios via prompt compression.\narXiv preprint arXiv:2310.06839.\n"}
{"page": 11, "bbox": [{"x": 0.5139797925949097, "y": 0.14003364741802216}, {"x": 0.8845925331115723, "y": 0.14003364741802216}, {"x": 0.8845925331115723, "y": 0.21362489461898804}, {"x": 0.5139797925949097, "y": 0.21362489461898804}], "text": "Xi Victoria Lin, Xilun Chen, Mingda Chen, Wei-\njia Shi, Maria Lomeli, Rich James, Pedro Ro-\ndriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis,\nLuke Zettlemoyer, and Scott Yih. 2023. Ra-dit:\nRetrieval-augmented dual instruction tuning. Arxiv,\nabs/2310.01352.\n"}
{"page": 11, "bbox": [{"x": 0.5133848786354065, "y": 0.2304457575082779}, {"x": 0.7108863592147827, "y": 0.22918419539928436}, {"x": 0.7108863592147827, "y": 0.2392767071723938}, {"x": 0.5133848786354065, "y": 0.24053826928138733}], "text": "Jerry Liu. 2022. LlamaIndex.\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.22708158195018768}, {"x": 0.48839977383613586, "y": 0.22708158195018768}, {"x": 0.48839977383613586, "y": 0.288898229598999}, {"x": 0.11838191747665405, "y": 0.288898229598999}], "text": "Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W.\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In Con-\nference on Empirical Methods in Natural Language\nProcessing.\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.30277544260025024}, {"x": 0.48839977383613586, "y": 0.30235493183135986}, {"x": 0.48839977383613586, "y": 0.35071489214897156}, {"x": 0.11838191747665405, "y": 0.35113540291786194}], "text": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. ArXiv, abs/1705.03551.\n"}
{"page": 11, "bbox": [{"x": 0.5133848786354065, "y": 0.25483599305152893}, {"x": 0.8845925331115723, "y": 0.25483599305152893}, {"x": 0.8845925331115723, "y": 0.4962153136730194}, {"x": 0.5133848786354065, "y": 0.4962153136730194}], "text": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024a. Lost in the middle: How language\nmodels use long contexts. Transactions of the Asso-\nciation for Computational Linguistics, 12:157–173.\nWenhao Liu, Xiaohua Wang, Muling Wu, Tianlong Li,\nChangze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan\nZhang, Xiaoqing Zheng, and Xuanjing Huang. 2023.\nAligning large language models with human pref-\nerences through representation engineering. arXiv\npreprint arXiv:2312.15997.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu\nLee, Mohammad Shoeybi, and Bryan Catanzaro.\n2024b. Chatqa: Surpassing gpt-4 on conversational\nqa and rag.\nLlamaIndex.\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.36501261591911316}, {"x": 0.48899465799331665, "y": 0.3645921051502228}, {"x": 0.48899465799331665, "y": 0.4158957004547119}, {"x": 0.11838191747665405, "y": 0.4163162410259247}], "text": "Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joon-\nsuk Park, and Jaewoo Kang. 2023. Tree of clarifica-\ntions: Answering ambiguous questions with retrieval-\naugmented large language models. arXiv preprint\n"}
{"page": 11, "bbox": [{"x": 0.1374182105064392, "y": 0.4175778031349182}, {"x": 0.2629387378692627, "y": 0.4175778031349182}, {"x": 0.2629387378692627, "y": 0.42556771636009216}, {"x": 0.1374182105064392, "y": 0.42556771636009216}], "text": "arXiv:2310.14696.\n"}
{"page": 11, "bbox": [{"x": 0.620464026927948, "y": 0.48696383833885193}, {"x": 0.7668054699897766, "y": 0.48696383833885193}, {"x": 0.7668054699897766, "y": 0.4949537515640259}, {"x": 0.620464026927948, "y": 0.4949537515640259}], "text": "Llamaindex website.\n"}
{"page": 11, "bbox": [{"x": 0.7864366173744202, "y": 0.4873843491077423}, {"x": 0.8834027647972107, "y": 0.48654332756996155}, {"x": 0.8834027647972107, "y": 0.49537426233291626}, {"x": 0.7864366173744202, "y": 0.4962153136730194}], "text": "https://www.\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.4411270022392273}, {"x": 0.48899465799331665, "y": 0.4407064616680145}, {"x": 0.48958954215049744, "y": 0.5647603273391724}, {"x": 0.11838191747665405, "y": 0.5651808381080627}], "text": "Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, Gábor Melis, and Ed-\nward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317–328.\nJing Yu Koh, Daniel Fried, and Russ R Salakhutdinov.\n2024. Generating images with multimodal language\nmodels. Advances in Neural Information Processing\nSystems, 36.\n"}
{"page": 11, "bbox": [{"x": 0.5336109399795532, "y": 0.5}, {"x": 0.8149910569190979, "y": 0.5}, {"x": 0.8149910569190979, "y": 0.5084104537963867}, {"x": 0.5336109399795532, "y": 0.5084104537963867}], "text": "llamaindex.com. Accessed: 2024-06-08.\n"}
{"page": 11, "bbox": [{"x": 0.5139797925949097, "y": 0.5243902206420898}, {"x": 0.8839976191520691, "y": 0.5239697098731995}, {"x": 0.8839976191520691, "y": 0.5874684453010559}, {"x": 0.5139797925949097, "y": 0.5878889560699463}], "text": "Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tian-\nhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, He-\nlen M. Meng, and James R. Glass. 2023. Sail: Search-\naugmented instruction learning. In Conference on\nEmpirical Methods in Natural Language Processing.\n"}
{"page": 11, "bbox": [{"x": 0.5133848786354065, "y": 0.6017661690711975}, {"x": 0.8834027647972107, "y": 0.6013456583023071}, {"x": 0.8834027647972107, "y": 0.6488645672798157}, {"x": 0.5133848786354065, "y": 0.6492851376533508}], "text": "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023a. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.5786374807357788}, {"x": 0.48839977383613586, "y": 0.5786374807357788}, {"x": 0.48839977383613586, "y": 0.9179983139038086}, {"x": 0.11778703331947327, "y": 0.9179983139038086}], "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc V. Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:453–466.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation. arXiv preprint arXiv:2202.01110.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, and Meishan Zhang. 2023. Towards\ngeneral text embeddings with multi-stage contrastive\nlearning. arXiv preprint arXiv:2308.03281.\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021a. Pyserini: A python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 2356-\n2362.\n"}
{"page": 11, "bbox": [{"x": 0.5127900242805481, "y": 0.6661059856414795}, {"x": 0.8845925331115723, "y": 0.6661059856414795}, {"x": 0.8845925331115723, "y": 0.9179983139038086}, {"x": 0.5127900242805481, "y": 0.9179983139038086}], "text": "Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and\nJimmy Lin. 2023b. Fine-tuning llama for multi-stage\ntext retrieval. arXiv preprint arXiv:2310.08319.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.\nDocument ranking with a pretrained sequence-to-\nsequence model. arXiv preprint arXiv:2003.06713.\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\nJimmy Lin. 2019. Multi-stage document ranking\nwith bert. arXiv preprint arXiv:1910.14424.\nOpenAI. 2023.\nGPT-4 technical report.\nabs/2303.08774.\n"}
{"page": 11, "bbox": [{"x": 0.838191568851471, "y": 0.8969722390174866}, {"x": 0.881618082523346, "y": 0.8965517282485962}, {"x": 0.881618082523346, "y": 0.9053826928138733}, {"x": 0.838191568851471, "y": 0.9058032035827637}], "text": "CORR,\n"}
{"page": 11, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17726\n"}
{"page": 12, "bbox": [{"x": 0.5145746469497681, "y": 0.08788898587226868}, {"x": 0.8834027647972107, "y": 0.0874684602022171}, {"x": 0.8834027647972107, "y": 0.16190075874328613}, {"x": 0.5145746469497681, "y": 0.1623212844133377}], "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. LLAMA: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\n"}
{"page": 12, "bbox": [{"x": 0.11838191747665405, "y": 0.08873002231121063}, {"x": 0.48899465799331665, "y": 0.08830950409173965}, {"x": 0.48899465799331665, "y": 0.2161480188369751}, {"x": 0.11838191747665405, "y": 0.21656854450702667}], "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Proceedings of the Conference\non Neural Information Processing Systems (NeurIPS\n2022).\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.23086626827716827}, {"x": 0.4866151213645935, "y": 0.232127845287323}, {"x": 0.4860202372074127, "y": 0.28132885694503784}, {"x": 0.11838191747665405, "y": 0.2800672948360443}], "text": "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoahA. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.29730865359306335}, {"x": 0.4866151213645935, "y": 0.29730865359306335}, {"x": 0.4866151213645935, "y": 0.35786375403404236}, {"x": 0.11897680163383484, "y": 0.35786375403404236}], "text": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. arXiv preprint\narXiv:2305.18290.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.18166527152061462}, {"x": 0.8839976191520691, "y": 0.18166527152061462}, {"x": 0.8839976191520691, "y": 0.47771236300468445}, {"x": 0.5139797925949097, "y": 0.47771236300468445}], "text": "Hugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cantón Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023b. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.37552565336227417}, {"x": 0.4866151213645935, "y": 0.37594616413116455}, {"x": 0.4866151213645935, "y": 0.4230445623397827}, {"x": 0.11897680163383484, "y": 0.42262405157089233}], "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\n"}
{"page": 12, "bbox": [{"x": 0.11838191747665405, "y": 0.4407064616680145}, {"x": 0.48839977383613586, "y": 0.44028595089912415}, {"x": 0.48839977383613586, "y": 0.48822540044784546}, {"x": 0.11838191747665405, "y": 0.48864591121673584}], "text": "Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nalla-\npati, Zhiheng Huang, and Bing Xiang. 2020. Beyond\n[cls] through ranking by generation. arXiv preprint\narXiv:2010.03073.\n"}
{"page": 12, "bbox": [{"x": 0.5145746469497681, "y": 0.4945332109928131}, {"x": 0.8839976191520691, "y": 0.4962153136730194}, {"x": 0.8834027647972107, "y": 0.5613961219787598}, {"x": 0.5139797925949097, "y": 0.5597140192985535}], "text": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, page\n539-554.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.5042052268981934}, {"x": 0.48899465799331665, "y": 0.5058873295783997}, {"x": 0.48839977383613586, "y": 0.5702270865440369}, {"x": 0.11838191747665405, "y": 0.5685449838638306}], "text": "Kunal Sawarkar, Abhilasha Mangal, and Shivam Raj\nSolanki. 2024.\nBlended rag: Improving rag\n(retriever-augmented generation) accuracy with se-\nmantic search and hybrid query-based retrievers.\narXiv preprint arXiv:2404.07220.\n"}
{"page": 12, "bbox": [{"x": 0.5133848786354065, "y": 0.5761143565177917}, {"x": 0.8845925331115723, "y": 0.5765349268913269}, {"x": 0.8845925331115723, "y": 0.6370899677276611}, {"x": 0.5133848786354065, "y": 0.6366694569587708}], "text": "Liang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.5832632184028625}, {"x": 0.48839977383613586, "y": 0.5836837887763977}, {"x": 0.48839977383613586, "y": 0.6467620134353638}, {"x": 0.11897680163383484, "y": 0.6463414430618286}], "text": "ES Shahul, Jithin James, Luis Espinosa Anke, and\nSteven Schockaert. 2023. Ragas: Automated evalu-\nation of retrieval augmented generation. In Confer-\nence of the European Chapter of the Association for\nComputational Linguistics.\n"}
{"page": 12, "bbox": [{"x": 0.5133848786354065, "y": 0.6547518968582153}, {"x": 0.8828078508377075, "y": 0.654331386089325}, {"x": 0.8828078508377075, "y": 0.6934398412704468}, {"x": 0.5133848786354065, "y": 0.6938604116439819}], "text": "Liang Wang, Nan Yang, and Furu Wei. 2023a.\nQuery2doc: Query expansion with large language\nmodels. arXiv preprint arXiv:2303.07678.\n"}
{"page": 12, "bbox": [{"x": 0.11838191747665405, "y": 0.6619007587432861}, {"x": 0.4878048896789551, "y": 0.6606391668319702}, {"x": 0.48839977383613586, "y": 0.7237173914909363}, {"x": 0.11897680163383484, "y": 0.7249789834022522}], "text": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\n"}
{"page": 12, "bbox": [{"x": 0.5133848786354065, "y": 0.7102607488632202}, {"x": 0.8822129964828491, "y": 0.7098401784896851}, {"x": 0.8822129964828491, "y": 0.7867956161499023}, {"x": 0.5133848786354065, "y": 0.7872161269187927}], "text": "Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing\nZheng, and Xuan-Jing Huang. 2023b. Hallucination\ndetection for generative large language models by\nbayesian sequential estimation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 15361-15371.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.7405382394790649}, {"x": 0.48839977383613586, "y": 0.7396972179412842}, {"x": 0.48839977383613586, "y": 0.7762826085090637}, {"x": 0.11897680163383484, "y": 0.7771236300468445}], "text": "Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. Asqa: Factoid questions meet\nlong-form answers. ArXiv, abs/2204.06092.\n"}
{"page": 12, "bbox": [{"x": 0.11838191747665405, "y": 0.7922624349594116}, {"x": 0.4878048896789551, "y": 0.7922624349594116}, {"x": 0.4878048896789551, "y": 0.8528174757957458}, {"x": 0.11838191747665405, "y": 0.8528174757957458}], "text": "Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evalua-\ntion of information retrieval models. arXiv preprint\narXiv:2104.08663.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.8040370345115662}, {"x": 0.8834027647972107, "y": 0.8031959533691406}, {"x": 0.8834027647972107, "y": 0.8528174757957458}, {"x": 0.5139797925949097, "y": 0.8536585569381714}], "text": "Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023c. Learning to fil-\nter context for retrieval-augmented generation. arXiv\npreprint arXiv:2311.08377.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.8671151995658875}, {"x": 0.4872100055217743, "y": 0.8675357699394226}, {"x": 0.4872100055217743, "y": 0.9196804165840149}, {"x": 0.11897680163383484, "y": 0.9192599058151245}], "text": "James Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification. ArXiv, abs/1803.05355.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.8704794049263}, {"x": 0.8822129964828491, "y": 0.8704794049263}, {"x": 0.8822129964828491, "y": 0.9179983139038086}, {"x": 0.5139797925949097, "y": 0.9179983139038086}], "text": "Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding. Preprint,\narXiv:2309.07597.\n"}
{"page": 12, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17727\n"}
{"page": 13, "bbox": [{"x": 0.11838191747665405, "y": 0.08788898587226868}, {"x": 0.48839977383613586, "y": 0.08662741631269455}, {"x": 0.48899465799331665, "y": 0.1358284205198288}, {"x": 0.11897680163383484, "y": 0.13708999752998352}], "text": "Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-\ncomp: Improving retrieval-augmented lms with com-\npression and selective augmentation. arXiv preprint\narXiv:2310.04408.\n"}
{"page": 13, "bbox": [{"x": 0.5139797925949097, "y": 0.0874684602022171}, {"x": 0.8834027647972107, "y": 0.08704794198274612}, {"x": 0.8839976191520691, "y": 0.22455845773220062}, {"x": 0.5145746469497681, "y": 0.224978968501091}], "text": "Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021.\nDeep query likelihood model for information re-\ntrieval. In Advances in Information Retrieval: 43rd\nEuropean Conference on IR Research, ECIR 2021,\nVirtual Event, March 28-April 1, 2021, Proceedings,\nPart II 43, pages 463–470. Springer.\nShengyao Zhuang and Guido Zuccon. 2021a. Fast pas-\nsage re-ranking with contextualized exact term match-\ning and efficient passage expansion. arXiv preprint\narXiv:2108.08513.\n"}
{"page": 13, "bbox": [{"x": 0.11897680163383484, "y": 0.15264928340911865}, {"x": 0.48839977383613586, "y": 0.15222875773906708}, {"x": 0.48839977383613586, "y": 0.21572750806808472}, {"x": 0.11897680163383484, "y": 0.2161480188369751}], "text": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\n"}
{"page": 13, "bbox": [{"x": 0.11897680163383484, "y": 0.22960470616817474}, {"x": 0.4872100055217743, "y": 0.22960470616817474}, {"x": 0.4872100055217743, "y": 0.29015979170799255}, {"x": 0.11897680163383484, "y": 0.29015979170799255}], "text": "Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Fei Huang. 2023. RRHF:\nRank responses to align language models with\nhuman feedback without tears. arXiv preprint\narXiv:2304.05302.\n"}
{"page": 13, "bbox": [{"x": 0.5139797925949097, "y": 0.23801514506340027}, {"x": 0.8839976191520691, "y": 0.2375946193933487}, {"x": 0.8839976191520691, "y": 0.3031959533691406}, {"x": 0.5139797925949097, "y": 0.3036164939403534}], "text": "Shengyao Zhuang and Guido Zuccon. 2021b. Tilde:\nTerm independent likelihood model for passage re-\nranking. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 1483–1492.\n"}
{"page": 13, "bbox": [{"x": 0.11957168579101562, "y": 0.3069806694984436}, {"x": 0.4878048896789551, "y": 0.30656012892723083}, {"x": 0.4878048896789551, "y": 0.343145489692688}, {"x": 0.11957168579101562, "y": 0.34356603026390076}], "text": "Hamed Zamani and Michael Bendersky. 2024. Stochas-\ntic rag: End-to-end retrieval-augmented generation\nthrough expected utility maximization.\n"}
{"page": 13, "bbox": [{"x": 0.11838191747665405, "y": 0.35828426480293274}, {"x": 0.4878048896789551, "y": 0.357443243265152}, {"x": 0.4878048896789551, "y": 0.40622371435165405}, {"x": 0.11838191747665405, "y": 0.4070647656917572}], "text": "Lingxi Zhang, Yue Yu, Kuan Wang, and Chao Zhang.\n2024a. Arl2: Aligning retrievers for black-box large\nlanguage models via self-guided adaptive relevance\nlabeling. ArXiv, abs/2402.13542.\n"}
{"page": 13, "bbox": [{"x": 0.11838191747665405, "y": 0.4217830002307892}, {"x": 0.4878048896789551, "y": 0.4217830002307892}, {"x": 0.4878048896789551, "y": 0.460470974445343}, {"x": 0.11838191747665405, "y": 0.460470974445343}], "text": "Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,\nand Jian-Yun Nie. 2023a. Retrieve anything to\naugment large language models. arXiv preprint\n"}
{"page": 13, "bbox": [{"x": 0.1374182105064392, "y": 0.4608915150165558}, {"x": 0.2635336220264435, "y": 0.4608915150165558}, {"x": 0.2635336220264435, "y": 0.46888139843940735}, {"x": 0.1374182105064392, "y": 0.46888139843940735}], "text": "arXiv:2310.07554.\n"}
{"page": 13, "bbox": [{"x": 0.11897680163383484, "y": 0.4861227869987488}, {"x": 0.48839977383613586, "y": 0.4861227869987488}, {"x": 0.48839977383613586, "y": 0.5475189089775085}, {"x": 0.11897680163383484, "y": 0.5475189089775085}], "text": "Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\nDanny Fox, Helen M. Meng, and James R. Glass.\n2023b. Interpretable unified language checking.\nArXiv, abs/2304.03728.\n"}
{"page": 13, "bbox": [{"x": 0.11897680163383484, "y": 0.5622372031211853}, {"x": 0.4878048896789551, "y": 0.5626577138900757}, {"x": 0.4878048896789551, "y": 0.6126997470855713}, {"x": 0.11897680163383484, "y": 0.6122792363166809}], "text": "Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng\nShen, Matei A. Zaharia, Ion Stoica, and Joseph E.\nGonzalez. 2024b. Raft: Adapting language model to\ndomain specific rag. ArXiv, abs/2403.10131.\n"}
{"page": 13, "bbox": [{"x": 0.11957168579101562, "y": 0.6265769600868225}, {"x": 0.48839977383613586, "y": 0.6265769600868225}, {"x": 0.48839977383613586, "y": 0.6896551847457886}, {"x": 0.11957168579101562, "y": 0.6896551847457886}], "text": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, et al. 2023c. Siren's song in the ai\nocean: a survey on hallucination in large language\nmodels. arXiv preprint arXiv:2309.01219.\n"}
{"page": 13, "bbox": [{"x": 0.11838191747665405, "y": 0.7035323977470398}, {"x": 0.48839977383613586, "y": 0.7031118869781494}, {"x": 0.48839977383613586, "y": 0.7670311331748962}, {"x": 0.11838191747665405, "y": 0.7674516439437866}], "text": "Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren\nWang, Yunteng Geng, Fangcheng Fu, Ling Yang,\nWentao Zhang, and Bin Cui. 2024. Retrieval-\naugmented generation for ai-generated content: A\nsurvey. arXiv preprint arXiv:2402.19473.\n"}
{"page": 13, "bbox": [{"x": 0.11838191747665405, "y": 0.7796467542648315}, {"x": 0.4878048896789551, "y": 0.7804877758026123}, {"x": 0.4878048896789551, "y": 0.8460891246795654}, {"x": 0.11838191747665405, "y": 0.8452481031417847}], "text": "Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai\nJiao, Xuan Long Do, Chengwei Qin, Bosheng\nDing, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al.\n2023a. Retrieving multimodal information for aug-\nmented generation: A survey. arXiv preprint\n"}
{"page": 13, "bbox": [{"x": 0.1374182105064392, "y": 0.8460891246795654}, {"x": 0.2629387378692627, "y": 0.8460891246795654}, {"x": 0.2629387378692627, "y": 0.8540790677070618}, {"x": 0.1374182105064392, "y": 0.8540790677070618}], "text": "arXiv:2303.10868.\n"}
{"page": 13, "bbox": [{"x": 0.11897680163383484, "y": 0.8704794049263}, {"x": 0.4878048896789551, "y": 0.8700588941574097}, {"x": 0.4878048896789551, "y": 0.9205214381217957}, {"x": 0.11897680163383484, "y": 0.920941948890686}], "text": "Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,\nMohammad Saleh, and Peter J Liu. 2023b. SLIC-HF:\nSequence likelihood calibration with human feed-\nback. arXiv preprint arXiv:2305.10425.\n"}
{"page": 13, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17728\n"}
{"page": 14, "bbox": [{"x": 0.1165972650051117, "y": 0.08620689809322357}, {"x": 0.4878048896789551, "y": 0.08536585420370102}, {"x": 0.48839977383613586, "y": 0.17073170840740204}, {"x": 0.11719214916229248, "y": 0.171572744846344}], "text": "A Experimental Details\nIn this section, we provide detailed experimental\nsettings for each module, covering dataset specifics,\ntraining parameters, and any additional experimen-\ntal results.\n"}
{"page": 14, "bbox": [{"x": 0.5133848786354065, "y": 0.08620689809322357}, {"x": 0.8851873874664307, "y": 0.08620689809322357}, {"x": 0.8851873874664307, "y": 0.20984020829200745}, {"x": 0.5133848786354065, "y": 0.20984020829200745}], "text": "zephyr-7b-alpha and gpt-3.5-turbo5 as gen-\neration model and evaluation model respectively.\nThe size of the chunk overlap is 20 tokens. First\nsixty pages of the document lyft_20216 are used\nas corpus, then prompting LLMs to generate about\none hundred and seventy queries according to cho-\nsen corpus. The impact of different chunk sizes is\nshown in Table 3.\n"}
{"page": 14, "bbox": [{"x": 0.11719214916229248, "y": 0.1875525712966919}, {"x": 0.48839977383613586, "y": 0.18797308206558228}, {"x": 0.48839977383613586, "y": 0.332632452249527}, {"x": 0.11719214916229248, "y": 0.3322119414806366}], "text": "A.1 Query Classification\nDatasets To develop the query classifier, we cre-\nated a comprehensive dataset consisting of 111K\nsamples covering 15 different types of tasks, with\n64K samples labeled as \"retrieval required\" and\n47K samples labeled as \"no retrieval required.\"\nThis dataset was constructed from a variety of spe-\ncialized sources, each contributing to a broad spec-\ntrum of task-specific data:\n"}
{"page": 14, "bbox": [{"x": 0.11838191747665405, "y": 0.3460891544818878}, {"x": 0.31707316637039185, "y": 0.34777122735977173}, {"x": 0.31707316637039185, "y": 0.35996636748313904}, {"x": 0.11838191747665405, "y": 0.35828426480293274}], "text": "• Code: code_alpaca_20k.\n"}
{"page": 14, "bbox": [{"x": 0.11957168579101562, "y": 0.36669468879699707}, {"x": 0.1249256432056427, "y": 0.36669468879699707}, {"x": 0.1249256432056427, "y": 0.3700588643550873}, {"x": 0.11957168579101562, "y": 0.3700588643550873}], "text": "•\n"}
{"page": 14, "bbox": [{"x": 0.11957168579101562, "y": 0.38267451524734497}, {"x": 0.12433075904846191, "y": 0.38267451524734497}, {"x": 0.12433075904846191, "y": 0.3860386908054352}, {"x": 0.11957168579101562, "y": 0.3860386908054352}], "text": "•\n"}
{"page": 14, "bbox": [{"x": 0.11897680163383484, "y": 0.39865434169769287}, {"x": 0.12433075904846191, "y": 0.39865434169769287}, {"x": 0.12433075904846191, "y": 0.4020185172557831}, {"x": 0.11897680163383484, "y": 0.4020185172557831}], "text": "•\n"}
{"page": 14, "bbox": [{"x": 0.11897680163383484, "y": 0.41505467891693115}, {"x": 0.12433075904846191, "y": 0.41505467891693115}, {"x": 0.12433075904846191, "y": 0.418418824672699}, {"x": 0.11897680163383484, "y": 0.418418824672699}], "text": "•\n"}
{"page": 14, "bbox": [{"x": 0.5127900242805481, "y": 0.22792261838912964}, {"x": 0.8845925331115723, "y": 0.22792261838912964}, {"x": 0.8845925331115723, "y": 0.6190075874328613}, {"x": 0.5127900242805481, "y": 0.6190075874328613}], "text": "Chunking Techniques To demonstrate the effec-\ntiveness of advanced chunking techniques, we use\nthe LLM-Embedder (Zhang et al., 2023a) model\nas embedding model. The smaller chunk size is\n175 tokens, the larger chunk size is 512 tokens and\nthe chunk overlap is 20 tokens. Techniques like\nsmall-to-big and sliding window improve retrieval\nquality by maintaining context and ensuring rele-\nvant information is retrieved. Detailed results are\nshown in Table 4.\nEmbedding Model Selection The embed-\nding model used for RAG needs to consider\nthe semantic space-matching problem between\nqueries and chunk blocks. We use the evalua-\ntion module of FlagEmbedding which uses the\ndataset namespace-Pt/msmarco³ as queries and\ndataset namespace-Pt/msmarco-corpus as cor-\npus to choose the appropriate open source em-\nbedding model. As shown in Table 5, LLM-\nEmbedder (Zhang et al., 2023a) achieves compa-\nrable results with BAAI/bge-large-en (Xiao et al.,\n2023), however, the size of the former is three times\nsmaller than that of the latter. Thus, we choose\nLLM-Embedder to build the vector database.\n"}
{"page": 14, "bbox": [{"x": 0.11838191747665405, "y": 0.36206895112991333}, {"x": 0.48839977383613586, "y": 0.3624894917011261}, {"x": 0.4878048896789551, "y": 0.5866274237632751}, {"x": 0.11778703331947327, "y": 0.5862069129943848}], "text": "Medical-related: medical_questions_pairs.\nSuggestion: oasst_quality_with_suggestions.\n· Roleplay: roleplay_alpaca.\nRewriting: merge_rewrite_13.3k.\n• Multi-task: Databricks-Dolly-15K (Conover\net al., 2023), which includes tasks such as closed\nQA, classification, information extraction, sum-\nmarization, and writing.\nFor other tasks not covered by these datasets, we\ngenerated corresponding samples using GPT-4.\nImplementation Details We choose BERT-base-\nmultilingual-cased as our classifier, with a batch\nsize of 16 and a learning rate of 1e-5. The evalua-\ntion of results is showcased in Table 2.\n"}
{"page": 14, "bbox": [{"x": 0.35574063658714294, "y": 0.6059713959693909}, {"x": 0.4051159918308258, "y": 0.6068124771118164}, {"x": 0.4051159918308258, "y": 0.6143818497657776}, {"x": 0.35574063658714294, "y": 0.613540768623352}], "text": "Metrics\n"}
{"page": 14, "bbox": [{"x": 0.15585960447788239, "y": 0.6122792363166809}, {"x": 0.19631171226501465, "y": 0.6118587255477905}, {"x": 0.19631171226501465, "y": 0.6194280982017517}, {"x": 0.15585960447788239, "y": 0.6198486089706421}], "text": "Model\n"}
{"page": 14, "bbox": [{"x": 0.31290897727012634, "y": 0.6253153681755066}, {"x": 0.44199880957603455, "y": 0.6253153681755066}, {"x": 0.44199880957603455, "y": 0.6324642300605774}, {"x": 0.31290897727012634, "y": 0.6324642300605774}], "text": "Acc Prec Rec F1\n"}
{"page": 14, "bbox": [{"x": 0.15585960447788239, "y": 0.6433978080749512}, {"x": 0.4479476511478424, "y": 0.6425567865371704}, {"x": 0.4479476511478424, "y": 0.6522287726402283}, {"x": 0.15585960447788239, "y": 0.653069794178009}], "text": "BERT-base-multilingual 0.95 0.96 0.94 0.95\n"}
{"page": 14, "bbox": [{"x": 0.7126710414886475, "y": 0.6610597372055054}, {"x": 0.7715645432472229, "y": 0.6597981452941895}, {"x": 0.7721594572067261, "y": 0.6686291098594666}, {"x": 0.7132658958435059, "y": 0.6698906421661377}], "text": "lyft 2021\n"}
{"page": 14, "bbox": [{"x": 0.5229030251502991, "y": 0.6661059856414795}, {"x": 0.5966686606407166, "y": 0.6656854748725891}, {"x": 0.5966686606407166, "y": 0.6732548475265503}, {"x": 0.5229030251502991, "y": 0.6736753582954407}], "text": "Chunk Size\n"}
{"page": 14, "bbox": [{"x": 0.1683521717786789, "y": 0.6719932556152344}, {"x": 0.4348601996898651, "y": 0.6719932556152344}, {"x": 0.4348601996898651, "y": 0.6820858120918274}, {"x": 0.1683521717786789, "y": 0.6820858120918274}], "text": "Table 2: Results of the Query Classifier.\n"}
{"page": 14, "bbox": [{"x": 0.6127305030822754, "y": 0.6770395040512085}, {"x": 0.7394407987594604, "y": 0.6774600744247437}, {"x": 0.7394407987594604, "y": 0.6867115497589111}, {"x": 0.6127305030822754, "y": 0.686290979385376}], "text": "Average Faithfulness\n"}
{"page": 14, "bbox": [{"x": 0.7560975551605225, "y": 0.6770395040512085}, {"x": 0.8732897043228149, "y": 0.6774600744247437}, {"x": 0.8732897043228149, "y": 0.6871320605278015}, {"x": 0.7560975551605225, "y": 0.6867115497589111}], "text": "Average Relevancy\n"}
{"page": 14, "bbox": [{"x": 0.6591314673423767, "y": 0.6892346739768982}, {"x": 0.6930398344993591, "y": 0.688814103603363}, {"x": 0.6930398344993591, "y": 0.6959629654884338}, {"x": 0.6591314673423767, "y": 0.696383535861969}], "text": "80.37\n"}
{"page": 14, "bbox": [{"x": 0.5223081707954407, "y": 0.690075695514679}, {"x": 0.5526472330093384, "y": 0.690075695514679}, {"x": 0.5526472330093384, "y": 0.6968040466308594}, {"x": 0.5223081707954407, "y": 0.6968040466308594}], "text": "2048\n"}
{"page": 14, "bbox": [{"x": 0.7977394461631775, "y": 0.690075695514679}, {"x": 0.8310529589653015, "y": 0.690075695514679}, {"x": 0.8310529589653015, "y": 0.6968040466308594}, {"x": 0.7977394461631775, "y": 0.6968040466308594}], "text": "91.11\n"}
{"page": 14, "bbox": [{"x": 0.11778703331947327, "y": 0.6955424547195435}, {"x": 0.43307554721832275, "y": 0.6955424547195435}, {"x": 0.43307554721832275, "y": 0.7068965435028076}, {"x": 0.11778703331947327, "y": 0.7068965435028076}], "text": "A.2 Experimental Details of Chunking\n"}
{"page": 14, "bbox": [{"x": 0.7965496778488159, "y": 0.7018502950668335}, {"x": 0.8316478133201599, "y": 0.7018502950668335}, {"x": 0.8316478133201599, "y": 0.7085786461830139}, {"x": 0.7965496778488159, "y": 0.7085786461830139}], "text": "95.56\n"}
{"page": 14, "bbox": [{"x": 0.5234979391098022, "y": 0.7022708058357239}, {"x": 0.5532421469688416, "y": 0.7022708058357239}, {"x": 0.5532421469688416, "y": 0.7089991569519043}, {"x": 0.5234979391098022, "y": 0.7089991569519043}], "text": "1024\n"}
{"page": 14, "bbox": [{"x": 0.6591314673423767, "y": 0.7022708058357239}, {"x": 0.6936347484588623, "y": 0.7022708058357239}, {"x": 0.6936347484588623, "y": 0.7089991569519043}, {"x": 0.6591314673423767, "y": 0.7089991569519043}], "text": "94.26\n"}
{"page": 14, "bbox": [{"x": 0.1635930985212326, "y": 0.7115222811698914}, {"x": 0.23200476169586182, "y": 0.711101770401001}, {"x": 0.23200476169586182, "y": 0.7199327349662781}, {"x": 0.1635930985212326, "y": 0.7203532457351685}], "text": "Methods\n"}
{"page": 14, "bbox": [{"x": 0.5223081707954407, "y": 0.7140454053878784}, {"x": 0.5449137687683105, "y": 0.7140454053878784}, {"x": 0.5449137687683105, "y": 0.7207737565040588}, {"x": 0.5223081707954407, "y": 0.7207737565040588}], "text": "512\n"}
{"page": 14, "bbox": [{"x": 0.6567519307136536, "y": 0.7140454053878784}, {"x": 0.6954193711280823, "y": 0.7140454053878784}, {"x": 0.6954193711280823, "y": 0.7207737565040588}, {"x": 0.6567519307136536, "y": 0.7207737565040588}], "text": "97.59\n"}
{"page": 14, "bbox": [{"x": 0.7971445322036743, "y": 0.7140454053878784}, {"x": 0.8310529589653015, "y": 0.7140454053878784}, {"x": 0.8310529589653015, "y": 0.7207737565040588}, {"x": 0.7971445322036743, "y": 0.7207737565040588}], "text": "97.41\n"}
{"page": 14, "bbox": [{"x": 0.7947649955749512, "y": 0.725820004940033}, {"x": 0.8340273499488831, "y": 0.725820004940033}, {"x": 0.8340273499488831, "y": 0.732127845287323}, {"x": 0.7947649955749512, "y": 0.732127845287323}], "text": "97.78\n"}
{"page": 14, "bbox": [{"x": 0.6591314673423767, "y": 0.725820004940033}, {"x": 0.6936347484588623, "y": 0.725820004940033}, {"x": 0.6936347484588623, "y": 0.7325483560562134}, {"x": 0.6591314673423767, "y": 0.7325483560562134}], "text": "97.22\n"}
{"page": 14, "bbox": [{"x": 0.5217132568359375, "y": 0.7253994941711426}, {"x": 0.545508623123169, "y": 0.7245584726333618}, {"x": 0.5466983914375305, "y": 0.7439024448394775}, {"x": 0.5229030251502991, "y": 0.7447434663772583}], "text": "256\n128\n"}
{"page": 14, "bbox": [{"x": 0.6591314673423767, "y": 0.7371740937232971}, {"x": 0.6936347484588623, "y": 0.7371740937232971}, {"x": 0.6936347484588623, "y": 0.7439024448394775}, {"x": 0.6591314673423767, "y": 0.7439024448394775}], "text": "95.74\n"}
{"page": 14, "bbox": [{"x": 0.7971445322036743, "y": 0.7375946044921875}, {"x": 0.8316478133201599, "y": 0.7375946044921875}, {"x": 0.8316478133201599, "y": 0.7439024448394775}, {"x": 0.7971445322036743, "y": 0.7439024448394775}], "text": "97.22\n"}
{"page": 14, "bbox": [{"x": 0.5443188548088074, "y": 0.7611438035964966}, {"x": 0.8500892519950867, "y": 0.7603027820587158}, {"x": 0.8500892519950867, "y": 0.7712363600730896}, {"x": 0.5443188548088074, "y": 0.7720773816108704}], "text": "Table 3: Comparison of different chunk sizes.\n"}
{"page": 14, "bbox": [{"x": 0.11778703331947327, "y": 0.7296047210693359}, {"x": 0.48899465799331665, "y": 0.7304457426071167}, {"x": 0.48839977383613586, "y": 0.9108494520187378}, {"x": 0.11719214916229248, "y": 0.910008430480957}], "text": "Chunk Size Finding the optimal chunk size\ninvolves a balance between some metrics such\nas faithfulness, relevancy. Faithfulness measures\nwhether the response is hallucinated or matches the\nretrieved texts. Relevancy measures whether the\nretrieved texts and response match queries. We use\nthe evaluation module of LlamaIndex(LlamaIndex)\nto calculate the metrics above. For embedding,\nwe use the text-embedding-ada-002³ model,\nwhich supports long input length. We choose\n3 https://platform.openai.com/docs/guides/embeddings/\n"}
{"page": 14, "bbox": [{"x": 0.5330160856246948, "y": 0.8288477659225464}, {"x": 0.8477097153663635, "y": 0.8301093578338623}, {"x": 0.8477097153663635, "y": 0.8410428762435913}, {"x": 0.5330160856246948, "y": 0.8397813439369202}], "text": "4 https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n"}
{"page": 14, "bbox": [{"x": 0.5330160856246948, "y": 0.8414633870124817}, {"x": 0.6769779920578003, "y": 0.8427249789237976}, {"x": 0.6769779920578003, "y": 0.8532379865646362}, {"x": 0.5330160856246948, "y": 0.8519764542579651}], "text": "Shttps://www.openai.com/\n"}
{"page": 14, "bbox": [{"x": 0.5133848786354065, "y": 0.8553406000137329}, {"x": 0.8703153133392334, "y": 0.8561816811561584}, {"x": 0.8703153133392334, "y": 0.8986543416976929}, {"x": 0.5133848786354065, "y": 0.8978132605552673}], "text": "https://raw.githubusercontent.com/run-llama/llama_index/\nmain/docs/docs/examples/data/10k/lyft_2021.pdf\nhttps://github.com/FlagOpen/FlagEmbedding\nhttps://huggingface.co/datasets/namespace-Pt/msmarco\n"}
{"page": 14, "bbox": [{"x": 0.5407495498657227, "y": 0.9007569551467896}, {"x": 0.8066626787185669, "y": 0.9015979766845703}, {"x": 0.8066626787185669, "y": 0.910008430480957}, {"x": 0.5407495498657227, "y": 0.9091673493385315}], "text": "https://huggingface.co/datasets/namespace-Pt/\n"}
{"page": 14, "bbox": [{"x": 0.11897680163383484, "y": 0.9125315546989441}, {"x": 0.21356335282325745, "y": 0.9125315546989441}, {"x": 0.21356335282325745, "y": 0.918418824672699}, {"x": 0.11897680163383484, "y": 0.918418824672699}], "text": "embedding-models\n"}
{"page": 14, "bbox": [{"x": 0.5139797925949097, "y": 0.9133725762367249}, {"x": 0.5966686606407166, "y": 0.9137930870056152}, {"x": 0.5966686606407166, "y": 0.9188393354415894}, {"x": 0.5139797925949097, "y": 0.918418824672699}], "text": "msmarco-corpus\n"}
{"page": 14, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.9314550161361694}, {"x": 0.5234979391098022, "y": 0.939444899559021}, {"x": 0.48007139563560486, "y": 0.939444899559021}], "text": "17729\n"}
{"page": 15, "bbox": [{"x": 0.2671029269695282, "y": 0.0874684602022171}, {"x": 0.38191553950309753, "y": 0.0874684602022171}, {"x": 0.38191553950309753, "y": 0.0946173220872879}, {"x": 0.2671029269695282, "y": 0.0946173220872879}], "text": "Sufficient information\n"}
{"page": 15, "bbox": [{"x": 0.7162403464317322, "y": 0.09545836597681046}, {"x": 0.8239143490791321, "y": 0.09503784775733948}, {"x": 0.8239143490791321, "y": 0.10092514753341675}, {"x": 0.7162403464317322, "y": 0.10134566575288773}], "text": "- Need to Retrieval\n"}
{"page": 15, "bbox": [{"x": 0.5484830737113953, "y": 0.09545836597681046}, {"x": 0.672218918800354, "y": 0.09545836597681046}, {"x": 0.672218918800354, "y": 0.10134566575288773}, {"x": 0.5484830737113953, "y": 0.10134566575288773}], "text": "- No Retrieval Needed\n"}
{"page": 15, "bbox": [{"x": 0.20761451125144958, "y": 0.1017661914229393}, {"x": 0.32064247131347656, "y": 0.10134566575288773}, {"x": 0.32064247131347656, "y": 0.107232965528965}, {"x": 0.20761451125144958, "y": 0.10765349119901657}], "text": "\"To be, or not to be, that is the\n"}
{"page": 15, "bbox": [{"x": 0.20820939540863037, "y": 0.11017661541700363}, {"x": 0.24628198146820068, "y": 0.11017661541700363}, {"x": 0.24628198146820068, "y": 0.11564339697360992}, {"x": 0.20820939540863037, "y": 0.11564339697360992}], "text": "question.\"\n"}
{"page": 15, "bbox": [{"x": 0.20820939540863037, "y": 0.11564339697360992}, {"x": 0.33491969108581543, "y": 0.11690495908260345}, {"x": 0.33491969108581543, "y": 0.1303616464138031}, {"x": 0.20820939540863037, "y": 0.12910008430480957}], "text": "Please translate this sentence into\nFrench.\n"}
{"page": 15, "bbox": [{"x": 0.28316476941108704, "y": 0.12405382841825485}, {"x": 0.3396787643432617, "y": 0.12363330274820328}, {"x": 0.3396787643432617, "y": 0.12825904786586761}, {"x": 0.28316476941108704, "y": 0.128679558634758}], "text": "<Translation>\n"}
{"page": 15, "bbox": [{"x": 0.5585960745811462, "y": 0.12573590874671936}, {"x": 0.7596668601036072, "y": 0.1265769600868225}, {"x": 0.7596668601036072, "y": 0.14003364741802216}, {"x": 0.5585960745811462, "y": 0.139192596077919}], "text": "Please give me a plan for holding a graduation party.\n< Planning>\n"}
{"page": 15, "bbox": [{"x": 0.37180250883102417, "y": 0.1017661914229393}, {"x": 0.4878048896789551, "y": 0.10218670964241028}, {"x": 0.4872100055217743, "y": 0.1804036945104599}, {"x": 0.3712076246738434, "y": 0.17998318374156952}], "text": "\"The Renaissance was a\ncultural transformation in\nEuropean history, marking the\nrevival of arts, sciences, and\nhumanistic thought. The\nfervor of artists and scholars\npropelled prosperity and\ninnovation in arts, literature,\nand science.\" Give me a\nsummary.\n<Summarization >\n"}
{"page": 15, "bbox": [{"x": 0.206424742937088, "y": 0.1408746838569641}, {"x": 0.33372992277145386, "y": 0.14045415818691254}, {"x": 0.33372992277145386, "y": 0.1677880585193634}, {"x": 0.206424742937088, "y": 0.16820858418941498}], "text": "\"Dave is attending his aunt's\nbrother funeral today.\"\nParaphrase the given information\neffectively.\n"}
{"page": 15, "bbox": [{"x": 0.2879238426685333, "y": 0.1623212844133377}, {"x": 0.33908388018608093, "y": 0.1623212844133377}, {"x": 0.33908388018608093, "y": 0.16862909495830536}, {"x": 0.2879238426685333, "y": 0.16862909495830536}], "text": "<Rewriting>\n"}
{"page": 15, "bbox": [{"x": 0.5585960745811462, "y": 0.15601345896720886}, {"x": 0.7751338481903076, "y": 0.1572750210762024}, {"x": 0.7751338481903076, "y": 0.17956265807151794}, {"x": 0.5585960745811462, "y": 0.17830109596252441}], "text": "If I want to travel from Los Angeles to New York and I\nwant to choose the cheapest mode of transportation,\nshould I drive or take a plane? <Decision making>\n"}
{"page": 15, "bbox": [{"x": 0.20939916372299194, "y": 0.17914213240146637}, {"x": 0.327781081199646, "y": 0.17914213240146637}, {"x": 0.327781081199646, "y": 0.2068965584039688}, {"x": 0.20939916372299194, "y": 0.2068965584039688}], "text": "Tom has three sisters, and each\nsister has a brother. How many\nsiblings are there in total?\n<Reasonning >\n"}
{"page": 15, "bbox": [{"x": 0.5585960745811462, "y": 0.19007569551467896}, {"x": 0.773944079875946, "y": 0.19007569551467896}, {"x": 0.773944079875946, "y": 0.21867115795612335}, {"x": 0.5585960745811462, "y": 0.21867115795612335}], "text": "I had a quarrel with my parents because they oppose my\nrelationship with my boyfriend, but we genuinely love\neach other. How should I persuade my parents to accept\nour relationship?\n< Suggestion>\n"}
{"page": 15, "bbox": [{"x": 0.37180250883102417, "y": 0.19386038184165955}, {"x": 0.48483046889305115, "y": 0.19343987107276917}, {"x": 0.48483046889305115, "y": 0.22371740639209747}, {"x": 0.37180250883102417, "y": 0.22413793206214905}], "text": "\"ChatGPT is a product of\nOpenAI.\"\nPlease provide the ownership\nrelationship.\n"}
{"page": 15, "bbox": [{"x": 0.20880427956581116, "y": 0.2161480188369751}, {"x": 0.32956573367118835, "y": 0.21656854450702667}, {"x": 0.32956573367118835, "y": 0.2304457575082779}, {"x": 0.20880427956581116, "y": 0.23002523183822632}], "text": "Identify who is football players:\nMessi, Jordan, Kobe.\n"}
{"page": 15, "bbox": [{"x": 0.3866746127605438, "y": 0.22455845773220062}, {"x": 0.4866151213645935, "y": 0.22371740639209747}, {"x": 0.4866151213645935, "y": 0.22960470616817474}, {"x": 0.3866746127605438, "y": 0.2304457575082779}], "text": "<Information extraction>\n"}
{"page": 15, "bbox": [{"x": 0.25639501214027405, "y": 0.232127845287323}, {"x": 0.311124324798584, "y": 0.232127845287323}, {"x": 0.311124324798584, "y": 0.2375946193933487}, {"x": 0.25639501214027405, "y": 0.2375946193933487}], "text": "<Closed QA >\n"}
{"page": 15, "bbox": [{"x": 0.5585960745811462, "y": 0.23423044383525848}, {"x": 0.7281380295753479, "y": 0.23423044383525848}, {"x": 0.7281380295753479, "y": 0.24011774361133575}, {"x": 0.5585960745811462, "y": 0.24011774361133575}], "text": "Which city will the next World Cup be held?\n"}
{"page": 15, "bbox": [{"x": 0.6597263813018799, "y": 0.24222035706043243}, {"x": 0.697204053401947, "y": 0.24222035706043243}, {"x": 0.697204053401947, "y": 0.2464255690574646}, {"x": 0.6597263813018799, "y": 0.2464255690574646}], "text": "< Search >\n"}
{"page": 15, "bbox": [{"x": 0.2629387378692627, "y": 0.24852816760540009}, {"x": 0.3866746127605438, "y": 0.24852816760540009}, {"x": 0.3866746127605438, "y": 0.25609755516052246}, {"x": 0.2629387378692627, "y": 0.25609755516052246}], "text": "Insufficient information\n"}
{"page": 15, "bbox": [{"x": 0.5574063062667847, "y": 0.2628259062767029}, {"x": 0.7733492255210876, "y": 0.2603027820587158}, {"x": 0.773944079875946, "y": 0.28343144059181213}, {"x": 0.5580011606216431, "y": 0.2859545946121216}], "text": "If you're currently a computer science student and your\ncomputer system encounters a malfunction, what should\nyou do?\n<Role-play>\n"}
{"page": 15, "bbox": [{"x": 0.36823320388793945, "y": 0.2640874683856964}, {"x": 0.48839977383613586, "y": 0.26366695761680603}, {"x": 0.48839977383613586, "y": 0.2838519811630249}, {"x": 0.36823320388793945, "y": 0.2842724919319153}], "text": "Please find a novel that is as\nfamous as \"One Hundred Years\nof Solitude\".\n<Search >\n"}
{"page": 15, "bbox": [{"x": 0.21296846866607666, "y": 0.26324641704559326}, {"x": 0.3325401544570923, "y": 0.2640874683856964}, {"x": 0.3319452702999115, "y": 0.2931034564971924}, {"x": 0.21237358450889587, "y": 0.29226240515708923}], "text": "\"French. Washington played a\ncrucial role in the American\nRevolutionary War, leading the\nContinental Army against the\n"}
{"page": 15, "bbox": [{"x": 0.21356335282325745, "y": 0.29352396726608276}, {"x": 0.2409280240535736, "y": 0.29352396726608276}, {"x": 0.2409280240535736, "y": 0.2977291941642761}, {"x": 0.21356335282325745, "y": 0.2977291941642761}], "text": "British.\n"}
{"page": 15, "bbox": [{"x": 0.37180250883102417, "y": 0.29941126704216003}, {"x": 0.4556811451911926, "y": 0.2985702157020569}, {"x": 0.4556811451911926, "y": 0.3111858665943146}, {"x": 0.37180250883102417, "y": 0.3120269179344177}], "text": "Q: 3,1 A: 3 Q: 2,5 A: 5\nQ: 5,7 A: ?\n"}
{"page": 15, "bbox": [{"x": 0.5585960745811462, "y": 0.29730865359306335}, {"x": 0.7733492255210876, "y": 0.2977291941642761}, {"x": 0.7733492255210876, "y": 0.3195962905883789}, {"x": 0.5585960745811462, "y": 0.3191757798194885}], "text": "Write an article about the geography of Europe, focusing\non the changes in rainfall in the western part of the\ncountry.\n<Writing>\n"}
{"page": 15, "bbox": [{"x": 0.21356335282325745, "y": 0.29941126704216003}, {"x": 0.33908388018608093, "y": 0.3002523183822632}, {"x": 0.33848899602890015, "y": 0.3212783932685852}, {"x": 0.21296846866607666, "y": 0.32043734192848206}], "text": "Please continue writing the\nabove paragraph.\n<Continuation writing>\n"}
{"page": 15, "bbox": [{"x": 0.40392622351646423, "y": 0.31370899081230164}, {"x": 0.4901844263076782, "y": 0.3141295313835144}, {"x": 0.4901844263076782, "y": 0.32043734192848206}, {"x": 0.40392622351646423, "y": 0.3200168311595917}], "text": "<In-context learning>\n"}
{"page": 15, "bbox": [{"x": 0.5877453684806824, "y": 0.3317914307117462}, {"x": 0.7626413106918335, "y": 0.332632452249527}, {"x": 0.7626413106918335, "y": 0.3427249789237976}, {"x": 0.5877453684806824, "y": 0.34188392758369446}], "text": "No Background Knowledge\n"}
{"page": 15, "bbox": [{"x": 0.24866151809692383, "y": 0.33305299282073975}, {"x": 0.4003569185733795, "y": 0.33305299282073975}, {"x": 0.4003569185733795, "y": 0.3423044681549072}, {"x": 0.24866151809692383, "y": 0.3423044681549072}], "text": "Background Knowledge\n"}
{"page": 15, "bbox": [{"x": 0.11838191747665405, "y": 0.3624894917011261}, {"x": 0.8810232281684875, "y": 0.3624894917011261}, {"x": 0.8810232281684875, "y": 0.38519763946533203}, {"x": 0.11838191747665405, "y": 0.38519763946533203}], "text": "Figure 2: Classification of retrieval requirements for different tasks. In cases where information is not provided, we\ndifferentiate tasks based on the functions of the model.\n"}
{"page": 15, "bbox": [{"x": 0.336109459400177, "y": 0.4179983139038086}, {"x": 0.39500296115875244, "y": 0.41715726256370544}, {"x": 0.39500296115875244, "y": 0.42598822712898254}, {"x": 0.336109459400177, "y": 0.4268292784690857}], "text": "lyft 2021\n"}
{"page": 15, "bbox": [{"x": 0.12730517983436584, "y": 0.42388561367988586}, {"x": 0.20523497462272644, "y": 0.42388561367988586}, {"x": 0.20523497462272644, "y": 0.43103447556495667}, {"x": 0.12730517983436584, "y": 0.43103447556495667}], "text": "Chunk Skill\n"}
{"page": 15, "bbox": [{"x": 0.23616895079612732, "y": 0.4343986511230469}, {"x": 0.3628792464733124, "y": 0.43313708901405334}, {"x": 0.3628792464733124, "y": 0.4428090751171112}, {"x": 0.23616895079612732, "y": 0.44407063722610474}], "text": "Average Faithfulness\n"}
{"page": 15, "bbox": [{"x": 0.3801308870315552, "y": 0.4343986511230469}, {"x": 0.4961332678794861, "y": 0.4339781403541565}, {"x": 0.4961332678794861, "y": 0.44365012645721436}, {"x": 0.3801308870315552, "y": 0.44407063722610474}], "text": "Average Relevancy\n"}
{"page": 15, "bbox": [{"x": 0.28256988525390625, "y": 0.44743481278419495}, {"x": 0.31766805052757263, "y": 0.44743481278419495}, {"x": 0.31766805052757263, "y": 0.45416316390037537}, {"x": 0.28256988525390625, "y": 0.45416316390037537}], "text": "95.74\n"}
{"page": 15, "bbox": [{"x": 0.12730517983436584, "y": 0.4461732506752014}, {"x": 0.17846520245075226, "y": 0.4465937614440918}, {"x": 0.17846520245075226, "y": 0.45626577734947205}, {"x": 0.12730517983436584, "y": 0.4558452367782593}], "text": "Original\n"}
{"page": 15, "bbox": [{"x": 0.4182034432888031, "y": 0.44701430201530457}, {"x": 0.457465797662735, "y": 0.44701430201530457}, {"x": 0.457465797662735, "y": 0.47771236300468445}, {"x": 0.4182034432888031, "y": 0.47771236300468445}], "text": "95.37\n95.37\n96.85\n"}
{"page": 15, "bbox": [{"x": 0.12730517983436584, "y": 0.4583683907985687}, {"x": 0.18619869649410248, "y": 0.4587889015674591}, {"x": 0.18619869649410248, "y": 0.4676198363304138}, {"x": 0.12730517983436584, "y": 0.46719932556152344}], "text": "small2big\n"}
{"page": 15, "bbox": [{"x": 0.2801903486251831, "y": 0.4592094123363495}, {"x": 0.319452702999115, "y": 0.4592094123363495}, {"x": 0.319452702999115, "y": 0.47771236300468445}, {"x": 0.2801903486251831, "y": 0.47771236300468445}], "text": "96.67\n97.41\n"}
{"page": 15, "bbox": [{"x": 0.12671029567718506, "y": 0.47056350111961365}, {"x": 0.21832242608070374, "y": 0.4693019390106201}, {"x": 0.21832242608070374, "y": 0.4785534143447876}, {"x": 0.12671029567718506, "y": 0.47981497645378113}], "text": "sliding window\n"}
{"page": 15, "bbox": [{"x": 0.14753122627735138, "y": 0.49537426233291626}, {"x": 0.4562760293483734, "y": 0.4945332109928131}, {"x": 0.4562760293483734, "y": 0.5046257376670837}, {"x": 0.14753122627735138, "y": 0.5054667592048645}], "text": "Table 4: Comparison of different chunk skills.\n"}
{"page": 15, "bbox": [{"x": 0.2153480052947998, "y": 0.5412111282348633}, {"x": 0.45865556597709656, "y": 0.5399495363235474}, {"x": 0.45925045013427734, "y": 0.5841042995452881}, {"x": 0.2159428894519806, "y": 0.5853658318519592}], "text": "Multiple Billion- Hybrid Cloud-\nIndex Type Scale Search Native\nX\nX\n☑\n"}
{"page": 15, "bbox": [{"x": 0.14515168964862823, "y": 0.5479394197463989}, {"x": 0.20523497462272644, "y": 0.5479394197463989}, {"x": 0.20523497462272644, "y": 0.5853658318519592}, {"x": 0.14515168964862823, "y": 0.5853658318519592}], "text": "Database\nWeaviate\nFaiss\n"}
{"page": 15, "bbox": [{"x": 0.14574657380580902, "y": 0.589991569519043}, {"x": 0.1933373063802719, "y": 0.5895710587501526}, {"x": 0.1933373063802719, "y": 0.5971404314041138}, {"x": 0.14574657380580902, "y": 0.5975610017776489}], "text": "Chroma\n"}
{"page": 15, "bbox": [{"x": 0.14515168964862823, "y": 0.6021867394447327}, {"x": 0.1897680014371872, "y": 0.602607250213623}, {"x": 0.1897680014371872, "y": 0.6223717331886292}, {"x": 0.14515168964862823, "y": 0.6219512224197388}], "text": "Qdrant\nMilvus\n"}
{"page": 15, "bbox": [{"x": 0.5127900242805481, "y": 0.4146341383457184}, {"x": 0.8845925331115723, "y": 0.4146341383457184}, {"x": 0.8851873874664307, "y": 0.8145500421524048}, {"x": 0.5133848786354065, "y": 0.8145500421524048}], "text": "Implementation Details For sparse retrieval,\nwe use the BM25 algorithm, which relies on the\nTF-IDF algorithm. For dense retrieval, we em-\nploy Contriever as our unsupervised contrastive\ntext encoder. Based on our evaluation of embed-\nding models, we implement our supervised dense\nretrieval using LLM-Embedder. We use the default\nimplementation of BM25 and Contriever from Py-\nserini (Lin et al., 2021a). The BM25 index is con-\nstructed using Lucene on MS MARCO collections,\nwhile the dense vector index is generated with Faiss\nemploying Flat configuration on the same dataset.\nFor query rewriting, we prompt Zephyr-7b-alpha¹0,\na model trained to act as a helpful assistant, to\nrewrite the original query. For query decompo-\nsition, we employ GPT-3.5-turbo-0125 to break\ndown the original query into multiple sub-queries.\nWe closely follow the implementation from HyDE\n(Gao et al., 2022), utilizing the more advanced\ninstruction-following language model, GPT-3.5-\nturbo-instruct, to generate hypothetical answers.\nThe model infers with a default temperature of 0.7,\nsampling up to a maximum of 512 tokens. Re-\ntrieval experiments and evaluation are conducted\nusing the Pyserini toolkit.\n"}
{"page": 15, "bbox": [{"x": 0.13563355803489685, "y": 0.6396130919456482}, {"x": 0.4693634808063507, "y": 0.6396130919456482}, {"x": 0.4693634808063507, "y": 0.6497056484222412}, {"x": 0.13563355803489685, "y": 0.6497056484222412}], "text": "Table 6: Comparison of Various Vector Databases\n"}
{"page": 15, "bbox": [{"x": 0.11719214916229248, "y": 0.6770395040512085}, {"x": 0.4259369373321533, "y": 0.6757779717445374}, {"x": 0.4259369373321533, "y": 0.6875525712966919}, {"x": 0.11719214916229248, "y": 0.688814103603363}], "text": "A.3 Experimental Details of Retrieval\n"}
{"page": 15, "bbox": [{"x": 0.1641879826784134, "y": 0.6938604116439819}, {"x": 0.2325996458530426, "y": 0.6938604116439819}, {"x": 0.2325996458530426, "y": 0.7018502950668335}, {"x": 0.1641879826784134, "y": 0.7018502950668335}], "text": "Methods\n"}
{"page": 15, "bbox": [{"x": 0.11838191747665405, "y": 0.7140454053878784}, {"x": 0.48839977383613586, "y": 0.714886486530304}, {"x": 0.4878048896789551, "y": 0.9230445623397827}, {"x": 0.11778703331947327, "y": 0.922203540802002}], "text": "Implementation details of the comparative experi-\nments of different retrieval methods are as below:\nDatasets We use the TREC DL 2019 (Craswell\net al., 2020) and 2020 (Craswell et al., 2021) pas-\nsage ranking datasets to evaluate the performance\nof different retrieval methods.\nMetrics Widely-used evaluation metrics for re-\ntrieval include mAP, nDCG@10, R@50 and R@ 1k.\nBoth mAP and nDCG@ 10 are order-aware metrics\nthat take the ranking of search results into account.\nIn contrast, R@k is an order-unaware metric. We\nalso report the average latency incurred by each\nmethod per query.\n"}
{"page": 15, "bbox": [{"x": 0.5127900242805481, "y": 0.8288477659225464}, {"x": 0.8810232281684875, "y": 0.8296887874603271}, {"x": 0.8810232281684875, "y": 0.8961312174797058}, {"x": 0.5127900242805481, "y": 0.8952901363372803}], "text": "A.3.1 HyDE with Different Concatenation of\nDocuments and Query\nTable 8 shows the impact of different concatenation\nstrategies for hypothetical documents and queries\n"}
{"page": 15, "bbox": [{"x": 0.5300416350364685, "y": 0.9083263278007507}, {"x": 0.8477097153663635, "y": 0.9095878601074219}, {"x": 0.8477097153663635, "y": 0.9205214381217957}, {"x": 0.5300416350364685, "y": 0.9192599058151245}], "text": "10https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n"}
{"page": 15, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17730\n"}
{"page": 16, "bbox": [{"x": 0.6008328199386597, "y": 0.08662741631269455}, {"x": 0.7525282502174377, "y": 0.08410429209470749}, {"x": 0.7525282502174377, "y": 0.0929352417588234}, {"x": 0.6008328199386597, "y": 0.09545836597681046}], "text": "namespace-Pt/msmarco\n"}
{"page": 16, "bbox": [{"x": 0.13920284807682037, "y": 0.09125315397977829}, {"x": 0.2587745487689972, "y": 0.08999159187078476}, {"x": 0.2587745487689972, "y": 0.09966358542442322}, {"x": 0.13920284807682037, "y": 0.10092514753341675}], "text": "Embedding Model\n"}
{"page": 16, "bbox": [{"x": 0.4949434995651245, "y": 0.10428931564092636}, {"x": 0.7019631266593933, "y": 0.10428931564092636}, {"x": 0.7019631266593933, "y": 0.11185870319604874}, {"x": 0.4949434995651245, "y": 0.11185870319604874}], "text": "MRR@1 MRR@10 MRR @ 100\n"}
{"page": 16, "bbox": [{"x": 0.5734681487083435, "y": 0.11648444086313248}, {"x": 0.6067816615104675, "y": 0.11648444086313248}, {"x": 0.6067816615104675, "y": 0.12279225885868073}, {"x": 0.5734681487083435, "y": 0.12279225885868073}], "text": "37.58\n"}
{"page": 16, "bbox": [{"x": 0.7150505781173706, "y": 0.10386879742145538}, {"x": 0.8602022528648376, "y": 0.10386879742145538}, {"x": 0.8602022528648376, "y": 0.1358284205198288}, {"x": 0.7150505781173706, "y": 0.1358284205198288}], "text": "R@1 R@10 R@100\n24.07 66.45 90.75\n22.63 64.12 90.13\n"}
{"page": 16, "bbox": [{"x": 0.6496133208274841, "y": 0.11648444086313248}, {"x": 0.6835216879844666, "y": 0.11690495908260345}, {"x": 0.6835216879844666, "y": 0.12363330274820328}, {"x": 0.6496133208274841, "y": 0.1232127845287323}], "text": "38.62\n"}
{"page": 16, "bbox": [{"x": 0.5044616460800171, "y": 0.11690495908260345}, {"x": 0.5389649271965027, "y": 0.11690495908260345}, {"x": 0.5389649271965027, "y": 0.12363330274820328}, {"x": 0.5044616460800171, "y": 0.12363330274820328}], "text": "24.79\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.11648444086313248}, {"x": 0.40392622351646423, "y": 0.11648444086313248}, {"x": 0.40392622351646423, "y": 0.12573590874671936}, {"x": 0.13979773223400116, "y": 0.12573590874671936}], "text": "BAAI/LLM-Embedder(Zhang et al., 2023a)\n"}
{"page": 16, "bbox": [{"x": 0.5044616460800171, "y": 0.12825904786586761}, {"x": 0.5395597815513611, "y": 0.12783852219581604}, {"x": 0.5395597815513611, "y": 0.13498738408088684}, {"x": 0.5044616460800171, "y": 0.13540790975093842}], "text": "23.34\n"}
{"page": 16, "bbox": [{"x": 0.5734681487083435, "y": 0.12825904786586761}, {"x": 0.6085663437843323, "y": 0.12825904786586761}, {"x": 0.6085663437843323, "y": 0.13498738408088684}, {"x": 0.5734681487083435, "y": 0.13498738408088684}], "text": "35.80\n"}
{"page": 16, "bbox": [{"x": 0.6502082347869873, "y": 0.12825904786586761}, {"x": 0.6847114562988281, "y": 0.12825904786586761}, {"x": 0.6847114562988281, "y": 0.13498738408088684}, {"x": 0.6502082347869873, "y": 0.13498738408088684}], "text": "36.94\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.12783852219581604}, {"x": 0.39381319284439087, "y": 0.1269974708557129}, {"x": 0.39381319284439087, "y": 0.13666947185993195}, {"x": 0.13979773223400116, "y": 0.1375105082988739}], "text": "BAAI/bge-base-en-v1.5(Xiao et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.5734681487083435, "y": 0.13961312174797058}, {"x": 0.6073765754699707, "y": 0.14003364741802216}, {"x": 0.6073765754699707, "y": 0.14718250930309296}, {"x": 0.5734681487083435, "y": 0.14676198363304138}], "text": "35.78\n"}
{"page": 16, "bbox": [{"x": 0.5044616460800171, "y": 0.14045415818691254}, {"x": 0.5389649271965027, "y": 0.14045415818691254}, {"x": 0.5389649271965027, "y": 0.14676198363304138}, {"x": 0.5044616460800171, "y": 0.14676198363304138}], "text": "23.27\n"}
{"page": 16, "bbox": [{"x": 0.6496133208274841, "y": 0.13961312174797058}, {"x": 0.6841166019439697, "y": 0.14045415818691254}, {"x": 0.6835216879844666, "y": 0.14760303497314453}, {"x": 0.6496133208274841, "y": 0.14676198363304138}], "text": "36.89\n"}
{"page": 16, "bbox": [{"x": 0.7150505781173706, "y": 0.14003364741802216}, {"x": 0.8530636429786682, "y": 0.14003364741802216}, {"x": 0.8530636429786682, "y": 0.14718250930309296}, {"x": 0.7150505781173706, "y": 0.14718250930309296}], "text": "22.65 63.92 89.80\n"}
{"page": 16, "bbox": [{"x": 0.14039261639118195, "y": 0.14003364741802216}, {"x": 0.4003569185733795, "y": 0.14003364741802216}, {"x": 0.4003569185733795, "y": 0.14886459708213806}, {"x": 0.14039261639118195, "y": 0.14886459708213806}], "text": "BAAI/bge-small-en-v1.5(Xiao et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.6496133208274841, "y": 0.1518082469701767}, {"x": 0.6835216879844666, "y": 0.1518082469701767}, {"x": 0.6835216879844666, "y": 0.15811605751514435}, {"x": 0.6496133208274841, "y": 0.15811605751514435}], "text": "38.59\n"}
{"page": 16, "bbox": [{"x": 0.7150505781173706, "y": 0.15096719563007355}, {"x": 0.8530636429786682, "y": 0.1501261591911316}, {"x": 0.8530636429786682, "y": 0.1589571088552475}, {"x": 0.7150505781173706, "y": 0.15979814529418945}], "text": "23.91 65.57 90.60\n"}
{"page": 16, "bbox": [{"x": 0.5044616460800171, "y": 0.1518082469701767}, {"x": 0.5395597815513611, "y": 0.15138772130012512}, {"x": 0.5395597815513611, "y": 0.15853658318519592}, {"x": 0.5044616460800171, "y": 0.1589571088552475}], "text": "24.63\n"}
{"page": 16, "bbox": [{"x": 0.5734681487083435, "y": 0.15222875773906708}, {"x": 0.6073765754699707, "y": 0.15222875773906708}, {"x": 0.6073765754699707, "y": 0.15853658318519592}, {"x": 0.5734681487083435, "y": 0.15853658318519592}], "text": "37.48\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.15096719563007355}, {"x": 0.3967876136302948, "y": 0.1501261591911316}, {"x": 0.3967876136302948, "y": 0.1606391966342926}, {"x": 0.13979773223400116, "y": 0.16148023307323456}], "text": "BAAI/bge-large-en-v1.5(Xiao et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.647233784198761, "y": 0.16358284652233124}, {"x": 0.6870909929275513, "y": 0.16316232085227966}, {"x": 0.6870909929275513, "y": 0.1698906570672989}, {"x": 0.647233784198761, "y": 0.17031118273735046}], "text": "38.73\n"}
{"page": 16, "bbox": [{"x": 0.5026769638061523, "y": 0.16358284652233124}, {"x": 0.5407495498657227, "y": 0.16316232085227966}, {"x": 0.5407495498657227, "y": 0.17031118273735046}, {"x": 0.5026769638061523, "y": 0.17073170840740204}], "text": "24.84\n"}
{"page": 16, "bbox": [{"x": 0.8191552758216858, "y": 0.16358284652233124}, {"x": 0.8542534112930298, "y": 0.16358284652233124}, {"x": 0.8542534112930298, "y": 0.17031118273735046}, {"x": 0.8191552758216858, "y": 0.17031118273735046}], "text": "90.64\n"}
{"page": 16, "bbox": [{"x": 0.5710886120796204, "y": 0.16400335729122162}, {"x": 0.6103509664535522, "y": 0.16400335729122162}, {"x": 0.6103509664535522, "y": 0.17031118273735046}, {"x": 0.5710886120796204, "y": 0.17031118273735046}], "text": "37.66\n"}
{"page": 16, "bbox": [{"x": 0.7126710414886475, "y": 0.16316232085227966}, {"x": 0.7995240688323975, "y": 0.16358284652233124}, {"x": 0.7995240688323975, "y": 0.1711522340774536}, {"x": 0.7126710414886475, "y": 0.17073170840740204}], "text": "24.13 66.09\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.16316232085227966}, {"x": 0.3652587831020355, "y": 0.1623212844133377}, {"x": 0.3652587831020355, "y": 0.17199327051639557}, {"x": 0.13979773223400116, "y": 0.17283432185649872}], "text": "BAAI/bge-large-en(Xiao et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.5044616460800171, "y": 0.17577797174453735}, {"x": 0.5383700132369995, "y": 0.17535744607448578}, {"x": 0.5383700132369995, "y": 0.18250630795955658}, {"x": 0.5044616460800171, "y": 0.18292683362960815}], "text": "23.28\n"}
{"page": 16, "bbox": [{"x": 0.5734681487083435, "y": 0.17577797174453735}, {"x": 0.6073765754699707, "y": 0.17577797174453735}, {"x": 0.6073765754699707, "y": 0.18250630795955658}, {"x": 0.5734681487083435, "y": 0.18250630795955658}], "text": "35.79\n"}
{"page": 16, "bbox": [{"x": 0.6496133208274841, "y": 0.17577797174453735}, {"x": 0.6841166019439697, "y": 0.17577797174453735}, {"x": 0.6841166019439697, "y": 0.18250630795955658}, {"x": 0.6496133208274841, "y": 0.18250630795955658}], "text": "36.91\n"}
{"page": 16, "bbox": [{"x": 0.7144556641578674, "y": 0.17619848251342773}, {"x": 0.7989292144775391, "y": 0.1749369204044342}, {"x": 0.7989292144775391, "y": 0.18250630795955658}, {"x": 0.7144556641578674, "y": 0.1837678700685501}], "text": "22.62 63.96\n"}
{"page": 16, "bbox": [{"x": 0.8185603618621826, "y": 0.17619848251342773}, {"x": 0.8542534112930298, "y": 0.17619848251342773}, {"x": 0.8542534112930298, "y": 0.18292683362960815}, {"x": 0.8185603618621826, "y": 0.18292683362960815}], "text": "89.67\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.17535744607448578}, {"x": 0.36882808804512024, "y": 0.17535744607448578}, {"x": 0.36882808804512024, "y": 0.18418839573860168}, {"x": 0.13979773223400116, "y": 0.18418839573860168}], "text": "BAAI/bge-small-en(Xiao et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.6496133208274841, "y": 0.18713204562664032}, {"x": 0.6835216879844666, "y": 0.18671151995658875}, {"x": 0.6835216879844666, "y": 0.19428090751171112}, {"x": 0.6496133208274841, "y": 0.1947014331817627}], "text": "37.07\n"}
{"page": 16, "bbox": [{"x": 0.5734681487083435, "y": 0.1875525712966919}, {"x": 0.6067816615104675, "y": 0.1875525712966919}, {"x": 0.6067816615104675, "y": 0.19428090751171112}, {"x": 0.5734681487083435, "y": 0.19428090751171112}], "text": "35.94\n"}
{"page": 16, "bbox": [{"x": 0.5044616460800171, "y": 0.18797308206558228}, {"x": 0.5389649271965027, "y": 0.18797308206558228}, {"x": 0.5389649271965027, "y": 0.19428090751171112}, {"x": 0.5044616460800171, "y": 0.19428090751171112}], "text": "23.47\n"}
{"page": 16, "bbox": [{"x": 0.7144556641578674, "y": 0.1875525712966919}, {"x": 0.8530636429786682, "y": 0.1875525712966919}, {"x": 0.8530636429786682, "y": 0.1947014331817627}, {"x": 0.7144556641578674, "y": 0.1947014331817627}], "text": "22.73 64.17 90.14\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.18713204562664032}, {"x": 0.3622843623161316, "y": 0.18671151995658875}, {"x": 0.3622843623161316, "y": 0.1963835209608078}, {"x": 0.13979773223400116, "y": 0.19680403172969818}], "text": "BAAI/bge-base-en(Xiao et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.5086258053779602, "y": 0.19974768161773682}, {"x": 0.535395622253418, "y": 0.19932717084884644}, {"x": 0.535395622253418, "y": 0.2056349813938141}, {"x": 0.5086258053779602, "y": 0.20605550706386566}], "text": "8.93\n"}
{"page": 16, "bbox": [{"x": 0.7186198830604553, "y": 0.19890664517879486}, {"x": 0.8542534112930298, "y": 0.19932717084884644}, {"x": 0.8542534112930298, "y": 0.20647603273391724}, {"x": 0.7186198830604553, "y": 0.20605550706386566}], "text": "8.67 32.28 60.36\n"}
{"page": 16, "bbox": [{"x": 0.5740630626678467, "y": 0.19974768161773682}, {"x": 0.6079714298248291, "y": 0.19974768161773682}, {"x": 0.6079714298248291, "y": 0.20647603273391724}, {"x": 0.5740630626678467, "y": 0.20647603273391724}], "text": "15.60\n"}
{"page": 16, "bbox": [{"x": 0.6502082347869873, "y": 0.19974768161773682}, {"x": 0.6841166019439697, "y": 0.19974768161773682}, {"x": 0.6841166019439697, "y": 0.20647603273391724}, {"x": 0.6502082347869873, "y": 0.20647603273391724}], "text": "16.71\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.19932717084884644}, {"x": 0.4229625165462494, "y": 0.19932717084884644}, {"x": 0.4229625165462494, "y": 0.20815812051296234}, {"x": 0.13979773223400116, "y": 0.20815812051296234}], "text": "Alibaba-NLP/gte-large-en-v1.5(Li et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.5746579170227051, "y": 0.2106812447309494}, {"x": 0.6079714298248291, "y": 0.21110177040100098}, {"x": 0.6079714298248291, "y": 0.21825063228607178}, {"x": 0.5746579170227051, "y": 0.2178301066160202}], "text": "13.23\n"}
{"page": 16, "bbox": [{"x": 0.7192147374153137, "y": 0.21110177040100098}, {"x": 0.8530636429786682, "y": 0.20984020829200745}, {"x": 0.8530636429786682, "y": 0.2178301066160202}, {"x": 0.7192147374153137, "y": 0.21909166872501373}], "text": "7.21 28.27 56.20\n"}
{"page": 16, "bbox": [{"x": 0.5092207193374634, "y": 0.21152228116989136}, {"x": 0.535395622253418, "y": 0.21152228116989136}, {"x": 0.535395622253418, "y": 0.2178301066160202}, {"x": 0.5092207193374634, "y": 0.2178301066160202}], "text": "7.42\n"}
{"page": 16, "bbox": [{"x": 0.6502082347869873, "y": 0.21110177040100098}, {"x": 0.6847114562988281, "y": 0.21110177040100098}, {"x": 0.6847114562988281, "y": 0.21825063228607178}, {"x": 0.6502082347869873, "y": 0.21825063228607178}], "text": "14.30\n"}
{"page": 16, "bbox": [{"x": 0.13920284807682037, "y": 0.2106812447309494}, {"x": 0.3372992277145386, "y": 0.20984020829200745}, {"x": 0.3372992277145386, "y": 0.2195121943950653}, {"x": 0.13920284807682037, "y": 0.22035323083400726}], "text": "thenlper/gte-base(Li et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.6502082347869873, "y": 0.22287636995315552}, {"x": 0.6841166019439697, "y": 0.22287636995315552}, {"x": 0.6841166019439697, "y": 0.22960470616817474}, {"x": 0.6502082347869873, "y": 0.22960470616817474}], "text": "15.95\n"}
{"page": 16, "bbox": [{"x": 0.5086258053779602, "y": 0.2232968807220459}, {"x": 0.535395622253418, "y": 0.2232968807220459}, {"x": 0.535395622253418, "y": 0.22960470616817474}, {"x": 0.5086258053779602, "y": 0.22960470616817474}], "text": "7.97\n"}
{"page": 16, "bbox": [{"x": 0.5740630626678467, "y": 0.2232968807220459}, {"x": 0.6073765754699707, "y": 0.2232968807220459}, {"x": 0.6073765754699707, "y": 0.22960470616817474}, {"x": 0.5740630626678467, "y": 0.22960470616817474}], "text": "14.81\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.22245584428310394}, {"x": 0.34265318512916565, "y": 0.221614807844162}, {"x": 0.34265318512916565, "y": 0.23128679394721985}, {"x": 0.13979773223400116, "y": 0.232127845287323}], "text": "thenlper/gte-small (Li et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.5086258053779602, "y": 0.23465096950531006}, {"x": 0.5348007082939148, "y": 0.23465096950531006}, {"x": 0.5348007082939148, "y": 0.24137930572032928}, {"x": 0.5086258053779602, "y": 0.24137930572032928}], "text": "8.07\n"}
{"page": 16, "bbox": [{"x": 0.6502082347869873, "y": 0.23423044383525848}, {"x": 0.6841166019439697, "y": 0.23465096950531006}, {"x": 0.6841166019439697, "y": 0.24179983139038086}, {"x": 0.6502082347869873, "y": 0.24137930572032928}], "text": "16.12\n"}
{"page": 16, "bbox": [{"x": 0.5740630626678467, "y": 0.23423044383525848}, {"x": 0.6073765754699707, "y": 0.23507149517536163}, {"x": 0.6073765754699707, "y": 0.24179983139038086}, {"x": 0.5740630626678467, "y": 0.2409587949514389}], "text": "15.02\n"}
{"page": 16, "bbox": [{"x": 0.13860797882080078, "y": 0.23423044383525848}, {"x": 0.48483046889305115, "y": 0.23423044383525848}, {"x": 0.48483046889305115, "y": 0.24390244483947754}, {"x": 0.13860797882080078, "y": 0.24390244483947754}], "text": "jinaai/jina-embeddings-v2-small-en(Günther et al., 2023)\n"}
{"page": 16, "bbox": [{"x": 0.5740630626678467, "y": 0.24600504338741302}, {"x": 0.6079714298248291, "y": 0.2464255690574646}, {"x": 0.6079714298248291, "y": 0.2535744309425354}, {"x": 0.5740630626678467, "y": 0.253153920173645}], "text": "18.23\n"}
{"page": 16, "bbox": [{"x": 0.6508030891418457, "y": 0.24600504338741302}, {"x": 0.6841166019439697, "y": 0.2464255690574646}, {"x": 0.6841166019439697, "y": 0.2535744309425354}, {"x": 0.6508030891418457, "y": 0.253153920173645}], "text": "19.41\n"}
{"page": 16, "bbox": [{"x": 0.5056514143943787, "y": 0.2464255690574646}, {"x": 0.5389649271965027, "y": 0.2464255690574646}, {"x": 0.5389649271965027, "y": 0.253153920173645}, {"x": 0.5056514143943787, "y": 0.253153920173645}], "text": "10.04\n"}
{"page": 16, "bbox": [{"x": 0.8191552758216858, "y": 0.2464255690574646}, {"x": 0.8536585569381714, "y": 0.2464255690574646}, {"x": 0.8536585569381714, "y": 0.2535744309425354}, {"x": 0.8191552758216858, "y": 0.2535744309425354}], "text": "68.42\n"}
{"page": 16, "bbox": [{"x": 0.7186198830604553, "y": 0.2232968807220459}, {"x": 0.854848325252533, "y": 0.2232968807220459}, {"x": 0.854848325252533, "y": 0.27754414081573486}, {"x": 0.7186198830604553, "y": 0.27754414081573486}], "text": "7.71 32.07 61.08\n7.87 32.55 60.36\n9.74 38.92\n9.35 39.00 66.11\n5.66 25.57 50.94\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.2464255690574646}, {"x": 0.3741820454597473, "y": 0.2464255690574646}, {"x": 0.3741820454597473, "y": 0.2552565038204193}, {"x": 0.13979773223400116, "y": 0.2552565038204193}], "text": "intfloat/e5-small-v2(Wang et al., 2022)\n"}
{"page": 16, "bbox": [{"x": 0.6502082347869873, "y": 0.25820016860961914}, {"x": 0.6847114562988281, "y": 0.2586206793785095}, {"x": 0.6847114562988281, "y": 0.2657695412635803}, {"x": 0.6502082347869873, "y": 0.26534903049468994}], "text": "19.03\n"}
{"page": 16, "bbox": [{"x": 0.5086258053779602, "y": 0.2590412199497223}, {"x": 0.5348007082939148, "y": 0.2590412199497223}, {"x": 0.5348007082939148, "y": 0.26534903049468994}, {"x": 0.5086258053779602, "y": 0.26534903049468994}], "text": "9.58\n"}
{"page": 16, "bbox": [{"x": 0.5740630626678467, "y": 0.2590412199497223}, {"x": 0.6079714298248291, "y": 0.2590412199497223}, {"x": 0.6079714298248291, "y": 0.2657695412635803}, {"x": 0.5740630626678467, "y": 0.2657695412635803}], "text": "17.94\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.2586206793785095}, {"x": 0.3712076246738434, "y": 0.2586206793785095}, {"x": 0.3712076246738434, "y": 0.2674516439437866}, {"x": 0.13979773223400116, "y": 0.2674516439437866}], "text": "intfloat/e5-large-v2(Wang et al., 2022)\n"}
{"page": 16, "bbox": [{"x": 0.5086258053779602, "y": 0.27081581950187683}, {"x": 0.535395622253418, "y": 0.27081581950187683}, {"x": 0.535395622253418, "y": 0.2771236300468445}, {"x": 0.5086258053779602, "y": 0.2771236300468445}], "text": "5.80\n"}
{"page": 16, "bbox": [{"x": 0.5740630626678467, "y": 0.27081581950187683}, {"x": 0.6073765754699707, "y": 0.27081581950187683}, {"x": 0.6073765754699707, "y": 0.2771236300468445}, {"x": 0.5740630626678467, "y": 0.2771236300468445}], "text": "11.26\n"}
{"page": 16, "bbox": [{"x": 0.6502082347869873, "y": 0.27081581950187683}, {"x": 0.6841166019439697, "y": 0.27081581950187683}, {"x": 0.6841166019439697, "y": 0.27754414081573486}, {"x": 0.6502082347869873, "y": 0.27754414081573486}], "text": "12.26\n"}
{"page": 16, "bbox": [{"x": 0.13979773223400116, "y": 0.2699747681617737}, {"x": 0.3866746127605438, "y": 0.2699747681617737}, {"x": 0.3866746127605438, "y": 0.27922624349594116}, {"x": 0.13979773223400116, "y": 0.27922624349594116}], "text": "sentence-transformers/all-mpnet-base-v2\n"}
{"page": 16, "bbox": [{"x": 0.2415229082107544, "y": 0.2947855293750763}, {"x": 0.757287323474884, "y": 0.2947855293750763}, {"x": 0.757287323474884, "y": 0.3057190775871277}, {"x": 0.2415229082107544, "y": 0.3057190775871277}], "text": "Table 5: Results for different embedding models on namespace-Pt/msmarco.\n"}
{"page": 16, "bbox": [{"x": 0.4051159918308258, "y": 0.31665265560150146}, {"x": 0.48542535305023193, "y": 0.31665265560150146}, {"x": 0.48542535305023193, "y": 0.32380151748657227}, {"x": 0.4051159918308258, "y": 0.32380151748657227}], "text": "TREC DL19\n"}
{"page": 16, "bbox": [{"x": 0.6936347484588623, "y": 0.31665265560150146}, {"x": 0.7745389938354492, "y": 0.31707316637039185}, {"x": 0.7745389938354492, "y": 0.32422202825546265}, {"x": 0.6936347484588623, "y": 0.32380151748657227}], "text": "TREC DL20\n"}
{"page": 16, "bbox": [{"x": 0.12671029567718506, "y": 0.3250630795955658}, {"x": 0.1760856658220291, "y": 0.3250630795955658}, {"x": 0.1760856658220291, "y": 0.3317914307117462}, {"x": 0.12671029567718506, "y": 0.3317914307117462}], "text": "Method\n"}
{"page": 16, "bbox": [{"x": 0.6448542475700378, "y": 0.32926830649375916}, {"x": 0.8732897043228149, "y": 0.33095037937164307}, {"x": 0.8732897043228149, "y": 0.3406223654747009}, {"x": 0.6448542475700378, "y": 0.338940292596817}], "text": "nDCG@10 R@50 R@lk Latency\n"}
{"page": 16, "bbox": [{"x": 0.3105294406414032, "y": 0.3322119414806366}, {"x": 0.3408685326576233, "y": 0.3317914307117462}, {"x": 0.3408685326576233, "y": 0.33809924125671387}, {"x": 0.3105294406414032, "y": 0.33851975202560425}], "text": "MAP\n"}
{"page": 16, "bbox": [{"x": 0.35633552074432373, "y": 0.33095037937164307}, {"x": 0.6293872594833374, "y": 0.33095037937164307}, {"x": 0.6293872594833374, "y": 0.3397813141345978}, {"x": 0.35633552074432373, "y": 0.3397813141345978}], "text": "nDCG@10 R@50 R@1k Latency mAP\n"}
{"page": 16, "bbox": [{"x": 0.12611541152000427, "y": 0.3444070518016815}, {"x": 0.20761451125144958, "y": 0.3444070518016815}, {"x": 0.20761451125144958, "y": 0.35281750559806824}, {"x": 0.12611541152000427, "y": 0.35281750559806824}], "text": "unsupervised\n"}
{"page": 16, "bbox": [{"x": 0.12671029567718506, "y": 0.35618165135383606}, {"x": 0.1641879826784134, "y": 0.35618165135383606}, {"x": 0.1641879826784134, "y": 0.3629100024700165}, {"x": 0.12671029567718506, "y": 0.3629100024700165}], "text": "BM25\n"}
{"page": 16, "bbox": [{"x": 0.6609161496162415, "y": 0.35492008924484253}, {"x": 0.8619869351387024, "y": 0.35449957847595215}, {"x": 0.8619869351387024, "y": 0.374684602022171}, {"x": 0.6609161496162415, "y": 0.3751051425933838}], "text": "47.96 46.18 78.63 0.29\n42.13 43.81 75.39 0.98\n"}
{"page": 16, "bbox": [{"x": 0.30874478816986084, "y": 0.3553406298160553}, {"x": 0.34324806928634644, "y": 0.3553406298160553}, {"x": 0.34324806928634644, "y": 0.37426409125328064}, {"x": 0.30874478816986084, "y": 0.37426409125328064}], "text": "30.13\n23.99\n"}
{"page": 16, "bbox": [{"x": 0.37239739298820496, "y": 0.35618165135383606}, {"x": 0.40749552845954895, "y": 0.35618165135383606}, {"x": 0.40749552845954895, "y": 0.37426409125328064}, {"x": 0.37239739298820496, "y": 0.37426409125328064}], "text": "50.58\n44.54\n"}
{"page": 16, "bbox": [{"x": 0.43783462047576904, "y": 0.35449957847595215}, {"x": 0.6317667961120605, "y": 0.35492008924484253}, {"x": 0.6317667961120605, "y": 0.37594616413116455}, {"x": 0.43783462047576904, "y": 0.37552565336227417}], "text": "38.32 75.01 0.07 28.56\n37.54 74.59 3.06 23.98\n"}
{"page": 16, "bbox": [{"x": 0.12671029567718506, "y": 0.3675357401371002}, {"x": 0.19036288559436798, "y": 0.3675357401371002}, {"x": 0.19036288559436798, "y": 0.374684602022171}, {"x": 0.12671029567718506, "y": 0.374684602022171}], "text": "Contriever\n"}
{"page": 16, "bbox": [{"x": 0.12611541152000427, "y": 0.3801513910293579}, {"x": 0.19214753806591034, "y": 0.3801513910293579}, {"x": 0.19214753806591034, "y": 0.3889823257923126}, {"x": 0.12611541152000427, "y": 0.3889823257923126}], "text": "supervised\n"}
{"page": 16, "bbox": [{"x": 0.12671029567718506, "y": 0.3910849392414093}, {"x": 0.22605592012405396, "y": 0.3910849392414093}, {"x": 0.22605592012405396, "y": 0.39865434169769287}, {"x": 0.12671029567718506, "y": 0.39865434169769287}], "text": "LLM-Embedder\n"}
{"page": 16, "bbox": [{"x": 0.30874478816986084, "y": 0.39150547981262207}, {"x": 0.34265318512916565, "y": 0.39150547981262207}, {"x": 0.34265318512916565, "y": 0.3982338011264801}, {"x": 0.30874478816986084, "y": 0.3982338011264801}], "text": "44.66\n"}
{"page": 16, "bbox": [{"x": 0.662105917930603, "y": 0.39192599058151245}, {"x": 0.6966091394424438, "y": 0.39192599058151245}, {"x": 0.6966091394424438, "y": 0.39865434169769287}, {"x": 0.662105917930603, "y": 0.39865434169769287}], "text": "68.76\n"}
{"page": 16, "bbox": [{"x": 0.7263533473014832, "y": 0.3910849392414093}, {"x": 0.8625817894935608, "y": 0.3910849392414093}, {"x": 0.8625817894935608, "y": 0.4108494520187378}, {"x": 0.7263533473014832, "y": 0.4108494520187378}], "text": "61.36 84.41 0.71\n59.63 83.45 2.06\n"}
{"page": 16, "bbox": [{"x": 0.543724000453949, "y": 0.39024388790130615}, {"x": 0.6317667961120605, "y": 0.3906644284725189}, {"x": 0.6317667961120605, "y": 0.4230445623397827}, {"x": 0.543724000453949, "y": 0.42262405157089233}], "text": "2.61 45.60\n7.80\n45.16\n14.98\n"}
{"page": 16, "bbox": [{"x": 0.30755501985549927, "y": 0.4032800793647766}, {"x": 0.34265318512916565, "y": 0.40285953879356384}, {"x": 0.34265318512916565, "y": 0.4104289412498474}, {"x": 0.30755501985549927, "y": 0.4108494520187378}], "text": "44.56\n"}
{"page": 16, "bbox": [{"x": 0.37358716130256653, "y": 0.39150547981262207}, {"x": 0.5234979391098022, "y": 0.39150547981262207}, {"x": 0.5234979391098022, "y": 0.42220354080200195}, {"x": 0.37358716130256653, "y": 0.42220354080200195}], "text": "70.20 49.06 84.48\n67.89 51.45 85.35\n48.66 82.62\n"}
{"page": 16, "bbox": [{"x": 0.6615110039710999, "y": 0.403700590133667}, {"x": 0.6960142850875854, "y": 0.403700590133667}, {"x": 0.6960142850875854, "y": 0.4104289412498474}, {"x": 0.6615110039710999, "y": 0.4104289412498474}], "text": "65.62\n"}
{"page": 16, "bbox": [{"x": 0.1421772688627243, "y": 0.40243902802467346}, {"x": 0.25580012798309326, "y": 0.40285953879356384}, {"x": 0.25580012798309326, "y": 0.4121110141277313}, {"x": 0.1421772688627243, "y": 0.41169050335884094}], "text": "+ Query Rewriting\n"}
{"page": 16, "bbox": [{"x": 0.5966686606407166, "y": 0.41505467891693115}, {"x": 0.6317667961120605, "y": 0.41547518968582153}, {"x": 0.6317667961120605, "y": 0.42220354080200195}, {"x": 0.5966686606407166, "y": 0.4217830002307892}], "text": "43.30\n"}
{"page": 16, "bbox": [{"x": 0.30874478816986084, "y": 0.41547518968582153}, {"x": 0.40809041261672974, "y": 0.41547518968582153}, {"x": 0.40809041261672974, "y": 0.42262405157089233}, {"x": 0.30874478816986084, "y": 0.42262405157089233}], "text": "41.93 66.10\n"}
{"page": 16, "bbox": [{"x": 0.6615110039710999, "y": 0.41547518968582153}, {"x": 0.6960142850875854, "y": 0.41547518968582153}, {"x": 0.6960142850875854, "y": 0.42262405157089233}, {"x": 0.6615110039710999, "y": 0.42262405157089233}], "text": "64.95\n"}
{"page": 16, "bbox": [{"x": 0.7257584929466248, "y": 0.41547518968582153}, {"x": 0.8607971668243408, "y": 0.4146341383457184}, {"x": 0.8607971668243408, "y": 0.42262405157089233}, {"x": 0.7257584929466248, "y": 0.4234651029109955}], "text": "57.74 84.18 2.01\n"}
{"page": 16, "bbox": [{"x": 0.1421772688627243, "y": 0.41547518968582153}, {"x": 0.2885187268257141, "y": 0.41547518968582153}, {"x": 0.2885187268257141, "y": 0.424726665019989}, {"x": 0.1421772688627243, "y": 0.424726665019989}], "text": "+ Query Decomposition\n"}
{"page": 16, "bbox": [{"x": 0.7263533473014832, "y": 0.4268292784690857}, {"x": 0.8607971668243408, "y": 0.4272497892379761}, {"x": 0.8607971668243408, "y": 0.4343986511230469}, {"x": 0.7263533473014832, "y": 0.4339781403541565}], "text": "63.80 88.03 2.14\n"}
{"page": 16, "bbox": [{"x": 0.6603212356567383, "y": 0.4268292784690857}, {"x": 0.698988676071167, "y": 0.4272497892379761}, {"x": 0.698988676071167, "y": 0.43481916189193726}, {"x": 0.6603212356567383, "y": 0.4343986511230469}], "text": "73.94\n"}
{"page": 16, "bbox": [{"x": 0.1421772688627243, "y": 0.4272497892379761}, {"x": 0.19274242222309113, "y": 0.42767030000686646}, {"x": 0.19274242222309113, "y": 0.4360807538032532}, {"x": 0.1421772688627243, "y": 0.4356602132320404}], "text": "+ HYDE\n"}
{"page": 16, "bbox": [{"x": 0.30874478816986084, "y": 0.4268292784690857}, {"x": 0.4104699492454529, "y": 0.4272497892379761}, {"x": 0.4098750650882721, "y": 0.4575273394584656}, {"x": 0.30814990401268005, "y": 0.4571067988872528}], "text": "50.87 75.44\n47.14 72.50\n73.34\n"}
{"page": 16, "bbox": [{"x": 0.4348601996898651, "y": 0.4264087378978729}, {"x": 0.6341463327407837, "y": 0.4272497892379761}, {"x": 0.6341463327407837, "y": 0.4587889015674591}, {"x": 0.4348601996898651, "y": 0.45794785022735596}], "text": "54.93 88.76 7.21 50.94\n51.13 89.08 3.20 47.72\n55.38 90.42 11.16 53.13\n"}
{"page": 16, "bbox": [{"x": 0.14158238470554352, "y": 0.439444899559021}, {"x": 0.2409280240535736, "y": 0.4373423159122467}, {"x": 0.2415229082107544, "y": 0.44701430201530457}, {"x": 0.1421772688627243, "y": 0.44911691546440125}], "text": "+ Hybrid Search\n"}
{"page": 16, "bbox": [{"x": 0.72397381067276, "y": 0.4390243887901306}, {"x": 0.8613920211791992, "y": 0.4390243887901306}, {"x": 0.8613920211791992, "y": 0.45794785022735596}, {"x": 0.72397381067276, "y": 0.45794785022735596}], "text": "64.32 88.04 0.77\n66.14 90.67 2.95\n"}
{"page": 16, "bbox": [{"x": 0.662105917930603, "y": 0.43860387802124023}, {"x": 0.6966091394424438, "y": 0.4390243887901306}, {"x": 0.6966091394424438, "y": 0.4583683907985687}, {"x": 0.662105917930603, "y": 0.45794785022735596}], "text": "69.80\n72.72\n"}
{"page": 16, "bbox": [{"x": 0.14158238470554352, "y": 0.45079898834228516}, {"x": 0.3456276059150696, "y": 0.44911691546440125}, {"x": 0.3456276059150696, "y": 0.4587889015674591}, {"x": 0.14158238470554352, "y": 0.460470974445343}], "text": "+ HYDE + Hybrid Search 52.13\n"}
{"page": 16, "bbox": [{"x": 0.11838191747665405, "y": 0.4751892387866974}, {"x": 0.8810232281684875, "y": 0.4751892387866974}, {"x": 0.8810232281684875, "y": 0.4978973865509033}, {"x": 0.11838191747665405, "y": 0.4978973865509033}], "text": "Table 7: Results for different retrieval methods on TREC DL19/20. The best result for each method is made bold\nand the second is underlined.\n"}
{"page": 16, "bbox": [{"x": 0.5133848786354065, "y": 0.517241358757019}, {"x": 0.881618082523346, "y": 0.5193439722061157}, {"x": 0.8810232281684875, "y": 0.5496215224266052}, {"x": 0.5127900242805481, "y": 0.5475189089775085}], "text": "Therefore, we selected a = 0.3 for our retrieval\nand main experiments.\n"}
{"page": 16, "bbox": [{"x": 0.11719214916229248, "y": 0.5197644829750061}, {"x": 0.48839977383613586, "y": 0.5189234614372253}, {"x": 0.48958954215049744, "y": 0.7666106224060059}, {"x": 0.11838191747665405, "y": 0.7674516439437866}], "text": "using HyDE. Concatenating multiple pseudo-\ndocuments with the original query can significantly\nenhance retrieval performance, though at the cost\nof increased latency, suggesting a trade-off between\nretrieval effectiveness and efficiency. However, in-\ndiscriminately increasing the number of hypothet-\nical documents does not yield significant benefits\nand substantially raises latency, indicating that us-\ning a single hypothetical document is sufficient.\nA.3.2 Hybrid Search with Different Weight on\nSparse Retrieval\nTable 9 presents the impact of different a values\nin hybrid search, where a controls the weighting\nbetween sparse retrieval and dense retrieval compo-\nnents. The relevance score is calculated as follows:\n"}
{"page": 16, "bbox": [{"x": 0.5127900242805481, "y": 0.5626577138900757}, {"x": 0.8845925331115723, "y": 0.5630782246589661}, {"x": 0.8839976191520691, "y": 0.8566021919250488}, {"x": 0.5121951103210449, "y": 0.8561816811561584}], "text": "A.4 Experimental Details of Reranking\nMethods\nDatasets Our experiments utilize the MS\nMARCO Passage ranking dataset, a substantial cor-\npus designed for machine reading comprehension\ntasks. This dataset comprises over 8.8 million pas-\nsages and 1 million queries. The training set con-\ntains approximately 398M tuples of queries paired\nwith corresponding positive and negative passages,\nwhile the development set comprises 6,980 queries,\npaired with their BM25 retrieval results, and pre-\nserves the top-1000 ranked candidate passages for\neach query. We evaluate the effectiveness of the\nmethods on the development set, as the test set is\nnot publicly available.\nMetrics The evaluation metrics MRR@1,\nMRR@10, MRR@ 1k and Hit Rate @ 10 are used.\nMRR@10 is the official metric proposed by MS\n"}
{"page": 16, "bbox": [{"x": 0.24687686562538147, "y": 0.7792262434959412}, {"x": 0.3581201732158661, "y": 0.7788057327270508}, {"x": 0.3581201732158661, "y": 0.788898229598999}, {"x": 0.24687686562538147, "y": 0.7893187403678894}], "text": "Sha Ss Sa\n"}
{"page": 16, "bbox": [{"x": 0.4687685966491699, "y": 0.7800672650337219}, {"x": 0.4866151213645935, "y": 0.7800672650337219}, {"x": 0.4866151213645935, "y": 0.788898229598999}, {"x": 0.4687685966491699, "y": 0.788898229598999}], "text": "(1)\n"}
{"page": 16, "bbox": [{"x": 0.11778703331947327, "y": 0.7960470914840698}, {"x": 0.48839977383613586, "y": 0.7960470914840698}, {"x": 0.48839977383613586, "y": 0.918418824672699}, {"x": 0.11778703331947327, "y": 0.918418824672699}], "text": "where Ss, Sd are the normalized relevance scores\nfrom sparse retrieval and dense retrieval respec-\ntively, and Sh is the total retrieval score.\nWe evaluated five different a values to determine\ntheir influence on performance. The results indicate\nthat an a value of 0.3 yields the best performance,\ndemonstrating that appropriate adjustment of a can\nenhance retrieval effectiveness to a certain extent.\n"}
{"page": 16, "bbox": [{"x": 0.5139797925949097, "y": 0.8608074188232422}, {"x": 0.5859607458114624, "y": 0.8608074188232422}, {"x": 0.5859607458114624, "y": 0.8687973022460938}, {"x": 0.5139797925949097, "y": 0.8687973022460938}], "text": "MARCO.\n"}
{"page": 16, "bbox": [{"x": 0.5133848786354065, "y": 0.8746846318244934}, {"x": 0.8822129964828491, "y": 0.8742640614509583}, {"x": 0.8822129964828491, "y": 0.9205214381217957}, {"x": 0.5133848786354065, "y": 0.920941948890686}], "text": "Implementation Details We follow and make\nmodifications to the implementation provided by\nPyGaggle (Nogueira et al., 2020) and TILDE\n"}
{"page": 16, "bbox": [{"x": 0.48007139563560486, "y": 0.931034505367279}, {"x": 0.5229030251502991, "y": 0.931034505367279}, {"x": 0.5229030251502991, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17731\n"}
{"page": 17, "bbox": [{"x": 0.6894705295562744, "y": 0.08873002231121063}, {"x": 0.7691850066184998, "y": 0.08830950409173965}, {"x": 0.7691850066184998, "y": 0.09587889164686203}, {"x": 0.6894705295562744, "y": 0.09629940986633301}], "text": "TREC DL20\n"}
{"page": 17, "bbox": [{"x": 0.40273645520210266, "y": 0.08915054798126221}, {"x": 0.4836407005786896, "y": 0.08915054798126221}, {"x": 0.4836407005786896, "y": 0.09629940986633301}, {"x": 0.40273645520210266, "y": 0.09629940986633301}], "text": "TREC DL19\n"}
{"page": 17, "bbox": [{"x": 0.13146936893463135, "y": 0.09671993553638458}, {"x": 0.22189173102378845, "y": 0.09756097197532654}, {"x": 0.22189173102378845, "y": 0.10639192909002304}, {"x": 0.13146936893463135, "y": 0.10555088520050049}], "text": "Configuration\n"}
{"page": 17, "bbox": [{"x": 0.3105294406414032, "y": 0.10386879742145538}, {"x": 0.8685306310653687, "y": 0.10386879742145538}, {"x": 0.8685306310653687, "y": 0.11312027275562286}, {"x": 0.3105294406414032, "y": 0.11312027275562286}], "text": "MAP nDCG@10 R@50 R@lk latency mAP nDCG@10 R@50 R@1k Latency\n"}
{"page": 17, "bbox": [{"x": 0.13087448477745056, "y": 0.11648444086313248}, {"x": 0.17013682425022125, "y": 0.11648444086313248}, {"x": 0.17013682425022125, "y": 0.12573590874671936}, {"x": 0.13087448477745056, "y": 0.12573590874671936}], "text": "HYDE\n"}
{"page": 17, "bbox": [{"x": 0.14574657380580902, "y": 0.1265769600868225}, {"x": 0.2456870973110199, "y": 0.12825904786586761}, {"x": 0.2450922131538391, "y": 0.13877207040786743}, {"x": 0.14515168964862823, "y": 0.13708999752998352}], "text": "w/ 1 pseudo-doc\n"}
{"page": 17, "bbox": [{"x": 0.4354550838470459, "y": 0.12783852219581604}, {"x": 0.6299821734428406, "y": 0.12783852219581604}, {"x": 0.6299821734428406, "y": 0.15937763452529907}, {"x": 0.4354550838470459, "y": 0.15937763452529907}], "text": "53.20 87.73 8.08 51.31\n54.93 88.76 7.21 50.94\n54.51 89.17 14.15 53.14\n"}
{"page": 17, "bbox": [{"x": 0.655562162399292, "y": 0.12783852219581604}, {"x": 0.8584176301956177, "y": 0.12783852219581604}, {"x": 0.8584176301956177, "y": 0.15937763452529907}, {"x": 0.655562162399292, "y": 0.15937763452529907}], "text": "70.37 63.28 87.81 2.09\n73.94 63.80 88.03 2.14\n73.65 65.79 88.67 3.44\n"}
{"page": 17, "bbox": [{"x": 0.37180250883102417, "y": 0.128679558634758}, {"x": 0.4104699492454529, "y": 0.128679558634758}, {"x": 0.4104699492454529, "y": 0.1589571088552475}, {"x": 0.37180250883102417, "y": 0.1589571088552475}], "text": "72.49\n75.44\n75.12\n"}
{"page": 17, "bbox": [{"x": 0.14574657380580902, "y": 0.12783852219581604}, {"x": 0.34681737422943115, "y": 0.12741799652576447}, {"x": 0.34681737422943115, "y": 0.16190075874328613}, {"x": 0.14574657380580902, "y": 0.1623212844133377}], "text": "48.77\nw/ 1 pseudo-doc + query 50.87\nw/ 8 pseudo-doc + query 51.64\n"}
{"page": 17, "bbox": [{"x": 0.21653777360916138, "y": 0.1766190081834793}, {"x": 0.782272458076477, "y": 0.1766190081834793}, {"x": 0.782272458076477, "y": 0.1875525712966919}, {"x": 0.21653777360916138, "y": 0.1875525712966919}], "text": "Table 8: HYDE with different concatenation of hypothetical documents and queries.\n"}
{"page": 17, "bbox": [{"x": 0.6817370653152466, "y": 0.2073170691728592}, {"x": 0.7638310790061951, "y": 0.20773759484291077}, {"x": 0.7638310790061951, "y": 0.21530698239803314}, {"x": 0.6817370653152466, "y": 0.21488645672798157}], "text": "TREC DL20\n"}
{"page": 17, "bbox": [{"x": 0.3622843623161316, "y": 0.2073170691728592}, {"x": 0.44259369373321533, "y": 0.2068965584039688}, {"x": 0.44259369373321533, "y": 0.23002523183822632}, {"x": 0.3622843623161316, "y": 0.2304457575082779}], "text": "TREC DL19\nR@50\n"}
{"page": 17, "bbox": [{"x": 0.12730517983436584, "y": 0.2161480188369751}, {"x": 0.2367638349533081, "y": 0.21656854450702667}, {"x": 0.2367638349533081, "y": 0.22582001984119415}, {"x": 0.12730517983436584, "y": 0.22539949417114258}], "text": "Hyperparameter\n"}
{"page": 17, "bbox": [{"x": 0.4527067244052887, "y": 0.22245584428310394}, {"x": 0.49256396293640137, "y": 0.2232968807220459}, {"x": 0.4919690787792206, "y": 0.23086626827716827}, {"x": 0.4521118402481079, "y": 0.23002523183822632}], "text": "R@ 1k\n"}
{"page": 17, "bbox": [{"x": 0.25698989629745483, "y": 0.2232968807220459}, {"x": 0.2879238426685333, "y": 0.22287636995315552}, {"x": 0.2879238426685333, "y": 0.23002523183822632}, {"x": 0.25698989629745483, "y": 0.2304457575082779}], "text": "MAP\n"}
{"page": 17, "bbox": [{"x": 0.3099345564842224, "y": 0.2232968807220459}, {"x": 0.3789411187171936, "y": 0.2232968807220459}, {"x": 0.3789411187171936, "y": 0.2304457575082779}, {"x": 0.3099345564842224, "y": 0.2304457575082779}], "text": "nDCG@10\n"}
{"page": 17, "bbox": [{"x": 0.6270077228546143, "y": 0.2211942821741104}, {"x": 0.8762641549110413, "y": 0.2232968807220459}, {"x": 0.8762641549110413, "y": 0.23254835605621338}, {"x": 0.6270077228546143, "y": 0.2304457575082779}], "text": "nDCG@ 10 R@50 R@1k Latency\n"}
{"page": 17, "bbox": [{"x": 0.5098155736923218, "y": 0.2232968807220459}, {"x": 0.605591893196106, "y": 0.22371740639209747}, {"x": 0.605591893196106, "y": 0.23254835605621338}, {"x": 0.5098155736923218, "y": 0.232127845287323}], "text": "latency MAP\n"}
{"page": 17, "bbox": [{"x": 0.12730517983436584, "y": 0.23549200594425201}, {"x": 0.21356335282325745, "y": 0.23423044383525848}, {"x": 0.21356335282325745, "y": 0.24264086782932281}, {"x": 0.12730517983436584, "y": 0.24390244483947754}], "text": "Hybrid Search\n"}
{"page": 17, "bbox": [{"x": 0.1421772688627243, "y": 0.2481076568365097}, {"x": 0.18619869649410248, "y": 0.24684609472751617}, {"x": 0.18679358065128326, "y": 0.25441548228263855}, {"x": 0.1427721530199051, "y": 0.2556770443916321}], "text": "a = 0.1\n"}
{"page": 17, "bbox": [{"x": 0.2552052438259125, "y": 0.24684609472751617}, {"x": 0.2897084951400757, "y": 0.24726660549640656}, {"x": 0.2891136109828949, "y": 0.26661059260368347}, {"x": 0.2546103596687317, "y": 0.2661900818347931}], "text": "46.00\n47.14\n"}
{"page": 17, "bbox": [{"x": 0.3242117762565613, "y": 0.24726660549640656}, {"x": 0.36347413063049316, "y": 0.24726660549640656}, {"x": 0.36347413063049316, "y": 0.26661059260368347}, {"x": 0.3242117762565613, "y": 0.26661059260368347}], "text": "70.87\n72.50\n"}
{"page": 17, "bbox": [{"x": 0.3967876136302948, "y": 0.24684609472751617}, {"x": 0.4913741946220398, "y": 0.24726660549640656}, {"x": 0.4913741946220398, "y": 0.26703113317489624}, {"x": 0.3967876136302948, "y": 0.26661059260368347}], "text": "49.24 88.89\n51.13 89.08\n"}
{"page": 17, "bbox": [{"x": 0.1421772688627243, "y": 0.25988224148750305}, {"x": 0.18738846480846405, "y": 0.2590412199497223}, {"x": 0.18798334896564484, "y": 0.2661900818347931}, {"x": 0.1427721530199051, "y": 0.26703113317489624}], "text": "a = 0.3\n"}
{"page": 17, "bbox": [{"x": 0.7126710414886475, "y": 0.2464255690574646}, {"x": 0.8655562400817871, "y": 0.2464255690574646}, {"x": 0.8655562400817871, "y": 0.3015138804912567}, {"x": 0.7126710414886475, "y": 0.3015138804912567}], "text": "63.36 87.32 0.90\n64.32 88.04 0.77\n64.90 87.86 0.87\n64.23 87.92\n63.22 87.76\n"}
{"page": 17, "bbox": [{"x": 0.25223082304000854, "y": 0.27081581950187683}, {"x": 0.2926829159259796, "y": 0.2712363302707672}, {"x": 0.2926829159259796, "y": 0.278385192155838}, {"x": 0.25223082304000854, "y": 0.27796468138694763}], "text": "47.36\n"}
{"page": 17, "bbox": [{"x": 0.3955978453159332, "y": 0.2712363302707672}, {"x": 0.48899465799331665, "y": 0.27081581950187683}, {"x": 0.48899465799331665, "y": 0.27796468138694763}, {"x": 0.3955978453159332, "y": 0.278385192155838}], "text": "52.71 88.09\n"}
{"page": 17, "bbox": [{"x": 0.3271861970424652, "y": 0.2712363302707672}, {"x": 0.36049970984458923, "y": 0.2712363302707672}, {"x": 0.36049970984458923, "y": 0.278385192155838}, {"x": 0.3271861970424652, "y": 0.278385192155838}], "text": "72.24\n"}
{"page": 17, "bbox": [{"x": 0.6412849426269531, "y": 0.24768713116645813}, {"x": 0.6793575286865234, "y": 0.24768713116645813}, {"x": 0.6793575286865234, "y": 0.3019343912601471}, {"x": 0.6412849426269531, "y": 0.3019343912601471}], "text": "69.05\n69.80\n68.12\n67.30\n65.55\n"}
{"page": 17, "bbox": [{"x": 0.3973824977874756, "y": 0.2464255690574646}, {"x": 0.6091611981391907, "y": 0.24726660549640656}, {"x": 0.6085663437843323, "y": 0.3036164939403534}, {"x": 0.3967876136302948, "y": 0.30277544260025024}], "text": "2.98 46.54\n3.20 47.72\n3.02 47.19\n52.40 88.01 3.15 45.82\n52.64 88.22 2.74 44.02\n"}
{"page": 17, "bbox": [{"x": 0.1421772688627243, "y": 0.27207738161087036}, {"x": 0.18738846480846405, "y": 0.27081581950187683}, {"x": 0.18798334896564484, "y": 0.27796468138694763}, {"x": 0.1427721530199051, "y": 0.27922624349594116}], "text": "a = 0.5\n"}
{"page": 17, "bbox": [{"x": 0.2546103596687317, "y": 0.28343144059181213}, {"x": 0.2897084951400757, "y": 0.28343144059181213}, {"x": 0.2897084951400757, "y": 0.2897392809391022}, {"x": 0.2546103596687317, "y": 0.2897392809391022}], "text": "47.21\n"}
{"page": 17, "bbox": [{"x": 0.3271861970424652, "y": 0.28343144059181213}, {"x": 0.35990482568740845, "y": 0.28343144059181213}, {"x": 0.35990482568740845, "y": 0.2897392809391022}, {"x": 0.3271861970424652, "y": 0.2897392809391022}], "text": "71.89\n"}
{"page": 17, "bbox": [{"x": 0.1421772688627243, "y": 0.28343144059181213}, {"x": 0.18738846480846405, "y": 0.28259041905403137}, {"x": 0.18738846480846405, "y": 0.29015979170799255}, {"x": 0.1421772688627243, "y": 0.2910008430480957}], "text": "a = 0.7\n"}
{"page": 17, "bbox": [{"x": 0.8370018005371094, "y": 0.28301092982292175}, {"x": 0.8637715578079224, "y": 0.28301092982292175}, {"x": 0.8637715578079224, "y": 0.3019343912601471}, {"x": 0.8370018005371094, "y": 0.3019343912601471}], "text": "1.02\n1.20\n"}
{"page": 17, "bbox": [{"x": 0.2546103596687317, "y": 0.2947855293750763}, {"x": 0.2897084951400757, "y": 0.2947855293750763}, {"x": 0.2897084951400757, "y": 0.3015138804912567}, {"x": 0.2546103596687317, "y": 0.3015138804912567}], "text": "46.35\n"}
{"page": 17, "bbox": [{"x": 0.3271861970424652, "y": 0.29520606994628906}, {"x": 0.36109459400177, "y": 0.29520606994628906}, {"x": 0.36109459400177, "y": 0.3015138804912567}, {"x": 0.3271861970424652, "y": 0.3015138804912567}], "text": "70.67\n"}
{"page": 17, "bbox": [{"x": 0.1421772688627243, "y": 0.29562658071517944}, {"x": 0.18679358065128326, "y": 0.2943650186061859}, {"x": 0.18738846480846405, "y": 0.3019343912601471}, {"x": 0.1427721530199051, "y": 0.3031959533691406}], "text": "a = 0.9\n"}
{"page": 17, "bbox": [{"x": 0.29565733671188354, "y": 0.3191757798194885}, {"x": 0.7031528949737549, "y": 0.3191757798194885}, {"x": 0.7031528949737549, "y": 0.32968881726264954}, {"x": 0.29565733671188354, "y": 0.32968881726264954}], "text": "Table 9: Results of hybrid search with different alpha values.\n"}
{"page": 17, "bbox": [{"x": 0.5133848786354065, "y": 0.3570227026939392}, {"x": 0.8851873874664307, "y": 0.35828426480293274}, {"x": 0.8834027647972107, "y": 0.6635828614234924}, {"x": 0.5116002559661865, "y": 0.6623212695121765}], "text": "model. This method is non-query-based, allowing\na comparison between query-based and non-query-\nbased approaches.\nDatasets We evaluated these methods on three\ndatasets: Natural Questions (NQ) (Kwiatkowski\net al., 2019), TriviaQA (Joshi et al., 2017), and\nHotpotQA (Yang et al., 2018).\nMetrics Evaluation metrics include the F1 score\nand the number of tokens changed after summa-\nrization to measure conciseness.\nImplementation Details For all methods, we\nuse Llama3-8B-Instruct as the generator model\nand set a summarization ratio of 0.4. For extrac-\ntive methods, importance scores determine the sen-\ntences retained. For abstractive methods, we con-\ntrol the maximum generation length using the sum-\nmarization ratio to align with extractive methods.\nExperiments are conducted on the NQ test set, Triv-\niaQA test set, and HotpotQA development set.\n"}
{"page": 17, "bbox": [{"x": 0.11600238084793091, "y": 0.3587048053741455}, {"x": 0.4878048896789551, "y": 0.35828426480293274}, {"x": 0.48899465799331665, "y": 0.7901598215103149}, {"x": 0.11719214916229248, "y": 0.7905803322792053}], "text": "(Zhuang and Zuccon, 2021b). For DLM-based\nreranking, we use monoT5 (Nogueira et al., 2020)\nbased on T5-base, monoBERT (Nogueira et al.,\n2019) based on BERT-large and RankLLAMA (Ma\net al., 2023b) based on Llama-2-7b. For TILDE\nreranking, we use TILDEv2 (Zhuang and Zuccon,\n2021a) based on BERT-base.\nTypically, 50 documents are retrieved as input\nfor the reranking module. The documents remain-\ning after the reranking and repacking phase can be\nfurther concentrated by assigning a top-k value or\na relevancy score threshold.\nResult Analysis Reranking results are shown in\nTable 10. We compare our results with a randomly\nshuffled ordering and the BM25 retrieval baseline.\nAll reranking methods demonstrate a notable in-\ncrease in performance across all metrics. Approxi-\nmately equal performance is achieved by monoT5\nand monoBERT, and RankLLaMA performs best,\neach ascending in latency. TILDEv2 is the fastest,\ntaking approximately 10 to 20 milliseconds per\nquery at the cost of performance. Additionally,\nTILDEv2 requires that the passages reranked be\nidentically included in the previously indexed col-\nlection. Preprocessing must be redone at inference\nfor new unseen passages, negating the efficiency\nadvantages.\n"}
{"page": 17, "bbox": [{"x": 0.5127900242805481, "y": 0.6774600744247437}, {"x": 0.8845925331115723, "y": 0.6770395040512085}, {"x": 0.8851873874664307, "y": 0.9196804165840149}, {"x": 0.5133848786354065, "y": 0.9201009273529053}], "text": "A.6 Experimental Details of Generator\nFine-tuning\nDatasets We fine-tune our model on several\nquestion answering(QA) and reading comprehen-\nsion datasets, including ASQA (Stelmakh et al.,\n2022), HotpotQA (Yang et al., 2018), Narra-\ntiveQA (Kočiskỳ et al., 2018), NQ (Kwiatkowski\net al., 2019), SQUAD (Rajpurkar et al., 2016), Triv-\niaQA (Joshi et al., 2017), TruthfulQA (Lin et al.,\n2021b). We use their train splits (for those con-\ntaining significantly more data entries than others,\nwe conducted a random sample). For evaluation,\nASQA (Stelmakh et al., 2022), HotpotQA (Yang\net al., 2018), NQ (Kwiatkowski et al., 2019), Trivi-\naQA (Joshi et al., 2017) are used. We evaluate our\n"}
{"page": 17, "bbox": [{"x": 0.11719214916229248, "y": 0.8065601587295532}, {"x": 0.48958954215049744, "y": 0.8065601587295532}, {"x": 0.48958954215049744, "y": 0.920941948890686}, {"x": 0.11719214916229248, "y": 0.920941948890686}], "text": "A.5 Experimental Details of Summarization\nMethods\nSelective Context Selective Context enhances\nLLM efficiency by identifying and removing re-\ndundant information in the input context. It evalu-\nates the informativeness of lexical units using self-\ninformation computed by a base causal language\n"}
{"page": 17, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17732\n"}
{"page": 18, "bbox": [{"x": 0.46757882833480835, "y": 0.08704794198274612}, {"x": 0.662105917930603, "y": 0.08873002231121063}, {"x": 0.662105917930603, "y": 0.09840201586484909}, {"x": 0.46757882833480835, "y": 0.09671993553638458}], "text": "MS MARCO Passage ranking\n"}
{"page": 18, "bbox": [{"x": 0.15645448863506317, "y": 0.09714045375585556}, {"x": 0.206424742937088, "y": 0.09714045375585556}, {"x": 0.206424742937088, "y": 0.10470984131097794}, {"x": 0.15645448863506317, "y": 0.10470984131097794}], "text": "Method\n"}
{"page": 18, "bbox": [{"x": 0.45449137687683105, "y": 0.10597140341997147}, {"x": 0.6745984554290771, "y": 0.10597140341997147}, {"x": 0.6745984554290771, "y": 0.11312027275562286}, {"x": 0.45449137687683105, "y": 0.11312027275562286}], "text": "MRR@1 MRR@10 MRR@1k\n"}
{"page": 18, "bbox": [{"x": 0.6948245167732239, "y": 0.10555088520050049}, {"x": 0.7733492255210876, "y": 0.10597140341997147}, {"x": 0.7733492255210876, "y": 0.11354079097509384}, {"x": 0.6948245167732239, "y": 0.11312027275562286}], "text": "Hit Rate@10\n"}
{"page": 18, "bbox": [{"x": 0.28613919019699097, "y": 0.10597140341997147}, {"x": 0.4342653155326843, "y": 0.10639192909002304}, {"x": 0.4342653155326843, "y": 0.11354079097509384}, {"x": 0.28613919019699097, "y": 0.11312027275562286}], "text": "Base Model # Params\n"}
{"page": 18, "bbox": [{"x": 0.7935752272605896, "y": 0.10597140341997147}, {"x": 0.8417608737945557, "y": 0.10555088520050049}, {"x": 0.8417608737945557, "y": 0.1143818348646164}, {"x": 0.7935752272605896, "y": 0.11480235308408737}], "text": "Latency\n"}
{"page": 18, "bbox": [{"x": 0.15645448863506317, "y": 0.11774600297212601}, {"x": 0.2456870973110199, "y": 0.11858704686164856}, {"x": 0.2456870973110199, "y": 0.12741799652576447}, {"x": 0.15645448863506317, "y": 0.1265769600868225}], "text": "w/o Reranking\n"}
{"page": 18, "bbox": [{"x": 0.15645448863506317, "y": 0.12825904786586761}, {"x": 0.2665080428123474, "y": 0.12994112074375153}, {"x": 0.2665080428123474, "y": 0.139192596077919}, {"x": 0.15645448863506317, "y": 0.1375105082988739}], "text": "Random Ordering\n"}
{"page": 18, "bbox": [{"x": 0.4646044075489044, "y": 0.1303616464138031}, {"x": 0.49851280450820923, "y": 0.1303616464138031}, {"x": 0.49851280450820923, "y": 0.1375105082988739}, {"x": 0.4646044075489044, "y": 0.1375105082988739}], "text": "0.011\n"}
{"page": 18, "bbox": [{"x": 0.5425342321395874, "y": 0.1303616464138031}, {"x": 0.5782272219657898, "y": 0.1303616464138031}, {"x": 0.5782272219657898, "y": 0.1375105082988739}, {"x": 0.5425342321395874, "y": 0.1375105082988739}], "text": "0.027\n"}
{"page": 18, "bbox": [{"x": 0.7162403464317322, "y": 0.12994112074375153}, {"x": 0.7501487135887146, "y": 0.13078217208385468}, {"x": 0.7495538592338562, "y": 0.13793103396892548}, {"x": 0.715645432472229, "y": 0.13708999752998352}], "text": "0.092\n"}
{"page": 18, "bbox": [{"x": 0.6252231001853943, "y": 0.13078217208385468}, {"x": 0.6603212356567383, "y": 0.13078217208385468}, {"x": 0.6603212356567383, "y": 0.13708999752998352}, {"x": 0.6252231001853943, "y": 0.13708999752998352}], "text": "0.068\n"}
{"page": 18, "bbox": [{"x": 0.543724000453949, "y": 0.1408746838569641}, {"x": 0.5776323676109314, "y": 0.14171572029590607}, {"x": 0.5770374536514282, "y": 0.14970563352108002}, {"x": 0.5431290864944458, "y": 0.14886459708213806}], "text": "11.65\n"}
{"page": 18, "bbox": [{"x": 0.15764425694942474, "y": 0.14213624596595764}, {"x": 0.19571682810783386, "y": 0.14213624596595764}, {"x": 0.19571682810783386, "y": 0.14886459708213806}, {"x": 0.15764425694942474, "y": 0.14886459708213806}], "text": "BM25\n"}
{"page": 18, "bbox": [{"x": 0.7168352007865906, "y": 0.14255677163600922}, {"x": 0.7513384819030762, "y": 0.14255677163600922}, {"x": 0.7513384819030762, "y": 0.14886459708213806}, {"x": 0.7168352007865906, "y": 0.14886459708213806}], "text": "24.63\n"}
{"page": 18, "bbox": [{"x": 0.46817371249198914, "y": 0.14255677163600922}, {"x": 0.4955383837223053, "y": 0.14255677163600922}, {"x": 0.4955383837223053, "y": 0.14928510785102844}, {"x": 0.46817371249198914, "y": 0.14928510785102844}], "text": "6.52\n"}
{"page": 18, "bbox": [{"x": 0.6264128684997559, "y": 0.14255677163600922}, {"x": 0.6603212356567383, "y": 0.14255677163600922}, {"x": 0.6603212356567383, "y": 0.14928510785102844}, {"x": 0.6264128684997559, "y": 0.14928510785102844}], "text": "12.59\n"}
{"page": 18, "bbox": [{"x": 0.15764425694942474, "y": 0.1534903347492218}, {"x": 0.25639501214027405, "y": 0.15475189685821533}, {"x": 0.25639501214027405, "y": 0.16316232085227966}, {"x": 0.15764425694942474, "y": 0.16190075874328613}], "text": "DLM Reranking\n"}
{"page": 18, "bbox": [{"x": 0.15704937279224396, "y": 0.16568544507026672}, {"x": 0.20761451125144958, "y": 0.16526493430137634}, {"x": 0.20761451125144958, "y": 0.17283432185649872}, {"x": 0.15704937279224396, "y": 0.1732548326253891}], "text": "monoT5\n"}
{"page": 18, "bbox": [{"x": 0.6252231001853943, "y": 0.1661059707403183}, {"x": 0.6597263813018799, "y": 0.16568544507026672}, {"x": 0.6597263813018799, "y": 0.17283432185649872}, {"x": 0.6252231001853943, "y": 0.1732548326253891}], "text": "32.40\n"}
{"page": 18, "bbox": [{"x": 0.7162403464317322, "y": 0.1661059707403183}, {"x": 0.7519333958625793, "y": 0.1661059707403183}, {"x": 0.7519333958625793, "y": 0.1732548326253891}, {"x": 0.7162403464317322, "y": 0.1732548326253891}], "text": "54.07\n"}
{"page": 18, "bbox": [{"x": 0.4646044075489044, "y": 0.16652649641036987}, {"x": 0.49791792035102844, "y": 0.16652649641036987}, {"x": 0.49791792035102844, "y": 0.17283432185649872}, {"x": 0.4646044075489044, "y": 0.17283432185649872}], "text": "21.62\n"}
{"page": 18, "bbox": [{"x": 0.5431290864944458, "y": 0.16652649641036987}, {"x": 0.5776323676109314, "y": 0.16652649641036987}, {"x": 0.5776323676109314, "y": 0.17283432185649872}, {"x": 0.5431290864944458, "y": 0.17283432185649872}], "text": "31.78\n"}
{"page": 18, "bbox": [{"x": 0.8060678243637085, "y": 0.16652649641036987}, {"x": 0.8286734223365784, "y": 0.16652649641036987}, {"x": 0.8286734223365784, "y": 0.17283432185649872}, {"x": 0.8060678243637085, "y": 0.17283432185649872}], "text": "4.5\n"}
{"page": 18, "bbox": [{"x": 0.2986317574977875, "y": 0.16652649641036987}, {"x": 0.34681737422943115, "y": 0.16652649641036987}, {"x": 0.34681737422943115, "y": 0.1732548326253891}, {"x": 0.2986317574977875, "y": 0.1732548326253891}], "text": "T5-base\n"}
{"page": 18, "bbox": [{"x": 0.3884592652320862, "y": 0.16652649641036987}, {"x": 0.4235574007034302, "y": 0.16652649641036987}, {"x": 0.4235574007034302, "y": 0.1732548326253891}, {"x": 0.3884592652320862, "y": 0.1732548326253891}], "text": "220M\n"}
{"page": 18, "bbox": [{"x": 0.6252231001853943, "y": 0.17746004462242126}, {"x": 0.6597263813018799, "y": 0.17788057029247284}, {"x": 0.6597263813018799, "y": 0.18460892140865326}, {"x": 0.6252231001853943, "y": 0.18418839573860168}], "text": "32.35\n"}
{"page": 18, "bbox": [{"x": 0.15704937279224396, "y": 0.17788057029247284}, {"x": 0.2284354567527771, "y": 0.17746004462242126}, {"x": 0.2284354567527771, "y": 0.18502943217754364}, {"x": 0.15704937279224396, "y": 0.18544995784759521}], "text": "monoBERT\n"}
{"page": 18, "bbox": [{"x": 0.3890541195869446, "y": 0.17788057029247284}, {"x": 0.4229625165462494, "y": 0.17830109596252441}, {"x": 0.4229625165462494, "y": 0.18502943217754364}, {"x": 0.3890541195869446, "y": 0.18460892140865326}], "text": "340M\n"}
{"page": 18, "bbox": [{"x": 0.5431290864944458, "y": 0.17830109596252441}, {"x": 0.5776323676109314, "y": 0.17830109596252441}, {"x": 0.5776323676109314, "y": 0.18460892140865326}, {"x": 0.5431290864944458, "y": 0.18460892140865326}], "text": "31.69\n"}
{"page": 18, "bbox": [{"x": 0.7168352007865906, "y": 0.17830109596252441}, {"x": 0.7507436275482178, "y": 0.17830109596252441}, {"x": 0.7507436275482178, "y": 0.18460892140865326}, {"x": 0.7168352007865906, "y": 0.18460892140865326}], "text": "53.38\n"}
{"page": 18, "bbox": [{"x": 0.8048780560493469, "y": 0.17788057029247284}, {"x": 0.8304580450057983, "y": 0.17788057029247284}, {"x": 0.8304580450057983, "y": 0.18544995784759521}, {"x": 0.8048780560493469, "y": 0.18544995784759521}], "text": "15.8\n"}
{"page": 18, "bbox": [{"x": 0.4646044075489044, "y": 0.17830109596252441}, {"x": 0.49851280450820923, "y": 0.178721621632576}, {"x": 0.49851280450820923, "y": 0.18544995784759521}, {"x": 0.4646044075489044, "y": 0.18502943217754364}], "text": "21.65\n"}
{"page": 18, "bbox": [{"x": 0.2879238426685333, "y": 0.17746004462242126}, {"x": 0.3581201732158661, "y": 0.17830109596252441}, {"x": 0.3581201732158661, "y": 0.18671151995658875}, {"x": 0.2879238426685333, "y": 0.1858704835176468}], "text": "BERT-large\n"}
{"page": 18, "bbox": [{"x": 0.6228435635566711, "y": 0.18965516984462738}, {"x": 0.662105917930603, "y": 0.1892346441745758}, {"x": 0.662105917930603, "y": 0.19680403172969818}, {"x": 0.6228435635566711, "y": 0.19722455739974976}], "text": "32.97\n"}
{"page": 18, "bbox": [{"x": 0.7144556641578674, "y": 0.18965516984462738}, {"x": 0.7537180185317993, "y": 0.19007569551467896}, {"x": 0.7537180185317993, "y": 0.19680403172969818}, {"x": 0.7144556641578674, "y": 0.1963835209608078}], "text": "54.53\n"}
{"page": 18, "bbox": [{"x": 0.8042831420898438, "y": 0.19007569551467896}, {"x": 0.8304580450057983, "y": 0.1892346441745758}, {"x": 0.8310529589653015, "y": 0.1963835209608078}, {"x": 0.8048780560493469, "y": 0.19722455739974976}], "text": "82.4\n"}
{"page": 18, "bbox": [{"x": 0.39857226610183716, "y": 0.19007569551467896}, {"x": 0.4140392541885376, "y": 0.19007569551467896}, {"x": 0.4140392541885376, "y": 0.19680403172969818}, {"x": 0.39857226610183716, "y": 0.19680403172969818}], "text": "7B\n"}
{"page": 18, "bbox": [{"x": 0.5401546955108643, "y": 0.19007569551467896}, {"x": 0.578822135925293, "y": 0.19007569551467896}, {"x": 0.578822135925293, "y": 0.19680403172969818}, {"x": 0.5401546955108643, "y": 0.19680403172969818}], "text": "32.35\n"}
{"page": 18, "bbox": [{"x": 0.15704937279224396, "y": 0.19007569551467896}, {"x": 0.23795360326766968, "y": 0.19049622118473053}, {"x": 0.23795360326766968, "y": 0.19722455739974976}, {"x": 0.15704937279224396, "y": 0.19680403172969818}], "text": "RankLLAMA\n"}
{"page": 18, "bbox": [{"x": 0.4616299867630005, "y": 0.19049622118473053}, {"x": 0.5008923411369324, "y": 0.19049622118473053}, {"x": 0.5008923411369324, "y": 0.19680403172969818}, {"x": 0.4616299867630005, "y": 0.19680403172969818}], "text": "22.08\n"}
{"page": 18, "bbox": [{"x": 0.28673407435417175, "y": 0.19007569551467896}, {"x": 0.3587150573730469, "y": 0.19007569551467896}, {"x": 0.3587150573730469, "y": 0.19764508306980133}, {"x": 0.28673407435417175, "y": 0.19764508306980133}], "text": "Llama-2-7b\n"}
{"page": 18, "bbox": [{"x": 0.15764425694942474, "y": 0.20142976939678192}, {"x": 0.26531827449798584, "y": 0.2018502950668335}, {"x": 0.26531827449798584, "y": 0.21026071906089783}, {"x": 0.15764425694942474, "y": 0.20984020829200745}], "text": "TILDE Reranking\n"}
{"page": 18, "bbox": [{"x": 0.8024985194206238, "y": 0.2140454202890396}, {"x": 0.8322427272796631, "y": 0.21362489461898804}, {"x": 0.8322427272796631, "y": 0.22035323083400726}, {"x": 0.8024985194206238, "y": 0.22077375650405884}], "text": "0.02\n"}
{"page": 18, "bbox": [{"x": 0.38964900374412537, "y": 0.21446593105793}, {"x": 0.4223676323890686, "y": 0.21446593105793}, {"x": 0.4223676323890686, "y": 0.22077375650405884}, {"x": 0.38964900374412537, "y": 0.22077375650405884}], "text": "110M\n"}
{"page": 18, "bbox": [{"x": 0.6258179545402527, "y": 0.21446593105793}, {"x": 0.6603212356567383, "y": 0.21446593105793}, {"x": 0.6603212356567383, "y": 0.22077375650405884}, {"x": 0.6258179545402527, "y": 0.22077375650405884}], "text": "28.60\n"}
{"page": 18, "bbox": [{"x": 0.7162403464317322, "y": 0.21446593105793}, {"x": 0.7513384819030762, "y": 0.21446593105793}, {"x": 0.7513384819030762, "y": 0.22077375650405884}, {"x": 0.7162403464317322, "y": 0.22077375650405884}], "text": "49.07\n"}
{"page": 18, "bbox": [{"x": 0.5431290864944458, "y": 0.2140454202890396}, {"x": 0.5782272219657898, "y": 0.2140454202890396}, {"x": 0.5782272219657898, "y": 0.2211942821741104}, {"x": 0.5431290864944458, "y": 0.2211942821741104}], "text": "27.83\n"}
{"page": 18, "bbox": [{"x": 0.15764425694942474, "y": 0.21446593105793}, {"x": 0.2153480052947998, "y": 0.21446593105793}, {"x": 0.2153480052947998, "y": 0.2211942821741104}, {"x": 0.15764425694942474, "y": 0.2211942821741104}], "text": "TILDEV2\n"}
{"page": 18, "bbox": [{"x": 0.2891136109828949, "y": 0.21446593105793}, {"x": 0.3575252890586853, "y": 0.21446593105793}, {"x": 0.3575252890586853, "y": 0.2211942821741104}, {"x": 0.2891136109828949, "y": 0.2211942821741104}], "text": "BERT-base\n"}
{"page": 18, "bbox": [{"x": 0.4651992917060852, "y": 0.21446593105793}, {"x": 0.49910768866539, "y": 0.21446593105793}, {"x": 0.49910768866539, "y": 0.2211942821741104}, {"x": 0.4651992917060852, "y": 0.2211942821741104}], "text": "18.57\n"}
{"page": 18, "bbox": [{"x": 0.11838191747665405, "y": 0.23885618150234222}, {"x": 0.8822129964828491, "y": 0.23549200594425201}, {"x": 0.8828078508377075, "y": 0.2746005058288574}, {"x": 0.11897680163383484, "y": 0.27796468138694763}], "text": "Table 10: Results of different reranking methods on the dev set of the MS MARCO Passage ranking dataset. For\neach query, the top-1000 candidate passages retrieved by BM25 are reranked. Latency is measured in seconds per\nquery.\n"}
{"page": 18, "bbox": [{"x": 0.34741225838661194, "y": 0.28511354327201843}, {"x": 0.3700178563594818, "y": 0.28511354327201843}, {"x": 0.3700178563594818, "y": 0.29352396726608276}, {"x": 0.34741225838661194, "y": 0.29352396726608276}], "text": "NQ\n"}
{"page": 18, "bbox": [{"x": 0.4776918590068817, "y": 0.2855340540409088}, {"x": 0.5098155736923218, "y": 0.2855340540409088}, {"x": 0.5098155736923218, "y": 0.2943650186061859}, {"x": 0.4776918590068817, "y": 0.2943650186061859}], "text": "TQA\n"}
{"page": 18, "bbox": [{"x": 0.595478892326355, "y": 0.2855340540409088}, {"x": 0.6627007722854614, "y": 0.2855340540409088}, {"x": 0.6627007722854614, "y": 0.2943650186061859}, {"x": 0.595478892326355, "y": 0.2943650186061859}], "text": "HotPotQA\n"}
{"page": 18, "bbox": [{"x": 0.7769185304641724, "y": 0.29015979170799255}, {"x": 0.8477097153663635, "y": 0.2880571782588959}, {"x": 0.8483045697212219, "y": 0.29730865359306335}, {"x": 0.7775133848190308, "y": 0.29941126704216003}], "text": "Avg. Token\n"}
{"page": 18, "bbox": [{"x": 0.7162403464317322, "y": 0.2897392809391022}, {"x": 0.7436050176620483, "y": 0.2905803322792053}, {"x": 0.7430101037025452, "y": 0.29941126704216003}, {"x": 0.715645432472229, "y": 0.2985702157020569}], "text": "Avg.\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.2914213538169861}, {"x": 0.19988101720809937, "y": 0.2914213538169861}, {"x": 0.19988101720809937, "y": 0.2981497049331665}, {"x": 0.14991076290607452, "y": 0.2981497049331665}], "text": "Method\n"}
{"page": 18, "bbox": [{"x": 0.37299227714538574, "y": 0.3019343912601471}, {"x": 0.4134443700313568, "y": 0.3019343912601471}, {"x": 0.4134443700313568, "y": 0.3090832531452179}, {"x": 0.37299227714538574, "y": 0.3090832531452179}], "text": "#token\n"}
{"page": 18, "bbox": [{"x": 0.5074360370635986, "y": 0.3019343912601471}, {"x": 0.5484830737113953, "y": 0.3019343912601471}, {"x": 0.5484830737113953, "y": 0.3090832531452179}, {"x": 0.5074360370635986, "y": 0.3090832531452179}], "text": "#token\n"}
{"page": 18, "bbox": [{"x": 0.587150514125824, "y": 0.3019343912601471}, {"x": 0.6020225882530212, "y": 0.3019343912601471}, {"x": 0.6020225882530212, "y": 0.3090832531452179}, {"x": 0.587150514125824, "y": 0.3090832531452179}], "text": "F1\n"}
{"page": 18, "bbox": [{"x": 0.6424747109413147, "y": 0.3019343912601471}, {"x": 0.6835216879844666, "y": 0.3019343912601471}, {"x": 0.6835216879844666, "y": 0.3090832531452179}, {"x": 0.6424747109413147, "y": 0.3090832531452179}], "text": "#token\n"}
{"page": 18, "bbox": [{"x": 0.31707316637039185, "y": 0.30235493183135986}, {"x": 0.3325401544570923, "y": 0.30235493183135986}, {"x": 0.3325401544570923, "y": 0.3086627423763275}, {"x": 0.31707316637039185, "y": 0.3086627423763275}], "text": "F1\n"}
{"page": 18, "bbox": [{"x": 0.4521118402481079, "y": 0.30235493183135986}, {"x": 0.46757882833480835, "y": 0.30235493183135986}, {"x": 0.46757882833480835, "y": 0.3086627423763275}, {"x": 0.4521118402481079, "y": 0.3086627423763275}], "text": "F1\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.3141295313835144}, {"x": 0.2665080428123474, "y": 0.3141295313835144}, {"x": 0.2665080428123474, "y": 0.3212783932685852}, {"x": 0.14991076290607452, "y": 0.3212783932685852}], "text": "w/o Summarization\n"}
{"page": 18, "bbox": [{"x": 0.5782272219657898, "y": 0.32590413093566895}, {"x": 0.612135648727417, "y": 0.3263246417045593}, {"x": 0.612135648727417, "y": 0.33305299282073975}, {"x": 0.5782272219657898, "y": 0.332632452249527}], "text": "33.92\n"}
{"page": 18, "bbox": [{"x": 0.30814990401268005, "y": 0.3263246417045593}, {"x": 0.34324806928634644, "y": 0.3263246417045593}, {"x": 0.34324806928634644, "y": 0.33305299282073975}, {"x": 0.30814990401268005, "y": 0.33305299282073975}], "text": "27.07\n"}
{"page": 18, "bbox": [{"x": 0.7132658958435059, "y": 0.3263246417045593}, {"x": 0.7477691769599915, "y": 0.3263246417045593}, {"x": 0.7477691769599915, "y": 0.33305299282073975}, {"x": 0.7132658958435059, "y": 0.33305299282073975}], "text": "31.53\n"}
{"page": 18, "bbox": [{"x": 0.38191553950309753, "y": 0.3267451524734497}, {"x": 0.404521107673645, "y": 0.3267451524734497}, {"x": 0.404521107673645, "y": 0.33305299282073975}, {"x": 0.38191553950309753, "y": 0.33305299282073975}], "text": "124\n"}
{"page": 18, "bbox": [{"x": 0.44259369373321533, "y": 0.3267451524734497}, {"x": 0.4770969748497009, "y": 0.3267451524734497}, {"x": 0.4770969748497009, "y": 0.33305299282073975}, {"x": 0.44259369373321533, "y": 0.33305299282073975}], "text": "33.61\n"}
{"page": 18, "bbox": [{"x": 0.5169541835784912, "y": 0.3263246417045593}, {"x": 0.5395597815513611, "y": 0.3263246417045593}, {"x": 0.5395597815513611, "y": 0.3334735035896301}, {"x": 0.5169541835784912, "y": 0.3334735035896301}], "text": "152\n"}
{"page": 18, "bbox": [{"x": 0.8030933737754822, "y": 0.3267451524734497}, {"x": 0.8245092034339905, "y": 0.3267451524734497}, {"x": 0.8245092034339905, "y": 0.33305299282073975}, {"x": 0.8030933737754822, "y": 0.33305299282073975}], "text": "139\n"}
{"page": 18, "bbox": [{"x": 0.6525877714157104, "y": 0.3267451524734497}, {"x": 0.6734086871147156, "y": 0.3267451524734497}, {"x": 0.6734086871147156, "y": 0.3334735035896301}, {"x": 0.6525877714157104, "y": 0.3334735035896301}], "text": "141\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.3263246417045593}, {"x": 0.23795360326766968, "y": 0.3267451524734497}, {"x": 0.23795360326766968, "y": 0.3355761170387268}, {"x": 0.14991076290607452, "y": 0.3351556062698364}], "text": "Origin Prompt\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.33851975202560425}, {"x": 0.2623438537120819, "y": 0.33851975202560425}, {"x": 0.2623438537120819, "y": 0.34566864371299744}, {"x": 0.14991076290607452, "y": 0.34566864371299744}], "text": "Extractive Method\n"}
{"page": 18, "bbox": [{"x": 0.4431885778903961, "y": 0.3502943515777588}, {"x": 0.4770969748497009, "y": 0.3502943515777588}, {"x": 0.4770969748497009, "y": 0.3570227026939392}, {"x": 0.4431885778903961, "y": 0.3570227026939392}], "text": "32.44\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.3502943515777588}, {"x": 0.18798334896564484, "y": 0.3502943515777588}, {"x": 0.18798334896564484, "y": 0.357443243265152}, {"x": 0.14991076290607452, "y": 0.357443243265152}], "text": "BM25\n"}
{"page": 18, "bbox": [{"x": 0.38488996028900146, "y": 0.35071489214897156}, {"x": 0.39976203441619873, "y": 0.35071489214897156}, {"x": 0.39976203441619873, "y": 0.3570227026939392}, {"x": 0.38488996028900146, "y": 0.3570227026939392}], "text": "40\n"}
{"page": 18, "bbox": [{"x": 0.5199286341667175, "y": 0.35071489214897156}, {"x": 0.5342058539390564, "y": 0.35071489214897156}, {"x": 0.5342058539390564, "y": 0.3570227026939392}, {"x": 0.5199286341667175, "y": 0.3570227026939392}], "text": "59\n"}
{"page": 18, "bbox": [{"x": 0.5782272219657898, "y": 0.3502943515777588}, {"x": 0.6127305030822754, "y": 0.35071489214897156}, {"x": 0.6127305030822754, "y": 0.357443243265152}, {"x": 0.5782272219657898, "y": 0.3570227026939392}], "text": "28.00\n"}
{"page": 18, "bbox": [{"x": 0.655562162399292, "y": 0.35071489214897156}, {"x": 0.6704342365264893, "y": 0.35071489214897156}, {"x": 0.6704342365264893, "y": 0.3570227026939392}, {"x": 0.655562162399292, "y": 0.3570227026939392}], "text": "63\n"}
{"page": 18, "bbox": [{"x": 0.7132658958435059, "y": 0.35071489214897156}, {"x": 0.7471743226051331, "y": 0.35071489214897156}, {"x": 0.7471743226051331, "y": 0.3570227026939392}, {"x": 0.7132658958435059, "y": 0.3570227026939392}], "text": "29.47\n"}
{"page": 18, "bbox": [{"x": 0.8060678243637085, "y": 0.35071489214897156}, {"x": 0.8209398984909058, "y": 0.35071489214897156}, {"x": 0.8209398984909058, "y": 0.3570227026939392}, {"x": 0.8060678243637085, "y": 0.3570227026939392}], "text": "54\n"}
{"page": 18, "bbox": [{"x": 0.30755501985549927, "y": 0.35071489214897156}, {"x": 0.34324806928634644, "y": 0.35071489214897156}, {"x": 0.34324806928634644, "y": 0.357443243265152}, {"x": 0.30755501985549927, "y": 0.357443243265152}], "text": "27.97\n"}
{"page": 18, "bbox": [{"x": 0.8060678243637085, "y": 0.36206895112991333}, {"x": 0.8209398984909058, "y": 0.36206895112991333}, {"x": 0.8209398984909058, "y": 0.36879730224609375}, {"x": 0.8060678243637085, "y": 0.36879730224609375}], "text": "56\n"}
{"page": 18, "bbox": [{"x": 0.30755501985549927, "y": 0.36164844036102295}, {"x": 0.34265318512916565, "y": 0.36206895112991333}, {"x": 0.34265318512916565, "y": 0.3692178428173065}, {"x": 0.30755501985549927, "y": 0.36879730224609375}], "text": "23.62\n"}
{"page": 18, "bbox": [{"x": 0.38488996028900146, "y": 0.3624894917011261}, {"x": 0.39976203441619873, "y": 0.3624894917011261}, {"x": 0.39976203441619873, "y": 0.36837679147720337}, {"x": 0.38488996028900146, "y": 0.36837679147720337}], "text": "42\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.36206895112991333}, {"x": 0.21415823698043823, "y": 0.36206895112991333}, {"x": 0.21415823698043823, "y": 0.3692178428173065}, {"x": 0.14991076290607452, "y": 0.3692178428173065}], "text": "Contriever\n"}
{"page": 18, "bbox": [{"x": 0.4431885778903961, "y": 0.36206895112991333}, {"x": 0.4776918590068817, "y": 0.3624894917011261}, {"x": 0.4776918590068817, "y": 0.3692178428173065}, {"x": 0.4431885778903961, "y": 0.36879730224609375}], "text": "33.79\n"}
{"page": 18, "bbox": [{"x": 0.5199286341667175, "y": 0.36206895112991333}, {"x": 0.5342058539390564, "y": 0.36206895112991333}, {"x": 0.5342058539390564, "y": 0.3692178428173065}, {"x": 0.5199286341667175, "y": 0.3692178428173065}], "text": "65\n"}
{"page": 18, "bbox": [{"x": 0.5782272219657898, "y": 0.3624894917011261}, {"x": 0.6133254170417786, "y": 0.3624894917011261}, {"x": 0.6133254170417786, "y": 0.36879730224609375}, {"x": 0.5782272219657898, "y": 0.36879730224609375}], "text": "23.64\n"}
{"page": 18, "bbox": [{"x": 0.655562162399292, "y": 0.3624894917011261}, {"x": 0.6698393821716309, "y": 0.3624894917011261}, {"x": 0.6698393821716309, "y": 0.36879730224609375}, {"x": 0.655562162399292, "y": 0.36879730224609375}], "text": "60\n"}
{"page": 18, "bbox": [{"x": 0.7132658958435059, "y": 0.3624894917011261}, {"x": 0.7471743226051331, "y": 0.3624894917011261}, {"x": 0.7471743226051331, "y": 0.36879730224609375}, {"x": 0.7132658958435059, "y": 0.36879730224609375}], "text": "27.02\n"}
{"page": 18, "bbox": [{"x": 0.5205234885215759, "y": 0.38267451524734497}, {"x": 0.5205234885215759, "y": 0.35113540291786194}, {"x": 0.5342058539390564, "y": 0.35113540291786194}, {"x": 0.5342058539390564, "y": 0.38267451524734497}], "text": "36\n"}
{"page": 18, "bbox": [{"x": 0.4431885778903961, "y": 0.3734230399131775}, {"x": 0.47650209069252014, "y": 0.3734230399131775}, {"x": 0.47650209069252014, "y": 0.3805719017982483}, {"x": 0.4431885778903961, "y": 0.3805719017982483}], "text": "35.32\n"}
{"page": 18, "bbox": [{"x": 0.5776323676109314, "y": 0.37384358048439026}, {"x": 0.6115407347679138, "y": 0.3730025291442871}, {"x": 0.6115407347679138, "y": 0.3801513910293579}, {"x": 0.5782272219657898, "y": 0.38099244236946106}], "text": "29.46\n"}
{"page": 18, "bbox": [{"x": 0.7132658958435059, "y": 0.3734230399131775}, {"x": 0.7477691769599915, "y": 0.37384358048439026}, {"x": 0.7477691769599915, "y": 0.38099244236946106}, {"x": 0.7132658958435059, "y": 0.3805719017982483}], "text": "30.87\n"}
{"page": 18, "bbox": [{"x": 0.30814990401268005, "y": 0.37426409125328064}, {"x": 0.34324806928634644, "y": 0.37426409125328064}, {"x": 0.34324806928634644, "y": 0.3805719017982483}, {"x": 0.30814990401268005, "y": 0.3805719017982483}], "text": "27.84\n"}
{"page": 18, "bbox": [{"x": 0.38548484444618225, "y": 0.37426409125328064}, {"x": 0.39976203441619873, "y": 0.37426409125328064}, {"x": 0.39976203441619873, "y": 0.3805719017982483}, {"x": 0.38548484444618225, "y": 0.3805719017982483}], "text": "34\n"}
{"page": 18, "bbox": [{"x": 0.5205234885215759, "y": 0.37426409125328064}, {"x": 0.535395622253418, "y": 0.37426409125328064}, {"x": 0.535395622253418, "y": 0.3805719017982483}, {"x": 0.5205234885215759, "y": 0.3805719017982483}], "text": "60\n"}
{"page": 18, "bbox": [{"x": 0.6549673080444336, "y": 0.37426409125328064}, {"x": 0.6704342365264893, "y": 0.37426409125328064}, {"x": 0.6704342365264893, "y": 0.3805719017982483}, {"x": 0.6549673080444336, "y": 0.3805719017982483}], "text": "58\n"}
{"page": 18, "bbox": [{"x": 0.8054729104042053, "y": 0.37426409125328064}, {"x": 0.8203450441360474, "y": 0.37426409125328064}, {"x": 0.8203450441360474, "y": 0.3805719017982483}, {"x": 0.8054729104042053, "y": 0.3805719017982483}], "text": "51\n"}
{"page": 18, "bbox": [{"x": 0.14931587874889374, "y": 0.3734230399131775}, {"x": 0.2712671160697937, "y": 0.37258198857307434}, {"x": 0.2712671160697937, "y": 0.38267451524734497}, {"x": 0.14931587874889374, "y": 0.3835155665874481}], "text": "Recomp (extractive)\n"}
{"page": 18, "bbox": [{"x": 0.14931587874889374, "y": 0.38519763946533203}, {"x": 0.26888757944107056, "y": 0.38477712869644165}, {"x": 0.26888757944107056, "y": 0.393187552690506}, {"x": 0.14931587874889374, "y": 0.39360806345939636}], "text": "Abstractive Method\n"}
{"page": 18, "bbox": [{"x": 0.8030933737754822, "y": 0.4280908405780792}, {"x": 0.8042831420898438, "y": 0.3502943515777588}, {"x": 0.8239143490791321, "y": 0.3502943515777588}, {"x": 0.8227245807647705, "y": 0.4280908405780792}], "text": "༅༴ ༤ཡ་\n"}
{"page": 18, "bbox": [{"x": 0.653777539730072, "y": 0.42935240268707275}, {"x": 0.6549673080444336, "y": 0.35071489214897156}, {"x": 0.6698393821716309, "y": 0.35071489214897156}, {"x": 0.6686496138572693, "y": 0.42935240268707275}], "text": "442 33\n"}
{"page": 18, "bbox": [{"x": 0.8060678243637085, "y": 0.3982338011264801}, {"x": 0.8203450441360474, "y": 0.3982338011264801}, {"x": 0.8203450441360474, "y": 0.4049621522426605}, {"x": 0.8060678243637085, "y": 0.4049621522426605}], "text": "67\n"}
{"page": 18, "bbox": [{"x": 0.30755501985549927, "y": 0.3982338011264801}, {"x": 0.34265318512916565, "y": 0.3982338011264801}, {"x": 0.34265318512916565, "y": 0.4053826630115509}, {"x": 0.30755501985549927, "y": 0.4053826630115509}], "text": "25.05\n"}
{"page": 18, "bbox": [{"x": 0.4431885778903961, "y": 0.3982338011264801}, {"x": 0.4770969748497009, "y": 0.3982338011264801}, {"x": 0.4770969748497009, "y": 0.4053826630115509}, {"x": 0.4431885778903961, "y": 0.4053826630115509}], "text": "34.25\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.3978132903575897}, {"x": 0.25282570719718933, "y": 0.3982338011264801}, {"x": 0.25282570719718933, "y": 0.40580320358276367}, {"x": 0.14991076290607452, "y": 0.4053826630115509}], "text": "SelectiveContext\n"}
{"page": 18, "bbox": [{"x": 0.38548484444618225, "y": 0.39865434169769287}, {"x": 0.4003569185733795, "y": 0.39865434169769287}, {"x": 0.4003569185733795, "y": 0.4049621522426605}, {"x": 0.38548484444618225, "y": 0.4049621522426605}], "text": "65\n"}
{"page": 18, "bbox": [{"x": 0.5758476853370667, "y": 0.39865434169769287}, {"x": 0.6151100397109985, "y": 0.39865434169769287}, {"x": 0.6151100397109985, "y": 0.4049621522426605}, {"x": 0.5758476853370667, "y": 0.4049621522426605}], "text": "34.43\n"}
{"page": 18, "bbox": [{"x": 0.6549673080444336, "y": 0.39865434169769287}, {"x": 0.6710291504859924, "y": 0.39865434169769287}, {"x": 0.6710291504859924, "y": 0.4049621522426605}, {"x": 0.6549673080444336, "y": 0.4049621522426605}], "text": "66\n"}
{"page": 18, "bbox": [{"x": 0.7132658958435059, "y": 0.39865434169769287}, {"x": 0.7471743226051331, "y": 0.39865434169769287}, {"x": 0.7471743226051331, "y": 0.4049621522426605}, {"x": 0.7132658958435059, "y": 0.4049621522426605}], "text": "31.24\n"}
{"page": 18, "bbox": [{"x": 0.5211184024810791, "y": 0.39865434169769287}, {"x": 0.535395622253418, "y": 0.39865434169769287}, {"x": 0.535395622253418, "y": 0.4053826630115509}, {"x": 0.5211184024810791, "y": 0.4053826630115509}], "text": "70\n"}
{"page": 18, "bbox": [{"x": 0.3842950761318207, "y": 0.4301934540271759}, {"x": 0.3842950761318207, "y": 0.39192599058151245}, {"x": 0.4003569185733795, "y": 0.39192599058151245}, {"x": 0.4003569185733795, "y": 0.4301934540271759}], "text": "5555\n"}
{"page": 18, "bbox": [{"x": 0.6549673080444336, "y": 0.41000840067863464}, {"x": 0.6710291504859924, "y": 0.41000840067863464}, {"x": 0.6710291504859924, "y": 0.41673675179481506}, {"x": 0.6549673080444336, "y": 0.41673675179481506}], "text": "57\n"}
{"page": 18, "bbox": [{"x": 0.30814990401268005, "y": 0.4104289412498474}, {"x": 0.34265318512916565, "y": 0.4104289412498474}, {"x": 0.34265318512916565, "y": 0.41673675179481506}, {"x": 0.30814990401268005, "y": 0.41673675179481506}], "text": "21.32\n"}
{"page": 18, "bbox": [{"x": 0.38548484444618225, "y": 0.4104289412498474}, {"x": 0.4003569185733795, "y": 0.4104289412498474}, {"x": 0.4003569185733795, "y": 0.41673675179481506}, {"x": 0.38548484444618225, "y": 0.41673675179481506}], "text": "51\n"}
{"page": 18, "bbox": [{"x": 0.7132658958435059, "y": 0.4104289412498474}, {"x": 0.7477691769599915, "y": 0.4104289412498474}, {"x": 0.7477691769599915, "y": 0.41673675179481506}, {"x": 0.7132658958435059, "y": 0.41673675179481506}], "text": "28.29\n"}
{"page": 18, "bbox": [{"x": 0.4431885778903961, "y": 0.4104289412498474}, {"x": 0.4776918590068817, "y": 0.4104289412498474}, {"x": 0.4776918590068817, "y": 0.41715726256370544}, {"x": 0.4431885778903961, "y": 0.41715726256370544}], "text": "32.81\n"}
{"page": 18, "bbox": [{"x": 0.5199286341667175, "y": 0.4104289412498474}, {"x": 0.535395622253418, "y": 0.4104289412498474}, {"x": 0.535395622253418, "y": 0.41715726256370544}, {"x": 0.5199286341667175, "y": 0.41715726256370544}], "text": "56\n"}
{"page": 18, "bbox": [{"x": 0.5776323676109314, "y": 0.4104289412498474}, {"x": 0.612135648727417, "y": 0.4104289412498474}, {"x": 0.612135648727417, "y": 0.41715726256370544}, {"x": 0.5776323676109314, "y": 0.41715726256370544}], "text": "30.79\n"}
{"page": 18, "bbox": [{"x": 0.8054729104042053, "y": 0.4104289412498474}, {"x": 0.8215348124504089, "y": 0.4104289412498474}, {"x": 0.8215348124504089, "y": 0.41715726256370544}, {"x": 0.8054729104042053, "y": 0.41715726256370544}], "text": "55\n"}
{"page": 18, "bbox": [{"x": 0.14991076290607452, "y": 0.41000840067863464}, {"x": 0.25223082304000854, "y": 0.41000840067863464}, {"x": 0.25223082304000854, "y": 0.4192598760128021}, {"x": 0.14991076290607452, "y": 0.4192598760128021}], "text": "LongLLMlingua\n"}
{"page": 18, "bbox": [{"x": 0.6549673080444336, "y": 0.4217830002307892}, {"x": 0.6704342365264893, "y": 0.4217830002307892}, {"x": 0.6704342365264893, "y": 0.4285113513469696}, {"x": 0.6549673080444336, "y": 0.4285113513469696}], "text": "57\n"}
{"page": 18, "bbox": [{"x": 0.7108863592147827, "y": 0.4213624894618988}, {"x": 0.7495538592338562, "y": 0.4217830002307892}, {"x": 0.7495538592338562, "y": 0.42893186211586}, {"x": 0.7108863592147827, "y": 0.4285113513469696}], "text": "32.85\n"}
{"page": 18, "bbox": [{"x": 0.8060678243637085, "y": 0.4217830002307892}, {"x": 0.8203450441360474, "y": 0.4217830002307892}, {"x": 0.8203450441360474, "y": 0.4285113513469696}, {"x": 0.8060678243637085, "y": 0.4285113513469696}], "text": "59\n"}
{"page": 18, "bbox": [{"x": 0.4402141571044922, "y": 0.4217830002307892}, {"x": 0.4794765114784241, "y": 0.42220354080200195}, {"x": 0.4794765114784241, "y": 0.42893186211586}, {"x": 0.4402141571044922, "y": 0.4285113513469696}], "text": "35.87\n"}
{"page": 18, "bbox": [{"x": 0.3057703673839569, "y": 0.42220354080200195}, {"x": 0.344437837600708, "y": 0.42220354080200195}, {"x": 0.344437837600708, "y": 0.4285113513469696}, {"x": 0.3057703673839569, "y": 0.4285113513469696}], "text": "33.68\n"}
{"page": 18, "bbox": [{"x": 0.38548484444618225, "y": 0.42220354080200195}, {"x": 0.39976203441619873, "y": 0.42220354080200195}, {"x": 0.39976203441619873, "y": 0.4285113513469696}, {"x": 0.38548484444618225, "y": 0.4285113513469696}], "text": "59\n"}
{"page": 18, "bbox": [{"x": 0.5205234885215759, "y": 0.42220354080200195}, {"x": 0.5348007082939148, "y": 0.42220354080200195}, {"x": 0.5348007082939148, "y": 0.4285113513469696}, {"x": 0.5205234885215759, "y": 0.4285113513469696}], "text": "61\n"}
{"page": 18, "bbox": [{"x": 0.5782272219657898, "y": 0.42220354080200195}, {"x": 0.612135648727417, "y": 0.42220354080200195}, {"x": 0.612135648727417, "y": 0.4285113513469696}, {"x": 0.5782272219657898, "y": 0.4285113513469696}], "text": "29.01\n"}
{"page": 18, "bbox": [{"x": 0.14931587874889374, "y": 0.4213624894618988}, {"x": 0.27781081199645996, "y": 0.42052143812179565}, {"x": 0.27781081199645996, "y": 0.4306139647960663}, {"x": 0.14931587874889374, "y": 0.43145501613616943}], "text": "Recomp (abstractive)\n"}
{"page": 18, "bbox": [{"x": 0.2801903486251831, "y": 0.4465937614440918}, {"x": 0.7186198830604553, "y": 0.4465937614440918}, {"x": 0.7186198830604553, "y": 0.4566862881183624}, {"x": 0.2801903486251831, "y": 0.4566862881183624}], "text": "Table 11: Comparison between different summarization methods.\n"}
{"page": 18, "bbox": [{"x": 0.7126710414886475, "y": 0.47771236300468445}, {"x": 0.8494943380355835, "y": 0.47813287377357483}, {"x": 0.8494943380355835, "y": 0.4924306273460388}, {"x": 0.7126710414886475, "y": 0.49201008677482605}], "text": "Models Trained with Different Method\nMgg\n"}
{"page": 18, "bbox": [{"x": 0.7834622263908386, "y": 0.48654332756996155}, {"x": 0.7929803729057312, "y": 0.48654332756996155}, {"x": 0.7929803729057312, "y": 0.49032801389694214}, {"x": 0.7834622263908386, "y": 0.49032801389694214}], "text": "M\n"}
{"page": 18, "bbox": [{"x": 0.7441998720169067, "y": 0.48696383833885193}, {"x": 0.7543129324913025, "y": 0.48696383833885193}, {"x": 0.7543129324913025, "y": 0.4907485246658325}, {"x": 0.7441998720169067, "y": 0.4907485246658325}], "text": "Mh\n"}
{"page": 18, "bbox": [{"x": 0.7447947859764099, "y": 0.4949537515640259}, {"x": 0.7555027008056641, "y": 0.4949537515640259}, {"x": 0.7555027008056641, "y": 0.4995794892311096}, {"x": 0.7447947859764099, "y": 0.4995794892311096}], "text": "Mg\n"}
{"page": 18, "bbox": [{"x": 0.7834622263908386, "y": 0.4945332109928131}, {"x": 0.7983343005180359, "y": 0.4949537515640259}, {"x": 0.7977394461631775, "y": 0.5004205107688904}, {"x": 0.7828673124313354, "y": 0.5}], "text": "Mgr\n"}
{"page": 18, "bbox": [{"x": 0.11838191747665405, "y": 0.47434818744659424}, {"x": 0.48839977383613586, "y": 0.4751892387866974}, {"x": 0.48839977383613586, "y": 0.5349032878875732}, {"x": 0.11838191747665405, "y": 0.5340622663497925}], "text": "model on their validation splits or manually split a\nsubset from the training set to avoid overlapping.\nThe exact number of entries in each train and test\nset is detailed in Table 13.\n"}
{"page": 18, "bbox": [{"x": 0.5365853905677795, "y": 0.5613961219787598}, {"x": 0.5365853905677795, "y": 0.5231286883354187}, {"x": 0.543724000453949, "y": 0.5231286883354187}, {"x": 0.543724000453949, "y": 0.5613961219787598}], "text": "Coverage Score\n"}
{"page": 18, "bbox": [{"x": 0.18738846480846405, "y": 0.5529857277870178}, {"x": 0.23557406663894653, "y": 0.5529857277870178}, {"x": 0.23557406663894653, "y": 0.560555100440979}, {"x": 0.18738846480846405, "y": 0.560555100440979}], "text": "Dataset\n"}
{"page": 18, "bbox": [{"x": 0.38072577118873596, "y": 0.5538267493247986}, {"x": 0.4176085591316223, "y": 0.5538267493247986}, {"x": 0.4176085591316223, "y": 0.560555100440979}, {"x": 0.38072577118873596, "y": 0.560555100440979}], "text": "#Eval\n"}
{"page": 18, "bbox": [{"x": 0.30041640996932983, "y": 0.5534062385559082}, {"x": 0.344437837600708, "y": 0.5534062385559082}, {"x": 0.344437837600708, "y": 0.5853658318519592}, {"x": 0.30041640996932983, "y": 0.5853658318519592}], "text": "#Train\n2,090\n15,000\n"}
{"page": 18, "bbox": [{"x": 0.18679358065128326, "y": 0.5647603273391724}, {"x": 0.2284354567527771, "y": 0.5651808381080627}, {"x": 0.2284354567527771, "y": 0.5748528242111206}, {"x": 0.18679358065128326, "y": 0.5744323134422302}], "text": "ASQA\n"}
{"page": 18, "bbox": [{"x": 0.3801308870315552, "y": 0.5647603273391724}, {"x": 0.41641879081726074, "y": 0.5643397569656372}, {"x": 0.41701367497444153, "y": 0.5853658318519592}, {"x": 0.38072577118873596, "y": 0.5857864022254944}], "text": "483\n7,405\n"}
{"page": 18, "bbox": [{"x": 0.18679358065128326, "y": 0.5773759484291077}, {"x": 0.2504461705684662, "y": 0.5773759484291077}, {"x": 0.2504461705684662, "y": 0.5870479345321655}, {"x": 0.18679358065128326, "y": 0.5870479345321655}], "text": "HotpotQA\n"}
{"page": 18, "bbox": [{"x": 0.18738846480846405, "y": 0.5891505479812622}, {"x": 0.24390244483947754, "y": 0.5891505479812622}, {"x": 0.24390244483947754, "y": 0.5979815125465393}, {"x": 0.18738846480846405, "y": 0.5979815125465393}], "text": "TriviaQA\n"}
{"page": 18, "bbox": [{"x": 0.3069601356983185, "y": 0.5895710587501526}, {"x": 0.41641879081726074, "y": 0.5895710587501526}, {"x": 0.41641879081726074, "y": 0.5979815125465393}, {"x": 0.3069601356983185, "y": 0.5979815125465393}], "text": "9,000 6,368\n"}
{"page": 18, "bbox": [{"x": 0.7965496778488159, "y": 0.5992430448532104}, {"x": 0.8096371293067932, "y": 0.5996635556221008}, {"x": 0.80904221534729, "y": 0.6059713959693909}, {"x": 0.7959547638893127, "y": 0.6055508852005005}], "text": "Dgr\n"}
{"page": 18, "bbox": [{"x": 0.5936942100524902, "y": 0.600084125995636}, {"x": 0.6061868071556091, "y": 0.600084125995636}, {"x": 0.6061868071556091, "y": 0.6051303744316101}, {"x": 0.5936942100524902, "y": 0.6051303744316101}], "text": "Dø\n"}
{"page": 18, "bbox": [{"x": 0.730517566204071, "y": 0.600084125995636}, {"x": 0.7400357127189636, "y": 0.600084125995636}, {"x": 0.7400357127189636, "y": 0.6051303744316101}, {"x": 0.730517566204071, "y": 0.6051303744316101}], "text": "Dr\n"}
{"page": 18, "bbox": [{"x": 0.6627007722854614, "y": 0.600084125995636}, {"x": 0.6734086871147156, "y": 0.600084125995636}, {"x": 0.6734086871147156, "y": 0.6063919067382812}, {"x": 0.6627007722854614, "y": 0.6063919067382812}], "text": "Dg\n"}
{"page": 18, "bbox": [{"x": 0.3789411187171936, "y": 0.6013456583023071}, {"x": 0.41582390666007996, "y": 0.6009251475334167}, {"x": 0.41582390666007996, "y": 0.6084945201873779}, {"x": 0.3789411187171936, "y": 0.6089150309562683}], "text": "8,006\n"}
{"page": 18, "bbox": [{"x": 0.29982152581214905, "y": 0.6013456583023071}, {"x": 0.3438429534435272, "y": 0.6009251475334167}, {"x": 0.3438429534435272, "y": 0.6089150309562683}, {"x": 0.29982152581214905, "y": 0.6093356013298035}], "text": "15,000\n"}
{"page": 18, "bbox": [{"x": 0.18679358065128326, "y": 0.6009251475334167}, {"x": 0.20999404788017273, "y": 0.6013456583023071}, {"x": 0.20939916372299194, "y": 0.6101766228675842}, {"x": 0.18619869649410248, "y": 0.6097561120986938}], "text": "NQ\n"}
{"page": 18, "bbox": [{"x": 0.18738846480846405, "y": 0.6118587255477905}, {"x": 0.26472339034080505, "y": 0.6126997470855713}, {"x": 0.26472339034080505, "y": 0.6215307116508484}, {"x": 0.18738846480846405, "y": 0.6206896305084229}], "text": "NarrativeQA\n"}
{"page": 18, "bbox": [{"x": 0.3069601356983185, "y": 0.6131202578544617}, {"x": 0.344437837600708, "y": 0.6126997470855713}, {"x": 0.344437837600708, "y": 0.621110200881958}, {"x": 0.3069601356983185, "y": 0.6215307116508484}], "text": "7,000\n"}
{"page": 18, "bbox": [{"x": 0.3069601356983185, "y": 0.6253153681755066}, {"x": 0.344437837600708, "y": 0.6244743466377258}, {"x": 0.344437837600708, "y": 0.6324642300605774}, {"x": 0.3069601356983185, "y": 0.6333053112030029}], "text": "67,00\n"}
{"page": 18, "bbox": [{"x": 0.18738846480846405, "y": 0.6244743466377258}, {"x": 0.2581796646118164, "y": 0.6240538358688354}, {"x": 0.2581796646118164, "y": 0.6450799107551575}, {"x": 0.18738846480846405, "y": 0.6455004215240479}], "text": "SQUAD\nTruthfulQA\n"}
{"page": 18, "bbox": [{"x": 0.32183223962783813, "y": 0.6370899677276611}, {"x": 0.344437837600708, "y": 0.6370899677276611}, {"x": 0.344437837600708, "y": 0.6433978080749512}, {"x": 0.32183223962783813, "y": 0.6433978080749512}], "text": "817\n"}
{"page": 18, "bbox": [{"x": 0.11838191747665405, "y": 0.6610597372055054}, {"x": 0.4860202372074127, "y": 0.6619007587432861}, {"x": 0.4860202372074127, "y": 0.6867115497589111}, {"x": 0.11838191747665405, "y": 0.6858704686164856}], "text": "Table 13: Number of examples in each Dataset used in\nthe fine-tuning experiments.\n"}
{"page": 18, "bbox": [{"x": 0.5133848786354065, "y": 0.6169049739837646}, {"x": 0.881618082523346, "y": 0.6169049739837646}, {"x": 0.881618082523346, "y": 0.7582001686096191}, {"x": 0.5133848786354065, "y": 0.7582001686096191}], "text": "Figure 3: Results of generator fine-tuning.\nused for fine-tuning and evaluation mainly follow\n(Lin et al., 2023). We train our generator for 3\nepochs and constrain the maximum length of the\nsequence to 1600, using a batch size of 4 and a\nlearning rate of 5e-5. During testing, we use a\nzero-shot setting.\nDetailed Results Table 12 shows our evaluation\nresults on each dataset.\n"}
{"page": 18, "bbox": [{"x": 0.11778703331947327, "y": 0.7119427919387817}, {"x": 0.4878048896789551, "y": 0.7119427919387817}, {"x": 0.4878048896789551, "y": 0.920941948890686}, {"x": 0.11778703331947327, "y": 0.920941948890686}], "text": "We use the dataset-provided documents as dgold\nfor each data entry. To obtain drandom we sample\nthe context of different entries within the same\ndataset, to make sure the distributions of drandom\nand dgold are roughly similar.\nMetrics We use the ground-truth coverage as our\nevaluation metric, considering that the answers of\nQA tasks are relatively short, while the generation\nlength of the model is sometimes hard to limit.\nImplementation Details We select Llama-2-\n7b (Touvron et al., 2023b) as the base model. For\nefficiency, we use LoRA (Hu et al., 2021) and int8\nquantization during training. The prompt templates\n"}
{"page": 18, "bbox": [{"x": 0.5127900242805481, "y": 0.7754415273666382}, {"x": 0.8839976191520691, "y": 0.7754415273666382}, {"x": 0.8839976191520691, "y": 0.920941948890686}, {"x": 0.5127900242805481, "y": 0.920941948890686}], "text": "A.7 Experimental Details of Comprehensive\nEvaluation\nTasks and Datasets We conducted extensive ex-\nperiments across various NLP tasks and datasets to\nassess the performance of RAG systems. Specif-\nically: (1) Commonsense Reasoning: We eval-\nuated on MMLU (Hendrycks et al., 2020), ARC-\nChallenge (Clark et al., 2018), and OpenbookQA\n(Mihaylov et al., 2018) datasets. (2) Fact Check-\n"}
{"page": 18, "bbox": [{"x": 0.48007139563560486, "y": 0.931034505367279}, {"x": 0.5246877074241638, "y": 0.931034505367279}, {"x": 0.5246877074241638, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17733\n"}
{"page": 19, "bbox": [{"x": 0.1850089281797409, "y": 0.08830950409173965}, {"x": 0.23616895079612732, "y": 0.08830950409173965}, {"x": 0.23616895079612732, "y": 0.09545836597681046}, {"x": 0.1850089281797409, "y": 0.09545836597681046}], "text": "Context\n"}
{"page": 19, "bbox": [{"x": 0.28316476941108704, "y": 0.08873002231121063}, {"x": 0.32540154457092285, "y": 0.08873002231121063}, {"x": 0.32540154457092285, "y": 0.09545836597681046}, {"x": 0.28316476941108704, "y": 0.09545836597681046}], "text": "Model\n"}
{"page": 19, "bbox": [{"x": 0.5698988437652588, "y": 0.08788898587226868}, {"x": 0.6365258693695068, "y": 0.08788898587226868}, {"x": 0.6365258693695068, "y": 0.09714045375585556}, {"x": 0.5698988437652588, "y": 0.09714045375585556}], "text": "HotpotQA\n"}
{"page": 19, "bbox": [{"x": 0.6847114562988281, "y": 0.08830950409173965}, {"x": 0.7263533473014832, "y": 0.08788898587226868}, {"x": 0.7263533473014832, "y": 0.09671993553638458}, {"x": 0.6847114562988281, "y": 0.09714045375585556}], "text": "ASQA\n"}
{"page": 19, "bbox": [{"x": 0.38132065534591675, "y": 0.08830950409173965}, {"x": 0.40392622351646423, "y": 0.08830950409173965}, {"x": 0.40392622351646423, "y": 0.09714045375585556}, {"x": 0.38132065534591675, "y": 0.09714045375585556}], "text": "NQ\n"}
{"page": 19, "bbox": [{"x": 0.4610351026058197, "y": 0.08830950409173965}, {"x": 0.5217132568359375, "y": 0.08873002231121063}, {"x": 0.5217132568359375, "y": 0.09714045375585556}, {"x": 0.4610351026058197, "y": 0.09671993553638458}], "text": "TriviaQA\n"}
{"page": 19, "bbox": [{"x": 0.7804877758026123, "y": 0.08873002231121063}, {"x": 0.8084473609924316, "y": 0.08873002231121063}, {"x": 0.8084473609924316, "y": 0.09714045375585556}, {"x": 0.7804877758026123, "y": 0.09714045375585556}], "text": "Avg.\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.10428931564092636}, {"x": 0.8120166659355164, "y": 0.10386879742145538}, {"x": 0.8120166659355164, "y": 0.1105971410870552}, {"x": 0.7769185304641724, "y": 0.11101765930652618}], "text": "37.96\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.10428931564092636}, {"x": 0.5080309510231018, "y": 0.10428931564092636}, {"x": 0.5080309510231018, "y": 0.11101765930652618}, {"x": 0.4735276699066162, "y": 0.11101765930652618}], "text": "60.44\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.10428931564092636}, {"x": 0.72218918800354, "y": 0.10428931564092636}, {"x": 0.72218918800354, "y": 0.11101765930652618}, {"x": 0.6882807612419128, "y": 0.11101765930652618}], "text": "37.89\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.10428931564092636}, {"x": 0.6210588812828064, "y": 0.10470984131097794}, {"x": 0.6210588812828064, "y": 0.11185870319604874}, {"x": 0.5859607458114624, "y": 0.11143818497657776}], "text": "23.73\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.10470984131097794}, {"x": 0.4092801809310913, "y": 0.10470984131097794}, {"x": 0.4092801809310913, "y": 0.11143818497657776}, {"x": 0.3753718137741089, "y": 0.11143818497657776}], "text": "29.78\n"}
{"page": 19, "bbox": [{"x": 0.2837596535682678, "y": 0.10386879742145538}, {"x": 0.30398571491241455, "y": 0.10386879742145538}, {"x": 0.30398571491241455, "y": 0.11269974708557129}, {"x": 0.2837596535682678, "y": 0.11269974708557129}], "text": "Мь\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.11942809075117111}, {"x": 0.4086852967739105, "y": 0.11942809075117111}, {"x": 0.4086852967739105, "y": 0.12615643441677094}, {"x": 0.3753718137741089, "y": 0.12615643441677094}], "text": "26.23\n"}
{"page": 19, "bbox": [{"x": 0.688875675201416, "y": 0.11942809075117111}, {"x": 0.72218918800354, "y": 0.11942809075117111}, {"x": 0.72218918800354, "y": 0.12615643441677094}, {"x": 0.688875675201416, "y": 0.12615643441677094}], "text": "32.30\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.11900757253170013}, {"x": 0.620464026927948, "y": 0.11942809075117111}, {"x": 0.620464026927948, "y": 0.1265769600868225}, {"x": 0.5859607458114624, "y": 0.12615643441677094}], "text": "26.67\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.11984860897064209}, {"x": 0.5086258053779602, "y": 0.11984860897064209}, {"x": 0.5086258053779602, "y": 0.1265769600868225}, {"x": 0.4735276699066162, "y": 0.1265769600868225}], "text": "58.26\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.11984860897064209}, {"x": 0.8114217519760132, "y": 0.11984860897064209}, {"x": 0.8114217519760132, "y": 0.1265769600868225}, {"x": 0.7775133848190308, "y": 0.1265769600868225}], "text": "35.87\n"}
{"page": 19, "bbox": [{"x": 0.2843545377254486, "y": 0.11816652864217758}, {"x": 0.3057703673839569, "y": 0.11900757253170013}, {"x": 0.3051754832267761, "y": 0.12910008430480957}, {"x": 0.2837596535682678, "y": 0.12825904786586761}], "text": "Mg\n"}
{"page": 19, "bbox": [{"x": 0.19512194395065308, "y": 0.13120269775390625}, {"x": 0.22546103596687317, "y": 0.13162320852279663}, {"x": 0.22486615180969238, "y": 0.1429772973060608}, {"x": 0.1945270597934723, "y": 0.14255677163600922}], "text": "Dø\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.13456685841083527}, {"x": 0.4098750650882721, "y": 0.13498738408088684}, {"x": 0.4098750650882721, "y": 0.14213624596595764}, {"x": 0.3753718137741089, "y": 0.14171572029590607}], "text": "31.10\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.13456685841083527}, {"x": 0.5080309510231018, "y": 0.13498738408088684}, {"x": 0.5080309510231018, "y": 0.14213624596595764}, {"x": 0.474122554063797, "y": 0.14171572029590607}], "text": "61.37\n"}
{"page": 19, "bbox": [{"x": 0.5865556001663208, "y": 0.13456685841083527}, {"x": 0.620464026927948, "y": 0.13498738408088684}, {"x": 0.620464026927948, "y": 0.14213624596595764}, {"x": 0.5865556001663208, "y": 0.14171572029590607}], "text": "28.40\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.13498738408088684}, {"x": 0.72218918800354, "y": 0.13456685841083527}, {"x": 0.72218918800354, "y": 0.14171572029590607}, {"x": 0.6882807612419128, "y": 0.14213624596595764}], "text": "39.96\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.13456685841083527}, {"x": 0.8114217519760132, "y": 0.13498738408088684}, {"x": 0.8114217519760132, "y": 0.14213624596595764}, {"x": 0.7769185304641724, "y": 0.14171572029590607}], "text": "40.21\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.1337258219718933}, {"x": 0.3057703673839569, "y": 0.13498738408088684}, {"x": 0.30458059906959534, "y": 0.1429772973060608}, {"x": 0.2843545377254486, "y": 0.14171572029590607}], "text": "Mr\n"}
{"page": 19, "bbox": [{"x": 0.5865556001663208, "y": 0.1501261591911316}, {"x": 0.620464026927948, "y": 0.1501261591911316}, {"x": 0.620464026927948, "y": 0.15685449540615082}, {"x": 0.5865556001663208, "y": 0.15685449540615082}], "text": "26.43\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.15054668486118317}, {"x": 0.7227840423583984, "y": 0.15054668486118317}, {"x": 0.7227840423583984, "y": 0.15685449540615082}, {"x": 0.6882807612419128, "y": 0.15685449540615082}], "text": "32.99\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.1501261591911316}, {"x": 0.8126115202903748, "y": 0.1501261591911316}, {"x": 0.8126115202903748, "y": 0.1572750210762024}, {"x": 0.7775133848190308, "y": 0.1572750210762024}], "text": "35.70\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.15054668486118317}, {"x": 0.4098750650882721, "y": 0.15054668486118317}, {"x": 0.4098750650882721, "y": 0.1572750210762024}, {"x": 0.3753718137741089, "y": 0.1572750210762024}], "text": "25.92\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.15054668486118317}, {"x": 0.5086258053779602, "y": 0.15054668486118317}, {"x": 0.5086258053779602, "y": 0.1572750210762024}, {"x": 0.4735276699066162, "y": 0.1572750210762024}], "text": "57.62\n"}
{"page": 19, "bbox": [{"x": 0.28613919019699097, "y": 0.14886459708213806}, {"x": 0.31171920895576477, "y": 0.1534903347492218}, {"x": 0.30874478816986084, "y": 0.16105970740318298}, {"x": 0.28316476941108704, "y": 0.15643398463726044}], "text": "Mgr\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.16568544507026672}, {"x": 0.5080309510231018, "y": 0.16568544507026672}, {"x": 0.5080309510231018, "y": 0.17241379618644714}, {"x": 0.474122554063797, "y": 0.17241379618644714}], "text": "58.07\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.16568544507026672}, {"x": 0.7227840423583984, "y": 0.16568544507026672}, {"x": 0.7227840423583984, "y": 0.17241379618644714}, {"x": 0.6882807612419128, "y": 0.17241379618644714}], "text": "33.75\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.16568544507026672}, {"x": 0.8114217519760132, "y": 0.16568544507026672}, {"x": 0.8114217519760132, "y": 0.17241379618644714}, {"x": 0.7775133848190308, "y": 0.17241379618644714}], "text": "36.39\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.16568544507026672}, {"x": 0.4092801809310913, "y": 0.1661059707403183}, {"x": 0.4092801809310913, "y": 0.17283432185649872}, {"x": 0.3753718137741089, "y": 0.17241379618644714}], "text": "26.69\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.1661059707403183}, {"x": 0.6210588812828064, "y": 0.1661059707403183}, {"x": 0.6210588812828064, "y": 0.17241379618644714}, {"x": 0.5859607458114624, "y": 0.17241379618644714}], "text": "27.04\n"}
{"page": 19, "bbox": [{"x": 0.2879238426685333, "y": 0.1627417951822281}, {"x": 0.31290897727012634, "y": 0.1698906570672989}, {"x": 0.30814990401268005, "y": 0.17830109596252441}, {"x": 0.28316476941108704, "y": 0.1711522340774536}], "text": "Mgg\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.18124474585056305}, {"x": 0.8114217519760132, "y": 0.18082422018051147}, {"x": 0.8114217519760132, "y": 0.18839360773563385}, {"x": 0.7775133848190308, "y": 0.18881413340568542}], "text": "63.26\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.18166527152061462}, {"x": 0.5080309510231018, "y": 0.18124474585056305}, {"x": 0.5080309510231018, "y": 0.18839360773563385}, {"x": 0.474122554063797, "y": 0.18881413340568542}], "text": "79.90\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.18166527152061462}, {"x": 0.6198691129684448, "y": 0.18166527152061462}, {"x": 0.6198691129684448, "y": 0.18839360773563385}, {"x": 0.5859607458114624, "y": 0.18839360773563385}], "text": "56.72\n"}
{"page": 19, "bbox": [{"x": 0.688875675201416, "y": 0.18166527152061462}, {"x": 0.72218918800354, "y": 0.18124474585056305}, {"x": 0.72218918800354, "y": 0.18839360773563385}, {"x": 0.688875675201416, "y": 0.18881413340568542}], "text": "71.64\n"}
{"page": 19, "bbox": [{"x": 0.3747769296169281, "y": 0.18166527152061462}, {"x": 0.4086852967739105, "y": 0.18166527152061462}, {"x": 0.4086852967739105, "y": 0.18881413340568542}, {"x": 0.3747769296169281, "y": 0.18881413340568542}], "text": "44.78\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.18082422018051147}, {"x": 0.3051754832267761, "y": 0.18166527152061462}, {"x": 0.30458059906959534, "y": 0.19049622118473053}, {"x": 0.2843545377254486, "y": 0.18965516984462738}], "text": "Mb\n"}
{"page": 19, "bbox": [{"x": 0.3747769296169281, "y": 0.1963835209608078}, {"x": 0.4098750650882721, "y": 0.19680403172969818}, {"x": 0.4098750650882721, "y": 0.20395290851593018}, {"x": 0.3747769296169281, "y": 0.2035323828458786}], "text": "85.72\n"}
{"page": 19, "bbox": [{"x": 0.5865556001663208, "y": 0.19722455739974976}, {"x": 0.6210588812828064, "y": 0.19722455739974976}, {"x": 0.6210588812828064, "y": 0.2035323828458786}, {"x": 0.5865556001663208, "y": 0.2035323828458786}], "text": "79.82\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.19680403172969818}, {"x": 0.8114217519760132, "y": 0.19680403172969818}, {"x": 0.8114217519760132, "y": 0.20395290851593018}, {"x": 0.7769185304641724, "y": 0.20395290851593018}], "text": "84.80\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.19722455739974976}, {"x": 0.5080309510231018, "y": 0.19722455739974976}, {"x": 0.5080309510231018, "y": 0.20395290851593018}, {"x": 0.474122554063797, "y": 0.20395290851593018}], "text": "88.16\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.19722455739974976}, {"x": 0.7227840423583984, "y": 0.19722455739974976}, {"x": 0.7227840423583984, "y": 0.20395290851593018}, {"x": 0.6882807612419128, "y": 0.20395290851593018}], "text": "85.51\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.19596299529075623}, {"x": 0.3057703673839569, "y": 0.19680403172969818}, {"x": 0.30458059906959534, "y": 0.2068965584039688}, {"x": 0.2837596535682678, "y": 0.20605550706386566}], "text": "Mg\n"}
{"page": 19, "bbox": [{"x": 0.19690659642219543, "y": 0.20815812051296234}, {"x": 0.2242712676525116, "y": 0.21026071906089783}, {"x": 0.22248661518096924, "y": 0.22203531861305237}, {"x": 0.19512194395065308, "y": 0.22035323083400726}], "text": "Dg\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.2123633325099945}, {"x": 0.4086852967739105, "y": 0.2123633325099945}, {"x": 0.4086852967739105, "y": 0.21909166872501373}, {"x": 0.3753718137741089, "y": 0.21909166872501373}], "text": "60.98\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.2123633325099945}, {"x": 0.5080309510231018, "y": 0.2123633325099945}, {"x": 0.5080309510231018, "y": 0.21909166872501373}, {"x": 0.474122554063797, "y": 0.21909166872501373}], "text": "80.20\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.2123633325099945}, {"x": 0.6210588812828064, "y": 0.21194280683994293}, {"x": 0.6210588812828064, "y": 0.21909166872501373}, {"x": 0.5859607458114624, "y": 0.2195121943950653}], "text": "65.73\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.21278385818004608}, {"x": 0.8114217519760132, "y": 0.21278385818004608}, {"x": 0.8114217519760132, "y": 0.21909166872501373}, {"x": 0.7775133848190308, "y": 0.21909166872501373}], "text": "68.60\n"}
{"page": 19, "bbox": [{"x": 0.2843545377254486, "y": 0.21194280683994293}, {"x": 0.3051754832267761, "y": 0.21278385818004608}, {"x": 0.30458059906959534, "y": 0.22035323083400726}, {"x": 0.2837596535682678, "y": 0.2195121943950653}], "text": "Mr\n"}
{"page": 19, "bbox": [{"x": 0.688875675201416, "y": 0.21278385818004608}, {"x": 0.7227840423583984, "y": 0.21278385818004608}, {"x": 0.7227840423583984, "y": 0.2195121943950653}, {"x": 0.688875675201416, "y": 0.2195121943950653}], "text": "67.49\n"}
{"page": 19, "bbox": [{"x": 0.5835812091827393, "y": 0.22750210762023926}, {"x": 0.622248649597168, "y": 0.2266610562801361}, {"x": 0.622248649597168, "y": 0.2338099181652069}, {"x": 0.5835812091827393, "y": 0.23465096950531006}], "text": "81.07\n"}
{"page": 19, "bbox": [{"x": 0.3747769296169281, "y": 0.22708158195018768}, {"x": 0.4092801809310913, "y": 0.22750210762023926}, {"x": 0.4092801809310913, "y": 0.23465096950531006}, {"x": 0.3747769296169281, "y": 0.23423044383525848}], "text": "87.60\n"}
{"page": 19, "bbox": [{"x": 0.6876859068870544, "y": 0.22750210762023926}, {"x": 0.72218918800354, "y": 0.22708158195018768}, {"x": 0.72218918800354, "y": 0.23423044383525848}, {"x": 0.6876859068870544, "y": 0.23465096950531006}], "text": "87.58\n"}
{"page": 19, "bbox": [{"x": 0.7745389938354492, "y": 0.22792261838912964}, {"x": 0.8132064342498779, "y": 0.22792261838912964}, {"x": 0.8132064342498779, "y": 0.23423044383525848}, {"x": 0.7745389938354492, "y": 0.23423044383525848}], "text": "86.05\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.22750210762023926}, {"x": 0.5080309510231018, "y": 0.22750210762023926}, {"x": 0.5080309510231018, "y": 0.23465096950531006}, {"x": 0.4735276699066162, "y": 0.23465096950531006}], "text": "87.94\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.22708158195018768}, {"x": 0.311124324798584, "y": 0.2287636697292328}, {"x": 0.3099345564842224, "y": 0.23801514506340027}, {"x": 0.2837596535682678, "y": 0.23633305728435516}], "text": "Mgr\n"}
{"page": 19, "bbox": [{"x": 0.3747769296169281, "y": 0.24264086782932281}, {"x": 0.4086852967739105, "y": 0.2430613934993744}, {"x": 0.4086852967739105, "y": 0.2502102553844452}, {"x": 0.3747769296169281, "y": 0.2497897446155548}], "text": "86.72\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.2430613934993744}, {"x": 0.8126115202903748, "y": 0.2430613934993744}, {"x": 0.8126115202903748, "y": 0.2497897446155548}, {"x": 0.7769185304641724, "y": 0.2497897446155548}], "text": "84.53\n"}
{"page": 19, "bbox": [{"x": 0.47114813327789307, "y": 0.24348191916942596}, {"x": 0.510410487651825, "y": 0.24348191916942596}, {"x": 0.510410487651825, "y": 0.2497897446155548}, {"x": 0.47114813327789307, "y": 0.2497897446155548}], "text": "88.35\n"}
{"page": 19, "bbox": [{"x": 0.688875675201416, "y": 0.24348191916942596}, {"x": 0.7227840423583984, "y": 0.24348191916942596}, {"x": 0.7227840423583984, "y": 0.2497897446155548}, {"x": 0.688875675201416, "y": 0.2497897446155548}], "text": "83.44\n"}
{"page": 19, "bbox": [{"x": 0.5865556001663208, "y": 0.2430613934993744}, {"x": 0.620464026927948, "y": 0.24348191916942596}, {"x": 0.620464026927948, "y": 0.25063079595565796}, {"x": 0.5865556001663208, "y": 0.2502102553844452}], "text": "79.59\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.24264086782932281}, {"x": 0.31171920895576477, "y": 0.24390244483947754}, {"x": 0.3105294406414032, "y": 0.2535744309425354}, {"x": 0.2837596535682678, "y": 0.25231286883354187}], "text": "Mgg\n"}
{"page": 19, "bbox": [{"x": 0.5853658318519592, "y": 0.2590412199497223}, {"x": 0.6210588812828064, "y": 0.2586206793785095}, {"x": 0.6210588812828064, "y": 0.26534903049468994}, {"x": 0.5853658318519592, "y": 0.2657695412635803}], "text": "21.57\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.2590412199497223}, {"x": 0.7227840423583984, "y": 0.2586206793785095}, {"x": 0.7227840423583984, "y": 0.26534903049468994}, {"x": 0.6882807612419128, "y": 0.2657695412635803}], "text": "28.79\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.2590412199497223}, {"x": 0.5086258053779602, "y": 0.2590412199497223}, {"x": 0.5086258053779602, "y": 0.2657695412635803}, {"x": 0.474122554063797, "y": 0.2657695412635803}], "text": "50.03\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.2590412199497223}, {"x": 0.8114217519760132, "y": 0.2586206793785095}, {"x": 0.8114217519760132, "y": 0.2657695412635803}, {"x": 0.7769185304641724, "y": 0.2661900818347931}], "text": "29.22\n"}
{"page": 19, "bbox": [{"x": 0.2843545377254486, "y": 0.25777965784072876}, {"x": 0.3051754832267761, "y": 0.2586206793785095}, {"x": 0.30458059906959534, "y": 0.26703113317489624}, {"x": 0.2837596535682678, "y": 0.2661900818347931}], "text": "Mb\n"}
{"page": 19, "bbox": [{"x": 0.3759666979312897, "y": 0.25946173071861267}, {"x": 0.4092801809310913, "y": 0.25946173071861267}, {"x": 0.4092801809310913, "y": 0.2657695412635803}, {"x": 0.3759666979312897, "y": 0.2657695412635803}], "text": "16.49\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.2737594544887543}, {"x": 0.8114217519760132, "y": 0.27417999505996704}, {"x": 0.8114217519760132, "y": 0.2809083163738251}, {"x": 0.7775133848190308, "y": 0.2804878056049347}], "text": "30.72\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.27417999505996704}, {"x": 0.5074360370635986, "y": 0.2737594544887543}, {"x": 0.5074360370635986, "y": 0.2809083163738251}, {"x": 0.4735276699066162, "y": 0.28132885694503784}], "text": "46.98\n"}
{"page": 19, "bbox": [{"x": 0.5853658318519592, "y": 0.27417999505996704}, {"x": 0.620464026927948, "y": 0.2737594544887543}, {"x": 0.620464026927948, "y": 0.2809083163738251}, {"x": 0.5853658318519592, "y": 0.28132885694503784}], "text": "24.36\n"}
{"page": 19, "bbox": [{"x": 0.688875675201416, "y": 0.2746005058288574}, {"x": 0.72218918800354, "y": 0.2746005058288574}, {"x": 0.72218918800354, "y": 0.2809083163738251}, {"x": 0.688875675201416, "y": 0.2809083163738251}], "text": "29.40\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.2746005058288574}, {"x": 0.4098750650882721, "y": 0.2746005058288574}, {"x": 0.4098750650882721, "y": 0.28132885694503784}, {"x": 0.3753718137741089, "y": 0.28132885694503784}], "text": "22.15\n"}
{"page": 19, "bbox": [{"x": 0.2843545377254486, "y": 0.2733389437198639}, {"x": 0.3063652515411377, "y": 0.27417999505996704}, {"x": 0.3051754832267761, "y": 0.2838519811630249}, {"x": 0.28316476941108704, "y": 0.28301092982292175}], "text": "Mg\n"}
{"page": 19, "bbox": [{"x": 0.19690659642219543, "y": 0.28679561614990234}, {"x": 0.2236763834953308, "y": 0.2876366674900055}, {"x": 0.22308149933815002, "y": 0.29899075627326965}, {"x": 0.19631171226501465, "y": 0.2981497049331665}], "text": "Dr\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.2893187403678894}, {"x": 0.5080309510231018, "y": 0.2897392809391022}, {"x": 0.5080309510231018, "y": 0.296888142824173}, {"x": 0.474122554063797, "y": 0.2964676320552826}], "text": "58.42\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.2897392809391022}, {"x": 0.6198691129684448, "y": 0.2893187403678894}, {"x": 0.6198691129684448, "y": 0.2964676320552826}, {"x": 0.5859607458114624, "y": 0.296888142824173}], "text": "29.64\n"}
{"page": 19, "bbox": [{"x": 0.688875675201416, "y": 0.2893187403678894}, {"x": 0.7233789563179016, "y": 0.2897392809391022}, {"x": 0.7233789563179016, "y": 0.296888142824173}, {"x": 0.688875675201416, "y": 0.2964676320552826}], "text": "39.54\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.28847771883010864}, {"x": 0.3057703673839569, "y": 0.2897392809391022}, {"x": 0.30458059906959534, "y": 0.2981497049331665}, {"x": 0.2837596535682678, "y": 0.296888142824173}], "text": "Mr\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.29015979170799255}, {"x": 0.4092801809310913, "y": 0.29015979170799255}, {"x": 0.4092801809310913, "y": 0.2964676320552826}, {"x": 0.3753718137741089, "y": 0.2964676320552826}], "text": "36.92\n"}
{"page": 19, "bbox": [{"x": 0.7763236165046692, "y": 0.29015979170799255}, {"x": 0.8120166659355164, "y": 0.29015979170799255}, {"x": 0.8120166659355164, "y": 0.2964676320552826}, {"x": 0.7763236165046692, "y": 0.2964676320552826}], "text": "41.13\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.3048780560493469}, {"x": 0.7227840423583984, "y": 0.3048780560493469}, {"x": 0.7227840423583984, "y": 0.31160637736320496}, {"x": 0.6882807612419128, "y": 0.31160637736320496}], "text": "27.95\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.3052985668182373}, {"x": 0.4098750650882721, "y": 0.3052985668182373}, {"x": 0.4098750650882721, "y": 0.31160637736320496}, {"x": 0.3753718137741089, "y": 0.31160637736320496}], "text": "23.63\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.3052985668182373}, {"x": 0.620464026927948, "y": 0.3052985668182373}, {"x": 0.620464026927948, "y": 0.31160637736320496}, {"x": 0.5859607458114624, "y": 0.31160637736320496}], "text": "24.17\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.3052985668182373}, {"x": 0.5080309510231018, "y": 0.3052985668182373}, {"x": 0.5080309510231018, "y": 0.3120269179344177}, {"x": 0.4735276699066162, "y": 0.3120269179344177}], "text": "45.01\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.3052985668182373}, {"x": 0.8114217519760132, "y": 0.3052985668182373}, {"x": 0.8114217519760132, "y": 0.3120269179344177}, {"x": 0.7775133848190308, "y": 0.3120269179344177}], "text": "30.19\n"}
{"page": 19, "bbox": [{"x": 0.28613919019699097, "y": 0.3031959533691406}, {"x": 0.31171920895576477, "y": 0.30782169103622437}, {"x": 0.30874478816986084, "y": 0.3158116042613983}, {"x": 0.28316476941108704, "y": 0.3111858665943146}], "text": "Mgr\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.32043734192848206}, {"x": 0.5080309510231018, "y": 0.32043734192848206}, {"x": 0.5080309510231018, "y": 0.3271656930446625}, {"x": 0.4735276699066162, "y": 0.3271656930446625}], "text": "43.83\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.32043734192848206}, {"x": 0.620464026927948, "y": 0.32043734192848206}, {"x": 0.620464026927948, "y": 0.3271656930446625}, {"x": 0.5859607458114624, "y": 0.3271656930446625}], "text": "23.23\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.32043734192848206}, {"x": 0.72397381067276, "y": 0.32043734192848206}, {"x": 0.72397381067276, "y": 0.3271656930446625}, {"x": 0.6882807612419128, "y": 0.3271656930446625}], "text": "27.33\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.32085785269737244}, {"x": 0.4092801809310913, "y": 0.32085785269737244}, {"x": 0.4092801809310913, "y": 0.3271656930446625}, {"x": 0.3753718137741089, "y": 0.3271656930446625}], "text": "21.08\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.32085785269737244}, {"x": 0.8120166659355164, "y": 0.32085785269737244}, {"x": 0.8120166659355164, "y": 0.3271656930446625}, {"x": 0.7775133848190308, "y": 0.3271656930446625}], "text": "28.87\n"}
{"page": 19, "bbox": [{"x": 0.28732895851135254, "y": 0.317914217710495}, {"x": 0.31231409311294556, "y": 0.3250630795955658}, {"x": 0.30755501985549927, "y": 0.3334735035896301}, {"x": 0.28256988525390625, "y": 0.3263246417045593}], "text": "Mgg\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.33641716837882996}, {"x": 0.5086258053779602, "y": 0.3359966278076172}, {"x": 0.5086258053779602, "y": 0.3427249789237976}, {"x": 0.4735276699066162, "y": 0.343145489692688}], "text": "81.27\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.33641716837882996}, {"x": 0.8102319836616516, "y": 0.3355761170387268}, {"x": 0.8102319836616516, "y": 0.3427249789237976}, {"x": 0.7769185304641724, "y": 0.34356603026390076}], "text": "58.52\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.33641716837882996}, {"x": 0.4086852967739105, "y": 0.3359966278076172}, {"x": 0.4086852967739105, "y": 0.343145489692688}, {"x": 0.3753718137741089, "y": 0.34356603026390076}], "text": "34.65\n"}
{"page": 19, "bbox": [{"x": 0.688875675201416, "y": 0.33641716837882996}, {"x": 0.7233789563179016, "y": 0.33641716837882996}, {"x": 0.7233789563179016, "y": 0.343145489692688}, {"x": 0.688875675201416, "y": 0.343145489692688}], "text": "65.42\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.3359966278076172}, {"x": 0.620464026927948, "y": 0.33641716837882996}, {"x": 0.620464026927948, "y": 0.34398654103279114}, {"x": 0.5859607458114624, "y": 0.34356603026390076}], "text": "52.75\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.3355761170387268}, {"x": 0.3057703673839569, "y": 0.3372581899166107}, {"x": 0.30458059906959534, "y": 0.3448275923728943}, {"x": 0.2837596535682678, "y": 0.343145489692688}], "text": "Mb\n"}
{"page": 19, "bbox": [{"x": 0.3747769296169281, "y": 0.3515559434890747}, {"x": 0.4092801809310913, "y": 0.3515559434890747}, {"x": 0.4092801809310913, "y": 0.35828426480293274}, {"x": 0.3747769296169281, "y": 0.35828426480293274}], "text": "85.00\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.35113540291786194}, {"x": 0.7227840423583984, "y": 0.3515559434890747}, {"x": 0.7227840423583984, "y": 0.3587048053741455}, {"x": 0.6882807612419128, "y": 0.35828426480293274}], "text": "83.02\n"}
{"page": 19, "bbox": [{"x": 0.7775133848190308, "y": 0.3519764542579651}, {"x": 0.8114217519760132, "y": 0.3515559434890747}, {"x": 0.8114217519760132, "y": 0.35786375403404236}, {"x": 0.7775133848190308, "y": 0.35828426480293274}], "text": "83.38\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.3519764542579651}, {"x": 0.5086258053779602, "y": 0.3519764542579651}, {"x": 0.5086258053779602, "y": 0.3587048053741455}, {"x": 0.474122554063797, "y": 0.3587048053741455}], "text": "87.33\n"}
{"page": 19, "bbox": [{"x": 0.5865556001663208, "y": 0.3519764542579651}, {"x": 0.6198691129684448, "y": 0.3519764542579651}, {"x": 0.6198691129684448, "y": 0.3587048053741455}, {"x": 0.5865556001663208, "y": 0.3587048053741455}], "text": "78.18\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.35071489214897156}, {"x": 0.3057703673839569, "y": 0.3515559434890747}, {"x": 0.30458059906959534, "y": 0.36122792959213257}, {"x": 0.2837596535682678, "y": 0.3603868782520294}], "text": "Mg\n"}
{"page": 19, "bbox": [{"x": 0.5859607458114624, "y": 0.36711522936820984}, {"x": 0.620464026927948, "y": 0.36711522936820984}, {"x": 0.620464026927948, "y": 0.37384358048439026}, {"x": 0.5859607458114624, "y": 0.37384358048439026}], "text": "63.82\n"}
{"page": 19, "bbox": [{"x": 0.19274242222309113, "y": 0.36333054304122925}, {"x": 0.2278405725955963, "y": 0.36501261591911316}, {"x": 0.22665080428123474, "y": 0.37762826681137085}, {"x": 0.19155265390872955, "y": 0.3763667047023773}], "text": "Dgr\n"}
{"page": 19, "bbox": [{"x": 0.3753718137741089, "y": 0.3675357401371002}, {"x": 0.4098750650882721, "y": 0.3675357401371002}, {"x": 0.4098750650882721, "y": 0.37384358048439026}, {"x": 0.3753718137741089, "y": 0.37384358048439026}], "text": "60.28\n"}
{"page": 19, "bbox": [{"x": 0.474122554063797, "y": 0.3675357401371002}, {"x": 0.5086258053779602, "y": 0.3675357401371002}, {"x": 0.5086258053779602, "y": 0.37384358048439026}, {"x": 0.474122554063797, "y": 0.37384358048439026}], "text": "79.32\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.36711522936820984}, {"x": 0.7227840423583984, "y": 0.36711522936820984}, {"x": 0.7227840423583984, "y": 0.37426409125328064}, {"x": 0.6882807612419128, "y": 0.37426409125328064}], "text": "67.29\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.3675357401371002}, {"x": 0.8114217519760132, "y": 0.36711522936820984}, {"x": 0.8114217519760132, "y": 0.37384358048439026}, {"x": 0.7769185304641724, "y": 0.37426409125328064}], "text": "67.68\n"}
{"page": 19, "bbox": [{"x": 0.2843545377254486, "y": 0.36669468879699707}, {"x": 0.3051754832267761, "y": 0.3675357401371002}, {"x": 0.30458059906959534, "y": 0.3751051425933838}, {"x": 0.2837596535682678, "y": 0.37426409125328064}], "text": "Mr\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.3822540044784546}, {"x": 0.8114217519760132, "y": 0.38141295313835144}, {"x": 0.8120166659355164, "y": 0.38856181502342224}, {"x": 0.7775133848190308, "y": 0.3894028663635254}], "text": "85.63\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.3822540044784546}, {"x": 0.5080309510231018, "y": 0.3818334639072418}, {"x": 0.5080309510231018, "y": 0.3889823257923126}, {"x": 0.4735276699066162, "y": 0.3894028663635254}], "text": "87.14\n"}
{"page": 19, "bbox": [{"x": 0.5865556001663208, "y": 0.3822540044784546}, {"x": 0.6198691129684448, "y": 0.3822540044784546}, {"x": 0.6198691129684448, "y": 0.3889823257923126}, {"x": 0.5865556001663208, "y": 0.3889823257923126}], "text": "79.95\n"}
{"page": 19, "bbox": [{"x": 0.6853063702583313, "y": 0.3822540044784546}, {"x": 0.7251635789871216, "y": 0.3822540044784546}, {"x": 0.7251635789871216, "y": 0.3889823257923126}, {"x": 0.6853063702583313, "y": 0.3889823257923126}], "text": "87.78\n"}
{"page": 19, "bbox": [{"x": 0.37239739298820496, "y": 0.38267451524734497}, {"x": 0.41225460171699524, "y": 0.38267451524734497}, {"x": 0.41225460171699524, "y": 0.3889823257923126}, {"x": 0.37239739298820496, "y": 0.3889823257923126}], "text": "87.63\n"}
{"page": 19, "bbox": [{"x": 0.2855443060398102, "y": 0.38141295313835144}, {"x": 0.3105294406414032, "y": 0.38309502601623535}, {"x": 0.3093396723270416, "y": 0.39234650135040283}, {"x": 0.2843545377254486, "y": 0.3906644284725189}], "text": "Mgr\n"}
{"page": 19, "bbox": [{"x": 0.4735276699066162, "y": 0.3978132903575897}, {"x": 0.5086258053779602, "y": 0.3978132903575897}, {"x": 0.5086258053779602, "y": 0.40454164147377014}, {"x": 0.4735276699066162, "y": 0.40454164147377014}], "text": "86.90\n"}
{"page": 19, "bbox": [{"x": 0.6882807612419128, "y": 0.3982338011264801}, {"x": 0.7227840423583984, "y": 0.3982338011264801}, {"x": 0.7227840423583984, "y": 0.40454164147377014}, {"x": 0.6882807612419128, "y": 0.40454164147377014}], "text": "83.85\n"}
{"page": 19, "bbox": [{"x": 0.7769185304641724, "y": 0.3982338011264801}, {"x": 0.8108268976211548, "y": 0.3982338011264801}, {"x": 0.8108268976211548, "y": 0.40454164147377014}, {"x": 0.7769185304641724, "y": 0.40454164147377014}], "text": "83.79\n"}
{"page": 19, "bbox": [{"x": 0.3747769296169281, "y": 0.3982338011264801}, {"x": 0.4092801809310913, "y": 0.3982338011264801}, {"x": 0.4092801809310913, "y": 0.4049621522426605}, {"x": 0.3747769296169281, "y": 0.4049621522426605}], "text": "86.31\n"}
{"page": 19, "bbox": [{"x": 0.5865556001663208, "y": 0.3982338011264801}, {"x": 0.620464026927948, "y": 0.3982338011264801}, {"x": 0.620464026927948, "y": 0.4049621522426605}, {"x": 0.5865556001663208, "y": 0.4049621522426605}], "text": "78.10\n"}
{"page": 19, "bbox": [{"x": 0.2849494218826294, "y": 0.3969722390174866}, {"x": 0.31171920895576477, "y": 0.3978132903575897}, {"x": 0.311124324798584, "y": 0.40832632780075073}, {"x": 0.2843545377254486, "y": 0.4074852764606476}], "text": "Mgg\n"}
{"page": 19, "bbox": [{"x": 0.19928613305091858, "y": 0.4234651029109955}, {"x": 0.800713837146759, "y": 0.4234651029109955}, {"x": 0.800713837146759, "y": 0.4339781403541565}, {"x": 0.19928613305091858, "y": 0.4339781403541565}], "text": "Table 12: Results of the model augmented with different contexts on various QA datasets.\n"}
{"page": 19, "bbox": [{"x": 0.18024985492229462, "y": 0.4554247260093689}, {"x": 0.2623438537120819, "y": 0.4554247260093689}, {"x": 0.2623438537120819, "y": 0.48275861144065857}, {"x": 0.18024985492229462, "y": 0.48275861144065857}], "text": "[Instruction]\n[Context]\n"}
{"page": 19, "bbox": [{"x": 0.29506245255470276, "y": 0.4503784775733948}, {"x": 0.8221296668052673, "y": 0.4533221125602722}, {"x": 0.8209398984909058, "y": 0.5849453210830688}, {"x": 0.2938726842403412, "y": 0.5820016860961914}], "text": "Please generate ten descriptions for the continuation task.\nFor example:\n1.\"French. Washington played a crucial role in the American Revolutionary War, leading\nthe Continental Army against the British.\" Please continue writing the above paragraph.\n2.\"The discovery of the double helix structure of DNA by James Watson and Francis\nCrick revolutionized the field of genetics, laying the foundation for modern molecular\nbiology and biotechnology.\" Please continue by discussing recent developments in\ngenetic research, such as CRISPR gene editing, and their potential ethical implications.\nTable 14: Template for generating task classification data.\n"}
{"page": 19, "bbox": [{"x": 0.11778703331947327, "y": 0.6118587255477905}, {"x": 0.48958954215049744, "y": 0.6122792363166809}, {"x": 0.48899465799331665, "y": 0.9205214381217957}, {"x": 0.11719214916229248, "y": 0.9201009273529053}], "text": "ing: Our evaluation encompassed the FEVER\n(Thorne et al., 2018) and PubHealth (Zhang et al.,\n2023b) datasets. (3) Open-Domain QA: We as-\nsessed on NQ (Kwiatkowski et al., 2019), Trivi-\naQA (Joshi et al., 2017), and WebQuestions (Be-\nrant et al., 2013) datasets. (4) MultiHop QA: Our\nevaluation included the HotPotQA (Yang et al.,\n2018), 2WikiMultiHopQA (Ho et al., 2020), and\nMuSiQue (Trivedi et al., 2022) datasets. For\nMusique, we followed the approach outlined in\n(Press et al., 2022) and focused solely on answer-\nable 2-hop questions. (5) Medical QA: We also as-\nsessed on the PubMedQA (Jin et al., 2019) dataset.\nIn each dataset, we randomly sub-sample 500 en-\ntries from the test set for our experiments. For\ndatasets without test set, we use develop set in-\nstead.\nTo assess RAG capabilities, we evenly collect a\ntotal of 500 entries from NQ, TriviaQA, HotPotQA,\n"}
{"page": 19, "bbox": [{"x": 0.5133848786354065, "y": 0.6122792363166809}, {"x": 0.8834027647972107, "y": 0.6122792363166809}, {"x": 0.8828078508377075, "y": 0.920941948890686}, {"x": 0.5127900242805481, "y": 0.920941948890686}], "text": "2WikiMultiHopQA and MuSiQue. Each entry is a\n\"question, gold document, gold answer\" triple.\nMetrics We use token-level F1 score and EM\nscore for Open-Domain QA and MultiHop QA\ntasks, and accuracy for others. We use a more\nlenient EM score, which evaluates performance\nbased on whether the model generations include\ngold answers instead of strictly exact matching\n(Asai et al., 2023).\nTowards RAG capabilities evaluation, we adopt\nfour metrics from RAGAs, including Faithfulness,\nContext Relevancy, Answer Relevancy, and An-\nswer Correctness. Faithfulness measures how fac-\ntually consistent the generated answer is with the\nretrieved context. An answer is considered faithful\nif all claims made can be directly inferred from\nthe provided context. Context Relevancy evaluates\nhow relevant the retrieved context is to the origi-\nnal query. Answer Relevancy assesses the perti-\n"}
{"page": 19, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.939444899559021}, {"x": 0.48007139563560486, "y": 0.939444899559021}], "text": "17734\n"}
{"page": 20, "bbox": [{"x": 0.4193932116031647, "y": 0.08830950409173965}, {"x": 0.5728732943534851, "y": 0.08620689809322357}, {"x": 0.5734681487083435, "y": 0.09756097197532654}, {"x": 0.41998809576034546, "y": 0.09966358542442322}], "text": "Text2image Retrieval\n"}
{"page": 20, "bbox": [{"x": 0.3313503861427307, "y": 0.11984860897064209}, {"x": 0.3842950761318207, "y": 0.11984860897064209}, {"x": 0.3842950761318207, "y": 0.1265769600868225}, {"x": 0.3313503861427307, "y": 0.1265769600868225}], "text": "Retrieval\n"}
{"page": 20, "bbox": [{"x": 0.19631171226501465, "y": 0.1408746838569641}, {"x": 0.30041640996932983, "y": 0.14213624596595764}, {"x": 0.30041640996932983, "y": 0.15096719563007355}, {"x": 0.19631171226501465, "y": 0.14970563352108002}], "text": "A dog is sleeping\n"}
{"page": 20, "bbox": [{"x": 0.3313503861427307, "y": 0.1677880585193634}, {"x": 0.3837001919746399, "y": 0.16736753284931183}, {"x": 0.3837001919746399, "y": 0.1749369204044342}, {"x": 0.3313503861427307, "y": 0.17535744607448578}], "text": "Retrieval\n"}
{"page": 20, "bbox": [{"x": 0.19690659642219543, "y": 0.19133725762367249}, {"x": 0.30041640996932983, "y": 0.19175778329372406}, {"x": 0.30041640996932983, "y": 0.2001682072877884}, {"x": 0.19690659642219543, "y": 0.19974768161773682}], "text": "A dog is sleeping\n"}
{"page": 20, "bbox": [{"x": 0.4193932116031647, "y": 0.23717409372329712}, {"x": 0.5698988437652588, "y": 0.23507149517536163}, {"x": 0.5698988437652588, "y": 0.24516400694847107}, {"x": 0.41998809576034546, "y": 0.24726660549640656}], "text": "Image2text Retrieval\n"}
{"page": 20, "bbox": [{"x": 0.33491969108581543, "y": 0.2590412199497223}, {"x": 0.3884592652320862, "y": 0.25946173071861267}, {"x": 0.3884592652320862, "y": 0.26703113317489624}, {"x": 0.33491969108581543, "y": 0.26661059260368347}], "text": "Retrieval\n"}
{"page": 20, "bbox": [{"x": 0.49375373125076294, "y": 0.2821698784828186}, {"x": 0.4943486154079437, "y": 0.25063079595565796}, {"x": 0.5252825617790222, "y": 0.25105130672454834}, {"x": 0.5246877074241638, "y": 0.28259041905403137}], "text": "W\n"}
{"page": 20, "bbox": [{"x": 0.562165379524231, "y": 0.2817493677139282}, {"x": 0.6686496138572693, "y": 0.28259041905403137}, {"x": 0.6686496138572693, "y": 0.29184189438819885}, {"x": 0.562165379524231, "y": 0.2910008430480957}], "text": "A dog is sleeping.\n"}
{"page": 20, "bbox": [{"x": 0.3616894781589508, "y": 0.32085785269737244}, {"x": 0.5044616460800171, "y": 0.32085785269737244}, {"x": 0.5044616460800171, "y": 0.32926830649375916}, {"x": 0.3616894781589508, "y": 0.32926830649375916}], "text": "A dog is drinking water\n"}
{"page": 20, "bbox": [{"x": 0.33491969108581543, "y": 0.349453330039978}, {"x": 0.3878643810749054, "y": 0.34903278946876526}, {"x": 0.3878643810749054, "y": 0.3557611405849457}, {"x": 0.33491969108581543, "y": 0.35618165135383606}], "text": "Retrieval\n"}
{"page": 20, "bbox": [{"x": 0.6543723940849304, "y": 0.3486122786998749}, {"x": 0.8120166659355164, "y": 0.34903278946876526}, {"x": 0.8114217519760132, "y": 0.4322960376739502}, {"x": 0.653777539730072, "y": 0.4318755269050598}], "text": "User Query\nLow Similarity\nHigh Similarity\nImage Generation Model\nImage Caption Model\n"}
{"page": 20, "bbox": [{"x": 0.38072577118873596, "y": 0.41337257623672485}, {"x": 0.48483046889305115, "y": 0.4137931168079376}, {"x": 0.48483046889305115, "y": 0.42262405157089233}, {"x": 0.38072577118873596, "y": 0.42220354080200195}], "text": "A dog is sleeping\n"}
{"page": 20, "bbox": [{"x": 0.11838191747665405, "y": 0.4785534143447876}, {"x": 0.8834027647972107, "y": 0.4785534143447876}, {"x": 0.8834027647972107, "y": 0.560555100440979}, {"x": 0.11838191747665405, "y": 0.560555100440979}], "text": "Figure 4: Workflow of multimodal retrieval. The upper section illustrates the text-to-image retrieval process.\nInitially, a text query is used to find images in the database with the highest similarity. If a high similarity is found,\nthe image is returned directly. If not, an image generation model is employed to create and return an appropriate\nimage. The lower section demonstrates the image-to-text retrieval process. Here, a user-provided image is matched\nwith images in the database to find the highest similarity. If a high similarity is identified, the pre-stored caption of\nthe matching image is returned. Otherwise, an image captioning model generates and returns a new caption.\n"}
{"page": 20, "bbox": [{"x": 0.11778703331947327, "y": 0.5895710587501526}, {"x": 0.48899465799331665, "y": 0.589991569519043}, {"x": 0.48899465799331665, "y": 0.6980655789375305}, {"x": 0.11778703331947327, "y": 0.6976450681686401}], "text": "nence of the generated answer to the original query.\nAnswer Correctness involves the accuracy of the\ngenerated answer when compared to the ground\ntruth. For example, Context Relevancy is calcu-\nlated from the proportion of sentences within the\nretrieved context that are relevant for answering the\ngiven question to all sentences:\n"}
{"page": 20, "bbox": [{"x": 0.4646044075489044, "y": 0.7169890403747559}, {"x": 0.4866151213645935, "y": 0.7169890403747559}, {"x": 0.4866151213645935, "y": 0.7270815968513489}, {"x": 0.4646044075489044, "y": 0.7270815968513489}], "text": "(2)\n"}
{"page": 20, "bbox": [{"x": 0.18857823312282562, "y": 0.7161480188369751}, {"x": 0.344437837600708, "y": 0.717409610748291}, {"x": 0.344437837600708, "y": 0.7279226183891296}, {"x": 0.18857823312282562, "y": 0.7266610860824585}], "text": "context relevancy =\n"}
{"page": 20, "bbox": [{"x": 0.36049970984458923, "y": 0.707317054271698}, {"x": 0.412849485874176, "y": 0.7077375650405884}, {"x": 0.412849485874176, "y": 0.7371740937232971}, {"x": 0.36049970984458923, "y": 0.7367535829544067}], "text": "|S|\n|Total|\n"}
{"page": 20, "bbox": [{"x": 0.5133848786354065, "y": 0.5874684453010559}, {"x": 0.8851873874664307, "y": 0.5878889560699463}, {"x": 0.8845925331115723, "y": 0.8599663376808167}, {"x": 0.5127900242805481, "y": 0.8595458269119263}], "text": "Implementation Details For Open-Domain QA\nand MultiHop QA datasets, we set the generation\nmodel's maximum new token number to 100 to-\nkens. For other datasets, we set it to 50 tokens.\nTo deal with excessively long retrieved documents,\nwe truncated the documents to 2048 words when\nevaluating RankLLaMA and LongLLMLingua.\nFor all datasets, we use greedy decoding dur-\ning generation. To better compare the capabilities\nof different RAG modules, we adopt the 0-shot\nevaluation setting, i.e., no in-context examples are\noffered. In the multiple choice and fact checking\ntasks, answers generated by the model may take a\nvariety of forms (e.g., \"the answer is A\" instead of\n\"A\"). Therefore, we preprocess the responses gen-\nerated by the model, applying regular expression\ntemplates to match them with gold labels.\n"}
{"page": 20, "bbox": [{"x": 0.11719214916229248, "y": 0.7464255690574646}, {"x": 0.4901844263076782, "y": 0.746846079826355}, {"x": 0.48958954215049744, "y": 0.9213625192642212}, {"x": 0.1165972650051117, "y": 0.920941948890686}], "text": "where S denotes the number of relevant sentences,\n|Total denotes the total number of sentences re-\ntrieved. All these metrics are evaluated using the\nRAGAS framework, with GPT-4 serving as the\njudge.\nAdditionally, we compute the cosine similarity\nbetween the retrieved document and the gold docu-\nment as Retrieval Similarity. The retrieved docu-\nment and gold document are fed into an embedding\nmodel, then the resulting embeddings are used to\ncompute the cosine similarity.\n"}
{"page": 20, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9314550161361694}, {"x": 0.5240927934646606, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17735\n"}
{"page": 21, "bbox": [{"x": 0.31766805052757263, "y": 0.13877207040786743}, {"x": 0.36823320388793945, "y": 0.13877207040786743}, {"x": 0.36823320388793945, "y": 0.14550042152404785}, {"x": 0.31766805052757263, "y": 0.14550042152404785}], "text": "Method\n"}
{"page": 21, "bbox": [{"x": 0.49910768866539, "y": 0.13835155963897705}, {"x": 0.680547297000885, "y": 0.13835155963897705}, {"x": 0.680547297000885, "y": 0.14760303497314453}, {"x": 0.49910768866539, "y": 0.14760303497314453}], "text": "CLIP Similarity LATENCY\n"}
{"page": 21, "bbox": [{"x": 0.3182629346847534, "y": 0.15096719563007355}, {"x": 0.3842950761318207, "y": 0.15054668486118317}, {"x": 0.3842950761318207, "y": 0.15769554674625397}, {"x": 0.3182629346847534, "y": 0.15811605751514435}], "text": "PRO2GEN\n"}
{"page": 21, "bbox": [{"x": 0.5330160856246948, "y": 0.15138772130012512}, {"x": 0.5669244527816772, "y": 0.15138772130012512}, {"x": 0.5669244527816772, "y": 0.15769554674625397}, {"x": 0.5330160856246948, "y": 0.15769554674625397}], "text": "0.266\n"}
{"page": 21, "bbox": [{"x": 0.6264128684997559, "y": 0.15054668486118317}, {"x": 0.6632956862449646, "y": 0.15054668486118317}, {"x": 0.6632956862449646, "y": 0.1698906570672989}, {"x": 0.6264128684997559, "y": 0.1698906570672989}], "text": "6.64S\n0.08S\n"}
{"page": 21, "bbox": [{"x": 0.3182629346847534, "y": 0.1627417951822281}, {"x": 0.38191553950309753, "y": 0.1623212844133377}, {"x": 0.38191553950309753, "y": 0.1698906570672989}, {"x": 0.3182629346847534, "y": 0.17031118273735046}], "text": "PRO2RET\n"}
{"page": 21, "bbox": [{"x": 0.5324211716651917, "y": 0.16316232085227966}, {"x": 0.5681142210960388, "y": 0.16316232085227966}, {"x": 0.5681142210960388, "y": 0.1694701462984085}, {"x": 0.5324211716651917, "y": 0.1694701462984085}], "text": "0.246\n"}
{"page": 21, "bbox": [{"x": 0.3182629346847534, "y": 0.17409588396549225}, {"x": 0.4770969748497009, "y": 0.1749369204044342}, {"x": 0.4770969748497009, "y": 0.1837678700685501}, {"x": 0.3182629346847534, "y": 0.18292683362960815}], "text": "PRO2RET(Need retrieval)\n"}
{"page": 21, "bbox": [{"x": 0.5324211716651917, "y": 0.1749369204044342}, {"x": 0.5681142210960388, "y": 0.1749369204044342}, {"x": 0.5681142210960388, "y": 0.19343987107276917}, {"x": 0.5324211716651917, "y": 0.19343987107276917}], "text": "0.258\n0.227\n"}
{"page": 21, "bbox": [{"x": 0.3182629346847534, "y": 0.18544995784759521}, {"x": 0.49256396293640137, "y": 0.18671151995658875}, {"x": 0.49256396293640137, "y": 0.19596299529075623}, {"x": 0.3182629346847534, "y": 0.1947014331817627}], "text": "PRO2RET(Need generation)\n"}
{"page": 21, "bbox": [{"x": 0.11838191747665405, "y": 0.21110177040100098}, {"x": 0.881618082523346, "y": 0.21110177040100098}, {"x": 0.881618082523346, "y": 0.32085785269737244}, {"x": 0.11838191747665405, "y": 0.32085785269737244}], "text": "Table 15: The results of text-to-image retrieval: PRO2GEN and PRO2RET represent using generation and retrieval\nmethods to return images. PRO2RET(Need retrieval) and PRO2RET(Need generation) refer to using prompts\nannotated as \"Need retrieval\" and \"Need generation\" for the retrieval process. \"Need retrieval\" represents there are\nexact pictures in retrieval sources well matching this prompt. \"Need generation\" represents there are no pictures\nin retrieval sources matching well this prompt. The retrieval time is significantly shorter than the generation time\nand the quality of retrieval is comparable to generation. The result of PRO2RET(Need retrieval) is better than\nPRO2RET(Need generation) demonstrating that expanding the size of the retrieval sources can improve outcomes\neffectively.\n"}
{"page": 21, "bbox": [{"x": 0.21058893203735352, "y": 0.453742653131485}, {"x": 0.28197500109672546, "y": 0.4550042152404785}, {"x": 0.2813801169395447, "y": 0.4680403769016266}, {"x": 0.20999404788017273, "y": 0.46677881479263306}], "text": "Prompt\n"}
{"page": 21, "bbox": [{"x": 0.3842950761318207, "y": 0.4520605504512787}, {"x": 0.8221296668052673, "y": 0.45416316390037537}, {"x": 0.8221296668052673, "y": 0.47056350111961365}, {"x": 0.3842950761318207, "y": 0.46846088767051697}], "text": "Result of retrieval Result of generation\n"}
{"page": 21, "bbox": [{"x": 0.18381915986537933, "y": 0.5584524869918823}, {"x": 0.34741225838661194, "y": 0.5592935085296631}, {"x": 0.34741225838661194, "y": 0.5731707215309143}, {"x": 0.18381915986537933, "y": 0.5723296999931335}], "text": "A tyrannosaurus\n"}
{"page": 21, "bbox": [{"x": 0.2028554379940033, "y": 0.7161480188369751}, {"x": 0.28732895851135254, "y": 0.7169890403747559}, {"x": 0.28732895851135254, "y": 0.7304457426071167}, {"x": 0.2028554379940033, "y": 0.7296047210693359}], "text": "A family\n"}
{"page": 21, "bbox": [{"x": 0.11838191747665405, "y": 0.8238015174865723}, {"x": 0.8822129964828491, "y": 0.8238015174865723}, {"x": 0.8822129964828491, "y": 0.8633305430412292}, {"x": 0.11838191747665405, "y": 0.8633305430412292}], "text": "Figure 5: Some cases of retrieval and generation methods: the generation model is less controllable, occasionally\nproducing errors or low-quality outputs. On the contrary, since the retrieval sources information from authoritative\nreferences, it consistently delivers high-quality results.\n"}
{"page": 21, "bbox": [{"x": 0.48007139563560486, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9314550161361694}, {"x": 0.5246877074241638, "y": 0.9398654103279114}, {"x": 0.48007139563560486, "y": 0.9398654103279114}], "text": "17736\n"}
{"page": 1, "bbox": [{"x": 0.19036288559436798, "y": 0.09503784775733948}, {"x": 0.8102319836616516, "y": 0.09503784775733948}, {"x": 0.8102319836616516, "y": 0.107232965528965}, {"x": 0.19036288559436798, "y": 0.107232965528965}], "text": "Precise Zero-Shot Dense Retrieval without Relevance Labels\n"}
{"page": 1, "bbox": [{"x": 0.23140987753868103, "y": 0.1408746838569641}, {"x": 0.33491969108581543, "y": 0.1375105082988739}, {"x": 0.3355145752429962, "y": 0.15054668486118317}, {"x": 0.23200476169586182, "y": 0.15391084551811218}], "text": "Luyu Gao* †\n"}
{"page": 1, "bbox": [{"x": 0.36049970984458923, "y": 0.1408746838569641}, {"x": 0.49791792035102844, "y": 0.13793103396892548}, {"x": 0.49851280450820923, "y": 0.15138772130012512}, {"x": 0.36109459400177, "y": 0.15433137118816376}], "text": "Xueguang Ma*‡\n"}
{"page": 1, "bbox": [{"x": 0.20939916372299194, "y": 0.13708999752998352}, {"x": 0.7941701412200928, "y": 0.13414634764194489}, {"x": 0.7947649955749512, "y": 0.20269133150577545}, {"x": 0.20999404788017273, "y": 0.2056349813938141}], "text": "Jimmy Lin Jamie Callant\n*Language Technologies Institute, Carnegie Mellon University\n#David R. Cheriton School of Computer Science, University of Waterloo\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin} @uwaterloo.ca\n"}
{"page": 1, "bbox": [{"x": 0.26472339034080505, "y": 0.2556770443916321}, {"x": 0.3396787643432617, "y": 0.2556770443916321}, {"x": 0.3396787643432617, "y": 0.26534903049468994}, {"x": 0.26472339034080505, "y": 0.26534903049468994}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.1469363421201706, "y": 0.2838519811630249}, {"x": 0.4610351026058197, "y": 0.2842724919319153}, {"x": 0.4604402184486389, "y": 0.6934398412704468}, {"x": 0.1463414579629898, "y": 0.6930193305015564}], "text": "While dense retrieval has been shown effec-\ntive and efficient across tasks and languages,\nit remains difficult to create effective fully\nzero-shot dense retrieval systems when no rel-\nevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning\nand encoding relevance. Instead, we pro-\npose to pivot through Hypothetical Document\nEmbeddings (HyDE). Given a query, HYDE first\nzero-shot instructs an instruction-following\nlanguage model (e.g. InstructGPT) to gen-\nerate a hypothetical document. The docu-\nment captures relevance patterns but is unreal\nand may contain false details. Then, an un-\nsupervised contrastively learned encoder (e.g.\nContriever) encodes the document into an\nembedding vector. This vector identifies a\nneighborhood in the corpus embedding space,\nwhere similar real documents are retrieved\nbased on vector similarity. This second step\nground the generated document to the actual\ncorpus, with the encoder's dense bottleneck\nfiltering out the incorrect details. Our exper-\niments show that HyDE significantly outper-\nforms the state-of-the-art unsupervised dense\nretriever Contriever and shows strong per-\nformance comparable to fine-tuned retrievers,\nacross various tasks (e.g. web search, QA, fact\nverification) and languages (e.g. sw, ko, ja).¹\n"}
{"page": 1, "bbox": [{"x": 0.029149316251277924, "y": 0.7232968807220459}, {"x": 0.02974420040845871, "y": 0.31665265560150146}, {"x": 0.06067816913127899, "y": 0.31665265560150146}, {"x": 0.060083284974098206, "y": 0.7232968807220459}], "text": "arXiv:2212.10496v1 [cs.IR] 20 Dec 2022\n"}
{"page": 1, "bbox": [{"x": 0.5127900242805481, "y": 0.25483599305152893}, {"x": 0.8845925331115723, "y": 0.25483599305152893}, {"x": 0.8845925331115723, "y": 0.918418824672699}, {"x": 0.5127900242805481, "y": 0.918418824672699}], "text": "pre-training (Izacard et al., 2021; Gao and Callan,\n2021; Lu et al., 2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\nOn the other hand, zero-shot dense retrieval still\nremains difficult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MS-\nMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged query-\ndocument pairs, is arguably the most commonly\nused. As argued by Izacard et al. (2021), in prac-\ntice, however, the existence of such a large dataset\ncannot always be assumed. Even MS-MARCO re-\nstricts commercial use and cannot be adopted in a\nvariety of real-world search scenarios.\nIn this paper, we aim to build effective fully\nzero-shot dense retrieval systems that require no\nrelevance supervision, work out-of-box and gener-\nalize across tasks. As supervision is not available,\nwe start by examining self-supervised representa-\ntion learning methods. Modern deep learning en-\nables two distinct learning algorithms. At the token\nlevel, generative large language models (LLM) pre-\ntrained on large corpus have demonstrated strong\nnatural language understanding (NLU) and gen-\neration (NLG) capabilities (Brown et al., 2020;\nChen et al., 2021; Rae et al., 2021; Hoffmann\net al., 2022; Thoppilan et al., 2022; Chowdhery\net al., 2022). At the document level, text (chunk)\nencoders pre-trained with contrastive objectives\nlearn to encode document-document similarity into\ninner-product (Izacard et al., 2021; Gao and Callan,\n2022). On top of these, one extra insight into LLM\nis borrowed: the LLMs further trained to follow\ninstructions can zero-shot generalize to diverse un-\nseen instructions (Ouyang et al., 2022; Sanh et al.,\n2022; Min et al., 2022; Wei et al., 2022). Ouyang\net al. (2022) show that with a small amount of data,\nGPT-3 (Brown et al., 2020) models can be aligned\n"}
{"page": 1, "bbox": [{"x": 0.11897680163383484, "y": 0.7094196677207947}, {"x": 0.2575847804546356, "y": 0.7094196677207947}, {"x": 0.2575847804546356, "y": 0.7190916538238525}, {"x": 0.11897680163383484, "y": 0.7190916538238525}], "text": "1 Introduction\n"}
{"page": 1, "bbox": [{"x": 0.11838191747665405, "y": 0.7342304587364197}, {"x": 0.4878048896789551, "y": 0.7342304587364197}, {"x": 0.4878048896789551, "y": 0.8591253161430359}, {"x": 0.11838191747665405, "y": 0.8591253161430359}], "text": "Dense retrieval (Lee et al., 2019; Karpukhin et al.,\n2020), the method of retrieving documents using\nsemantic embedding similarities, has been shown\nsuccessful across tasks like web search, question\nanswering, and fact verification. A variety of meth-\nods such as negative mining (Xiong et al., 2021; Qu\net al., 2021), distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofstätter et al., 2021) and task-specific\n"}
{"page": 1, "bbox": [{"x": 0.15704937279224396, "y": 0.8717409372329712}, {"x": 0.2718620002269745, "y": 0.8708999156951904}, {"x": 0.2718620002269745, "y": 0.8801513910293579}, {"x": 0.15704937279224396, "y": 0.8809924125671387}], "text": "Equal contribution.\n"}
{"page": 1, "bbox": [{"x": 0.46341463923454285, "y": 0.8877207636833191}, {"x": 0.48839977383613586, "y": 0.8877207636833191}, {"x": 0.48839977383613586, "y": 0.8948696255683899}, {"x": 0.46341463923454285, "y": 0.8948696255683899}], "text": "pre-\n"}
{"page": 1, "bbox": [{"x": 0.11838191747665405, "y": 0.882674515247345}, {"x": 0.48839977383613586, "y": 0.8835155367851257}, {"x": 0.48839977383613586, "y": 0.9175778031349182}, {"x": 0.11838191747665405, "y": 0.9167367815971375}], "text": "'No models were trained or fine-tuned in making this\nprint. Our open source code is available at https://github.\ncom/texttron/hyde.\n"}
{"page": 2, "bbox": [{"x": 0.12195122241973877, "y": 0.08915054798126221}, {"x": 0.2754313051700592, "y": 0.08915054798126221}, {"x": 0.2754313051700592, "y": 0.11396130919456482}, {"x": 0.12195122241973877, "y": 0.11396130919456482}], "text": "write a passage to answer the question\nhow long does it take to remove\nwisdom tooth\n"}
{"page": 2, "bbox": [{"x": 0.35455086827278137, "y": 0.10092514753341675}, {"x": 0.40809041261672974, "y": 0.10134566575288773}, {"x": 0.40809041261672974, "y": 0.11354079097509384}, {"x": 0.35455086827278137, "y": 0.11312027275562286}], "text": "HYDE\n"}
{"page": 2, "bbox": [{"x": 0.7424152493476868, "y": 0.09377628564834595}, {"x": 0.8744794726371765, "y": 0.09419680386781693}, {"x": 0.8744794726371765, "y": 0.1232127845287323}, {"x": 0.7424152493476868, "y": 0.12279225885868073}], "text": "How wisdom teeth are removed...\nSome ... a few minutes, whereas\nothers can take 20 minutes or\nlonger....\n"}
{"page": 2, "bbox": [{"x": 0.4687685966491699, "y": 0.10807400941848755}, {"x": 0.5746579170227051, "y": 0.10765349119901657}, {"x": 0.5746579170227051, "y": 0.12825904786586761}, {"x": 0.4687685966491699, "y": 0.128679558634758}], "text": "It usually takes between 30\nminutes and two hours to\nremove a wisdom tooth...\n"}
{"page": 2, "bbox": [{"x": 0.12195122241973877, "y": 0.12952060997486115}, {"x": 0.3016061782836914, "y": 0.1303616464138031}, {"x": 0.3016061782836914, "y": 0.15264928340911865}, {"x": 0.12195122241973877, "y": 0.1518082469701767}], "text": "write a scientific paper passage to answer\nthe question\nHow has the COVID-19 pandemic impacted\n"}
{"page": 2, "bbox": [{"x": 0.4729327857494354, "y": 0.14045415818691254}, {"x": 0.5841760635375977, "y": 0.1408746838569641}, {"x": 0.5841760635375977, "y": 0.14676198363304138}, {"x": 0.4729327857494354, "y": 0.1463414579629898}], "text": "..depression and anxiety had\n"}
{"page": 2, "bbox": [{"x": 0.6180844902992249, "y": 0.1480235457420349}, {"x": 0.6787626147270203, "y": 0.1480235457420349}, {"x": 0.6787626147270203, "y": 0.15475189685821533}, {"x": 0.6180844902992249, "y": 0.15475189685821533}], "text": "Contriever\n"}
{"page": 2, "bbox": [{"x": 0.7412254810333252, "y": 0.1408746838569641}, {"x": 0.8738846182823181, "y": 0.14171572029590607}, {"x": 0.8738846182823181, "y": 0.16358284652233124}, {"x": 0.7412254810333252, "y": 0.1627417951822281}], "text": "... two studies investigating\nCOVID-19 patients ... significantly\nhigher level of depressive ...\n"}
{"page": 2, "bbox": [{"x": 0.4693634808063507, "y": 0.1480235457420349}, {"x": 0.5770374536514282, "y": 0.1480235457420349}, {"x": 0.5770374536514282, "y": 0.16105970740318298}, {"x": 0.4693634808063507, "y": 0.16105970740318298}], "text": "increased by 20% since the\nstart of the pandemic...\n"}
{"page": 2, "bbox": [{"x": 0.12254610657691956, "y": 0.15433137118816376}, {"x": 0.18381915986537933, "y": 0.15433137118816376}, {"x": 0.18381915986537933, "y": 0.15937763452529907}, {"x": 0.12254610657691956, "y": 0.15937763452529907}], "text": "mental health?\n"}
{"page": 2, "bbox": [{"x": 0.3759666979312897, "y": 0.1589571088552475}, {"x": 0.41165971755981445, "y": 0.15853658318519592}, {"x": 0.41165971755981445, "y": 0.1677880585193634}, {"x": 0.3759666979312897, "y": 0.16820858418941498}], "text": "GPT\n"}
{"page": 2, "bbox": [{"x": 0.465794175863266, "y": 0.17031118273735046}, {"x": 0.5835812091827393, "y": 0.17073170840740204}, {"x": 0.5835812091827393, "y": 0.18502943217754364}, {"x": 0.465794175863266, "y": 0.18460892140865326}], "text": "인간이 불을 사용한 기록은 약\n1800만년 전부터 나타난다...\n"}
{"page": 2, "bbox": [{"x": 0.12254610657691956, "y": 0.17788057029247284}, {"x": 0.2795954644680023, "y": 0.17746004462242126}, {"x": 0.2795954644680023, "y": 0.19133725762367249}, {"x": 0.12254610657691956, "y": 0.19175778329372406}], "text": "write a passage in Korean to answer the\nquestion in detail\n"}
{"page": 2, "bbox": [{"x": 0.7418203353881836, "y": 0.1804036945104599}, {"x": 0.871505081653595, "y": 0.1804036945104599}, {"x": 0.871505081653595, "y": 0.20269133150577545}, {"x": 0.7418203353881836, "y": 0.20269133150577545}], "text": "... 불을 처음 사용한 시기는 호모\n에렉투스가 살았던 142만 년 전으\n로 거슬러간다...\n"}
{"page": 2, "bbox": [{"x": 0.12314099073410034, "y": 0.19259881973266602}, {"x": 0.24033313989639282, "y": 0.19217829406261444}, {"x": 0.24033313989639282, "y": 0.19890664517879486}, {"x": 0.12314099073410034, "y": 0.19932717084884644}], "text": "인간은 언제 불을 사용했는가?\n"}
{"page": 2, "bbox": [{"x": 0.3402736485004425, "y": 0.21572750806808472}, {"x": 0.3866746127605438, "y": 0.21572750806808472}, {"x": 0.3866746127605438, "y": 0.21993272006511688}, {"x": 0.3402736485004425, "y": 0.21993272006511688}], "text": "instruction\n"}
{"page": 2, "bbox": [{"x": 0.6751933097839355, "y": 0.2161480188369751}, {"x": 0.7370612621307373, "y": 0.2161480188369751}, {"x": 0.7370612621307373, "y": 0.22035323083400726}, {"x": 0.6751933097839355, "y": 0.22035323083400726}], "text": "real document\n"}
{"page": 2, "bbox": [{"x": 0.5288518667221069, "y": 0.2161480188369751}, {"x": 0.6180844902992249, "y": 0.21530698239803314}, {"x": 0.6180844902992249, "y": 0.22035323083400726}, {"x": 0.5288518667221069, "y": 0.2211942821741104}], "text": "generated document\n"}
{"page": 2, "bbox": [{"x": 0.44556811451911926, "y": 0.21698907017707825}, {"x": 0.4699583649635315, "y": 0.21740958094596863}, {"x": 0.4699583649635315, "y": 0.2211942821741104}, {"x": 0.44556811451911926, "y": 0.22077375650405884}], "text": "query\n"}
{"page": 2, "bbox": [{"x": 0.11838191747665405, "y": 0.2392767071723938}, {"x": 0.879833459854126, "y": 0.2392767071723938}, {"x": 0.879833459854126, "y": 0.2645079791545868}, {"x": 0.11838191747665405, "y": 0.2645079791545868}], "text": "Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries\nwithout changing the underlying GPT-3 and Contriever/mContriever models.\n"}
{"page": 2, "bbox": [{"x": 0.5121951103210449, "y": 0.2931034564971924}, {"x": 0.8810232281684875, "y": 0.29394447803497314}, {"x": 0.8810232281684875, "y": 0.33809924125671387}, {"x": 0.5121951103210449, "y": 0.3372581899166107}], "text": "sets, covering tasks like Web Search, Question\nAnswering, Fact Verification and languages like\nSwahili, Korean, Japanese.\n"}
{"page": 2, "bbox": [{"x": 0.5133848786354065, "y": 0.35492008924484253}, {"x": 0.6716240048408508, "y": 0.35492008924484253}, {"x": 0.6716240048408508, "y": 0.3645921051502228}, {"x": 0.5133848786354065, "y": 0.3645921051502228}], "text": "2 Related Works\n"}
{"page": 2, "bbox": [{"x": 0.1165972650051117, "y": 0.2943650186061859}, {"x": 0.48899465799331665, "y": 0.29394447803497314}, {"x": 0.48958954215049744, "y": 0.9188393354415894}, {"x": 0.11719214916229248, "y": 0.9192599058151245}], "text": "to human intent to follow instructions.\nWith these ingredients, we propose to\npivot through Hypothetical Document\nEmbeddings (HyDE), and decompose dense\nretrieval into two tasks, a generative task per-\nformed by an instruction-following language\nmodel and a document-document similarity task\nperformed by a contrastive encoder (Figure 1).\nFirst, we feed the query to the generative model\nand instruct it to \"write a document that answers\nthe question\", i.e. a hypothetical document.\nWe expect the generative process to capture\n\"relevance\" by giving an example; the generated\ndocument is not real, can contain factual errors but\nis like a relevant document. In the second step,\nwe use an unsupervised contrastive encoder to\nencode this document into an embedding vector.\nHere, we expect the encoder's dense bottleneck\nto serve a lossy compressor, where the extra\n(hallucinated) details are filtered out from the\nembedding. We use this vector to search against\nthe corpus embeddings. The most similar real\ndocuments are retrieved and returned. The retrieval\nleverages document-document similarity encoded\nin the inner-product during contrastive training.\nNote that, interestingly, with HyDE factorization,\nthe query-document similarity score is no longer\nexplicitly modeled nor computed. Instead, the\nretrieval task is cast into two NLU and NLG tasks.\nHYDE appears unsupervised. No model is trained\nin HyDE: both the generative model and the con-\ntrastive encoder remain intact. Supervision signals\nwere only involved in instruction learning of our\nbackbone LLM.\nIn our experiments, we show HyDE using Instruct-\nGPT (Ouyang et al., 2022) and Contriever (Izacard\net al., 2021) as backbone models significantly out-\nperforms the previous state-of-the-art Contriever-\nonly zero-shot no-relevance system on 11 queries\n"}
{"page": 2, "bbox": [{"x": 0.5116002559661865, "y": 0.38099244236946106}, {"x": 0.8845925331115723, "y": 0.38099244236946106}, {"x": 0.8839976191520691, "y": 0.9163162112236023}, {"x": 0.5110053420066833, "y": 0.9163162112236023}], "text": "Dense Retrieval (Lee et al., 2019; Karpukhin\net al., 2020) has been extensively studied after the\nemergence of pre-trained Transformer language\nmodels (Devlin et al., 2019). Researchers stud-\nied the metric learning problems, such as training\nloss (Karpukhin et al., 2020) and negative sam-\npling (Xiong et al., 2021; Qu et al., 2021), and also\nintroduced distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofstätter et al., 2021). Later works studied\nthe second stage pre-training of language model\nspecifically for retrieval (Izacard et al., 2021; Gao\nand Callan, 2021; Lu et al., 2021; Gao and Callan,\n2022; Liu and Shao, 2022).\nThe popularity of dense retrieval can be partially\nattributed to the rich and successful research in very\nefficient minimum inner product search (MIPS) at\nvery large (billion) scales (Johnson et al., 2017).\nInstructions-Following Language Models\nSoon after the emergence of LLMs, several groups\nof researchers discover that LLMs trained on data\nconsisting of instructions and their execution can\nzero-shot generalize to perform new tasks with new\ninstructions (Ouyang et al., 2022; Sanh et al., 2022;\nMin et al., 2022; Wei et al., 2022). This can be\ndone by standard supervised sequence-to-sequence\nlearning or more effectively with reinforcement\nlearning (Ouyang et al., 2022).\nConcurrent to us, Asai et al. (2022) studied\n\"Task-aware Retrieval with Instructions\". They\nfine-tuned dense encoders that can also encode\ntask-specific instruction prepended to query. In\ncomparison, we use an unsupervised encoder and\nhandle different tasks and their instruction with an\n"}
{"page": 3, "bbox": [{"x": 0.5133848786354065, "y": 0.08830950409173965}, {"x": 0.6609161496162415, "y": 0.08830950409173965}, {"x": 0.6609161496162415, "y": 0.09714045375585556}, {"x": 0.5133848786354065, "y": 0.09714045375585556}], "text": "3.1 Preliminaries\n"}
{"page": 3, "bbox": [{"x": 0.11897680163383484, "y": 0.08830950409173965}, {"x": 0.4860202372074127, "y": 0.08788898587226868}, {"x": 0.4860202372074127, "y": 0.11269974708557129}, {"x": 0.11897680163383484, "y": 0.11312027275562286}], "text": "instruction following generative LLM, as described\nabove.\n"}
{"page": 3, "bbox": [{"x": 0.5133848786354065, "y": 0.10681244730949402}, {"x": 0.8834027647972107, "y": 0.10681244730949402}, {"x": 0.8834027647972107, "y": 0.20058873295783997}, {"x": 0.5133848786354065, "y": 0.20058873295783997}], "text": "Dense retrieval models similarity between query\nand document with inner product similarity. Given\na query q and document d, it uses two encoder\nfunction enc, and encd to map them into d dimen-\nsion vectors Vq, Vd, whose inner product is used\nas similarity measurement.\n"}
{"page": 3, "bbox": [{"x": 0.11838191747665405, "y": 0.14129520952701569}, {"x": 0.48958954215049744, "y": 0.14171572029590607}, {"x": 0.48899465799331665, "y": 0.5328006744384766}, {"x": 0.11778703331947327, "y": 0.5323801636695862}], "text": "Zero-Shot Dense Retrieval The tasks of zero-\nshot (dense) retrieval are arguably empirically de-\nfined by Thakur et al. (2021) for the neural re-\ntrieval community. Their BEIR benchmark con-\nsists of diverse retrieval tasks. The paper and\nmany follow-up research generally consider the\nTransfer Learning setup where the dense re-\ntriever is first learned using a diverse and richly\nsupervised corpus and query collection, namely\nMS-MARCO (Thakur et al., 2021; Wang et al.,\n2022; Yu et al., 2022).\nHowever, as stated by Izacard et al. (2021), such\na large collection can rarely be assumed. In this\npaper, therefore, we study the problem of building\neffective dense retrieval systems without relevance\nlabels. Similar to Izacard et al. (2021), we also\ndo not assume access to the test time corpora for\ntraining. This is a more realistic setup and prevents\nover-engineering on the test corpora.\nBy the definition in Sachan et al. (2022), our\nsetup can be roughly considered as \"unsuper-\nvised\". Strictly, as with Sachan et al. (2022), the\nonly supervision resides in the LLM, in the pro-\ncessing of learning to follow instructions.\n"}
{"page": 3, "bbox": [{"x": 0.5127900242805481, "y": 0.21825063228607178}, {"x": 0.8839976191520691, "y": 0.21825063228607178}, {"x": 0.8839976191520691, "y": 0.46677881479263306}, {"x": 0.5127900242805481, "y": 0.46677881479263306}], "text": "sim(q, d) = (enc, (q), enc¿(d)) = (Vq, Vd) (1)\nFor zero-shot retrieval, we consider L query sets\nQ1, Q2,,QL and their corresponding search cor-\npus, document sets D1, D2, ..., DL. Denote the\nj-th query from i-th set query set Qi as qij. We\nneed to fully define mapping functions enca and\nencd without access to any query set Qi, document\nset Di, or any relevance judgment rij.\nThe difficulty of zero-shot dense retrieval lies\nprecisely in Equation 1: it requires learning of two\nembedding functions (for query and document re-\nspectively) into the same embedding space where\ninner product captures relevance. Without rele-\nvance judgments/scores to fit, learning becomes\nintractable.\n"}
{"page": 3, "bbox": [{"x": 0.5139797925949097, "y": 0.48317915201187134}, {"x": 0.6026175022125244, "y": 0.48486122488975525}, {"x": 0.6020225882530212, "y": 0.49537426233291626}, {"x": 0.5133848786354065, "y": 0.49369218945503235}], "text": "3.2 HYDE\n"}
{"page": 3, "bbox": [{"x": 0.5133848786354065, "y": 0.5033641457557678}, {"x": 0.8839976191520691, "y": 0.5042052268981934}, {"x": 0.8834027647972107, "y": 0.6307821869850159}, {"x": 0.5127900242805481, "y": 0.6299411058425903}], "text": "HYDE circumvents the aforementioned learning\nproblem by performing search in document-\nonly embedding space that captures document-\ndocument similarity. This can be easily learned\nusing unsupervised contrastive learning (Izacard\net al., 2021; Gao et al., 2021; Gao and Callan,\n2022). We set document encoder encd directly as a\ncontrastive encoder enccon\n"}
{"page": 3, "bbox": [{"x": 0.8584176301956177, "y": 0.6476030349731445}, {"x": 0.881618082523346, "y": 0.6476030349731445}, {"x": 0.881618082523346, "y": 0.6585366129875183}, {"x": 0.8584176301956177, "y": 0.6585366129875183}], "text": "(2)\n"}
{"page": 3, "bbox": [{"x": 0.6281974911689758, "y": 0.6476030349731445}, {"x": 0.7668054699897766, "y": 0.6488645672798157}, {"x": 0.7668054699897766, "y": 0.6602186560630798}, {"x": 0.6281974911689758, "y": 0.6589571237564087}], "text": "f = encα = enccon\n"}
{"page": 3, "bbox": [{"x": 0.11778703331947327, "y": 0.556770384311676}, {"x": 0.48839977383613586, "y": 0.556770384311676}, {"x": 0.48839977383613586, "y": 0.8086627125740051}, {"x": 0.11778703331947327, "y": 0.8086627125740051}], "text": "Generative Retrieval Generative search is a new\nclass of retrieval methods that use neural generative\nmodels as search indices (Metzler et al., 2021; Tay\net al., 2022; Bevilacqua et al., 2022; Lee et al.,\n2022). These models use (constrained) decoding\nto generate document identifiers, such as id and\nsub-string, which map directly to real documents.\nThey have to go through special training procedures\nover relevance data; effective search may also need\nto use novel forms of search indices (Bevilacqua\net al., 2022; Lee et al., 2022). In comparison, our\nmethod uses the standard MIPS index and requires\nno training or training data. Our generative model\nproduces an intermediate hypothetical document\nto be fed into a dense encoder, instead of a real\ndocument.\n"}
{"page": 3, "bbox": [{"x": 0.5133848786354065, "y": 0.6749369502067566}, {"x": 0.8834027647972107, "y": 0.6757779717445374}, {"x": 0.8834027647972107, "y": 0.7211942672729492}, {"x": 0.5133848786354065, "y": 0.7203532457351685}], "text": "This function is also denoted as f for simplic-\nity. This unsupervised contrastive encoder will\nbe shared by all incoming document corpus.\n"}
{"page": 3, "bbox": [{"x": 0.5883402824401855, "y": 0.7380151152610779}, {"x": 0.8810232281684875, "y": 0.7380151152610779}, {"x": 0.8810232281684875, "y": 0.7502102851867676}, {"x": 0.5883402824401855, "y": 0.7502102851867676}], "text": "f(d) Vd E D₁ U D₂ U ... U DĽ (3)\n"}
{"page": 3, "bbox": [{"x": 0.5419393181800842, "y": 0.7409588098526001}, {"x": 0.5806067585945129, "y": 0.7401177287101746}, {"x": 0.5812016725540161, "y": 0.7489486932754517}, {"x": 0.5425342321395874, "y": 0.7497897148132324}], "text": "Vd =\n"}
{"page": 3, "bbox": [{"x": 0.5133848786354065, "y": 0.7670311331748962}, {"x": 0.8828078508377075, "y": 0.7661900520324707}, {"x": 0.8834027647972107, "y": 0.8427249789237976}, {"x": 0.5139797925949097, "y": 0.8435660004615784}], "text": "To build the query vector, we consider in addition\nan instruction following LM, InstructLM. It takes a\nquery q and a textual instruction INST and follows\nthem to perform the task specified by INST. For\nsimplicity, denote,\n"}
{"page": 3, "bbox": [{"x": 0.11897680163383484, "y": 0.8389402627944946}, {"x": 0.2617489695549011, "y": 0.8397813439369202}, {"x": 0.2617489695549011, "y": 0.8523969650268555}, {"x": 0.11897680163383484, "y": 0.8515559434890747}], "text": "3 Methodology\n"}
{"page": 3, "bbox": [{"x": 0.5657346844673157, "y": 0.8608074188232422}, {"x": 0.8810232281684875, "y": 0.8608074188232422}, {"x": 0.8810232281684875, "y": 0.8734230399131775}, {"x": 0.5657346844673157, "y": 0.8734230399131775}], "text": "g(q, INST) = InstructLM(q, INST) (4)\n"}
{"page": 3, "bbox": [{"x": 0.11838191747665405, "y": 0.8742640614509583}, {"x": 0.48839977383613586, "y": 0.8742640614509583}, {"x": 0.48839977383613586, "y": 0.918418824672699}, {"x": 0.11838191747665405, "y": 0.918418824672699}], "text": "In this section, we first formally define the prob-\nlem of (zero-shot) dense retrieval. Then we will\nintroduce how HyDE is designed to solve it.\n"}
{"page": 3, "bbox": [{"x": 0.5127900242805481, "y": 0.8906643986701965}, {"x": 0.8834027647972107, "y": 0.8910849690437317}, {"x": 0.8834027647972107, "y": 0.9192599058151245}, {"x": 0.5127900242805481, "y": 0.9188393354415894}], "text": "Now we can use g to map queries to \"hypotheti-\ncal\" documents by sampling from g, setting INST\n"}
{"page": 4, "bbox": [{"x": 0.11778703331947327, "y": 0.08873002231121063}, {"x": 0.48958954215049744, "y": 0.08873002231121063}, {"x": 0.48958954215049744, "y": 0.3090832531452179}, {"x": 0.11778703331947327, "y": 0.3090832531452179}], "text": "to be \"write a paragraph that answers the\nquestion\". The generated document is not real,\ncan and is likely to be ungrounded factually (Brown\net al., 2020; Thoppilan et al., 2022). We only re-\nquire it to capture relevance pattern. This is done\nby generating documents, i.e. providing exam-\nples. Critically, here we offload relevance mod-\neling from representation learning model to an\nNLG model that generalizes significantly more eas-\nily, naturally, and effectively (Brown et al., 2020;\nOuyang et al., 2022). Generating examples also\nreplaces explicit modeling of relevance scores.\nWe can now encode the generated document using\nthe document encoder f. Write,\n"}
{"page": 4, "bbox": [{"x": 0.11600238084793091, "y": 0.3195962905883789}, {"x": 0.49256396293640137, "y": 0.32253995537757874}, {"x": 0.4901844263076782, "y": 0.4730866253376007}, {"x": 0.11362284421920776, "y": 0.47014299035072327}], "text": "E[v] =E[f(g(qij, INST;))]\n(5)\nFormally, g defines a probability distribution based\non the chain rule. In this paper, we simply consider\nthe expectation value, assuming the distribution of\nVaij\nis uni-modal, i.e. the query is not ambiguous.\nThe study of ambiguous queries and diversity is\nleft to future work. We estimate Equation 5 by\nsampling N documents from 9, [d1, d2, ..., ɗN].\n"}
{"page": 4, "bbox": [{"x": 0.5133848786354065, "y": 0.08830950409173965}, {"x": 0.8839976191520691, "y": 0.08830950409173965}, {"x": 0.8839976191520691, "y": 0.8406223654747009}, {"x": 0.5133848786354065, "y": 0.8406223654747009}], "text": "sample from InstructGPT using the OpenAI play-\nground default temperature of 0.7 for open-ended\ngenerations. We use the English-only Contriever\nmodel for English retrieval tasks and multilingual\nmContriever for non-English tasks. We conducted\nretrieval experiments with the Pyserini toolkit (Lin\net al., 2021a).\nDatasets We consider web search query sets\nTREC DL19 (Craswell et al., 2020a) and\nDL20 (Craswell et al., 2020b); they are based on\nthe MS-MARCO dataset (Bajaj et al., 2016). We\nalso use a diverse collection of 6 low-resource\ndatasets from the BEIR dataset (Thakur et al.,\n2021). For non-English retrieval, we consider\nSwahili, Korean, Japanese, and Bengali from the\nMr.Tydi dataset (Zhang et al., 2021).\nWe use different instructions for each dataset.\nThey share a similar structure but have different\nquantifiers to control the exact form of the gener-\nated hypothetical documents. These instructions\ncan be found in subsection A.1.\nCompared Systems Contriever models,\nContriever and mContriever, serve as our major\nbaseline. They are trained using unsupervised\ncontrastive learning. HyDE retrievers share the\nexact same embedding spaces with them. The\nonly difference is how the query vector is built.\nThese comparisons allow us to easily examine\nthe effect of HyDE. The classical heuristic-based\nlexical retriever BM25 is also included.\nSeveral systems that involve fine-tuning on mas-\nsive relevance data are also included as refer-\nences. We consider models fine-tuned on MS-\nMARCO and transferred, DPR and ANCE, from\nthe BEIR paper. For multilingual, we include\nthe mDPR model from Mr.Tydi paper and MS-\nMARCO fine-tuned mBERT and XLM-R from\nthe Contriever paper. We also include the state-of-\nthe-art transfer learning models: Contriever and\nmContriever fine-tuned on MS-MARCO, denoted\nContriever FT and mContriever FT. These mod-\nels have run through the state-of-the-art retrieval\nmodel training pipeline that involves second-stage\nretrieval-specific pre-training (Lee et al., 2019) and\na few rounds of fine-tuning (Qu et al., 2021); they\nshould be considered empirical upper bounds.\n"}
{"page": 4, "bbox": [{"x": 0.251041054725647, "y": 0.478973925113678}, {"x": 0.259369432926178, "y": 0.478973925113678}, {"x": 0.259369432926178, "y": 0.4861227869987488}, {"x": 0.251041054725647, "y": 0.4861227869987488}], "text": "1\n"}
{"page": 4, "bbox": [{"x": 0.46341463923454285, "y": 0.48696383833885193}, {"x": 0.48423558473587036, "y": 0.48696383833885193}, {"x": 0.48423558473587036, "y": 0.4978973865509033}, {"x": 0.46341463923454285, "y": 0.4978973865509033}], "text": "(6)\n"}
{"page": 4, "bbox": [{"x": 0.3700178563594818, "y": 0.4861227869987488}, {"x": 0.412849485874176, "y": 0.48654332756996155}, {"x": 0.412849485874176, "y": 0.49873843789100647}, {"x": 0.3700178563594818, "y": 0.4983179271221161}], "text": "f(dk)\n"}
{"page": 4, "bbox": [{"x": 0.30339083075523376, "y": 0.4835996627807617}, {"x": 0.3325401544570923, "y": 0.4835996627807617}, {"x": 0.3325401544570923, "y": 0.5025231242179871}, {"x": 0.30339083075523376, "y": 0.5025231242179871}], "text": "Σ\n"}
{"page": 4, "bbox": [{"x": 0.22546103596687317, "y": 0.49201008677482605}, {"x": 0.23497918248176575, "y": 0.49201008677482605}, {"x": 0.23497918248176575, "y": 0.4945332109928131}, {"x": 0.22546103596687317, "y": 0.4945332109928131}], "text": "-\n"}
{"page": 4, "bbox": [{"x": 0.19155265390872955, "y": 0.48654332756996155}, {"x": 0.21891731023788452, "y": 0.49032801389694214}, {"x": 0.2159428894519806, "y": 0.5016821026802063}, {"x": 0.18857823312282562, "y": 0.49747687578201294}], "text": "√ qij\n"}
{"page": 4, "bbox": [{"x": 0.24747174978256226, "y": 0.4962153136730194}, {"x": 0.2623438537120819, "y": 0.4962153136730194}, {"x": 0.2623438537120819, "y": 0.5046257376670837}, {"x": 0.24747174978256226, "y": 0.5046257376670837}], "text": "N\n"}
{"page": 4, "bbox": [{"x": 0.26888757944107056, "y": 0.50630784034729}, {"x": 0.3670434355735779, "y": 0.5075693726539612}, {"x": 0.3670434355735779, "y": 0.5176619291305542}, {"x": 0.26888757944107056, "y": 0.5164003372192383}], "text": "dk~g(qij, INSTį)\n"}
{"page": 4, "bbox": [{"x": 0.2766210734844208, "y": 0.524810791015625}, {"x": 0.2885187268257141, "y": 0.524810791015625}, {"x": 0.2885187268257141, "y": 0.5311185717582703}, {"x": 0.2766210734844208, "y": 0.5311185717582703}], "text": "N\n"}
{"page": 4, "bbox": [{"x": 0.26829269528388977, "y": 0.5353237986564636}, {"x": 0.3402736485004425, "y": 0.5349032878875732}, {"x": 0.3402736485004425, "y": 0.550462543964386}, {"x": 0.26829269528388977, "y": 0.5508831143379211}], "text": "Σ f(dk)\n"}
{"page": 4, "bbox": [{"x": 0.46400952339172363, "y": 0.5386879444122314}, {"x": 0.48542535305023193, "y": 0.5386879444122314}, {"x": 0.48542535305023193, "y": 0.5487805008888245}, {"x": 0.46400952339172363, "y": 0.5487805008888245}], "text": "(7)\n"}
{"page": 4, "bbox": [{"x": 0.22546103596687317, "y": 0.5428931713104248}, {"x": 0.2367638349533081, "y": 0.5428931713104248}, {"x": 0.2367638349533081, "y": 0.5462573766708374}, {"x": 0.22546103596687317, "y": 0.5462573766708374}], "text": "=\n"}
{"page": 4, "bbox": [{"x": 0.26948246359825134, "y": 0.5563498735427856}, {"x": 0.29506245255470276, "y": 0.556770384311676}, {"x": 0.29506245255470276, "y": 0.5630782246589661}, {"x": 0.26948246359825134, "y": 0.5626577138900757}], "text": "k=1\n"}
{"page": 4, "bbox": [{"x": 0.11719214916229248, "y": 0.5769554376602173}, {"x": 0.4878048896789551, "y": 0.5786374807357788}, {"x": 0.4878048896789551, "y": 0.5916736721992493}, {"x": 0.11719214916229248, "y": 0.589991569519043}], "text": "We also consider the query as a possible hypothesis,\n"}
{"page": 4, "bbox": [{"x": 0.29208803176879883, "y": 0.6068124771118164}, {"x": 0.30398571491241455, "y": 0.6068124771118164}, {"x": 0.30398571491241455, "y": 0.6131202578544617}, {"x": 0.29208803176879883, "y": 0.6131202578544617}], "text": "N\n"}
{"page": 4, "bbox": [{"x": 0.2498512864112854, "y": 0.6118587255477905}, {"x": 0.2575847804546356, "y": 0.6118587255477905}, {"x": 0.2575847804546356, "y": 0.6194280982017517}, {"x": 0.2498512864112854, "y": 0.6194280982017517}], "text": "1\n"}
{"page": 4, "bbox": [{"x": 0.2813801169395447, "y": 0.6169049739837646}, {"x": 0.48542535305023193, "y": 0.6190075874328613}, {"x": 0.48483046889305115, "y": 0.6341463327407837}, {"x": 0.2807852327823639, "y": 0.632043719291687}], "text": "[ƒ (dk) + f (ij)] (8)\n"}
{"page": 4, "bbox": [{"x": 0.22962522506713867, "y": 0.6282590627670288}, {"x": 0.2766210734844208, "y": 0.6291000843048096}, {"x": 0.27602618932724, "y": 0.638351559638977}, {"x": 0.22903034090995789, "y": 0.6375105381011963}], "text": "N+1\n"}
{"page": 4, "bbox": [{"x": 0.2849494218826294, "y": 0.6379310488700867}, {"x": 0.311124324798584, "y": 0.638351559638977}, {"x": 0.311124324798584, "y": 0.6450799107551575}, {"x": 0.2849494218826294, "y": 0.6446593999862671}], "text": "k=1\n"}
{"page": 4, "bbox": [{"x": 0.4051159918308258, "y": 0.6619007587432861}, {"x": 0.4217727482318878, "y": 0.6623212695121765}, {"x": 0.42117786407470703, "y": 0.6690496206283569}, {"x": 0.404521107673645, "y": 0.6686291098594666}], "text": "qij\n"}
{"page": 4, "bbox": [{"x": 0.11838191747665405, "y": 0.6551724076271057}, {"x": 0.4878048896789551, "y": 0.6560134291648865}, {"x": 0.4872100055217743, "y": 0.7964676022529602}, {"x": 0.11778703331947327, "y": 0.7956265807151794}], "text": "Inner product is computed between ŵ₁₁; and the\nset of all document vectors {f(d)\\d € Di}. The\nmost similar documents are retrieved. Here the\nencoder function of serves as a lossy compressor\nthat outputs dense vectors, where the extra details\nare filtered and left out from the vector. It further\ngrounds the hypothetical vector to the actual corpus\nand the real documents. The full HYDE system is\nillustrated in Figure 1.\n"}
{"page": 4, "bbox": [{"x": 0.1165972650051117, "y": 0.811185896396637}, {"x": 0.259369432926178, "y": 0.8120269179344177}, {"x": 0.259369432926178, "y": 0.824642539024353}, {"x": 0.1165972650051117, "y": 0.8238015174865723}], "text": "4 Experiments\n"}
{"page": 4, "bbox": [{"x": 0.11778703331947327, "y": 0.836837649345398}, {"x": 0.20404520630836487, "y": 0.8389402627944946}, {"x": 0.20345032215118408, "y": 0.8490328192710876}, {"x": 0.11719214916229248, "y": 0.846930205821991}], "text": "4.1 Setup\n"}
{"page": 4, "bbox": [{"x": 0.5121951103210449, "y": 0.8540790677070618}, {"x": 0.6496133208274841, "y": 0.8540790677070618}, {"x": 0.6496133208274841, "y": 0.8629100322723389}, {"x": 0.5121951103210449, "y": 0.8629100322723389}], "text": "4.2 Web Search\n"}
{"page": 4, "bbox": [{"x": 0.11838191747665405, "y": 0.8587048053741455}, {"x": 0.4872100055217743, "y": 0.8587048053741455}, {"x": 0.4872100055217743, "y": 0.9179983139038086}, {"x": 0.11838191747665405, "y": 0.9179983139038086}], "text": "Implementation We implement HyDE using\nInstructGPT, a GPT-3 model from the instruct\nseries (text-davinci-003; Ouyang et al. (2022))\nand Contriever models (Izacard et al., 2021). We\n"}
{"page": 4, "bbox": [{"x": 0.5139797925949097, "y": 0.8746846318244934}, {"x": 0.8822129964828491, "y": 0.8742640614509583}, {"x": 0.8822129964828491, "y": 0.9175778031349182}, {"x": 0.5139797925949097, "y": 0.9179983139038086}], "text": "In Table 1, we show retrieval results on TREC\nDL19 and TREC DL20. We see HyDE bring sizable\nimprovements to Contriever across the board for\n"}
{"page": 5, "bbox": [{"x": 0.6341463327407837, "y": 0.08999159187078476}, {"x": 0.6680546998977661, "y": 0.08957106620073318}, {"x": 0.6680546998977661, "y": 0.09629940986633301}, {"x": 0.6341463327407837, "y": 0.09671993553638458}], "text": "DL20\n"}
{"page": 5, "bbox": [{"x": 0.4271267056465149, "y": 0.08999159187078476}, {"x": 0.4610351026058197, "y": 0.08999159187078476}, {"x": 0.4610351026058197, "y": 0.09671993553638458}, {"x": 0.4271267056465149, "y": 0.09671993553638458}], "text": "DL19\n"}
{"page": 5, "bbox": [{"x": 0.603807270526886, "y": 0.10092514753341675}, {"x": 0.7453896403312683, "y": 0.09882254153490067}, {"x": 0.7453896403312683, "y": 0.1089150533080101}, {"x": 0.603807270526886, "y": 0.11101765930652618}], "text": "ndcg@10 recall@1k\n"}
{"page": 5, "bbox": [{"x": 0.47531232237815857, "y": 0.1017661914229393}, {"x": 0.5383700132369995, "y": 0.1017661914229393}, {"x": 0.5383700132369995, "y": 0.1089150533080101}, {"x": 0.47531232237815857, "y": 0.1089150533080101}], "text": "recall @ 1k\n"}
{"page": 5, "bbox": [{"x": 0.3967876136302948, "y": 0.1017661914229393}, {"x": 0.4556811451911926, "y": 0.10050462931394577}, {"x": 0.4562760293483734, "y": 0.10933557897806168}, {"x": 0.3973824977874756, "y": 0.1105971410870552}], "text": "ndcg@10\n"}
{"page": 5, "bbox": [{"x": 0.5580011606216431, "y": 0.1034482792019844}, {"x": 0.5841760635375977, "y": 0.10386879742145538}, {"x": 0.5835812091827393, "y": 0.11101765930652618}, {"x": 0.5574063062667847, "y": 0.1105971410870552}], "text": "map\n"}
{"page": 5, "bbox": [{"x": 0.35098156332969666, "y": 0.10428931564092636}, {"x": 0.37656158208847046, "y": 0.10470984131097794}, {"x": 0.37656158208847046, "y": 0.11143818497657776}, {"x": 0.35098156332969666, "y": 0.11101765930652618}], "text": "map\n"}
{"page": 5, "bbox": [{"x": 0.2546103596687317, "y": 0.11984860897064209}, {"x": 0.4051159918308258, "y": 0.12068965286016464}, {"x": 0.4051159918308258, "y": 0.12952060997486115}, {"x": 0.2546103596687317, "y": 0.128679558634758}], "text": "w/o relevance judgement\n"}
{"page": 5, "bbox": [{"x": 0.412849485874176, "y": 0.1320437341928482}, {"x": 0.4396192729473114, "y": 0.13246425986289978}, {"x": 0.4396192729473114, "y": 0.13961312174797058}, {"x": 0.412849485874176, "y": 0.139192596077919}], "text": "50.6\n"}
{"page": 5, "bbox": [{"x": 0.35098156332969666, "y": 0.13288477063179016}, {"x": 0.3753718137741089, "y": 0.13288477063179016}, {"x": 0.3753718137741089, "y": 0.139192596077919}, {"x": 0.35098156332969666, "y": 0.139192596077919}], "text": "30.1\n"}
{"page": 5, "bbox": [{"x": 0.49375373125076294, "y": 0.13288477063179016}, {"x": 0.5199286341667175, "y": 0.13288477063179016}, {"x": 0.5199286341667175, "y": 0.139192596077919}, {"x": 0.49375373125076294, "y": 0.139192596077919}], "text": "75.0\n"}
{"page": 5, "bbox": [{"x": 0.5574063062667847, "y": 0.13288477063179016}, {"x": 0.5841760635375977, "y": 0.13288477063179016}, {"x": 0.5841760635375977, "y": 0.139192596077919}, {"x": 0.5574063062667847, "y": 0.139192596077919}], "text": "28.6\n"}
{"page": 5, "bbox": [{"x": 0.6192742586135864, "y": 0.13288477063179016}, {"x": 0.645449161529541, "y": 0.13288477063179016}, {"x": 0.645449161529541, "y": 0.139192596077919}, {"x": 0.6192742586135864, "y": 0.139192596077919}], "text": "48.0\n"}
{"page": 5, "bbox": [{"x": 0.7013682126998901, "y": 0.13288477063179016}, {"x": 0.7275431156158447, "y": 0.13288477063179016}, {"x": 0.7275431156158447, "y": 0.139192596077919}, {"x": 0.7013682126998901, "y": 0.139192596077919}], "text": "78.6\n"}
{"page": 5, "bbox": [{"x": 0.2546103596687317, "y": 0.13246425986289978}, {"x": 0.29149314761161804, "y": 0.13246425986289978}, {"x": 0.29149314761161804, "y": 0.13961312174797058}, {"x": 0.2546103596687317, "y": 0.13961312174797058}], "text": "BM25\n"}
{"page": 5, "bbox": [{"x": 0.2540154755115509, "y": 0.14381833374500275}, {"x": 0.31766805052757263, "y": 0.1429772973060608}, {"x": 0.31766805052757263, "y": 0.15096719563007355}, {"x": 0.2540154755115509, "y": 0.1518082469701767}], "text": "Contriever\n"}
{"page": 5, "bbox": [{"x": 0.35038667917251587, "y": 0.14381833374500275}, {"x": 0.37715646624565125, "y": 0.14423885941505432}, {"x": 0.37715646624565125, "y": 0.15138772130012512}, {"x": 0.35038667917251587, "y": 0.15096719563007355}], "text": "24.0\n"}
{"page": 5, "bbox": [{"x": 0.41225460171699524, "y": 0.14423885941505432}, {"x": 0.43783462047576904, "y": 0.14381833374500275}, {"x": 0.43783462047576904, "y": 0.15096719563007355}, {"x": 0.41225460171699524, "y": 0.15138772130012512}], "text": "44.5\n"}
{"page": 5, "bbox": [{"x": 0.5580011606216431, "y": 0.1446593701839447}, {"x": 0.5835812091827393, "y": 0.1446593701839447}, {"x": 0.5835812091827393, "y": 0.15096719563007355}, {"x": 0.5580011606216431, "y": 0.15096719563007355}], "text": "24.0\n"}
{"page": 5, "bbox": [{"x": 0.6198691129684448, "y": 0.1446593701839447}, {"x": 0.645449161529541, "y": 0.1446593701839447}, {"x": 0.645449161529541, "y": 0.15096719563007355}, {"x": 0.6198691129684448, "y": 0.15096719563007355}], "text": "42.1\n"}
{"page": 5, "bbox": [{"x": 0.7007733583450317, "y": 0.14339780807495117}, {"x": 0.7269482612609863, "y": 0.14423885941505432}, {"x": 0.7263533473014832, "y": 0.15222875773906708}, {"x": 0.7001784443855286, "y": 0.15138772130012512}], "text": "75.4\n"}
{"page": 5, "bbox": [{"x": 0.49375373125076294, "y": 0.1446593701839447}, {"x": 0.5199286341667175, "y": 0.1446593701839447}, {"x": 0.5199286341667175, "y": 0.15138772130012512}, {"x": 0.49375373125076294, "y": 0.15138772130012512}], "text": "74.6\n"}
{"page": 5, "bbox": [{"x": 0.4134443700313568, "y": 0.1551724076271057}, {"x": 0.4396192729473114, "y": 0.1555929332971573}, {"x": 0.4396192729473114, "y": 0.16316232085227966}, {"x": 0.4134443700313568, "y": 0.1627417951822281}], "text": "61.3\n"}
{"page": 5, "bbox": [{"x": 0.7007733583450317, "y": 0.15601345896720886}, {"x": 0.7269482612609863, "y": 0.15601345896720886}, {"x": 0.7269482612609863, "y": 0.1623212844133377}, {"x": 0.7007733583450317, "y": 0.1623212844133377}], "text": "84.4\n"}
{"page": 5, "bbox": [{"x": 0.5580011606216431, "y": 0.15601345896720886}, {"x": 0.5829862952232361, "y": 0.15601345896720886}, {"x": 0.5829862952232361, "y": 0.1627417951822281}, {"x": 0.5580011606216431, "y": 0.1627417951822281}], "text": "38.2\n"}
{"page": 5, "bbox": [{"x": 0.3497917950153351, "y": 0.15643398463726044}, {"x": 0.37656158208847046, "y": 0.15643398463726044}, {"x": 0.37656158208847046, "y": 0.1627417951822281}, {"x": 0.3497917950153351, "y": 0.1627417951822281}], "text": "41.8\n"}
{"page": 5, "bbox": [{"x": 0.49375373125076294, "y": 0.15643398463726044}, {"x": 0.5193337202072144, "y": 0.15643398463726044}, {"x": 0.5193337202072144, "y": 0.1627417951822281}, {"x": 0.49375373125076294, "y": 0.1627417951822281}], "text": "88.0\n"}
{"page": 5, "bbox": [{"x": 0.620464026927948, "y": 0.15643398463726044}, {"x": 0.645449161529541, "y": 0.15643398463726044}, {"x": 0.645449161529541, "y": 0.1627417951822281}, {"x": 0.620464026927948, "y": 0.1627417951822281}], "text": "57.9\n"}
{"page": 5, "bbox": [{"x": 0.2546103596687317, "y": 0.15643398463726044}, {"x": 0.2837596535682678, "y": 0.15685449540615082}, {"x": 0.2837596535682678, "y": 0.16484440863132477}, {"x": 0.2546103596687317, "y": 0.1644238829612732}], "text": "HYDE\n"}
{"page": 5, "bbox": [{"x": 0.2546103596687317, "y": 0.17367535829544067}, {"x": 0.3973824977874756, "y": 0.1749369204044342}, {"x": 0.3973824977874756, "y": 0.1837678700685501}, {"x": 0.2546103596687317, "y": 0.18250630795955658}], "text": "w/ relevance judgement\n"}
{"page": 5, "bbox": [{"x": 0.35098156332969666, "y": 0.18544995784759521}, {"x": 0.37715646624565125, "y": 0.18629099428653717}, {"x": 0.37656158208847046, "y": 0.19386038184165955}, {"x": 0.35038667917251587, "y": 0.1930193454027176}], "text": "36.5\n"}
{"page": 5, "bbox": [{"x": 0.412849485874176, "y": 0.18671151995658875}, {"x": 0.4390243887901306, "y": 0.18629099428653717}, {"x": 0.4390243887901306, "y": 0.19343987107276917}, {"x": 0.412849485874176, "y": 0.19386038184165955}], "text": "62.2\n"}
{"page": 5, "bbox": [{"x": 0.5574063062667847, "y": 0.18671151995658875}, {"x": 0.5829862952232361, "y": 0.18629099428653717}, {"x": 0.5829862952232361, "y": 0.19343987107276917}, {"x": 0.5574063062667847, "y": 0.19386038184165955}], "text": "41.8\n"}
{"page": 5, "bbox": [{"x": 0.620464026927948, "y": 0.18671151995658875}, {"x": 0.6466389298439026, "y": 0.18671151995658875}, {"x": 0.6466389298439026, "y": 0.19343987107276917}, {"x": 0.620464026927948, "y": 0.19343987107276917}], "text": "65.3\n"}
{"page": 5, "bbox": [{"x": 0.2546103596687317, "y": 0.18713204562664032}, {"x": 0.2837596535682678, "y": 0.18713204562664032}, {"x": 0.2837596535682678, "y": 0.19343987107276917}, {"x": 0.2546103596687317, "y": 0.19343987107276917}], "text": "DPR\n"}
{"page": 5, "bbox": [{"x": 0.49375373125076294, "y": 0.18713204562664032}, {"x": 0.518738865852356, "y": 0.18713204562664032}, {"x": 0.518738865852356, "y": 0.19343987107276917}, {"x": 0.49375373125076294, "y": 0.19343987107276917}], "text": "76.9\n"}
{"page": 5, "bbox": [{"x": 0.7007733583450317, "y": 0.18713204562664032}, {"x": 0.7263533473014832, "y": 0.18713204562664032}, {"x": 0.7263533473014832, "y": 0.19343987107276917}, {"x": 0.7007733583450317, "y": 0.19343987107276917}], "text": "81.4\n"}
{"page": 5, "bbox": [{"x": 0.35098156332969666, "y": 0.1980656087398529}, {"x": 0.3753718137741089, "y": 0.19764508306980133}, {"x": 0.3753718137741089, "y": 0.20479394495487213}, {"x": 0.35098156332969666, "y": 0.2052144706249237}], "text": "37.1\n"}
{"page": 5, "bbox": [{"x": 0.2540154755115509, "y": 0.1984861195087433}, {"x": 0.29506245255470276, "y": 0.1980656087398529}, {"x": 0.29506245255470276, "y": 0.2052144706249237}, {"x": 0.2540154755115509, "y": 0.2056349813938141}], "text": "ANCE\n"}
{"page": 5, "bbox": [{"x": 0.4134443700313568, "y": 0.1980656087398529}, {"x": 0.43842950463294983, "y": 0.1980656087398529}, {"x": 0.43842950463294983, "y": 0.2056349813938141}, {"x": 0.4134443700313568, "y": 0.2056349813938141}], "text": "64.5\n"}
{"page": 5, "bbox": [{"x": 0.620464026927948, "y": 0.1984861195087433}, {"x": 0.6466389298439026, "y": 0.19890664517879486}, {"x": 0.6466389298439026, "y": 0.2052144706249237}, {"x": 0.620464026927948, "y": 0.20479394495487213}], "text": "64.6\n"}
{"page": 5, "bbox": [{"x": 0.49375373125076294, "y": 0.1984861195087433}, {"x": 0.5199286341667175, "y": 0.1984861195087433}, {"x": 0.5199286341667175, "y": 0.2056349813938141}, {"x": 0.49375373125076294, "y": 0.2056349813938141}], "text": "75.5\n"}
{"page": 5, "bbox": [{"x": 0.5574063062667847, "y": 0.19890664517879486}, {"x": 0.5829862952232361, "y": 0.19890664517879486}, {"x": 0.5829862952232361, "y": 0.2052144706249237}, {"x": 0.5574063062667847, "y": 0.2052144706249237}], "text": "40.8\n"}
{"page": 5, "bbox": [{"x": 0.7007733583450317, "y": 0.19890664517879486}, {"x": 0.7281380295753479, "y": 0.19890664517879486}, {"x": 0.7281380295753479, "y": 0.2056349813938141}, {"x": 0.7007733583450317, "y": 0.2056349813938141}], "text": "77.6\n"}
{"page": 5, "bbox": [{"x": 0.2534205913543701, "y": 0.20941968262195587}, {"x": 0.37656158208847046, "y": 0.2073170691728592}, {"x": 0.37715646624565125, "y": 0.2178301066160202}, {"x": 0.2540154755115509, "y": 0.21993272006511688}], "text": "ContrieverFT 41.7\n"}
{"page": 5, "bbox": [{"x": 0.4134443700313568, "y": 0.2106812447309494}, {"x": 0.43842950463294983, "y": 0.21110177040100098}, {"x": 0.43842950463294983, "y": 0.21825063228607178}, {"x": 0.4134443700313568, "y": 0.2178301066160202}], "text": "62.1\n"}
{"page": 5, "bbox": [{"x": 0.620464026927948, "y": 0.21110177040100098}, {"x": 0.6460440158843994, "y": 0.21110177040100098}, {"x": 0.6460440158843994, "y": 0.2178301066160202}, {"x": 0.620464026927948, "y": 0.2178301066160202}], "text": "63.2\n"}
{"page": 5, "bbox": [{"x": 0.49375373125076294, "y": 0.21152228116989136}, {"x": 0.5199286341667175, "y": 0.21152228116989136}, {"x": 0.5199286341667175, "y": 0.2178301066160202}, {"x": 0.49375373125076294, "y": 0.2178301066160202}], "text": "83.6\n"}
{"page": 5, "bbox": [{"x": 0.5574063062667847, "y": 0.21152228116989136}, {"x": 0.5841760635375977, "y": 0.21152228116989136}, {"x": 0.5841760635375977, "y": 0.2178301066160202}, {"x": 0.5574063062667847, "y": 0.2178301066160202}], "text": "43.6\n"}
{"page": 5, "bbox": [{"x": 0.7001784443855286, "y": 0.21152228116989136}, {"x": 0.7269482612609863, "y": 0.21152228116989136}, {"x": 0.7269482612609863, "y": 0.2178301066160202}, {"x": 0.7001784443855286, "y": 0.2178301066160202}], "text": "85.8\n"}
{"page": 5, "bbox": [{"x": 0.11838191747665405, "y": 0.23843565583229065}, {"x": 0.8810232281684875, "y": 0.23843565583229065}, {"x": 0.8810232281684875, "y": 0.26366695761680603}, {"x": 0.11838191747665405, "y": 0.26366695761680603}], "text": "Table 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked\nbold. DPR, ANCE and Contriever FT are in-domain supervised models that are finetuned on MS MARCO training\n"}
{"page": 5, "bbox": [{"x": 0.11897680163383484, "y": 0.2674516439437866}, {"x": 0.1505056470632553, "y": 0.267872154712677}, {"x": 0.1505056470632553, "y": 0.27544155716896057}, {"x": 0.11897680163383484, "y": 0.2750210165977478}], "text": "data.\n"}
{"page": 5, "bbox": [{"x": 0.6811421513557434, "y": 0.29941126704216003}, {"x": 0.765615701675415, "y": 0.29899075627326965}, {"x": 0.765615701675415, "y": 0.30656012892723083}, {"x": 0.6811421513557434, "y": 0.3069806694984436}], "text": "TREC-NEWS\n"}
{"page": 5, "bbox": [{"x": 0.32956573367118835, "y": 0.29899075627326965}, {"x": 0.6603212356567383, "y": 0.2998317778110504}, {"x": 0.6603212356567383, "y": 0.30992430448532104}, {"x": 0.32956573367118835, "y": 0.3090832531452179}], "text": "Scifact Arguana Trec-Covid FiQA DBPedia\n"}
{"page": 5, "bbox": [{"x": 0.4651992917060852, "y": 0.3183347284793854}, {"x": 0.5348007082939148, "y": 0.3183347284793854}, {"x": 0.5348007082939148, "y": 0.3254835903644562}, {"x": 0.4651992917060852, "y": 0.3254835903644562}], "text": "nDCG@10\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.32926830649375916}, {"x": 0.3842950761318207, "y": 0.3305298686027527}, {"x": 0.3837001919746399, "y": 0.3498738408088684}, {"x": 0.2325996458530426, "y": 0.3486122786998749}], "text": "w/o relevance judgement\nBM25\n"}
{"page": 5, "bbox": [{"x": 0.40392622351646423, "y": 0.3410429060459137}, {"x": 0.42891135811805725, "y": 0.3414634168148041}, {"x": 0.42891135811805725, "y": 0.3486122786998749}, {"x": 0.40392622351646423, "y": 0.3481917679309845}], "text": "39.7\n"}
{"page": 5, "bbox": [{"x": 0.4830458164215088, "y": 0.34188392758369446}, {"x": 0.5092207193374634, "y": 0.3410429060459137}, {"x": 0.5098155736923218, "y": 0.3481917679309845}, {"x": 0.4836407005786896, "y": 0.34903278946876526}], "text": "59.5\n"}
{"page": 5, "bbox": [{"x": 0.3372992277145386, "y": 0.34188392758369446}, {"x": 0.3628792464733124, "y": 0.3423044681549072}, {"x": 0.3628792464733124, "y": 0.34903278946876526}, {"x": 0.3372992277145386, "y": 0.3486122786998749}], "text": "67.9\n"}
{"page": 5, "bbox": [{"x": 0.6198691129684448, "y": 0.3423044681549072}, {"x": 0.645449161529541, "y": 0.3423044681549072}, {"x": 0.645449161529541, "y": 0.3486122786998749}, {"x": 0.6198691129684448, "y": 0.3486122786998749}], "text": "31.8\n"}
{"page": 5, "bbox": [{"x": 0.7108863592147827, "y": 0.3423044681549072}, {"x": 0.7358714938163757, "y": 0.3423044681549072}, {"x": 0.7358714938163757, "y": 0.3486122786998749}, {"x": 0.7108863592147827, "y": 0.3486122786998749}], "text": "39.5\n"}
{"page": 5, "bbox": [{"x": 0.5550267696380615, "y": 0.3423044681549072}, {"x": 0.5817965269088745, "y": 0.3423044681549072}, {"x": 0.5817965269088745, "y": 0.34903278946876526}, {"x": 0.5550267696380615, "y": 0.34903278946876526}], "text": "23.6\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.3532380163669586}, {"x": 0.29625222086906433, "y": 0.35281750559806824}, {"x": 0.29625222086906433, "y": 0.3603868782520294}, {"x": 0.2331945300102234, "y": 0.3608073890209198}], "text": "Contriever\n"}
{"page": 5, "bbox": [{"x": 0.3372992277145386, "y": 0.353658527135849}, {"x": 0.3616894781589508, "y": 0.353658527135849}, {"x": 0.3616894781589508, "y": 0.3603868782520294}, {"x": 0.3372992277145386, "y": 0.3603868782520294}], "text": "64.9\n"}
{"page": 5, "bbox": [{"x": 0.4836407005786896, "y": 0.3532380163669586}, {"x": 0.5098155736923218, "y": 0.3532380163669586}, {"x": 0.5098155736923218, "y": 0.3608073890209198}, {"x": 0.4836407005786896, "y": 0.3608073890209198}], "text": "27.3\n"}
{"page": 5, "bbox": [{"x": 0.5550267696380615, "y": 0.353658527135849}, {"x": 0.5812016725540161, "y": 0.3532380163669586}, {"x": 0.5812016725540161, "y": 0.3603868782520294}, {"x": 0.5550267696380615, "y": 0.3608073890209198}], "text": "24.5\n"}
{"page": 5, "bbox": [{"x": 0.7108863592147827, "y": 0.35407906770706177}, {"x": 0.7358714938163757, "y": 0.35407906770706177}, {"x": 0.7358714938163757, "y": 0.3603868782520294}, {"x": 0.7108863592147827, "y": 0.3603868782520294}], "text": "34.8\n"}
{"page": 5, "bbox": [{"x": 0.40392622351646423, "y": 0.35407906770706177}, {"x": 0.42891135811805725, "y": 0.35407906770706177}, {"x": 0.42891135811805725, "y": 0.3608073890209198}, {"x": 0.40392622351646423, "y": 0.3608073890209198}], "text": "37.9\n"}
{"page": 5, "bbox": [{"x": 0.6198691129684448, "y": 0.35407906770706177}, {"x": 0.645449161529541, "y": 0.35407906770706177}, {"x": 0.645449161529541, "y": 0.3608073890209198}, {"x": 0.6198691129684448, "y": 0.3608073890209198}], "text": "29.2\n"}
{"page": 5, "bbox": [{"x": 0.6198691129684448, "y": 0.3658536672592163}, {"x": 0.645449161529541, "y": 0.3658536672592163}, {"x": 0.645449161529541, "y": 0.37216147780418396}, {"x": 0.6198691129684448, "y": 0.37216147780418396}], "text": "36.8\n"}
{"page": 5, "bbox": [{"x": 0.40333133935928345, "y": 0.3658536672592163}, {"x": 0.4306960105895996, "y": 0.3658536672592163}, {"x": 0.4306960105895996, "y": 0.37258198857307434}, {"x": 0.40333133935928345, "y": 0.37258198857307434}], "text": "46.6\n"}
{"page": 5, "bbox": [{"x": 0.5550267696380615, "y": 0.3658536672592163}, {"x": 0.5817965269088745, "y": 0.3658536672592163}, {"x": 0.5817965269088745, "y": 0.37258198857307434}, {"x": 0.5550267696380615, "y": 0.37258198857307434}], "text": "27.3\n"}
{"page": 5, "bbox": [{"x": 0.4836407005786896, "y": 0.3662741780281067}, {"x": 0.5098155736923218, "y": 0.3662741780281067}, {"x": 0.5098155736923218, "y": 0.37258198857307434}, {"x": 0.4836407005786896, "y": 0.37258198857307434}], "text": "59.3\n"}
{"page": 5, "bbox": [{"x": 0.7096965909004211, "y": 0.3662741780281067}, {"x": 0.7370612621307373, "y": 0.3662741780281067}, {"x": 0.7370612621307373, "y": 0.37258198857307434}, {"x": 0.7096965909004211, "y": 0.37258198857307434}], "text": "44.0\n"}
{"page": 5, "bbox": [{"x": 0.3367043435573578, "y": 0.3658536672592163}, {"x": 0.3622843623161316, "y": 0.3658536672592163}, {"x": 0.3622843623161316, "y": 0.3730025291442871}, {"x": 0.3367043435573578, "y": 0.3730025291442871}], "text": "69.1\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.3662741780281067}, {"x": 0.2623438537120819, "y": 0.3662741780281067}, {"x": 0.2623438537120819, "y": 0.374684602022171}, {"x": 0.2331945300102234, "y": 0.374684602022171}], "text": "HYDE\n"}
{"page": 5, "bbox": [{"x": 0.2325996458530426, "y": 0.38309502601623535}, {"x": 0.37656158208847046, "y": 0.38477712869644165}, {"x": 0.37656158208847046, "y": 0.39402860403060913}, {"x": 0.2325996458530426, "y": 0.39234650135040283}], "text": "w/ relevance judgement\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.39571067690849304}, {"x": 0.26115408539772034, "y": 0.3961312174797058}, {"x": 0.26115408539772034, "y": 0.4032800793647766}, {"x": 0.2331945300102234, "y": 0.40285953879356384}], "text": "DPR\n"}
{"page": 5, "bbox": [{"x": 0.4051159918308258, "y": 0.3961312174797058}, {"x": 0.42891135811805725, "y": 0.3961312174797058}, {"x": 0.42891135811805725, "y": 0.40285953879356384}, {"x": 0.4051159918308258, "y": 0.40285953879356384}], "text": "17.5\n"}
{"page": 5, "bbox": [{"x": 0.48423558473587036, "y": 0.3961312174797058}, {"x": 0.5086258053779602, "y": 0.39571067690849304}, {"x": 0.5086258053779602, "y": 0.40285953879356384}, {"x": 0.48423558473587036, "y": 0.4032800793647766}], "text": "33.2\n"}
{"page": 5, "bbox": [{"x": 0.5550267696380615, "y": 0.3965517282485962}, {"x": 0.5817965269088745, "y": 0.3965517282485962}, {"x": 0.5817965269088745, "y": 0.40285953879356384}, {"x": 0.5550267696380615, "y": 0.40285953879356384}], "text": "29.5\n"}
{"page": 5, "bbox": [{"x": 0.3367043435573578, "y": 0.3961312174797058}, {"x": 0.3622843623161316, "y": 0.3965517282485962}, {"x": 0.3622843623161316, "y": 0.4032800793647766}, {"x": 0.3367043435573578, "y": 0.40285953879356384}], "text": "31.8\n"}
{"page": 5, "bbox": [{"x": 0.6192742586135864, "y": 0.3961312174797058}, {"x": 0.6466389298439026, "y": 0.3961312174797058}, {"x": 0.6466389298439026, "y": 0.4032800793647766}, {"x": 0.6192742586135864, "y": 0.4032800793647766}], "text": "26.3\n"}
{"page": 5, "bbox": [{"x": 0.7114812731742859, "y": 0.3965517282485962}, {"x": 0.7352766394615173, "y": 0.3965517282485962}, {"x": 0.7352766394615173, "y": 0.4032800793647766}, {"x": 0.7114812731742859, "y": 0.4032800793647766}], "text": "16.1\n"}
{"page": 5, "bbox": [{"x": 0.3372992277145386, "y": 0.4074852764606476}, {"x": 0.3628792464733124, "y": 0.40790581703186035}, {"x": 0.3628792464733124, "y": 0.41505467891693115}, {"x": 0.3372992277145386, "y": 0.4146341383457184}], "text": "50.7\n"}
{"page": 5, "bbox": [{"x": 0.4836407005786896, "y": 0.40790581703186035}, {"x": 0.510410487651825, "y": 0.40790581703186035}, {"x": 0.510410487651825, "y": 0.4146341383457184}, {"x": 0.4836407005786896, "y": 0.4146341383457184}], "text": "65.4\n"}
{"page": 5, "bbox": [{"x": 0.40333133935928345, "y": 0.40832632780075073}, {"x": 0.42950624227523804, "y": 0.40832632780075073}, {"x": 0.42950624227523804, "y": 0.4146341383457184}, {"x": 0.40333133935928345, "y": 0.4146341383457184}], "text": "41.5\n"}
{"page": 5, "bbox": [{"x": 0.5556216239929199, "y": 0.40832632780075073}, {"x": 0.5812016725540161, "y": 0.40832632780075073}, {"x": 0.5812016725540161, "y": 0.4146341383457184}, {"x": 0.5556216239929199, "y": 0.4146341383457184}], "text": "30.0\n"}
{"page": 5, "bbox": [{"x": 0.7108863592147827, "y": 0.40832632780075073}, {"x": 0.7358714938163757, "y": 0.40832632780075073}, {"x": 0.7358714938163757, "y": 0.4146341383457184}, {"x": 0.7108863592147827, "y": 0.4146341383457184}], "text": "38.2\n"}
{"page": 5, "bbox": [{"x": 0.2325996458530426, "y": 0.40790581703186035}, {"x": 0.27364665269851685, "y": 0.40790581703186035}, {"x": 0.27364665269851685, "y": 0.41505467891693115}, {"x": 0.2325996458530426, "y": 0.41505467891693115}], "text": "ANCE\n"}
{"page": 5, "bbox": [{"x": 0.6192742586135864, "y": 0.40832632780075073}, {"x": 0.645449161529541, "y": 0.40832632780075073}, {"x": 0.645449161529541, "y": 0.41505467891693115}, {"x": 0.6192742586135864, "y": 0.41505467891693115}], "text": "28.1\n"}
{"page": 5, "bbox": [{"x": 0.2325996458530426, "y": 0.41883936524391174}, {"x": 0.3628792464733124, "y": 0.41673675179481506}, {"x": 0.36347413063049316, "y": 0.4268292784690857}, {"x": 0.2331945300102234, "y": 0.42893186211586}], "text": "ContrieverFT 67.7\n"}
{"page": 5, "bbox": [{"x": 0.40273645520210266, "y": 0.42052143812179565}, {"x": 0.4301011264324188, "y": 0.4201009273529053}, {"x": 0.4301011264324188, "y": 0.4268292784690857}, {"x": 0.40273645520210266, "y": 0.4272497892379761}], "text": "44.6\n"}
{"page": 5, "bbox": [{"x": 0.4836407005786896, "y": 0.42052143812179565}, {"x": 0.5098155736923218, "y": 0.4201009273529053}, {"x": 0.5098155736923218, "y": 0.4268292784690857}, {"x": 0.4836407005786896, "y": 0.4272497892379761}], "text": "59.6\n"}
{"page": 5, "bbox": [{"x": 0.6192742586135864, "y": 0.42052143812179565}, {"x": 0.6460440158843994, "y": 0.4201009273529053}, {"x": 0.6460440158843994, "y": 0.4268292784690857}, {"x": 0.6192742586135864, "y": 0.4272497892379761}], "text": "41.3\n"}
{"page": 5, "bbox": [{"x": 0.5550267696380615, "y": 0.42052143812179565}, {"x": 0.5812016725540161, "y": 0.42052143812179565}, {"x": 0.5812016725540161, "y": 0.4272497892379761}, {"x": 0.5550267696380615, "y": 0.4272497892379761}], "text": "32.9\n"}
{"page": 5, "bbox": [{"x": 0.7096965909004211, "y": 0.42052143812179565}, {"x": 0.7358714938163757, "y": 0.42052143812179565}, {"x": 0.7358714938163757, "y": 0.4272497892379761}, {"x": 0.7096965909004211, "y": 0.4272497892379761}], "text": "42.8\n"}
{"page": 5, "bbox": [{"x": 0.4616299867630005, "y": 0.4390243887901306}, {"x": 0.5377751588821411, "y": 0.4390243887901306}, {"x": 0.5377751588821411, "y": 0.4461732506752014}, {"x": 0.4616299867630005, "y": 0.4461732506752014}], "text": "Recall@100\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.449957937002182}, {"x": 0.3837001919746399, "y": 0.45121949911117554}, {"x": 0.3837001919746399, "y": 0.46005046367645264}, {"x": 0.2331945300102234, "y": 0.4587889015674591}], "text": "w/o relevance judgement\n"}
{"page": 5, "bbox": [{"x": 0.5556216239929199, "y": 0.4629940986633301}, {"x": 0.5812016725540161, "y": 0.4625735878944397}, {"x": 0.5812016725540161, "y": 0.4693019390106201}, {"x": 0.5556216239929199, "y": 0.4697224497795105}], "text": "54.0\n"}
{"page": 5, "bbox": [{"x": 0.336109459400177, "y": 0.4625735878944397}, {"x": 0.3628792464733124, "y": 0.4629940986633301}, {"x": 0.3622843623161316, "y": 0.47056350111961365}, {"x": 0.3355145752429962, "y": 0.47014299035072327}], "text": "92.5\n"}
{"page": 5, "bbox": [{"x": 0.4830458164215088, "y": 0.46341463923454285}, {"x": 0.5098155736923218, "y": 0.46341463923454285}, {"x": 0.5098155736923218, "y": 0.4697224497795105}, {"x": 0.4830458164215088, "y": 0.4697224497795105}], "text": "49.8\n"}
{"page": 5, "bbox": [{"x": 0.6192742586135864, "y": 0.46341463923454285}, {"x": 0.645449161529541, "y": 0.46341463923454285}, {"x": 0.645449161529541, "y": 0.4697224497795105}, {"x": 0.6192742586135864, "y": 0.4697224497795105}], "text": "46.8\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.46341463923454285}, {"x": 0.27007734775543213, "y": 0.46341463923454285}, {"x": 0.27007734775543213, "y": 0.47014299035072327}, {"x": 0.2331945300102234, "y": 0.47014299035072327}], "text": "BM25\n"}
{"page": 5, "bbox": [{"x": 0.40333133935928345, "y": 0.46341463923454285}, {"x": 0.42950624227523804, "y": 0.46341463923454285}, {"x": 0.42950624227523804, "y": 0.47014299035072327}, {"x": 0.40333133935928345, "y": 0.47014299035072327}], "text": "93.2\n"}
{"page": 5, "bbox": [{"x": 0.7091017365455627, "y": 0.46341463923454285}, {"x": 0.7358714938163757, "y": 0.46341463923454285}, {"x": 0.7358714938163757, "y": 0.47014299035072327}, {"x": 0.7091017365455627, "y": 0.47014299035072327}], "text": "44.7\n"}
{"page": 5, "bbox": [{"x": 0.2325996458530426, "y": 0.47434818744659424}, {"x": 0.29625222086906433, "y": 0.47392767667770386}, {"x": 0.29625222086906433, "y": 0.48149704933166504}, {"x": 0.2325996458530426, "y": 0.4819175899028778}], "text": "Contriever\n"}
{"page": 5, "bbox": [{"x": 0.40392622351646423, "y": 0.47392767667770386}, {"x": 0.42950624227523804, "y": 0.47434818744659424}, {"x": 0.42891135811805725, "y": 0.4819175899028778}, {"x": 0.40333133935928345, "y": 0.48149704933166504}], "text": "90.1\n"}
{"page": 5, "bbox": [{"x": 0.5550267696380615, "y": 0.47434818744659424}, {"x": 0.5806067585945129, "y": 0.47392767667770386}, {"x": 0.5812016725540161, "y": 0.48149704933166504}, {"x": 0.5556216239929199, "y": 0.4819175899028778}], "text": "56.2\n"}
{"page": 5, "bbox": [{"x": 0.336109459400177, "y": 0.474768728017807}, {"x": 0.3628792464733124, "y": 0.474768728017807}, {"x": 0.3628792464733124, "y": 0.48149704933166504}, {"x": 0.336109459400177, "y": 0.48149704933166504}], "text": "92.6\n"}
{"page": 5, "bbox": [{"x": 0.7096965909004211, "y": 0.474768728017807}, {"x": 0.7358714938163757, "y": 0.474768728017807}, {"x": 0.7358714938163757, "y": 0.48149704933166504}, {"x": 0.7096965909004211, "y": 0.48149704933166504}], "text": "42.3\n"}
{"page": 5, "bbox": [{"x": 0.48483046889305115, "y": 0.4751892387866974}, {"x": 0.5092207193374634, "y": 0.4751892387866974}, {"x": 0.5092207193374634, "y": 0.48149704933166504}, {"x": 0.48483046889305115, "y": 0.48149704933166504}], "text": "17.2\n"}
{"page": 5, "bbox": [{"x": 0.6192742586135864, "y": 0.474768728017807}, {"x": 0.6466389298439026, "y": 0.474768728017807}, {"x": 0.6466389298439026, "y": 0.4819175899028778}, {"x": 0.6192742586135864, "y": 0.4819175899028778}], "text": "45.3\n"}
{"page": 5, "bbox": [{"x": 0.5556216239929199, "y": 0.48654332756996155}, {"x": 0.5800119042396545, "y": 0.48654332756996155}, {"x": 0.5800119042396545, "y": 0.4932716488838196}, {"x": 0.5556216239929199, "y": 0.4932716488838196}], "text": "62.1\n"}
{"page": 5, "bbox": [{"x": 0.6192742586135864, "y": 0.48654332756996155}, {"x": 0.6460440158843994, "y": 0.48654332756996155}, {"x": 0.6460440158843994, "y": 0.4932716488838196}, {"x": 0.6192742586135864, "y": 0.4932716488838196}], "text": "47.2\n"}
{"page": 5, "bbox": [{"x": 0.4830458164215088, "y": 0.48696383833885193}, {"x": 0.5092207193374634, "y": 0.48696383833885193}, {"x": 0.5092207193374634, "y": 0.4932716488838196}, {"x": 0.4830458164215088, "y": 0.4932716488838196}], "text": "41.4\n"}
{"page": 5, "bbox": [{"x": 0.7096965909004211, "y": 0.48696383833885193}, {"x": 0.7364664077758789, "y": 0.48696383833885193}, {"x": 0.7364664077758789, "y": 0.4932716488838196}, {"x": 0.7096965909004211, "y": 0.4932716488838196}], "text": "50.9\n"}
{"page": 5, "bbox": [{"x": 0.3367043435573578, "y": 0.48654332756996155}, {"x": 0.3622843623161316, "y": 0.48654332756996155}, {"x": 0.3622843623161316, "y": 0.49369218945503235}, {"x": 0.3367043435573578, "y": 0.49369218945503235}], "text": "96.4\n"}
{"page": 5, "bbox": [{"x": 0.40333133935928345, "y": 0.48696383833885193}, {"x": 0.42891135811805725, "y": 0.48696383833885193}, {"x": 0.42891135811805725, "y": 0.49369218945503235}, {"x": 0.40333133935928345, "y": 0.49369218945503235}], "text": "97.9\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.4861227869987488}, {"x": 0.2623438537120819, "y": 0.48654332756996155}, {"x": 0.2617489695549011, "y": 0.49579477310180664}, {"x": 0.2325996458530426, "y": 0.49537426233291626}], "text": "HYDE\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.5042052268981934}, {"x": 0.3759666979312897, "y": 0.5058873295783997}, {"x": 0.3759666979312897, "y": 0.514718234539032}, {"x": 0.2331945300102234, "y": 0.5130361914634705}], "text": "w/ relevance judgement\n"}
{"page": 5, "bbox": [{"x": 0.2331945300102234, "y": 0.5164003372192383}, {"x": 0.2617489695549011, "y": 0.5168208479881287}, {"x": 0.2617489695549011, "y": 0.5239697098731995}, {"x": 0.2331945300102234, "y": 0.5235491991043091}], "text": "DPR\n"}
{"page": 5, "bbox": [{"x": 0.3367043435573578, "y": 0.5168208479881287}, {"x": 0.3628792464733124, "y": 0.5168208479881287}, {"x": 0.3628792464733124, "y": 0.5235491991043091}, {"x": 0.3367043435573578, "y": 0.5235491991043091}], "text": "72.7\n"}
{"page": 5, "bbox": [{"x": 0.5556216239929199, "y": 0.5168208479881287}, {"x": 0.5806067585945129, "y": 0.5164003372192383}, {"x": 0.5806067585945129, "y": 0.5235491991043091}, {"x": 0.5556216239929199, "y": 0.5239697098731995}], "text": "34.2\n"}
{"page": 5, "bbox": [{"x": 0.4836407005786896, "y": 0.517241358757019}, {"x": 0.5092207193374634, "y": 0.517241358757019}, {"x": 0.5092207193374634, "y": 0.5235491991043091}, {"x": 0.4836407005786896, "y": 0.5235491991043091}], "text": "21.2\n"}
{"page": 5, "bbox": [{"x": 0.620464026927948, "y": 0.517241358757019}, {"x": 0.6448542475700378, "y": 0.517241358757019}, {"x": 0.6448542475700378, "y": 0.5235491991043091}, {"x": 0.620464026927948, "y": 0.5235491991043091}], "text": "34.9\n"}
{"page": 5, "bbox": [{"x": 0.7096965909004211, "y": 0.517241358757019}, {"x": 0.7352766394615173, "y": 0.517241358757019}, {"x": 0.7352766394615173, "y": 0.5235491991043091}, {"x": 0.7096965909004211, "y": 0.5235491991043091}], "text": "21.5\n"}
{"page": 5, "bbox": [{"x": 0.40392622351646423, "y": 0.517241358757019}, {"x": 0.42831647396087646, "y": 0.517241358757019}, {"x": 0.42831647396087646, "y": 0.5239697098731995}, {"x": 0.40392622351646423, "y": 0.5239697098731995}], "text": "75.1\n"}
{"page": 5, "bbox": [{"x": 0.3372992277145386, "y": 0.5281749367713928}, {"x": 0.3628792464733124, "y": 0.5290159583091736}, {"x": 0.3622843623161316, "y": 0.5361648201942444}, {"x": 0.3367043435573578, "y": 0.5353237986564636}], "text": "81.6\n"}
{"page": 5, "bbox": [{"x": 0.2325996458530426, "y": 0.5290159583091736}, {"x": 0.2724568843841553, "y": 0.5285954475402832}, {"x": 0.2724568843841553, "y": 0.535744309425354}, {"x": 0.2325996458530426, "y": 0.5361648201942444}], "text": "ANCE\n"}
{"page": 5, "bbox": [{"x": 0.40333133935928345, "y": 0.5290159583091736}, {"x": 0.42950624227523804, "y": 0.5290159583091736}, {"x": 0.42950624227523804, "y": 0.535744309425354}, {"x": 0.40333133935928345, "y": 0.535744309425354}], "text": "93.7\n"}
{"page": 5, "bbox": [{"x": 0.5556216239929199, "y": 0.5285954475402832}, {"x": 0.5812016725540161, "y": 0.5290159583091736}, {"x": 0.5812016725540161, "y": 0.5361648201942444}, {"x": 0.5556216239929199, "y": 0.535744309425354}], "text": "58.1\n"}
{"page": 5, "bbox": [{"x": 0.4830458164215088, "y": 0.5294365286827087}, {"x": 0.510410487651825, "y": 0.5294365286827087}, {"x": 0.510410487651825, "y": 0.535744309425354}, {"x": 0.4830458164215088, "y": 0.535744309425354}], "text": "45.7\n"}
{"page": 5, "bbox": [{"x": 0.620464026927948, "y": 0.5294365286827087}, {"x": 0.6442593932151794, "y": 0.5294365286827087}, {"x": 0.6442593932151794, "y": 0.535744309425354}, {"x": 0.620464026927948, "y": 0.535744309425354}], "text": "31.9\n"}
{"page": 5, "bbox": [{"x": 0.7102915048599243, "y": 0.5294365286827087}, {"x": 0.7346817255020142, "y": 0.5294365286827087}, {"x": 0.7346817255020142, "y": 0.535744309425354}, {"x": 0.7102915048599243, "y": 0.535744309425354}], "text": "39.8\n"}
{"page": 5, "bbox": [{"x": 0.2325996458530426, "y": 0.5399495363235474}, {"x": 0.30874478816986084, "y": 0.5382674336433411}, {"x": 0.3093396723270416, "y": 0.5479394197463989}, {"x": 0.2331945300102234, "y": 0.5496215224266052}], "text": "ContrieverFT\n"}
{"page": 5, "bbox": [{"x": 0.7096965909004211, "y": 0.5412111282348633}, {"x": 0.7364664077758789, "y": 0.5416316390037537}, {"x": 0.7364664077758789, "y": 0.5483599901199341}, {"x": 0.7096965909004211, "y": 0.5479394197463989}], "text": "49.2\n"}
{"page": 5, "bbox": [{"x": 0.40333133935928345, "y": 0.5416316390037537}, {"x": 0.42891135811805725, "y": 0.5416316390037537}, {"x": 0.42891135811805725, "y": 0.5483599901199341}, {"x": 0.40333133935928345, "y": 0.5483599901199341}], "text": "97.7\n"}
{"page": 5, "bbox": [{"x": 0.4836407005786896, "y": 0.5416316390037537}, {"x": 0.5092207193374634, "y": 0.5416316390037537}, {"x": 0.5092207193374634, "y": 0.5483599901199341}, {"x": 0.4836407005786896, "y": 0.5483599901199341}], "text": "40.7\n"}
{"page": 5, "bbox": [{"x": 0.5556216239929199, "y": 0.5412111282348633}, {"x": 0.5817965269088745, "y": 0.5412111282348633}, {"x": 0.5817965269088745, "y": 0.5487805008888245}, {"x": 0.5556216239929199, "y": 0.5487805008888245}], "text": "65.6\n"}
{"page": 5, "bbox": [{"x": 0.3367043435573578, "y": 0.5416316390037537}, {"x": 0.3622843623161316, "y": 0.542052149772644}, {"x": 0.3622843623161316, "y": 0.5492010116577148}, {"x": 0.3367043435573578, "y": 0.5487805008888245}], "text": "94.7\n"}
{"page": 5, "bbox": [{"x": 0.6198691129684448, "y": 0.542052149772644}, {"x": 0.645449161529541, "y": 0.542052149772644}, {"x": 0.645449161529541, "y": 0.5487805008888245}, {"x": 0.6198691129684448, "y": 0.5487805008888245}], "text": "54.1\n"}
{"page": 5, "bbox": [{"x": 0.12611541152000427, "y": 0.568965494632721}, {"x": 0.8720999360084534, "y": 0.568965494632721}, {"x": 0.8720999360084534, "y": 0.5798990726470947}, {"x": 0.12611541152000427, "y": 0.5798990726470947}], "text": "Table 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked bold.\n"}
{"page": 5, "bbox": [{"x": 0.11838191747665405, "y": 0.6080740094184875}, {"x": 0.4860202372074127, "y": 0.6076534986495972}, {"x": 0.4860202372074127, "y": 0.6686291098594666}, {"x": 0.11838191747665405, "y": 0.6690496206283569}], "text": "both precision-oriented and recall metrics. While\nunsupervised Contriever can underperform the\nclassical BM25 approach, HYDE outperforms BM25\nby large margins.\n"}
{"page": 5, "bbox": [{"x": 0.5127900242805481, "y": 0.6093356013298035}, {"x": 0.8839976191520691, "y": 0.6093356013298035}, {"x": 0.8839976191520691, "y": 0.9188393354415894}, {"x": 0.5127900242805481, "y": 0.9188393354415894}], "text": "4.3 Low Resource Retrieval\nIn Table 2, we show retrieval results on low-\nresource tasks from BEIR. Similar to web\nsearch, HYDE again brings sizable improvements to\nContriever across the board in terms of both ndcg\nand recall. HYDE is only outperformed by BM25 on\none dataset, TREC-Covid but with a tiny 0.2 mar-\ngin; in comparison, the underlying Contriever\nunderperforms by more than 50%.\nWe also observe HyDE demonstrates strong\nperformance compared to fine-tuned models.\nHYDE generally shows better performance than\nANCE and DPR, even though the two are\nfine-tuned on MS-MARCO and ANCE also in-\nvolves some sophisticated hard negative techniques.\nContriever shows performance advantages on\nFiQA and DBPedia. These involve retrieval of fi-\nnancial posts or entities respectively. We believe\nthe performance difference can be attributed to the\n"}
{"page": 5, "bbox": [{"x": 0.11838191747665405, "y": 0.7291842103004456}, {"x": 0.48899465799331665, "y": 0.7291842103004456}, {"x": 0.48899465799331665, "y": 0.9158957004547119}, {"x": 0.11838191747665405, "y": 0.9158957004547119}], "text": "HYDE remains competitive even when compared\nto fine-tuned models. Note that TREC DL19/20\nare search tasks defined on MS-MARCO and\nthere, all the fine-tuned models are richly super-\nvised. On TREC DL19, HYDE shows comparable\nmap and ndcg@ 10 to Contriever FT and best re-\ncall@1k. On DL20, HyDE gets around 10% lower\nmap and ndcg@ 10 than Contriever FT and sim-\nilar recall @1k. The ANCE model shows better\nndcg@ 10 numbers than HyDE but lower recall, sug-\ngesting it may be biased to a subset of queries\nand/or relevant documents.\n"}
{"page": 6, "bbox": [{"x": 0.7245687246322632, "y": 0.08915054798126221}, {"x": 0.8138012886047363, "y": 0.08873002231121063}, {"x": 0.8138012886047363, "y": 0.09671993553638458}, {"x": 0.7245687246322632, "y": 0.09714045375585556}], "text": "DL19 DL20\n"}
{"page": 6, "bbox": [{"x": 0.24033313989639282, "y": 0.08915054798126221}, {"x": 0.4646044075489044, "y": 0.08915054798126221}, {"x": 0.4646044075489044, "y": 0.09756097197532654}, {"x": 0.24033313989639282, "y": 0.09756097197532654}], "text": "Swahili Korean Japanese Bengali\n"}
{"page": 6, "bbox": [{"x": 0.5800119042396545, "y": 0.08999159187078476}, {"x": 0.6198691129684448, "y": 0.08999159187078476}, {"x": 0.6198691129684448, "y": 0.09671993553638458}, {"x": 0.5800119042396545, "y": 0.09671993553638458}], "text": "Model\n"}
{"page": 6, "bbox": [{"x": 0.1427721530199051, "y": 0.10513035953044891}, {"x": 0.27840569615364075, "y": 0.10639192909002304}, {"x": 0.27840569615364075, "y": 0.11480235308408737}, {"x": 0.1427721530199051, "y": 0.11354079097509384}], "text": "w/o relevance judgement\n"}
{"page": 6, "bbox": [{"x": 0.7834622263908386, "y": 0.10849453508853912}, {"x": 0.80904221534729, "y": 0.1089150533080101}, {"x": 0.80904221534729, "y": 0.1160639226436615}, {"x": 0.7834622263908386, "y": 0.11564339697360992}], "text": "42.1\n"}
{"page": 6, "bbox": [{"x": 0.5800119042396545, "y": 0.10849453508853912}, {"x": 0.6442593932151794, "y": 0.1089150533080101}, {"x": 0.6442593932151794, "y": 0.11648444086313248}, {"x": 0.5800119042396545, "y": 0.1160639226436615}], "text": "Contriever\n"}
{"page": 6, "bbox": [{"x": 0.7281380295753479, "y": 0.10933557897806168}, {"x": 0.7543129324913025, "y": 0.10933557897806168}, {"x": 0.7543129324913025, "y": 0.11564339697360992}, {"x": 0.7281380295753479, "y": 0.11564339697360992}], "text": "44.5\n"}
{"page": 6, "bbox": [{"x": 0.36823320388793945, "y": 0.11690495908260345}, {"x": 0.3920285403728485, "y": 0.11690495908260345}, {"x": 0.3920285403728485, "y": 0.12279225885868073}, {"x": 0.36823320388793945, "y": 0.12279225885868073}], "text": "21.2\n"}
{"page": 6, "bbox": [{"x": 0.4312908947467804, "y": 0.11690495908260345}, {"x": 0.45508626103401184, "y": 0.11690495908260345}, {"x": 0.45508626103401184, "y": 0.12279225885868073}, {"x": 0.4312908947467804, "y": 0.12279225885868073}], "text": "41.8\n"}
{"page": 6, "bbox": [{"x": 0.14336703717708588, "y": 0.11690495908260345}, {"x": 0.1760856658220291, "y": 0.11690495908260345}, {"x": 0.1760856658220291, "y": 0.1232127845287323}, {"x": 0.14336703717708588, "y": 0.1232127845287323}], "text": "BM25\n"}
{"page": 6, "bbox": [{"x": 0.24925640225410461, "y": 0.11690495908260345}, {"x": 0.2712671160697937, "y": 0.11690495908260345}, {"x": 0.2712671160697937, "y": 0.1232127845287323}, {"x": 0.24925640225410461, "y": 0.1232127845287323}], "text": "38.9\n"}
{"page": 6, "bbox": [{"x": 0.3063652515411377, "y": 0.11732548475265503}, {"x": 0.33016061782836914, "y": 0.11732548475265503}, {"x": 0.33016061782836914, "y": 0.1232127845287323}, {"x": 0.3063652515411377, "y": 0.1232127845287323}], "text": "28.5\n"}
{"page": 6, "bbox": [{"x": 0.5794169902801514, "y": 0.11984860897064209}, {"x": 0.6567519307136536, "y": 0.11858704686164856}, {"x": 0.6573468446731567, "y": 0.12783852219581604}, {"x": 0.5800119042396545, "y": 0.12910008430480957}], "text": "ContrieverFT\n"}
{"page": 6, "bbox": [{"x": 0.7293277978897095, "y": 0.12068965286016464}, {"x": 0.7537180185317993, "y": 0.1215306967496872}, {"x": 0.7531231641769409, "y": 0.128679558634758}, {"x": 0.7287328839302063, "y": 0.12783852219581604}], "text": "62.1\n"}
{"page": 6, "bbox": [{"x": 0.784057080745697, "y": 0.1215306967496872}, {"x": 0.8102319836616516, "y": 0.1215306967496872}, {"x": 0.8102319836616516, "y": 0.12825904786586761}, {"x": 0.784057080745697, "y": 0.12825904786586761}], "text": "63.2\n"}
{"page": 6, "bbox": [{"x": 0.24866151809692383, "y": 0.12741799652576447}, {"x": 0.2718620002269745, "y": 0.12741799652576447}, {"x": 0.2718620002269745, "y": 0.13330529630184174}, {"x": 0.24866151809692383, "y": 0.13330529630184174}], "text": "38.3\n"}
{"page": 6, "bbox": [{"x": 0.369422972202301, "y": 0.12741799652576447}, {"x": 0.3920285403728485, "y": 0.12741799652576447}, {"x": 0.3920285403728485, "y": 0.13330529630184174}, {"x": 0.369422972202301, "y": 0.13330529630184174}], "text": "19.5\n"}
{"page": 6, "bbox": [{"x": 0.4318857789039612, "y": 0.12741799652576447}, {"x": 0.45508626103401184, "y": 0.12741799652576447}, {"x": 0.45508626103401184, "y": 0.13330529630184174}, {"x": 0.4318857789039612, "y": 0.13330529630184174}], "text": "35.3\n"}
{"page": 6, "bbox": [{"x": 0.14336703717708588, "y": 0.12741799652576447}, {"x": 0.2111838161945343, "y": 0.12741799652576447}, {"x": 0.2111838161945343, "y": 0.1337258219718933}, {"x": 0.14336703717708588, "y": 0.1337258219718933}], "text": "mContriever\n"}
{"page": 6, "bbox": [{"x": 0.3063652515411377, "y": 0.12741799652576447}, {"x": 0.33016061782836914, "y": 0.12741799652576447}, {"x": 0.33016061782836914, "y": 0.1337258219718933}, {"x": 0.3063652515411377, "y": 0.1337258219718933}], "text": "22.3\n"}
{"page": 6, "bbox": [{"x": 0.3069601356983185, "y": 0.13793103396892548}, {"x": 0.33016061782836914, "y": 0.1375105082988739}, {"x": 0.33016061782836914, "y": 0.14423885941505432}, {"x": 0.3069601356983185, "y": 0.1446593701839447}], "text": "30.6\n"}
{"page": 6, "bbox": [{"x": 0.24806663393974304, "y": 0.13835155963897705}, {"x": 0.2718620002269745, "y": 0.13793103396892548}, {"x": 0.2718620002269745, "y": 0.14423885941505432}, {"x": 0.24806663393974304, "y": 0.1446593701839447}], "text": "41.7\n"}
{"page": 6, "bbox": [{"x": 0.4306960105895996, "y": 0.13835155963897705}, {"x": 0.45508626103401184, "y": 0.13835155963897705}, {"x": 0.45508626103401184, "y": 0.14423885941505432}, {"x": 0.4306960105895996, "y": 0.14423885941505432}], "text": "41.3\n"}
{"page": 6, "bbox": [{"x": 0.36882808804512024, "y": 0.13835155963897705}, {"x": 0.3914336562156677, "y": 0.13877207040786743}, {"x": 0.3914336562156677, "y": 0.1446593701839447}, {"x": 0.36882808804512024, "y": 0.14423885941505432}], "text": "30.7\n"}
{"page": 6, "bbox": [{"x": 0.14336703717708588, "y": 0.13793103396892548}, {"x": 0.17073170840740204, "y": 0.13835155963897705}, {"x": 0.17073170840740204, "y": 0.14592094719409943}, {"x": 0.14336703717708588, "y": 0.14550042152404785}], "text": "HYDE\n"}
{"page": 6, "bbox": [{"x": 0.5800119042396545, "y": 0.14045415818691254}, {"x": 0.6091611981391907, "y": 0.14003364741802216}, {"x": 0.6097561120986938, "y": 0.1480235457420349}, {"x": 0.5806067585945129, "y": 0.1484440714120865}], "text": "HYDE\n"}
{"page": 6, "bbox": [{"x": 0.5794169902801514, "y": 0.15138772130012512}, {"x": 0.6627007722854614, "y": 0.15096719563007355}, {"x": 0.6627007722854614, "y": 0.15853658318519592}, {"x": 0.5794169902801514, "y": 0.1589571088552475}], "text": "w/ Contriever\n"}
{"page": 6, "bbox": [{"x": 0.1421772688627243, "y": 0.1534903347492218}, {"x": 0.2724568843841553, "y": 0.15475189685821533}, {"x": 0.2724568843841553, "y": 0.1627417951822281}, {"x": 0.1421772688627243, "y": 0.16148023307323456}], "text": "w/ relevance judgement\n"}
{"page": 6, "bbox": [{"x": 0.7834622263908386, "y": 0.16316232085227966}, {"x": 0.8096371293067932, "y": 0.16358284652233124}, {"x": 0.8096371293067932, "y": 0.17073170840740204}, {"x": 0.7834622263908386, "y": 0.17031118273735046}], "text": "52.9\n"}
{"page": 6, "bbox": [{"x": 0.6002379655838013, "y": 0.1627417951822281}, {"x": 0.7043426632881165, "y": 0.16316232085227966}, {"x": 0.7043426632881165, "y": 0.171572744846344}, {"x": 0.6002379655838013, "y": 0.1711522340774536}], "text": "w/ Flan-T5 (11b)\n"}
{"page": 6, "bbox": [{"x": 0.7281380295753479, "y": 0.16358284652233124}, {"x": 0.7537180185317993, "y": 0.16358284652233124}, {"x": 0.7537180185317993, "y": 0.17073170840740204}, {"x": 0.7281380295753479, "y": 0.17073170840740204}], "text": "48.9\n"}
{"page": 6, "bbox": [{"x": 0.25223082304000854, "y": 0.16484440863132477}, {"x": 0.26888757944107056, "y": 0.16484440863132477}, {"x": 0.26888757944107056, "y": 0.171572744846344}, {"x": 0.25223082304000854, "y": 0.171572744846344}], "text": "7.3\n"}
{"page": 6, "bbox": [{"x": 0.369422972202301, "y": 0.16526493430137634}, {"x": 0.39083877205848694, "y": 0.16526493430137634}, {"x": 0.39083877205848694, "y": 0.1711522340774536}, {"x": 0.369422972202301, "y": 0.1711522340774536}], "text": "18.1\n"}
{"page": 6, "bbox": [{"x": 0.4312908947467804, "y": 0.16526493430137634}, {"x": 0.45449137687683105, "y": 0.16526493430137634}, {"x": 0.45449137687683105, "y": 0.1711522340774536}, {"x": 0.4312908947467804, "y": 0.1711522340774536}], "text": "25.8\n"}
{"page": 6, "bbox": [{"x": 0.1427721530199051, "y": 0.16526493430137634}, {"x": 0.17906008660793304, "y": 0.16526493430137634}, {"x": 0.17906008660793304, "y": 0.171572744846344}, {"x": 0.1427721530199051, "y": 0.171572744846344}], "text": "mDPR\n"}
{"page": 6, "bbox": [{"x": 0.3063652515411377, "y": 0.16526493430137634}, {"x": 0.33016061782836914, "y": 0.16526493430137634}, {"x": 0.33016061782836914, "y": 0.171572744846344}, {"x": 0.3063652515411377, "y": 0.171572744846344}], "text": "21.9\n"}
{"page": 6, "bbox": [{"x": 0.7834622263908386, "y": 0.1749369204044342}, {"x": 0.8084473609924316, "y": 0.17451639473438263}, {"x": 0.8084473609924316, "y": 0.182085782289505}, {"x": 0.784057080745697, "y": 0.18250630795955658}], "text": "53.8\n"}
{"page": 6, "bbox": [{"x": 0.7287328839302063, "y": 0.17577797174453735}, {"x": 0.7549077868461609, "y": 0.17577797174453735}, {"x": 0.7549077868461609, "y": 0.182085782289505}, {"x": 0.7287328839302063, "y": 0.182085782289505}], "text": "53.8\n"}
{"page": 6, "bbox": [{"x": 0.24925640225410461, "y": 0.17619848251342773}, {"x": 0.2724568843841553, "y": 0.17619848251342773}, {"x": 0.2724568843841553, "y": 0.182085782289505}, {"x": 0.24925640225410461, "y": 0.182085782289505}], "text": "37.4\n"}
{"page": 6, "bbox": [{"x": 0.36882808804512024, "y": 0.17619848251342773}, {"x": 0.3914336562156677, "y": 0.17619848251342773}, {"x": 0.3914336562156677, "y": 0.182085782289505}, {"x": 0.36882808804512024, "y": 0.182085782289505}], "text": "27.1\n"}
{"page": 6, "bbox": [{"x": 0.4318857789039612, "y": 0.17619848251342773}, {"x": 0.45449137687683105, "y": 0.17619848251342773}, {"x": 0.45449137687683105, "y": 0.182085782289505}, {"x": 0.4318857789039612, "y": 0.182085782289505}], "text": "35.1\n"}
{"page": 6, "bbox": [{"x": 0.1427721530199051, "y": 0.17619848251342773}, {"x": 0.18738846480846405, "y": 0.17619848251342773}, {"x": 0.18738846480846405, "y": 0.18250630795955658}, {"x": 0.1427721530199051, "y": 0.18250630795955658}], "text": "MBERT\n"}
{"page": 6, "bbox": [{"x": 0.3063652515411377, "y": 0.17619848251342773}, {"x": 0.33016061782836914, "y": 0.17619848251342773}, {"x": 0.33016061782836914, "y": 0.18250630795955658}, {"x": 0.3063652515411377, "y": 0.18250630795955658}], "text": "28.1\n"}
{"page": 6, "bbox": [{"x": 0.6002379655838013, "y": 0.17451639473438263}, {"x": 0.698988676071167, "y": 0.17535744607448578}, {"x": 0.698988676071167, "y": 0.18418839573860168}, {"x": 0.6002379655838013, "y": 0.18334734439849854}], "text": "w/ Cohere (52b)\n"}
{"page": 6, "bbox": [{"x": 0.1427721530199051, "y": 0.18671151995658875}, {"x": 0.18619869649410248, "y": 0.18629099428653717}, {"x": 0.18619869649410248, "y": 0.19259881973266602}, {"x": 0.1427721530199051, "y": 0.1930193454027176}], "text": "XLM-R\n"}
{"page": 6, "bbox": [{"x": 0.3069601356983185, "y": 0.18671151995658875}, {"x": 0.33016061782836914, "y": 0.18671151995658875}, {"x": 0.33016061782836914, "y": 0.19259881973266602}, {"x": 0.3069601356983185, "y": 0.19259881973266602}], "text": "32.2\n"}
{"page": 6, "bbox": [{"x": 0.36823320388793945, "y": 0.18671151995658875}, {"x": 0.3914336562156677, "y": 0.18671151995658875}, {"x": 0.3914336562156677, "y": 0.19259881973266602}, {"x": 0.36823320388793945, "y": 0.19259881973266602}], "text": "24.8\n"}
{"page": 6, "bbox": [{"x": 0.4312908947467804, "y": 0.18671151995658875}, {"x": 0.45389649271965027, "y": 0.18671151995658875}, {"x": 0.45389649271965027, "y": 0.19259881973266602}, {"x": 0.4312908947467804, "y": 0.19259881973266602}], "text": "41.7\n"}
{"page": 6, "bbox": [{"x": 0.24925640225410461, "y": 0.18713204562664032}, {"x": 0.2718620002269745, "y": 0.18713204562664032}, {"x": 0.2718620002269745, "y": 0.1930193454027176}, {"x": 0.24925640225410461, "y": 0.1930193454027176}], "text": "35.1\n"}
{"page": 6, "bbox": [{"x": 0.7287328839302063, "y": 0.1875525712966919}, {"x": 0.7555027008056641, "y": 0.1875525712966919}, {"x": 0.7555027008056641, "y": 0.19428090751171112}, {"x": 0.7287328839302063, "y": 0.19428090751171112}], "text": "61.3\n"}
{"page": 6, "bbox": [{"x": 0.7834622263908386, "y": 0.18713204562664032}, {"x": 0.80904221534729, "y": 0.1875525712966919}, {"x": 0.80904221534729, "y": 0.1947014331817627}, {"x": 0.7834622263908386, "y": 0.19428090751171112}], "text": "57.9\n"}
{"page": 6, "bbox": [{"x": 0.6002379655838013, "y": 0.18629099428653717}, {"x": 0.6924449801445007, "y": 0.1875525712966919}, {"x": 0.6924449801445007, "y": 0.19680403172969818}, {"x": 0.6002379655838013, "y": 0.19554246962070465}], "text": "w/ GPT (175b)\n"}
{"page": 6, "bbox": [{"x": 0.1427721530199051, "y": 0.1963835209608078}, {"x": 0.22248661518096924, "y": 0.19512194395065308}, {"x": 0.22248661518096924, "y": 0.2035323828458786}, {"x": 0.1427721530199051, "y": 0.20479394495487213}], "text": "mContrieverFT\n"}
{"page": 6, "bbox": [{"x": 0.36882808804512024, "y": 0.1980656087398529}, {"x": 0.3914336562156677, "y": 0.1980656087398529}, {"x": 0.3914336562156677, "y": 0.2035323828458786}, {"x": 0.36882808804512024, "y": 0.2035323828458786}], "text": "32.4\n"}
{"page": 6, "bbox": [{"x": 0.4312908947467804, "y": 0.19764508306980133}, {"x": 0.45508626103401184, "y": 0.19764508306980133}, {"x": 0.45508626103401184, "y": 0.20395290851593018}, {"x": 0.4312908947467804, "y": 0.20395290851593018}], "text": "42.3\n"}
{"page": 6, "bbox": [{"x": 0.24866151809692383, "y": 0.1980656087398529}, {"x": 0.2712671160697937, "y": 0.1980656087398529}, {"x": 0.2712671160697937, "y": 0.20395290851593018}, {"x": 0.24866151809692383, "y": 0.20395290851593018}], "text": "51.2\n"}
{"page": 6, "bbox": [{"x": 0.3069601356983185, "y": 0.1980656087398529}, {"x": 0.32956573367118835, "y": 0.1980656087398529}, {"x": 0.32956573367118835, "y": 0.20395290851593018}, {"x": 0.3069601356983185, "y": 0.20395290851593018}], "text": "34.2\n"}
{"page": 6, "bbox": [{"x": 0.5794169902801514, "y": 0.1980656087398529}, {"x": 0.6751933097839355, "y": 0.19680403172969818}, {"x": 0.6751933097839355, "y": 0.2068965584039688}, {"x": 0.5794169902801514, "y": 0.20815812051296234}], "text": "w/ ContrieverFT\n"}
{"page": 6, "bbox": [{"x": 0.7287328839302063, "y": 0.21194280683994293}, {"x": 0.7543129324913025, "y": 0.21152228116989136}, {"x": 0.7543129324913025, "y": 0.21825063228607178}, {"x": 0.7287328839302063, "y": 0.21867115795612335}], "text": "60.2\n"}
{"page": 6, "bbox": [{"x": 0.784057080745697, "y": 0.21152228116989136}, {"x": 0.80904221534729, "y": 0.21194280683994293}, {"x": 0.80904221534729, "y": 0.2195121943950653}, {"x": 0.784057080745697, "y": 0.21909166872501373}], "text": "62.1\n"}
{"page": 6, "bbox": [{"x": 0.6008328199386597, "y": 0.21110177040100098}, {"x": 0.7049375176429749, "y": 0.21194280683994293}, {"x": 0.7049375176429749, "y": 0.2211942821741104}, {"x": 0.6008328199386597, "y": 0.22035323083400726}], "text": "w/ Flan-T5 (11b)\n"}
{"page": 6, "bbox": [{"x": 0.784057080745697, "y": 0.22371740639209747}, {"x": 0.80904221534729, "y": 0.2232968807220459}, {"x": 0.80904221534729, "y": 0.23002523183822632}, {"x": 0.784057080745697, "y": 0.2304457575082779}], "text": "63.1\n"}
{"page": 6, "bbox": [{"x": 0.7287328839302063, "y": 0.22413793206214905}, {"x": 0.7549077868461609, "y": 0.22413793206214905}, {"x": 0.7549077868461609, "y": 0.2304457575082779}, {"x": 0.7287328839302063, "y": 0.2304457575082779}], "text": "61.4\n"}
{"page": 6, "bbox": [{"x": 0.6008328199386597, "y": 0.22287636995315552}, {"x": 0.698988676071167, "y": 0.22371740639209747}, {"x": 0.698988676071167, "y": 0.232127845287323}, {"x": 0.6008328199386597, "y": 0.23128679394721985}], "text": "w/ Cohere (52b)\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.22455845773220062}, {"x": 0.4860202372074127, "y": 0.22455845773220062}, {"x": 0.4860202372074127, "y": 0.24852816760540009}, {"x": 0.11838191747665405, "y": 0.24852816760540009}], "text": "Table 3: MRR@100 on Mr.Tydi. Best performing w/o\nrelevance and overall system(s) are marked bold.\n"}
{"page": 6, "bbox": [{"x": 0.7293277978897095, "y": 0.23465096950531006}, {"x": 0.8096371293067932, "y": 0.23507149517536163}, {"x": 0.8096371293067932, "y": 0.2430613934993744}, {"x": 0.7293277978897095, "y": 0.24264086782932281}], "text": "67.4 63.5\n"}
{"page": 6, "bbox": [{"x": 0.6002379655838013, "y": 0.2338099181652069}, {"x": 0.6918500661849976, "y": 0.23507149517536163}, {"x": 0.6918500661849976, "y": 0.24432295560836792}, {"x": 0.6002379655838013, "y": 0.2430613934993744}], "text": "w/ GPT (175b)\n"}
{"page": 6, "bbox": [{"x": 0.8405711054801941, "y": 0.2624053955078125}, {"x": 0.881618082523346, "y": 0.26324641704559326}, {"x": 0.881618082523346, "y": 0.2716568410396576}, {"x": 0.8405711054801941, "y": 0.27081581950187683}], "text": "Effect\n"}
{"page": 6, "bbox": [{"x": 0.5133848786354065, "y": 0.26324641704559326}, {"x": 0.8828078508377075, "y": 0.2628259062767029}, {"x": 0.8828078508377075, "y": 0.31370899081230164}, {"x": 0.5133848786354065, "y": 0.3141295313835144}], "text": "Table 4: NDCG@10 on TREC DL19/20.\nof changing different instruction LMs and using fine-\ntuned encoder. Best w/o relevance and overall models\nare marked bold.\n"}
{"page": 6, "bbox": [{"x": 0.11897680163383484, "y": 0.27417999505996704}, {"x": 0.48839977383613586, "y": 0.2767031192779541}, {"x": 0.4878048896789551, "y": 0.3052985668182373}, {"x": 0.11838191747665405, "y": 0.30277544260025024}], "text": "under-specification of the instruction; more elabo-\nrative instructions may help.\n"}
{"page": 6, "bbox": [{"x": 0.5133848786354065, "y": 0.34735071659088135}, {"x": 0.881618082523346, "y": 0.34777122735977173}, {"x": 0.8810232281684875, "y": 0.4697224497795105}, {"x": 0.5127900242805481, "y": 0.4693019390106201}], "text": "models bring improvement to the unsupervised\nContriever, with larger models bringing larger\nimprovements. At the time when this paper is\nwritten, the Cohere model is still experimental\nwithout much detail disclosed. We can only\ntentatively hypothesize that training techniques\nmay have also played some role in the performance\ndifference.\n"}
{"page": 6, "bbox": [{"x": 0.11719214916229248, "y": 0.3183347284793854}, {"x": 0.48839977383613586, "y": 0.3174937069416046}, {"x": 0.48958954215049744, "y": 0.6253153681755066}, {"x": 0.11838191747665405, "y": 0.6261564493179321}], "text": "4.4 Multilingual Retrieval\nMultilingual setup poses several additional chal-\nlenges to HyDE. The small-sized contrastive en-\ncoder gets saturated as the number of languages\nscales (Conneau et al., 2020; Izacard et al., 2021).\nMeanwhile, our generative LLM faces an opposite\nissue: with languages of not as high resource as\nEnglish or French, the high capacity LLM can get\nunder-trained (Hoffmann et al., 2022).\nNevertheless, in Table 3, we still find HYDE\nable to improve the mContriever model. It can\noutperform non-Contriever models fine-tuned on\nand transferred from MS-MARCO. On the other\nhand, we do observe some margins between HyDE\nand fine-tuned mContrieverFT. Since HyDE and\nmContriever FT use similar contrastive encoders,\nwe hypothesize this is because the non-English lan-\nguages we considered are under-trained in both\npre-training and instruction learning stages.\n"}
{"page": 6, "bbox": [{"x": 0.5127900242805481, "y": 0.48864591121673584}, {"x": 0.8839976191520691, "y": 0.48822540044784546}, {"x": 0.8845925331115723, "y": 0.7640874981880188}, {"x": 0.5133848786354065, "y": 0.7645080089569092}], "text": "5.2 HyDE with Fine-tuned Encoder\nTo begin with, HyDE with fine-tuned encoder is\nnot the intended usage: HYDE is more powerful\nand irreplaceable when few relevance labels are\npresent. Here we are interested to find out if\nand how HYDE embedding can affect fine-tuned en-\ncoders. In Table 4, we see that less powerful instruc-\ntion LMs can negatively impact the overall perfor-\nmance of the fine-tuned retriever. (To remind our\nreaders, ContrieverFT is in-domain supervisedly\nfine-tuned for TREC DL19/20). The performance\ndegradations remain small. On the other hand, we\nalso observe the InstructGPT model able to fur-\nther bring up the performance, especially on DL19.\nThis suggests that there may still exist certain fac-\ntors not captured by the fine-tuned encoder but only\nby the generative model.\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.6387720704078674}, {"x": 0.22129684686660767, "y": 0.6400336623191833}, {"x": 0.22070196270942688, "y": 0.6522287726402283}, {"x": 0.11778703331947327, "y": 0.6509671807289124}], "text": "5 Analysis\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.6656854748725891}, {"x": 0.4878048896789551, "y": 0.665264904499054}, {"x": 0.4878048896789551, "y": 0.7548360228538513}, {"x": 0.11838191747665405, "y": 0.7552565336227417}], "text": "The generative LLM and contrastive encoder make\nup the backbone of HyDE. In this section, we study\nthe effect of changing their realizations. In partic-\nular, we consider smaller language models (LM)\nand fine-tuned encoders. We conduct our studies\non TREC DL19/20.\n"}
{"page": 6, "bbox": [{"x": 0.5145746469497681, "y": 0.782590389251709}, {"x": 0.6406900882720947, "y": 0.782590389251709}, {"x": 0.6406900882720947, "y": 0.792682945728302}, {"x": 0.5145746469497681, "y": 0.792682945728302}], "text": "6 Conclusion\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.7720773816108704}, {"x": 0.4872100055217743, "y": 0.7720773816108704}, {"x": 0.4872100055217743, "y": 0.9058032035827637}, {"x": 0.11838191747665405, "y": 0.9058032035827637}], "text": "5.1 Effect of Different Generative Models\nIn Table 4, we show HYDE using other\ninstruction-following language models. In\nparticular, we consider a 52-billion Cohere\nmodel (command-xlarge-20221108) and a\n11-billion FLAN model (FLAN-T5-xxl; Wei\net al. (2022)).² Generally, we observe that all\n2Model sizes are from https://crfm.stanford.edu/\n"}
{"page": 6, "bbox": [{"x": 0.5121951103210449, "y": 0.8095037937164307}, {"x": 0.8839976191520691, "y": 0.809924304485321}, {"x": 0.8839976191520691, "y": 0.9192599058151245}, {"x": 0.5121951103210449, "y": 0.9188393354415894}], "text": "At the end of the paper, we encourage the readers\nto take a moment and reflect on the HYDE model.\nCompare it to some of the other recently seen re-\ntrievers or re-ranker. These other models probably\ndiffer in their architecture, training method, and/or\ntask, but probably all of them involve modeling\nrelevance scores between a pair of query and docu-\n"}
{"page": 6, "bbox": [{"x": 0.11897680163383484, "y": 0.9087468385696411}, {"x": 0.2504461705684662, "y": 0.9087468385696411}, {"x": 0.2504461705684662, "y": 0.9158957004547119}, {"x": 0.11897680163383484, "y": 0.9158957004547119}], "text": "helm/v1.0/?models.\n"}
{"page": 7, "bbox": [{"x": 0.5139797925949097, "y": 0.08915054798126221}, {"x": 0.8839976191520691, "y": 0.08915054798126221}, {"x": 0.8839976191520691, "y": 0.2800672948360443}, {"x": 0.5139797925949097, "y": 0.2800672948360443}], "text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\n"}
{"page": 7, "bbox": [{"x": 0.11719214916229248, "y": 0.08830950409173965}, {"x": 0.48899465799331665, "y": 0.08788898587226868}, {"x": 0.48958954215049744, "y": 0.6316232085227966}, {"x": 0.11778703331947327, "y": 0.632043719291687}], "text": "ment. Dense retrievers consider vector similarities\nwhile self-attentive re-rankers regression scores. In\ncomparison, the concept of relevance in HYDE is\ncaptured by an NLG model and the language gener-\nation process. We demonstrate in many cases, HYDE\ncan be as effective as dense retrievers that learn to\nmodel numerical relevance scores. So, is numeri-\ncal relevance just a statistical artifact of language\nunderstanding? Will a weak retriever theoretically\nsuffice as the NLU & NLG models rapidly become\nstronger? Rushing to conclusions is not smart;\nmore works need to be done to get answers. With\nthis paper, we just want to raise these questions.\nConcretely in this paper, we introduce a new\nparadigm of interactions between LLM and dense\nencoder/retriever. We demonstrate (part of) rel-\nevance modeling and instruction understanding\ncan be delegated to the more powerful and flex-\nible LLM. As a consequence, the need for rele-\nvance labels is removed. We are excited to see\nhow this can be generalized further to more so-\nphisticated tasks like multi-hop retrieval/QA and\nconversational search.\nWe argue HYDE is also of practical use though not\nnecessarily over the entire lifespan of a search sys-\ntem. At the very beginning of the life of the search\nsystem, serving queries using HyDE offers perfor-\nmance comparable to a fine-tuned model, which\nno other relevance-free model can offer. As the\nsearch log grows, a supervised dense retriever can\nbe gradually rolled out. As the dense retriever\ngrows stronger, more queries will be routed to it,\nwith only less common and emerging ones going\nto HYDE backend.\n"}
{"page": 7, "bbox": [{"x": 0.5139797925949097, "y": 0.296888142824173}, {"x": 0.8845925331115723, "y": 0.2964676320552826}, {"x": 0.8851873874664307, "y": 0.554247260093689}, {"x": 0.5145746469497681, "y": 0.5546677708625793}], "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\nuating large language models trained on code.\n"}
{"page": 7, "bbox": [{"x": 0.11897680163383484, "y": 0.6686291098594666}, {"x": 0.2111838161945343, "y": 0.6682085990905762}, {"x": 0.2111838161945343, "y": 0.677880585193634}, {"x": 0.11897680163383484, "y": 0.6783010959625244}], "text": "References\n"}
{"page": 7, "bbox": [{"x": 0.11838191747665405, "y": 0.6938604116439819}, {"x": 0.4878048896789551, "y": 0.6938604116439819}, {"x": 0.4878048896789551, "y": 0.7413793206214905}, {"x": 0.11838191747665405, "y": 0.7413793206214905}], "text": "Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware re-\ntrieval with instructions.\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.5693860650062561}, {"x": 0.8839976191520691, "y": 0.5693860650062561}, {"x": 0.8839976191520691, "y": 0.8666946887969971}, {"x": 0.5133848786354065, "y": 0.8666946887969971}], "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\n"}
{"page": 7, "bbox": [{"x": 0.11897680163383484, "y": 0.761564314365387}, {"x": 0.4878048896789551, "y": 0.7611438035964966}, {"x": 0.4878048896789551, "y": 0.8372582197189331}, {"x": 0.11897680163383484, "y": 0.8376787304878235}], "text": "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\nwary, and Tong Wang. 2016. Ms marco: A human\ngenerated machine reading comprehension dataset.\n"}
{"page": 7, "bbox": [{"x": 0.11897680163383484, "y": 0.8557611703872681}, {"x": 0.48839977383613586, "y": 0.8557611703872681}, {"x": 0.48839977383613586, "y": 0.9167367815971375}, {"x": 0.11897680163383484, "y": 0.9167367815971375}], "text": "Michele Bevilacqua, Giuseppe Ottaviano, Patrick\nLewis, Wen-tau Yih, Sebastian Riedel, and Fabio\nPetroni. 2022. Autoregressive search engines: Gen-\nerating substrings as document identifiers. CORR,\nabs/2204.10628.\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.881412923336029}, {"x": 0.8834027647972107, "y": 0.881412923336029}, {"x": 0.8834027647972107, "y": 0.918418824672699}, {"x": 0.5133848786354065, "y": 0.918418824672699}], "text": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\n"}
{"page": 8, "bbox": [{"x": 0.5324211716651917, "y": 0.08788898587226868}, {"x": 0.8834027647972107, "y": 0.08915054798126221}, {"x": 0.8834027647972107, "y": 0.11354079097509384}, {"x": 0.5324211716651917, "y": 0.11227922886610031}], "text": "SIGIR '21, page 113-122, New York, NY, USA. As-\nsociation for Computing Machinery.\n"}
{"page": 8, "bbox": [{"x": 0.13622844219207764, "y": 0.08915054798126221}, {"x": 0.48899465799331665, "y": 0.08873002231121063}, {"x": 0.48899465799331665, "y": 0.16400335729122162}, {"x": 0.13622844219207764, "y": 0.1644238829612732}], "text": "moyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440-\n8451, Online. Association for Computational Lin-\nguistics.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.12531539797782898}, {"x": 0.8834027647972107, "y": 0.12531539797782898}, {"x": 0.8834027647972107, "y": 0.18629099428653717}, {"x": 0.5139797925949097, "y": 0.18629099428653717}], "text": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Towards unsupervised\ndense information retrieval with contrastive learning.\nCORR, abs/2112.09118.\n"}
{"page": 8, "bbox": [{"x": 0.11838191747665405, "y": 0.17830109596252441}, {"x": 0.4860202372074127, "y": 0.17830109596252441}, {"x": 0.4860202372074127, "y": 0.21488645672798157}, {"x": 0.11838191747665405, "y": 0.21488645672798157}], "text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\nCampos, and Ellen M. Voorhees. 2020a. Overview\nof the trec 2019 deep learning track.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.2001682072877884}, {"x": 0.8834027647972107, "y": 0.20058873295783997}, {"x": 0.8834027647972107, "y": 0.23549200594425201}, {"x": 0.5139797925949097, "y": 0.23507149517536163}], "text": "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with gpus. CORR,\nabs/1702.08734.\n"}
{"page": 8, "bbox": [{"x": 0.11838191747665405, "y": 0.22792261838912964}, {"x": 0.48839977383613586, "y": 0.2283431440591812}, {"x": 0.48839977383613586, "y": 0.2767031192779541}, {"x": 0.11838191747665405, "y": 0.27628257870674133}], "text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz,\nDaniel Fernando Campos, and Ellen M. Voorhees.\n2020b. Overview of the trec 2020 deep learning\ntrack. ArXiv, abs/2003.07820.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.2497897446155548}, {"x": 0.8845925331115723, "y": 0.24936921894550323}, {"x": 0.8845925331115723, "y": 0.35113540291786194}, {"x": 0.5139797925949097, "y": 0.3515559434890747}], "text": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 6769-\n6781, Online. Association for Computational Lin-\nguistics.\n"}
{"page": 8, "bbox": [{"x": 0.11838191747665405, "y": 0.29184189438819885}, {"x": 0.48839977383613586, "y": 0.29184189438819885}, {"x": 0.48839977383613586, "y": 0.40622371435165405}, {"x": 0.11838191747665405, "y": 0.40622371435165405}], "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171-4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.36375105381011963}, {"x": 0.8834027647972107, "y": 0.36417156457901}, {"x": 0.8834027647972107, "y": 0.4768713116645813}, {"x": 0.5139797925949097, "y": 0.4764508008956909}], "text": "Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon\nSeo. 2022. Generative multi-hop retrieval.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6086–6096, Florence,\nItaly. Association for Computational Linguistics.\n"}
{"page": 8, "bbox": [{"x": 0.11838191747665405, "y": 0.4192598760128021}, {"x": 0.48839977383613586, "y": 0.4201009273529053}, {"x": 0.48839977383613586, "y": 0.49705633521080017}, {"x": 0.11838191747665405, "y": 0.4962153136730194}], "text": "Luyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 981-993,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\n"}
{"page": 8, "bbox": [{"x": 0.5133848786354065, "y": 0.4890664517879486}, {"x": 0.8839976191520691, "y": 0.48864591121673584}, {"x": 0.8839976191520691, "y": 0.5908326506614685}, {"x": 0.5133848786354065, "y": 0.5912531614303589}], "text": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021a. Pyserini: A Python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Annual\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR\n2021), pages 2356-2362.\n"}
{"page": 8, "bbox": [{"x": 0.11838191747665405, "y": 0.5092514753341675}, {"x": 0.48839977383613586, "y": 0.5084104537963867}, {"x": 0.48899465799331665, "y": 0.5967199206352234}, {"x": 0.11897680163383484, "y": 0.5975610017776489}], "text": "Luyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense\npassage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2843-2853,\nDublin, Ireland. Association for Computational Lin-\nguistics.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.6034482717514038}, {"x": 0.8839976191520691, "y": 0.6038687825202942}, {"x": 0.8839976191520691, "y": 0.690075695514679}, {"x": 0.5139797925949097, "y": 0.6896551847457886}], "text": "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2021b. In-batch negatives for knowledge distillation\nwith tightly-coupled teachers for dense retrieval. In\nProceedings of the 6th Workshop on Representation\nLearning for NLP (RepLANLP-2021), pages 163–\n173, Online. Association for Computational Linguis-\ntics.\n"}
{"page": 8, "bbox": [{"x": 0.11897680163383484, "y": 0.6105971336364746}, {"x": 0.48899465799331665, "y": 0.6101766228675842}, {"x": 0.48899465799331665, "y": 0.6993271708488464}, {"x": 0.11897680163383484, "y": 0.6997476816177368}], "text": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894-6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.7047939300537109}, {"x": 0.8822129964828491, "y": 0.7043734192848206}, {"x": 0.8822129964828491, "y": 0.7401177287101746}, {"x": 0.5139797925949097, "y": 0.7405382394790649}], "text": "Zheng Liu and Yingxia Shao. 2022. Retromae: Pre-\ntraining retrieval-oriented transformers via masked\nauto-encoder. ArXiv, abs/2205.12035.\n"}
{"page": 8, "bbox": [{"x": 0.11838191747665405, "y": 0.7144659161567688}, {"x": 0.48839977383613586, "y": 0.7144659161567688}, {"x": 0.48839977383613586, "y": 0.8288477659225464}, {"x": 0.11838191747665405, "y": 0.8288477659225464}], "text": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.7539949417114258}, {"x": 0.8839976191520691, "y": 0.7535744309425354}, {"x": 0.8839976191520691, "y": 0.867956280708313}, {"x": 0.5139797925949097, "y": 0.8683767914772034}], "text": "Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed\nMalik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu,\nand Arnold Overwijk. 2021. Less is more: Pre-\ntrain a strong Siamese encoder for dense text re-\ntrieval using a weak decoder. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2780-2791, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\n"}
{"page": 8, "bbox": [{"x": 0.11897680163383484, "y": 0.8406223654747009}, {"x": 0.48958954215049744, "y": 0.8414633870124817}, {"x": 0.48899465799331665, "y": 0.9192599058151245}, {"x": 0.11838191747665405, "y": 0.918418824672699}], "text": "Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\nficiently teaching an effective dense retriever with\nbalanced topic aware sampling. In Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\n"}
{"page": 8, "bbox": [{"x": 0.5133848786354065, "y": 0.881412923336029}, {"x": 0.8834027647972107, "y": 0.881412923336029}, {"x": 0.8834027647972107, "y": 0.9179983139038086}, {"x": 0.5133848786354065, "y": 0.9179983139038086}], "text": "Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork.\n2021. Rethinking search: making domain experts\nout of dilettantes. SIGIR Forum, 55(1):13:1-13:27.\n"}
{"page": 9, "bbox": [{"x": 0.11778703331947327, "y": 0.08873002231121063}, {"x": 0.48958954215049744, "y": 0.08873002231121063}, {"x": 0.48958954215049744, "y": 0.178721621632576}, {"x": 0.11778703331947327, "y": 0.178721621632576}], "text": "Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791-2809, Seattle, United States.\nAssociation for Computational Linguistics.\n"}
{"page": 9, "bbox": [{"x": 0.11838191747665405, "y": 0.19512194395065308}, {"x": 0.48899465799331665, "y": 0.19512194395065308}, {"x": 0.48899465799331665, "y": 0.29520606994628906}, {"x": 0.11838191747665405, "y": 0.29520606994628906}], "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow in-\nstructions with human feedback.\n"}
{"page": 9, "bbox": [{"x": 0.5139797925949097, "y": 0.0874684602022171}, {"x": 0.8839976191520691, "y": 0.0874684602022171}, {"x": 0.8839976191520691, "y": 0.40664422512054443}, {"x": 0.5139797925949097, "y": 0.40664422512054443}], "text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V. Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Févry, Ja-\nson Alan Fries, Ryan Teehan, Teven Le Scao, Stella\nBiderman, Leo Gao, Thomas Wolf, and Alexan-\nder M. Rush. 2022. Multitask prompted training\nenables zero-shot task generalization. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Prakash Gupta, Tal Schuster, William W.\nCohen, and Donald Metzler. 2022. Transformer\nmemory as a differentiable search index. CORR,\nabs/2202.06991.\n"}
{"page": 9, "bbox": [{"x": 0.11897680163383484, "y": 0.31539109349250793}, {"x": 0.48899465799331665, "y": 0.3158116042613983}, {"x": 0.48899465799331665, "y": 0.44365012645721436}, {"x": 0.11897680163383484, "y": 0.443229615688324}], "text": "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5835-5847, Online. Association for Computational\nLinguistics.\n"}
{"page": 9, "bbox": [{"x": 0.5133848786354065, "y": 0.4217830002307892}, {"x": 0.8828078508377075, "y": 0.4213624894618988}, {"x": 0.8828078508377075, "y": 0.48149704933166504}, {"x": 0.5133848786354065, "y": 0.4819175899028778}], "text": "Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogenous benchmark for zero-shot evalu-\nation of information retrieval models. CORR,\nabs/2104.08663.\n"}
{"page": 9, "bbox": [{"x": 0.5133848786354065, "y": 0.4966358244419098}, {"x": 0.8839976191520691, "y": 0.4966358244419098}, {"x": 0.8839976191520691, "y": 0.7657695412635803}, {"x": 0.5133848786354065, "y": 0.7657695412635803}], "text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kath-\nleen S. Meier-Hellstern, Meredith Ringel Morris,\nTulsee Doshi, Renelito Delos Santos, Toju Duke,\nJohnny Soraker, Ben Zevenbergen, Vinodkumar\nPrabhakaran, Mark Diaz, Ben Hutchinson, Kristen\nOlson, Alejandra Molina, Erin Hoffman-John, Josh\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd H. Chi, and Quoc Le. 2022. Lamda: Lan-\nCORR,\nguage models for dialog applications.\nabs/2201.08239.\n"}
{"page": 9, "bbox": [{"x": 0.11838191747665405, "y": 0.46173253655433655}, {"x": 0.48839977383613586, "y": 0.46173253655433655}, {"x": 0.48839977383613586, "y": 0.8502943515777588}, {"x": 0.11838191747665405, "y": 0.8502943515777588}], "text": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d'Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake Hecht-\nman, Laura Weidinger, Iason Gabriel, William Isaac,\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\nway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher.\n"}
{"page": 9, "bbox": [{"x": 0.5139797925949097, "y": 0.7800672650337219}, {"x": 0.8845925331115723, "y": 0.7800672650337219}, {"x": 0.8845925331115723, "y": 0.9171572923660278}, {"x": 0.5139797925949097, "y": 0.9171572923660278}], "text": "Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. GPL: Generative pseudo label-\ning for unsupervised domain adaptation of dense re-\ntrieval. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2345-2360, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\n"}
{"page": 9, "bbox": [{"x": 0.11838191747665405, "y": 0.867956280708313}, {"x": 0.48899465799331665, "y": 0.8687973022460938}, {"x": 0.48899465799331665, "y": 0.918418824672699}, {"x": 0.11838191747665405, "y": 0.9175778031349182}], "text": "Devendra Singh Sachan, Mike Lewis, Mandar Joshi,\nArmen Aghajanyan, Wen-tau Yih, Joelle Pineau, and\nLuke Zettlemoyer. 2022. Improving passage re-\ntrieval with zero-shot question generation.\n"}
{"page": 10, "bbox": [{"x": 0.13682332634925842, "y": 0.08915054798126221}, {"x": 0.48839977383613586, "y": 0.08873002231121063}, {"x": 0.48839977383613586, "y": 0.15096719563007355}, {"x": 0.13682332634925842, "y": 0.15138772130012512}], "text": "drew M. Dai, and Quoc V. Le. 2022. Finetuned lan-\nguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\n"}
{"page": 10, "bbox": [{"x": 0.11778703331947327, "y": 0.1644238829612732}, {"x": 0.48899465799331665, "y": 0.1644238829612732}, {"x": 0.48899465799331665, "y": 0.38982337713241577}, {"x": 0.11778703331947327, "y": 0.38982337713241577}], "text": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\nArnold Overwijk. 2022. Coco-dr: Combating dis-\ntribution shifts in zero-shot dense retrieval with con-\ntrastive and distributionally robust learning. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing.\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.\n2021. Mr. TyDi: A multi-lingual benchmark for\ndense retrieval. arXiv:2108.08787.\n"}
{"page": 11, "bbox": [{"x": 0.11719214916229248, "y": 0.08662741631269455}, {"x": 0.23557406663894653, "y": 0.08704794198274612}, {"x": 0.23557406663894653, "y": 0.09966358542442322}, {"x": 0.11719214916229248, "y": 0.09924305975437164}], "text": "A Appendix\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.11227922886610031}, {"x": 0.2587745487689972, "y": 0.11227922886610031}, {"x": 0.2587745487689972, "y": 0.12111017853021622}, {"x": 0.11778703331947327, "y": 0.12111017853021622}], "text": "A.1 Instructions\n"}
{"page": 11, "bbox": [{"x": 0.11897680163383484, "y": 0.13288477063179016}, {"x": 0.15883402526378632, "y": 0.13330529630184174}, {"x": 0.15883402526378632, "y": 0.14213624596595764}, {"x": 0.11897680163383484, "y": 0.14171572029590607}], "text": "A.1.1\n"}
{"page": 11, "bbox": [{"x": 0.17787031829357147, "y": 0.13330529630184174}, {"x": 0.2712671160697937, "y": 0.13330529630184174}, {"x": 0.2712671160697937, "y": 0.14213624596595764}, {"x": 0.17787031829357147, "y": 0.14213624596595764}], "text": "Web Search\n"}
{"page": 11, "bbox": [{"x": 0.14396192133426666, "y": 0.15811605751514435}, {"x": 0.4776918590068817, "y": 0.15811605751514435}, {"x": 0.4776918590068817, "y": 0.18502943217754364}, {"x": 0.14396192133426666, "y": 0.18502943217754364}], "text": "Please write a passage to answer the question\nQuestion: [QUESTION]\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.19049622118473053}, {"x": 0.20820939540863037, "y": 0.19049622118473053}, {"x": 0.20820939540863037, "y": 0.20100925862789154}, {"x": 0.14455680549144745, "y": 0.20100925862789154}], "text": "Passage:\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.23170731961727142}, {"x": 0.23616895079612732, "y": 0.23170731961727142}, {"x": 0.23616895079612732, "y": 0.24053826928138733}, {"x": 0.11778703331947327, "y": 0.24053826928138733}], "text": "A.1.2 SciFact\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.253153920173645}, {"x": 0.6228435635566711, "y": 0.25651809573173523}, {"x": 0.622248649597168, "y": 0.2859545946121216}, {"x": 0.14396192133426666, "y": 0.28259041905403137}], "text": "Please write a scientific paper passage to support/refute the claim\nClaim: [Claim]\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.2893187403678894}, {"x": 0.20820939540863037, "y": 0.2893187403678894}, {"x": 0.20820939540863037, "y": 0.29941126704216003}, {"x": 0.14455680549144745, "y": 0.29941126704216003}], "text": "Passage:\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.3288477659225464}, {"x": 0.24747174978256226, "y": 0.33137089014053345}, {"x": 0.24687686562538147, "y": 0.3423044681549072}, {"x": 0.11719214916229248, "y": 0.3397813141345978}], "text": "A.1.3 Arguana\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.3557611405849457}, {"x": 0.4949434995651245, "y": 0.35786375403404236}, {"x": 0.4949434995651245, "y": 0.3700588643550873}, {"x": 0.14455680549144745, "y": 0.3679562509059906}], "text": "Please write a counter argument for the passage\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.3717409670352936}, {"x": 0.30398571491241455, "y": 0.37258198857307434}, {"x": 0.30339083075523376, "y": 0.4015979766845703}, {"x": 0.14396192133426666, "y": 0.40075692534446716}], "text": "Passage: [PASSAGE]\nCounter Argument:\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.43145501613616943}, {"x": 0.294467568397522, "y": 0.43145501613616943}, {"x": 0.294467568397522, "y": 0.4407064616680145}, {"x": 0.11778703331947327, "y": 0.4407064616680145}], "text": "A.1.4 TREC-COVID\n"}
{"page": 11, "bbox": [{"x": 0.14396192133426666, "y": 0.45626577734947205}, {"x": 0.5942891240119934, "y": 0.45626577734947205}, {"x": 0.5942891240119934, "y": 0.48317915201187134}, {"x": 0.14396192133426666, "y": 0.48317915201187134}], "text": "Please write a scientific paper passage to answer the question\nQuestion: [QUESTION]\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.4890664517879486}, {"x": 0.20761451125144958, "y": 0.4890664517879486}, {"x": 0.20761451125144958, "y": 0.49873843789100647}, {"x": 0.14455680549144745, "y": 0.49873843789100647}], "text": "Passage:\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.5277544260025024}, {"x": 0.22248661518096924, "y": 0.5298570394515991}, {"x": 0.22189173102378845, "y": 0.5403700470924377}, {"x": 0.11719214916229248, "y": 0.5382674336433411}], "text": "A.1.5 FIQA\n"}
{"page": 11, "bbox": [{"x": 0.14396192133426666, "y": 0.5550882816314697}, {"x": 0.5960737466812134, "y": 0.5563498735427856}, {"x": 0.5960737466812134, "y": 0.5849453210830688}, {"x": 0.14396192133426666, "y": 0.5836837887763977}], "text": "Please write a financial article passage to answer the question\nQuestion: [QUESTION]\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.5891505479812622}, {"x": 0.20761451125144958, "y": 0.5895710587501526}, {"x": 0.20761451125144958, "y": 0.600084125995636}, {"x": 0.14455680549144745, "y": 0.5996635556221008}], "text": "Passage:\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.6286795735359192}, {"x": 0.30339083075523376, "y": 0.6303616762161255}, {"x": 0.30339083075523376, "y": 0.6417157053947449}, {"x": 0.11778703331947327, "y": 0.6400336623191833}], "text": "A.1.6 DBPedia-Entity\n"}
{"page": 11, "bbox": [{"x": 0.14396192133426666, "y": 0.6576955318450928}, {"x": 0.48126116394996643, "y": 0.6576955318450928}, {"x": 0.48126116394996643, "y": 0.6850294470787048}, {"x": 0.14396192133426666, "y": 0.6850294470787048}], "text": "Please write a passage to answer the question.\nQuestion: [QUESTION]\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.6909167170524597}, {"x": 0.20761451125144958, "y": 0.6909167170524597}, {"x": 0.20761451125144958, "y": 0.7005887031555176}, {"x": 0.14455680549144745, "y": 0.7005887031555176}], "text": "Passage:\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.7317073345184326}, {"x": 0.2891136109828949, "y": 0.7317073345184326}, {"x": 0.2891136109828949, "y": 0.7405382394790649}, {"x": 0.11838191747665405, "y": 0.7405382394790649}], "text": "A.1.7 TREC-NEWS\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.7565180659294128}, {"x": 0.46817371249198914, "y": 0.7565180659294128}, {"x": 0.46817371249198914, "y": 0.7682926654815674}, {"x": 0.14455680549144745, "y": 0.7682926654815674}], "text": "Please write a news passage about the topic.\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.7724978923797607}, {"x": 0.26115408539772034, "y": 0.7724978923797607}, {"x": 0.26115408539772034, "y": 0.7994112968444824}, {"x": 0.14455680549144745, "y": 0.7994112968444824}], "text": "Topic: [TOPIC]\nPassage:\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.8280067443847656}, {"x": 0.2456870973110199, "y": 0.8292682766914368}, {"x": 0.2456870973110199, "y": 0.8410428762435913}, {"x": 0.11838191747665405, "y": 0.8397813439369202}], "text": "A.1.8 Mr.TyDi\n"}
{"page": 11, "bbox": [{"x": 0.14396192133426666, "y": 0.8566021919250488}, {"x": 0.8143962025642395, "y": 0.8574432134628296}, {"x": 0.8143962025642395, "y": 0.8856181502342224}, {"x": 0.14396192133426666, "y": 0.8847771286964417}], "text": "Please write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\nQuestion: [QUESTION]\n"}
{"page": 11, "bbox": [{"x": 0.14455680549144745, "y": 0.8902438879013062}, {"x": 0.2070196270942688, "y": 0.8902438879013062}, {"x": 0.2070196270942688, "y": 0.9003364443778992}, {"x": 0.14455680549144745, "y": 0.9003364443778992}], "text": "Passage:\n"}
{"page": 1, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 1, "bbox": [{"x": 0.1769055724143982, "y": 0.10153846442699432}, {"x": 0.8219567537307739, "y": 0.10153846442699432}, {"x": 0.8219567537307739, "y": 0.14461538195610046}, {"x": 0.1769055724143982, "y": 0.14461538195610046}], "text": "SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\nCRITIQUE THROUGH SELF-REFLECTION\n"}
{"page": 1, "bbox": [{"x": 0.1848691701889038, "y": 0.17274725437164307}, {"x": 0.725824773311615, "y": 0.17406593263149261}, {"x": 0.725824773311615, "y": 0.20395603775978088}, {"x": 0.1848691701889038, "y": 0.20263735949993134}], "text": "Akari Asait, Zeqiu Wu, Yizhong Wang+s, Avirup Sil‡, Hannaneh Hajishirzi†⁹\n*University of Washington § Allen Institute for AI\n"}
{"page": 1, "bbox": [{"x": 0.5455062389373779, "y": 0.18945054709911346}, {"x": 0.6689419746398926, "y": 0.190769225358963}, {"x": 0.6689419746398926, "y": 0.20087912678718567}, {"x": 0.5455062389373779, "y": 0.19956043362617493}], "text": "#IBM Research AI\n"}
{"page": 1, "bbox": [{"x": 0.18714448809623718, "y": 0.20483516156673431}, {"x": 0.8259385824203491, "y": 0.20483516156673431}, {"x": 0.8259385824203491, "y": 0.21714285016059875}, {"x": 0.18714448809623718, "y": 0.21714285016059875}], "text": "{akari, zeqiuwu, yizhongw, hannaneh}@cs.washington.edu, avi@us.ibm.com\n"}
{"page": 1, "bbox": [{"x": 0.4550625681877136, "y": 0.25582417845726013}, {"x": 0.5449374318122864, "y": 0.25626373291015625}, {"x": 0.5449374318122864, "y": 0.2659340798854828}, {"x": 0.4550625681877136, "y": 0.2654944956302643}], "text": "ABSTRACT\n"}
{"page": 1, "bbox": [{"x": 0.233788400888443, "y": 0.2883516550064087}, {"x": 0.7684869170188904, "y": 0.2883516550064087}, {"x": 0.7684869170188904, "y": 0.5485714077949524}, {"x": 0.233788400888443, "y": 0.5485714077949524}], "text": "Despite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\neration (SELF-RAG) that enhances an LM's quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called reflection tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models.-\n"}
{"page": 1, "bbox": [{"x": 0.027872582897543907, "y": 0.7059340476989746}, {"x": 0.029010238125920296, "y": 0.2716483473777771}, {"x": 0.05915813520550728, "y": 0.2716483473777771}, {"x": 0.05802047625184059, "y": 0.7059340476989746}], "text": "arXiv:2310.11511v1 [cs.CL] 17 Oct 2023\n"}
{"page": 1, "bbox": [{"x": 0.17804323136806488, "y": 0.5793406367301941}, {"x": 0.18543799221515656, "y": 0.5793406367301941}, {"x": 0.18543799221515656, "y": 0.5894505381584167}, {"x": 0.17804323136806488, "y": 0.5894505381584167}], "text": "1\n"}
{"page": 1, "bbox": [{"x": 0.20705346763134003, "y": 0.5789011120796204}, {"x": 0.3361774682998657, "y": 0.5797802209854126}, {"x": 0.3361774682998657, "y": 0.590329647064209}, {"x": 0.20705346763134003, "y": 0.5894505381584167}], "text": "INTRODUCTION\n"}
{"page": 1, "bbox": [{"x": 0.17519909143447876, "y": 0.6109890341758728}, {"x": 0.8253697156906128, "y": 0.6109890341758728}, {"x": 0.8253697156906128, "y": 0.9243956208229065}, {"x": 0.17519909143447876, "y": 0.9243956208229065}], "text": "State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an\nLLM's generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n¹Our code and trained models are available at https://selfrag.github.io/.\n"}
{"page": 1, "bbox": [{"x": 0.4960182011127472, "y": 0.9507692456245422}, {"x": 0.5017064809799194, "y": 0.9507692456245422}, {"x": 0.5017064809799194, "y": 0.9586813449859619}, {"x": 0.4960182011127472, "y": 0.9586813449859619}], "text": "1\n"}
{"page": 2, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 2, "bbox": [{"x": 0.19965870678424835, "y": 0.10725274682044983}, {"x": 0.3913538157939911, "y": 0.10725274682044983}, {"x": 0.3913538157939911, "y": 0.11428571492433548}, {"x": 0.19965870678424835, "y": 0.11428571492433548}], "text": "Retrieval-Augmented Generation (RAG)\n"}
{"page": 2, "bbox": [{"x": 0.45164960622787476, "y": 0.10725274682044983}, {"x": 0.7656427621841431, "y": 0.10725274682044983}, {"x": 0.7656427621841431, "y": 0.11428571492433548}, {"x": 0.45164960622787476, "y": 0.11428571492433548}], "text": "Ours: Self-reflective Retrieval-Augmented Generation (Self-RAG)\n"}
{"page": 2, "bbox": [{"x": 0.6507394909858704, "y": 0.12439560145139694}, {"x": 0.7616609930992126, "y": 0.12351648509502411}, {"x": 0.7616609930992126, "y": 0.12923076748847961}, {"x": 0.6507394909858704, "y": 0.13010989129543304}], "text": "Step 1: Retrieve on demand\n"}
{"page": 2, "bbox": [{"x": 0.19738338887691498, "y": 0.1257142871618271}, {"x": 0.3646188974380493, "y": 0.12527473270893097}, {"x": 0.3646188974380493, "y": 0.1318681389093399}, {"x": 0.19738338887691498, "y": 0.13230769336223602}], "text": "Prompt How did US states get their names?\n"}
{"page": 2, "bbox": [{"x": 0.424914687871933, "y": 0.1261538416147232}, {"x": 0.5927190184593201, "y": 0.1261538416147232}, {"x": 0.5927190184593201, "y": 0.1318681389093399}, {"x": 0.424914687871933, "y": 0.1318681389093399}], "text": "Prompt How did US states get their names?\n"}
{"page": 2, "bbox": [{"x": 0.2349260449409485, "y": 0.14461538195610046}, {"x": 0.3538111448287964, "y": 0.14417582750320435}, {"x": 0.3538111448287964, "y": 0.14989010989665985}, {"x": 0.2349260449409485, "y": 0.15032966434955597}], "text": "Step 1: Retrieve K documents\n"}
{"page": 2, "bbox": [{"x": 0.4653014838695526, "y": 0.14725275337696075}, {"x": 0.6831626892089844, "y": 0.14725275337696075}, {"x": 0.6831626892089844, "y": 0.15340659022331238}, {"x": 0.4653014838695526, "y": 0.15340659022331238}], "text": "US states got their names from a variety of sources. Retrieve\n"}
{"page": 2, "bbox": [{"x": 0.2377701997756958, "y": 0.16175824403762817}, {"x": 0.24630261957645416, "y": 0.16175824403762817}, {"x": 0.24630261957645416, "y": 0.16747252643108368}, {"x": 0.2377701997756958, "y": 0.16747252643108368}], "text": "1\n"}
{"page": 2, "bbox": [{"x": 0.2542662024497986, "y": 0.15736263990402222}, {"x": 0.3993174135684967, "y": 0.15824176371097565}, {"x": 0.3993174135684967, "y": 0.17186813056468964}, {"x": 0.2542662024497986, "y": 0.1709890067577362}], "text": "Of the fifty states, eleven are named\nafter an individual person.\n"}
{"page": 2, "bbox": [{"x": 0.4226393699645996, "y": 0.16615384817123413}, {"x": 0.5693970322608948, "y": 0.16615384817123413}, {"x": 0.5693970322608948, "y": 0.17186813056468964}, {"x": 0.4226393699645996, "y": 0.17186813056468964}], "text": "Step 2: Generate segment in parallel\n"}
{"page": 2, "bbox": [{"x": 0.23549488186836243, "y": 0.179340660572052}, {"x": 0.39476677775382996, "y": 0.17978021502494812}, {"x": 0.39476677775382996, "y": 0.19428572058677673}, {"x": 0.23549488186836243, "y": 0.19384615123271942}], "text": "2 Popular names by states. In Texas,\nEmma is a popular baby name.\n"}
{"page": 2, "bbox": [{"x": 0.5699658989906311, "y": 0.18813186883926392}, {"x": 0.6228668689727783, "y": 0.18813186883926392}, {"x": 0.6228668689727783, "y": 0.19428572058677673}, {"x": 0.5699658989906311, "y": 0.19428572058677673}], "text": "Prompt + 2\n"}
{"page": 2, "bbox": [{"x": 0.7013651728630066, "y": 0.18593406677246094}, {"x": 0.753697395324707, "y": 0.18593406677246094}, {"x": 0.753697395324707, "y": 0.21054944396018982}, {"x": 0.7013651728630066, "y": 0.21054944396018982}], "text": "Prompt + 3\n↓\n"}
{"page": 2, "bbox": [{"x": 0.45392492413520813, "y": 0.18857142329216003}, {"x": 0.5039817690849304, "y": 0.18857142329216003}, {"x": 0.5039817690849304, "y": 0.21142856776714325}, {"x": 0.45392492413520813, "y": 0.21142856776714325}], "text": "Prompt + 1\n↓\n"}
{"page": 2, "bbox": [{"x": 0.18885096907615662, "y": 0.20351648330688477}, {"x": 0.22411832213401794, "y": 0.2043956071138382}, {"x": 0.22411832213401794, "y": 0.20967033505439758}, {"x": 0.18885096907615662, "y": 0.20879121124744415}], "text": "Retriever\n"}
{"page": 2, "bbox": [{"x": 0.2542662024497986, "y": 0.2017582356929779}, {"x": 0.40614333748817444, "y": 0.2017582356929779}, {"x": 0.40614333748817444, "y": 0.21450549364089966}, {"x": 0.2542662024497986, "y": 0.21450549364089966}], "text": "California was named after a fictional\nisland in a Spanish book.\n"}
{"page": 2, "bbox": [{"x": 0.6831626892089844, "y": 0.21978022158145905}, {"x": 0.8014789819717407, "y": 0.2210988998413086}, {"x": 0.8014789819717407, "y": 0.2272527515888214}, {"x": 0.6831626892089844, "y": 0.22593407332897186}], "text": "Relevant California's name has its\n"}
{"page": 2, "bbox": [{"x": 0.5688282251358032, "y": 0.2237362563610077}, {"x": 0.594994306564331, "y": 0.2237362563610077}, {"x": 0.594994306564331, "y": 0.2272527515888214}, {"x": 0.5688282251358032, "y": 0.2272527515888214}], "text": "Irrelevant\n"}
{"page": 2, "bbox": [{"x": 0.4197952151298523, "y": 0.21626374125480652}, {"x": 0.5295790433883667, "y": 0.2184615433216095}, {"x": 0.5290102362632751, "y": 0.23868131637573242}, {"x": 0.41922640800476074, "y": 0.23648351430892944}], "text": "Relevant 11 of 50 state names\ncome from persons.\n"}
{"page": 2, "bbox": [{"x": 0.18828213214874268, "y": 0.2272527515888214}, {"x": 0.3697383403778076, "y": 0.2272527515888214}, {"x": 0.3697383403778076, "y": 0.2325274795293808}, {"x": 0.18828213214874268, "y": 0.2325274795293808}], "text": "Step 2: Prompt LM with K docs and generate\n"}
{"page": 2, "bbox": [{"x": 0.5637087821960449, "y": 0.22329670190811157}, {"x": 0.668373167514801, "y": 0.22329670190811157}, {"x": 0.668373167514801, "y": 0.23999999463558197}, {"x": 0.5637087821960449, "y": 0.23999999463558197}], "text": "Texas is named\nafter a Native American tribe.\n"}
{"page": 2, "bbox": [{"x": 0.5130830407142639, "y": 0.2320879101753235}, {"x": 0.5420932769775391, "y": 0.2320879101753235}, {"x": 0.5420932769775391, "y": 0.23604395985603333}, {"x": 0.5130830407142639, "y": 0.23604395985603333}], "text": "Supported\n"}
{"page": 2, "bbox": [{"x": 0.6769055724143982, "y": 0.23032967746257782}, {"x": 0.7946529984474182, "y": 0.2298901081085205}, {"x": 0.7946529984474182, "y": 0.2461538463830948}, {"x": 0.6769055724143982, "y": 0.2465934008359909}], "text": "origins in a 16th-century novel\nLas Sergas de Esplandián.\n"}
{"page": 2, "bbox": [{"x": 0.7855517864227295, "y": 0.24219779670238495}, {"x": 0.8077360391616821, "y": 0.24219779670238495}, {"x": 0.8077360391616821, "y": 0.2461538463830948}, {"x": 0.7855517864227295, "y": 0.2461538463830948}], "text": "Partially\n"}
{"page": 2, "bbox": [{"x": 0.36234357953071594, "y": 0.23956044018268585}, {"x": 0.4032992124557495, "y": 0.23956044018268585}, {"x": 0.4032992124557495, "y": 0.2492307722568512}, {"x": 0.36234357953071594, "y": 0.2492307722568512}], "text": "123\n"}
{"page": 2, "bbox": [{"x": 0.18031854927539825, "y": 0.24175824224948883}, {"x": 0.35608646273612976, "y": 0.24175824224948883}, {"x": 0.35608646273612976, "y": 0.24791209399700165}, {"x": 0.18031854927539825, "y": 0.24791209399700165}], "text": "Prompt How did US states get their names? +\n"}
{"page": 2, "bbox": [{"x": 0.4226393699645996, "y": 0.25406593084335327}, {"x": 0.6194539070129395, "y": 0.25406593084335327}, {"x": 0.6194539070129395, "y": 0.25934067368507385}, {"x": 0.4226393699645996, "y": 0.25934067368507385}], "text": "Step 3: Critique outputs and select best segment\n"}
{"page": 2, "bbox": [{"x": 0.22411832213401794, "y": 0.25978022813796997}, {"x": 0.39988622069358826, "y": 0.25978022813796997}, {"x": 0.39988622069358826, "y": 0.3138461410999298}, {"x": 0.22411832213401794, "y": 0.3138461410999298}], "text": "US states got their names from a variety of\nsources. Eleven states are named after an\nindividual person (e.g. California was named\nafter Christopher Columbus). Some states\nincluing Texas and Utah, are named after\nmerican tribe No information in passages\n"}
{"page": 2, "bbox": [{"x": 0.18828213214874268, "y": 0.2914285659790039}, {"x": 0.19965870678424835, "y": 0.2914285659790039}, {"x": 0.19965870678424835, "y": 0.29538461565971375}, {"x": 0.18828213214874268, "y": 0.29538461565971375}], "text": "LM\n"}
{"page": 2, "bbox": [{"x": 0.47098976373672485, "y": 0.29582417011260986}, {"x": 0.49317407608032227, "y": 0.296263724565506}, {"x": 0.49317407608032227, "y": 0.3002197742462158}, {"x": 0.47098976373672485, "y": 0.2997802197933197}], "text": "Retrieve\n"}
{"page": 2, "bbox": [{"x": 0.5017064809799194, "y": 0.29582417011260986}, {"x": 0.5574516654014587, "y": 0.29802197217941284}, {"x": 0.5568827986717224, "y": 0.30417582392692566}, {"x": 0.5011376738548279, "y": 0.3019780218601227}], "text": "→ Repeat....\n"}
{"page": 2, "bbox": [{"x": 0.5784983038902283, "y": 0.29010990262031555}, {"x": 0.8174061179161072, "y": 0.29054945707321167}, {"x": 0.8174061179161072, "y": 0.3160439431667328}, {"x": 0.5784983038902283, "y": 0.31560438871383667}], "text": "US states got their names from a variety of sources. 11 of 50\nstates names are come from persons. 26 states are named\nafter Native Americans, including Utah.\n"}
{"page": 2, "bbox": [{"x": 0.19852104783058167, "y": 0.3081318736076355}, {"x": 0.24459613859653473, "y": 0.3085714280605316}, {"x": 0.24459613859653473, "y": 0.3138461410999298}, {"x": 0.19852104783058167, "y": 0.3134065866470337}], "text": "Contradictory\n"}
{"page": 2, "bbox": [{"x": 0.4237770140171051, "y": 0.32967033982276917}, {"x": 0.6279863715171814, "y": 0.32967033982276917}, {"x": 0.6279863715171814, "y": 0.3358241617679596}, {"x": 0.4237770140171051, "y": 0.3358241617679596}], "text": "Prompt: Write an essay of your best summer vacation\n"}
{"page": 2, "bbox": [{"x": 0.18941979110240936, "y": 0.3305494487285614}, {"x": 0.39362913370132446, "y": 0.3305494487285614}, {"x": 0.39362913370132446, "y": 0.3367033004760742}, {"x": 0.18941979110240936, "y": 0.3367033004760742}], "text": "Prompt: Write an essay of your best summer vacation\n"}
{"page": 2, "bbox": [{"x": 0.4755403995513916, "y": 0.35164836049079895}, {"x": 0.8248009085655212, "y": 0.35164836049079895}, {"x": 0.8248009085655212, "y": 0.35736262798309326}, {"x": 0.4755403995513916, "y": 0.35736262798309326}], "text": "No Retrieval My best summer vacation is when my family and I embarked on a road trip along...\n"}
{"page": 2, "bbox": [{"x": 0.34698522090911865, "y": 0.35164836049079895}, {"x": 0.404436856508255, "y": 0.3529670238494873}, {"x": 0.40386801958084106, "y": 0.3591208755970001}, {"x": 0.34698522090911865, "y": 0.35780221223831177}], "text": "→ My best...\n"}
{"page": 2, "bbox": [{"x": 0.1757679134607315, "y": 0.37978023290634155}, {"x": 0.8236632347106934, "y": 0.3806593418121338}, {"x": 0.8236632347106934, "y": 0.40747252106666565}, {"x": 0.1757679134607315, "y": 0.4065934121608734}], "text": "Figure 1: Overview of SELF-RAG. SELF-RAG learns to retrieve, critique, and generate text passages\nto enhance overall generation quality, factuality, and verifiability.\n"}
{"page": 2, "bbox": [{"x": 0.17519909143447876, "y": 0.4364835023880005}, {"x": 0.8248009085655212, "y": 0.4364835023880005}, {"x": 0.8248009085655212, "y": 0.8167033195495605}, {"x": 0.17519909143447876, "y": 0.8167033195495605}], "text": "consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\ngeneration quality. Moreover, SELF-RAG provides citations for each segment with its self-assessment\nof whether the output is supported by the passage, leading to easier fact verification.\nSELF-RAG trains an arbitrary LM to generate text with reflection tokens by unifying them as the\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\nassess its own predictions after each generated segment as an integral part of the generation output.\nSELF-RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints,\nwhich are defined by reflection token predictions. In particular, our inference-time algorithm enables\nus to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\nmodels' behaviors to user preferences by leveraging reflection tokens through segment-level beam\nsearch using the weighted linear sum of the reflection token probabilities as segment score.\nEmpirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF-\nRAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF-RAG outperforms\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\nreflection tokens for overall performance improvements as well as test-time model customizations\n(e.g., balancing the trade-off between citation previsions and completeness).\n"}
{"page": 2, "bbox": [{"x": 0.1757679134607315, "y": 0.8399999737739563}, {"x": 0.3447099030017853, "y": 0.8399999737739563}, {"x": 0.3447099030017853, "y": 0.850109875202179}, {"x": 0.1757679134607315, "y": 0.850109875202179}], "text": "2 RELATED WORK\n"}
{"page": 2, "bbox": [{"x": 0.17633675038814545, "y": 0.8712087869644165}, {"x": 0.8236632347106934, "y": 0.8712087869644165}, {"x": 0.8236632347106934, "y": 0.923956036567688}, {"x": 0.17633675038814545, "y": 0.923956036567688}], "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input\nspace of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\nimprovements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\net al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\n"}
{"page": 2, "bbox": [{"x": 0.4948805570602417, "y": 0.951208770275116}, {"x": 0.5034129619598389, "y": 0.951208770275116}, {"x": 0.5034129619598389, "y": 0.9586813449859619}, {"x": 0.4948805570602417, "y": 0.9586813449859619}], "text": "2\n"}
{"page": 3, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 3, "bbox": [{"x": 0.17519909143447876, "y": 0.10681318491697311}, {"x": 0.8270761966705322, "y": 0.10681318491697311}, {"x": 0.8270761966705322, "y": 0.6316483616828918}, {"x": 0.17519909143447876, "y": 0.6316483616828918}], "text": "of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\nConcurrent RAG work. A few concurrent works² on RAG propose new training or prompting\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\ninstruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\na summarization model to filter out or compress retrieved passages before using them to prompt the\nLM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\non retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n"}
{"page": 3, "bbox": [{"x": 0.1769055724143982, "y": 0.6571428775787354}, {"x": 0.7457337975502014, "y": 0.6571428775787354}, {"x": 0.7457337975502014, "y": 0.6690109968185425}, {"x": 0.1769055724143982, "y": 0.6690109968185425}], "text": "3 SELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\n"}
{"page": 3, "bbox": [{"x": 0.1757679134607315, "y": 0.689230740070343}, {"x": 0.8253697156906128, "y": 0.689230740070343}, {"x": 0.8253697156906128, "y": 0.7837362885475159}, {"x": 0.1757679134607315, "y": 0.7837362885475159}], "text": "We introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\nSELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM's original creativity and versatility. Our end-to-end training\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\nor confirm the output's relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n"}
{"page": 3, "bbox": [{"x": 0.1769055724143982, "y": 0.8052747249603271}, {"x": 0.5199089646339417, "y": 0.8057143092155457}, {"x": 0.5199089646339417, "y": 0.8149450421333313}, {"x": 0.1769055724143982, "y": 0.8145055174827576}], "text": "3.1 PROBLEM FORMALIZATION AND OVERVIEW\n"}
{"page": 3, "bbox": [{"x": 0.1757679134607315, "y": 0.8320879340171814}, {"x": 0.8236632347106934, "y": 0.8320879340171814}, {"x": 0.8236632347106934, "y": 0.870769202709198}, {"x": 0.1757679134607315, "y": 0.870769202709198}], "text": "Formally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\nsegments y [Y1, ..., YT], where yt indicates a sequence of tokens for the t-th segment.³ Generated\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\n"}
{"page": 3, "bbox": [{"x": 0.25255972146987915, "y": 0.8540659546852112}, {"x": 0.25255972146987915, "y": 0.8505494594573975}, {"x": 0.2622298002243042, "y": 0.8505494594573975}, {"x": 0.2622298002243042, "y": 0.8540659546852112}], "text": "=\n"}
{"page": 3, "bbox": [{"x": 0.19567690789699554, "y": 0.8857142925262451}, {"x": 0.49829351902008057, "y": 0.8870329856872559}, {"x": 0.49829351902008057, "y": 0.8980219960212708}, {"x": 0.19567690789699554, "y": 0.89670330286026}], "text": "2All work is arXived within a week of this preprint.\n"}
{"page": 3, "bbox": [{"x": 0.17633675038814545, "y": 0.898901104927063}, {"x": 0.8230944275856018, "y": 0.8971428275108337}, {"x": 0.8230944275856018, "y": 0.921758234500885}, {"x": 0.17633675038814545, "y": 0.9235165119171143}], "text": "³In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n"}
{"page": 3, "bbox": [{"x": 0.4948805570602417, "y": 0.951208770275116}, {"x": 0.5039817690849304, "y": 0.951208770275116}, {"x": 0.5039817690849304, "y": 0.9582417607307434}, {"x": 0.4948805570602417, "y": 0.9582417607307434}], "text": "3\n"}
{"page": 4, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 4, "bbox": [{"x": 0.5568827986717224, "y": 0.10857142508029938}, {"x": 0.6211603879928589, "y": 0.10813187062740326}, {"x": 0.6211603879928589, "y": 0.11648351699113846}, {"x": 0.5568827986717224, "y": 0.11692307889461517}], "text": "Definitions\n"}
{"page": 4, "bbox": [{"x": 0.37542662024497986, "y": 0.10945054888725281}, {"x": 0.41695109009742737, "y": 0.10945054888725281}, {"x": 0.41695109009742737, "y": 0.11868131905794144}, {"x": 0.37542662024497986, "y": 0.11868131905794144}], "text": "Output\n"}
{"page": 4, "bbox": [{"x": 0.18771331012248993, "y": 0.10945054888725281}, {"x": 0.21672354638576508, "y": 0.10945054888725281}, {"x": 0.21672354638576508, "y": 0.11956044286489487}, {"x": 0.18771331012248993, "y": 0.11956044286489487}], "text": "Туре\n"}
{"page": 4, "bbox": [{"x": 0.2997724711894989, "y": 0.10989011079072952}, {"x": 0.33162686228752136, "y": 0.10989011079072952}, {"x": 0.33162686228752136, "y": 0.11912088096141815}, {"x": 0.2997724711894989, "y": 0.11912088096141815}], "text": "Input\n"}
{"page": 4, "bbox": [{"x": 0.2997724711894989, "y": 0.12791208922863007}, {"x": 0.34300342202186584, "y": 0.13010989129543304}, {"x": 0.3424345850944519, "y": 0.13846154510974884}, {"x": 0.29920363426208496, "y": 0.13626374304294586}], "text": "x / x,y\n"}
{"page": 4, "bbox": [{"x": 0.19169510900974274, "y": 0.12967033684253693}, {"x": 0.23094426095485687, "y": 0.12967033684253693}, {"x": 0.23094426095485687, "y": 0.16131867468357086}, {"x": 0.19169510900974274, "y": 0.16131867468357086}], "text": "Retrieve\nISREL\nISSUP\n"}
{"page": 4, "bbox": [{"x": 0.29920363426208496, "y": 0.14329670369625092}, {"x": 0.32366326451301575, "y": 0.1397802233695984}, {"x": 0.3253697454929352, "y": 0.14769230782985687}, {"x": 0.3009101152420044, "y": 0.1512087881565094}], "text": "x, d\n"}
{"page": 4, "bbox": [{"x": 0.2997724711894989, "y": 0.1542857140302658}, {"x": 0.33674630522727966, "y": 0.15340659022331238}, {"x": 0.3373151421546936, "y": 0.1626373678445816}, {"x": 0.30034130811691284, "y": 0.16351647675037384}], "text": "x, d, y\n"}
{"page": 4, "bbox": [{"x": 0.3748577833175659, "y": 0.12747253477573395}, {"x": 0.5375426411628723, "y": 0.12747253477573395}, {"x": 0.5375426411628723, "y": 0.18945054709911346}, {"x": 0.3748577833175659, "y": 0.18945054709911346}], "text": "{yes, no, continue}\n{relevant, irrelevant}\n{fully supported, partially\nsupported, no support}\n{5, 4, 3, 2, 1}\n"}
{"page": 4, "bbox": [{"x": 0.5563139915466309, "y": 0.12747253477573395}, {"x": 0.8128555417060852, "y": 0.12791208922863007}, {"x": 0.8128555417060852, "y": 0.1903296709060669}, {"x": 0.5563139915466309, "y": 0.18989011645317078}], "text": "Decides when to retrieve with R\nd provides useful information to solve x.\nAll of the verification-worthy statement in y\nis supported by d.\ny is a useful response to x.\n"}
{"page": 4, "bbox": [{"x": 0.19453924894332886, "y": 0.18065933883190155}, {"x": 0.2246871441602707, "y": 0.18065933883190155}, {"x": 0.2246871441602707, "y": 0.18637362122535706}, {"x": 0.19453924894332886, "y": 0.18637362122535706}], "text": "ISUSE\n"}
{"page": 4, "bbox": [{"x": 0.30034130811691284, "y": 0.18153846263885498}, {"x": 0.3230944275856018, "y": 0.1819780170917511}, {"x": 0.3230944275856018, "y": 0.18901099264621735}, {"x": 0.30034130811691284, "y": 0.18857142329216003}], "text": "X, Y\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.2070329636335373}, {"x": 0.8236632347106934, "y": 0.2070329636335373}, {"x": 0.8236632347106934, "y": 0.2465934008359909}, {"x": 0.17519909143447876, "y": 0.2465934008359909}], "text": "Table 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\nthe most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\n"}
{"page": 4, "bbox": [{"x": 0.17463025450706482, "y": 0.26725274324417114}, {"x": 0.40273037552833557, "y": 0.2659340798854828}, {"x": 0.40273037552833557, "y": 0.2764835059642792}, {"x": 0.17463025450706482, "y": 0.2778021991252899}], "text": "Algorithm 1 SELF-RAG Inference\n"}
{"page": 4, "bbox": [{"x": 0.17633675038814545, "y": 0.2857142984867096}, {"x": 0.76450514793396, "y": 0.2861538529396057}, {"x": 0.76450514793396, "y": 0.3274725377559662}, {"x": 0.17633675038814545, "y": 0.3270329535007477}], "text": "Require: Generator LM M, Retriever R, Large-scale passage collections {d₁, ..., dN}\n1: Input: input prompt x and preceding generation y<t, Output: next output segment Yt\n2: M predicts Retrieve given (x, y<t)\n"}
{"page": 4, "bbox": [{"x": 0.18430034816265106, "y": 0.3274725377559662}, {"x": 0.35836178064346313, "y": 0.3283516466617584}, {"x": 0.35836178064346313, "y": 0.3393406569957733}, {"x": 0.18430034816265106, "y": 0.3384615480899811}], "text": "3: if Retrieve == Yes then\n"}
{"page": 4, "bbox": [{"x": 0.7565415501594543, "y": 0.34285715222358704}, {"x": 0.8225256204605103, "y": 0.34241756796836853}, {"x": 0.8225256204605103, "y": 0.35120880603790283}, {"x": 0.7565415501594543, "y": 0.35164836049079895}], "text": "▷ Retrieve\n"}
{"page": 4, "bbox": [{"x": 0.22810012102127075, "y": 0.34065935015678406}, {"x": 0.6063708662986755, "y": 0.3415384590625763}, {"x": 0.6063708662986755, "y": 0.35428571701049805}, {"x": 0.22810012102127075, "y": 0.3534066081047058}], "text": "Retrieve relevant text passages D using R given (x, yt−1)\n"}
{"page": 4, "bbox": [{"x": 0.18373151123523712, "y": 0.3441758155822754}, {"x": 0.1951080709695816, "y": 0.3441758155822754}, {"x": 0.1951080709695816, "y": 0.35120880603790283}, {"x": 0.18373151123523712, "y": 0.35120880603790283}], "text": "4:\n"}
{"page": 4, "bbox": [{"x": 0.18373151123523712, "y": 0.35780221223831177}, {"x": 0.1962457299232483, "y": 0.35780221223831177}, {"x": 0.1962457299232483, "y": 0.36527472734451294}, {"x": 0.18373151123523712, "y": 0.36527472734451294}], "text": "5:\n"}
{"page": 4, "bbox": [{"x": 0.7519909143447876, "y": 0.35692307353019714}, {"x": 0.8236632347106934, "y": 0.35692307353019714}, {"x": 0.8236632347106934, "y": 0.3806593418121338}, {"x": 0.7519909143447876, "y": 0.3806593418121338}], "text": "▷ Generate\n▷ Critique\n"}
{"page": 4, "bbox": [{"x": 0.22810012102127075, "y": 0.356483519077301}, {"x": 0.670648455619812, "y": 0.3551648259162903}, {"x": 0.670648455619812, "y": 0.381538450717926}, {"x": 0.22810012102127075, "y": 0.38285714387893677}], "text": "M predicts [ISREL] given x, d and yt given x, d, y<t for each d = D\nM predicts ISSUP and ISUSE given x, yt, d for each dЄ D\n"}
{"page": 4, "bbox": [{"x": 0.18430034816265106, "y": 0.37142857909202576}, {"x": 0.1951080709695816, "y": 0.37142857909202576}, {"x": 0.1951080709695816, "y": 0.37890109419822693}, {"x": 0.18430034816265106, "y": 0.37890109419822693}], "text": "6:\n"}
{"page": 4, "bbox": [{"x": 0.22810012102127075, "y": 0.38197803497314453}, {"x": 0.34186574816703796, "y": 0.3841758370399475}, {"x": 0.3412969410419464, "y": 0.3947252631187439}, {"x": 0.2275312840938568, "y": 0.3925274610519409}], "text": "Rank based on\n"}
{"page": 4, "bbox": [{"x": 0.6569966077804565, "y": 0.38461539149284363}, {"x": 0.8185437917709351, "y": 0.38461539149284363}, {"x": 0.8185437917709351, "y": 0.3929670453071594}, {"x": 0.6569966077804565, "y": 0.3929670453071594}], "text": "▷ Detailed in Section 3.3\n"}
{"page": 4, "bbox": [{"x": 0.18430034816265106, "y": 0.38549450039863586}, {"x": 0.19567690789699554, "y": 0.38549450039863586}, {"x": 0.19567690789699554, "y": 0.3929670453071594}, {"x": 0.18430034816265106, "y": 0.3929670453071594}], "text": "7:\n"}
{"page": 4, "bbox": [{"x": 0.35722410678863525, "y": 0.3872527480125427}, {"x": 0.3879408538341522, "y": 0.3872527480125427}, {"x": 0.3879408538341522, "y": 0.3929670453071594}, {"x": 0.35722410678863525, "y": 0.3929670453071594}], "text": "ISREL\n"}
{"page": 4, "bbox": [{"x": 0.4715586006641388, "y": 0.3863736391067505}, {"x": 0.5011376738548279, "y": 0.3872527480125427}, {"x": 0.5005688071250916, "y": 0.39384615421295166}, {"x": 0.47098976373672485, "y": 0.3929670453071594}], "text": "ISUSE\n"}
{"page": 4, "bbox": [{"x": 0.41524460911750793, "y": 0.3863736391067505}, {"x": 0.45676904916763306, "y": 0.38813185691833496}, {"x": 0.4562002420425415, "y": 0.3947252631187439}, {"x": 0.414675772190094, "y": 0.3929670453071594}], "text": "ISSUP,\n"}
{"page": 4, "bbox": [{"x": 0.26678043603897095, "y": 0.38769230246543884}, {"x": 0.2804323136806488, "y": 0.38769230246543884}, {"x": 0.2804323136806488, "y": 0.3956044018268585}, {"x": 0.26678043603897095, "y": 0.3956044018268585}], "text": "Yt\n"}
{"page": 4, "bbox": [{"x": 0.39590445160865784, "y": 0.3916483521461487}, {"x": 0.3993174135684967, "y": 0.3916483521461487}, {"x": 0.3993174135684967, "y": 0.3942857086658478}, {"x": 0.39590445160865784, "y": 0.3942857086658478}], "text": "\"\n"}
{"page": 4, "bbox": [{"x": 0.18430034816265106, "y": 0.3978022038936615}, {"x": 0.19567690789699554, "y": 0.3978022038936615}, {"x": 0.19567690789699554, "y": 0.4065934121608734}, {"x": 0.18430034816265106, "y": 0.4065934121608734}], "text": "8:\n"}
{"page": 4, "bbox": [{"x": 0.2042093276977539, "y": 0.3973626494407654}, {"x": 0.3771331012248993, "y": 0.3973626494407654}, {"x": 0.3771331012248993, "y": 0.40791207551956177}, {"x": 0.2042093276977539, "y": 0.40791207551956177}], "text": "else if Retrieve == = No then\n"}
{"page": 4, "bbox": [{"x": 0.18373151123523712, "y": 0.41362637281417847}, {"x": 0.1951080709695816, "y": 0.41362637281417847}, {"x": 0.1951080709695816, "y": 0.4206593334674835}, {"x": 0.18373151123523712, "y": 0.4206593334674835}], "text": "9:\n"}
{"page": 4, "bbox": [{"x": 0.22810012102127075, "y": 0.4123076796531677}, {"x": 0.3953356146812439, "y": 0.4123076796531677}, {"x": 0.3953356146812439, "y": 0.42417582869529724}, {"x": 0.22810012102127075, "y": 0.42417582869529724}], "text": "Mgen predicts yt given x\n"}
{"page": 4, "bbox": [{"x": 0.7519909143447876, "y": 0.4114285707473755}, {"x": 0.8236632347106934, "y": 0.4114285707473755}, {"x": 0.8236632347106934, "y": 0.436923086643219}, {"x": 0.7519909143447876, "y": 0.436923086643219}], "text": "▷ Generate\n▷ Critique\n"}
{"page": 4, "bbox": [{"x": 0.17804323136806488, "y": 0.42461538314819336}, {"x": 0.44766780734062195, "y": 0.4259340763092041}, {"x": 0.44766780734062195, "y": 0.43824175000190735}, {"x": 0.17804323136806488, "y": 0.436923086643219}], "text": "10: Mgen predicts ISUSE given x, yt\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.473406583070755}, {"x": 0.8248009085655212, "y": 0.473406583070755}, {"x": 0.8248009085655212, "y": 0.7551648616790771}, {"x": 0.17519909143447876, "y": 0.7551648616790771}], "text": "Inference overview. Figure 1 and Algorithm 1 present an overview of SELF-RAG at inference. For\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\npassage's relevance, the next response segment, and a critique token to evaluate if the information in\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\nutility of the response.4 To generate each segment, SELF-RAG processes multiple passages in parallel\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\nd₁ is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)\nand d3 output is only partially supported while d₁ are fully supported.\nTraining overview. SELF-RAG enables an arbitrary LM to generate text with reflection tokens\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\nvocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\nwith interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\nC (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\ntrain the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\nM to generate reflection tokens by itself without relying on the critic at inference time.\n"}
{"page": 4, "bbox": [{"x": 0.17633675038814545, "y": 0.7771428823471069}, {"x": 0.36348122358322144, "y": 0.7771428823471069}, {"x": 0.36348122358322144, "y": 0.7854945063591003}, {"x": 0.17633675038814545, "y": 0.7854945063591003}], "text": "3.2 SELF-RAG TRAINING\n"}
{"page": 4, "bbox": [{"x": 0.17633675038814545, "y": 0.8030769228935242}, {"x": 0.8248009085655212, "y": 0.8030769228935242}, {"x": 0.8248009085655212, "y": 0.8276923298835754}, {"x": 0.17633675038814545, "y": 0.8276923298835754}], "text": "Here, we describe the supervised data collection and training of two models, the critic C (Section 3.2.1)\nand the generator M (Section 3.2.2).\n"}
{"page": 4, "bbox": [{"x": 0.1769055724143982, "y": 0.8483516573905945}, {"x": 0.4431171715259552, "y": 0.8483516573905945}, {"x": 0.4431171715259552, "y": 0.8571428656578064}, {"x": 0.1769055724143982, "y": 0.8571428656578064}], "text": "3.2.1 TRAINING THE CRITIC MODEL\n"}
{"page": 4, "bbox": [{"x": 0.1757679134607315, "y": 0.8725274801254272}, {"x": 0.8248009085655212, "y": 0.8725274801254272}, {"x": 0.8248009085655212, "y": 0.8980219960212708}, {"x": 0.1757679134607315, "y": 0.8980219960212708}], "text": "Data collection for critic model. Manual annotation of reflection tokens for each segment is\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n"}
{"page": 4, "bbox": [{"x": 0.19453924894332886, "y": 0.9120879173278809}, {"x": 0.8191125988960266, "y": 0.9120879173278809}, {"x": 0.8191125988960266, "y": 0.9243956208229065}, {"x": 0.19453924894332886, "y": 0.9243956208229065}], "text": "*We follow Liu et al. (2023a) in using a “perceived\" utility value that is independent of retrieved passages.\n"}
{"page": 4, "bbox": [{"x": 0.4948805570602417, "y": 0.951208770275116}, {"x": 0.5045506358146667, "y": 0.951208770275116}, {"x": 0.5045506358146667, "y": 0.9582417607307434}, {"x": 0.4948805570602417, "y": 0.9582417607307434}], "text": "4\n"}
{"page": 5, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 5, "bbox": [{"x": 0.4317406117916107, "y": 0.11120878905057907}, {"x": 0.594994306564331, "y": 0.11120878905057907}, {"x": 0.594994306564331, "y": 0.11736264079809189}, {"x": 0.4317406117916107, "y": 0.11736264079809189}], "text": "Input: How did US states get their names?\n"}
{"page": 5, "bbox": [{"x": 0.19681456685066223, "y": 0.1120879128575325}, {"x": 0.39362913370132446, "y": 0.1120879128575325}, {"x": 0.39362913370132446, "y": 0.11824175715446472}, {"x": 0.19681456685066223, "y": 0.11824175715446472}], "text": "Input: Write an essay of your best summer vacation\n"}
{"page": 5, "bbox": [{"x": 0.4306029677391052, "y": 0.12967033684253693}, {"x": 0.7918089032173157, "y": 0.12967033684253693}, {"x": 0.7918089032173157, "y": 0.14329670369625092}, {"x": 0.4306029677391052, "y": 0.14329670369625092}], "text": "Output: 1 of 50 states names come from persons. For instance, Louisiana was named in honor\nof King Louis XIV of France and Georgia was named after King George II.\n"}
{"page": 5, "bbox": [{"x": 0.1962457299232483, "y": 0.13054944574832916}, {"x": 0.414675772190094, "y": 0.13054944574832916}, {"x": 0.414675772190094, "y": 0.15208791196346283}, {"x": 0.1962457299232483, "y": 0.15208791196346283}], "text": "Output: My best summer vacation was a magical escape\nto the coastal town of Santorini. The azure waters,\ncharming white-washed building are unforgettable.\n"}
{"page": 5, "bbox": [{"x": 0.7059158086776733, "y": 0.15516483783721924}, {"x": 0.7411831617355347, "y": 0.15516483783721924}, {"x": 0.7411831617355347, "y": 0.1595604419708252}, {"x": 0.7059158086776733, "y": 0.1595604419708252}], "text": "Retriever\n"}
{"page": 5, "bbox": [{"x": 0.44653013348579407, "y": 0.15736263990402222}, {"x": 0.48122867941856384, "y": 0.15736263990402222}, {"x": 0.48122867941856384, "y": 0.1621977984905243}, {"x": 0.44653013348579407, "y": 0.1621977984905243}], "text": "Critic LM\n"}
{"page": 5, "bbox": [{"x": 0.6638225317001343, "y": 0.1819780170917511}, {"x": 0.6871444582939148, "y": 0.1819780170917511}, {"x": 0.6871444582939148, "y": 0.18549451231956482}, {"x": 0.6638225317001343, "y": 0.18549451231956482}], "text": "Retrieve\n"}
{"page": 5, "bbox": [{"x": 0.4306029677391052, "y": 0.16923077404499054}, {"x": 0.8009101152420044, "y": 0.16879120469093323}, {"x": 0.8009101152420044, "y": 0.20923076570034027}, {"x": 0.4306029677391052, "y": 0.20967033505439758}], "text": "Augmented Output: Retrieve 1 <p>Of the fifty states, eleven are named after an individual person</p>.\nSupported\nRelevant 11 of 50 states' names come from person.\n2 <p>LOUISIANA: Named in\nhonor of Louis XIV of France.</p>. Relevant For instance, Louisiana was named after King Louis XIV, and\nGeorgia was named after King George II. Partially Util: 5\n"}
{"page": 5, "bbox": [{"x": 0.19795222580432892, "y": 0.17274725437164307}, {"x": 0.41524460911750793, "y": 0.17274725437164307}, {"x": 0.41524460911750793, "y": 0.20923076570034027}, {"x": 0.19795222580432892, "y": 0.20923076570034027}], "text": "Augmented Output: No Retrieval My best summer\nvacation was a magical escape to the coastal town of\nSantorini. No Retrieval The azure waters, charming white-\nwashed building are unforgettable experience. Util: 5\n"}
{"page": 5, "bbox": [{"x": 0.17633675038814545, "y": 0.2320879101753235}, {"x": 0.8248009085655212, "y": 0.2320879101753235}, {"x": 0.8248009085655212, "y": 0.257582426071167}, {"x": 0.17633675038814545, "y": 0.257582426071167}], "text": "Figure 2: SELF-RAG training examples. The left example does not require retrieval while the right\none requires retrieval; thus, passages are inserted. More examples are in Appendix Table 4.\n"}
{"page": 5, "bbox": [{"x": 0.17519909143447876, "y": 0.2879121005535126}, {"x": 0.8259385824203491, "y": 0.2879121005535126}, {"x": 0.8259385824203491, "y": 0.4646153748035431}, {"x": 0.17519909143447876, "y": 0.4646153748035431}], "text": "used to generate such feedback (Liu et al., 2023b). However, depending on such proprietary LMs\ncan raise API costs and diminish reproducibility (Chen et al., 2023). We create supervised data by\nprompting GPT-4 to generate reflection tokens and then distill their knowledge into an in-house C.\nFor each group of reflection tokens, we randomly sample instances from the original training data:\n{X sample, Ysample} ~ {X,Y}. As different reflection token groups have their own definitions and\ninput, as shown in Table 1, we use different instruction prompts for them. Here, we use [Retrieve] as\nan example. We prompt GPT-4 with a type-specific instruction (\"Given an instruction, make a\njudgment on whether finding some external documents from the web helps to generate a better\nresponse.\") followed by few-shot demonstrations I the original task input x and output y to predict\nan appropriate reflection token as text: p(r|I, x, y). Manual assessment reveals that GPT-4 reflection\ntoken predictions show high agreement with human evaluations. We collect 4k-20k supervised\ntraining data for each type and combine them to form training data for C. Appendix Section D shows\nthe full list of instructions, and A.1 contains more details and our analysis.\n"}
{"page": 5, "bbox": [{"x": 0.1757679134607315, "y": 0.4839560389518738}, {"x": 0.8248009085655212, "y": 0.4817582368850708}, {"x": 0.8253697156906128, "y": 0.5331867933273315}, {"x": 0.17633675038814545, "y": 0.5353845953941345}], "text": "Critic learning. After we collect training data Dcritic, we initialize C with a pre-trained LM and\ntrain it on Dcritic using a standard conditional language modeling objective, maximizing likelihood:\nmax E((x,y),r)~Dcritic log pc (r|x, y), r for reflection tokens.\n(1)\n"}
{"page": 5, "bbox": [{"x": 0.31058019399642944, "y": 0.5314285755157471}, {"x": 0.319112628698349, "y": 0.5314285755157471}, {"x": 0.319112628698349, "y": 0.538021981716156}, {"x": 0.31058019399642944, "y": 0.538021981716156}], "text": "с\n"}
{"page": 5, "bbox": [{"x": 0.1757679134607315, "y": 0.5481318831443787}, {"x": 0.8253697156906128, "y": 0.5481318831443787}, {"x": 0.8253697156906128, "y": 0.5876923203468323}, {"x": 0.1757679134607315, "y": 0.5876923203468323}], "text": "Though the initial model can be any pre-trained LM, we use the same one as the generator LM\n(i.e., Llama 2-7B; Touvron et al. 2023) for C initialization. The critic achieves a higher than 90%\nagreement with GPT-4-based predictions on most reflection token categories (Appendix Table 5).\n"}
{"page": 5, "bbox": [{"x": 0.17633675038814545, "y": 0.6070329546928406}, {"x": 0.4806598424911499, "y": 0.6070329546928406}, {"x": 0.4806598424911499, "y": 0.6158241629600525}, {"x": 0.17633675038814545, "y": 0.6158241629600525}], "text": "3.2.2 TRAINING THE GENERATOR MODEL\n"}
{"page": 5, "bbox": [{"x": 0.17519909143447876, "y": 0.6303296685218811}, {"x": 0.8259385824203491, "y": 0.6303296685218811}, {"x": 0.8259385824203491, "y": 0.8035165071487427}, {"x": 0.17519909143447876, "y": 0.8035165071487427}], "text": "Data collection for generator. Given an input-output pair (x, y), we augment the original output\ny using the retrieval and critic models to create supervised data that precisely mimics the SELF-\nRAG inference-time process (Section 3.1). For each segment yt Є y, we run C to assess whether\nadditional passages could help to enhance generation. If retrieval is required, the retrieval special\ntoken Retrieve =Yes is added, and R retrieves the top K passages, D. For each passage, C further\nevaluates whether the passage is relevant and predicts ISREL. If a passage is relevant, C further\nevaluates whether the passage supports the model generation and predicts [ISSUP]. Critique tokens\n| ISSUP] are appended after the retrieved passage or generations. At the end of the output, y\n(or yT), C predicts the overall utility token ISUSE, and an augmented output with reflection tokens\nand the original input pair is added to Dgen. See the example training data in Figure 2.\nGenerator learning. We train the generator model M by training on the curated corpus augmented\nwith reflection tokens Dgen using the standard next token objective:\n"}
{"page": 5, "bbox": [{"x": 0.18316268920898438, "y": 0.7292307615280151}, {"x": 0.2428896427154541, "y": 0.7279120683670044}, {"x": 0.24345847964286804, "y": 0.7353846430778503}, {"x": 0.18373151123523712, "y": 0.7367032766342163}], "text": "ISREL and\n"}
{"page": 5, "bbox": [{"x": 0.8048919439315796, "y": 0.8136263489723206}, {"x": 0.8253697156906128, "y": 0.8136263489723206}, {"x": 0.8253697156906128, "y": 0.8237362504005432}, {"x": 0.8048919439315796, "y": 0.8237362504005432}], "text": "(2)\n"}
{"page": 5, "bbox": [{"x": 0.3850966989994049, "y": 0.8140659332275391}, {"x": 0.6131967902183533, "y": 0.8114285469055176}, {"x": 0.6131967902183533, "y": 0.825054943561554}, {"x": 0.3850966989994049, "y": 0.8276923298835754}], "text": "max E(x,y,r)~Dgen log PM(y, r|x).\n"}
{"page": 5, "bbox": [{"x": 0.3924914598464966, "y": 0.8237362504005432}, {"x": 0.4084186553955078, "y": 0.8237362504005432}, {"x": 0.4084186553955078, "y": 0.8298901319503784}, {"x": 0.3924914598464966, "y": 0.8298901319503784}], "text": "M\n"}
{"page": 5, "bbox": [{"x": 0.17633675038814545, "y": 0.841318666934967}, {"x": 0.8248009085655212, "y": 0.841318666934967}, {"x": 0.8248009085655212, "y": 0.8808791041374207}, {"x": 0.17633675038814545, "y": 0.8808791041374207}], "text": "Unlike C training (Eq. 1), M learns to predict the target output as well as the reflection tokens. During\ntraining, we mask out the retrieved text chunks (surrounded by <p> and </p> in Figure 2) for loss\ncalculation and expand the original vocabulary V with a set of reflection tokens {[Critique] [Retrieve]}.\n"}
{"page": 5, "bbox": [{"x": 0.1757679134607315, "y": 0.8984615206718445}, {"x": 0.8242321014404297, "y": 0.8984615206718445}, {"x": 0.8242321014404297, "y": 0.9243956208229065}, {"x": 0.1757679134607315, "y": 0.9243956208229065}], "text": "Connections to prior work on learning with critique. Recent work incorporates additional\ncritique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on\n"}
{"page": 5, "bbox": [{"x": 0.49544936418533325, "y": 0.951208770275116}, {"x": 0.5028441548347473, "y": 0.951208770275116}, {"x": 0.5028441548347473, "y": 0.9582417607307434}, {"x": 0.49544936418533325, "y": 0.9582417607307434}], "text": "5\n"}
{"page": 6, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 6, "bbox": [{"x": 0.1757679134607315, "y": 0.10725274682044983}, {"x": 0.8248009085655212, "y": 0.10725274682044983}, {"x": 0.8248009085655212, "y": 0.1876923143863678}, {"x": 0.1757679134607315, "y": 0.1876923143863678}], "text": "separate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\nlearns to generate special tokens to evaluate its own prediction after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.20791208744049072}, {"x": 0.372013658285141, "y": 0.20835164189338684}, {"x": 0.372013658285141, "y": 0.21670329570770264}, {"x": 0.1769055724143982, "y": 0.21626374125480652}], "text": "3.3 SELF-RAG INFERENCE\n"}
{"page": 6, "bbox": [{"x": 0.17463025450706482, "y": 0.23340658843517303}, {"x": 0.8253697156906128, "y": 0.23340658843517303}, {"x": 0.8253697156906128, "y": 0.5120879411697388}, {"x": 0.17463025450706482, "y": 0.5120879411697388}], "text": "Generating reflection tokens to self-evaluate its own output makes SELF-RAG controllable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\ncontrol to meet these distinct objectives during the inference process.\nAdaptive retrieval with threshold. SELF-RAG dynamically decides when to retrieve text passages by\npredicting Retrieve]. Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\nability of generating the [Retrieve |=Yes token normalized over all output tokens in [Retrieve] surpasses a\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\nCritique token type. For each critique token group G (e.g., ISREL]), we denote its score at timestamp\nt ass, and we compute a segment score as follows:\n"}
{"page": 6, "bbox": [{"x": 0.8048919439315796, "y": 0.5217582583427429}, {"x": 0.8248009085655212, "y": 0.5217582583427429}, {"x": 0.8248009085655212, "y": 0.5323076844215393}, {"x": 0.8048919439315796, "y": 0.5323076844215393}], "text": "(3)\n"}
{"page": 6, "bbox": [{"x": 0.436860054731369, "y": 0.5195604562759399}, {"x": 0.6843003630638123, "y": 0.5208791494369507}, {"x": 0.6843003630638123, "y": 0.5349450707435608}, {"x": 0.436860054731369, "y": 0.53362637758255}], "text": "· P(yt|x, d, y<t)) + S([Critique] where\n"}
{"page": 6, "bbox": [{"x": 0.31569966673851013, "y": 0.5213186740875244}, {"x": 0.4124004542827606, "y": 0.5226373672485352}, {"x": 0.4118316173553467, "y": 0.5340659618377686}, {"x": 0.31569966673851013, "y": 0.5327472686767578}], "text": "f(yt, d, Critique\n"}
{"page": 6, "bbox": [{"x": 0.4260523319244385, "y": 0.5301098823547363}, {"x": 0.4260523319244385, "y": 0.5261538624763489}, {"x": 0.4357224106788635, "y": 0.5261538624763489}, {"x": 0.4357224106788635, "y": 0.5301098823547363}], "text": "=\n"}
{"page": 6, "bbox": [{"x": 0.44368600845336914, "y": 0.5472527742385864}, {"x": 0.4562002420425415, "y": 0.5450549721717834}, {"x": 0.4584755301475525, "y": 0.5525274872779846}, {"x": 0.4459613263607025, "y": 0.5547252893447876}], "text": ",G\n"}
{"page": 6, "bbox": [{"x": 0.40784981846809387, "y": 0.5454944968223572}, {"x": 0.5284414291381836, "y": 0.5441758036613464}, {"x": 0.5290102362632751, "y": 0.5613186955451965}, {"x": 0.4084186553955078, "y": 0.5626373887062073}], "text": "Σws for G =\n"}
{"page": 6, "bbox": [{"x": 0.594994306564331, "y": 0.5485714077949524}, {"x": 0.6860068440437317, "y": 0.5481318831443787}, {"x": 0.6860068440437317, "y": 0.558681309223175}, {"x": 0.594994306564331, "y": 0.5591208934783936}], "text": "ISSUP, ISUSE},\n"}
{"page": 6, "bbox": [{"x": 0.8048919439315796, "y": 0.5481318831443787}, {"x": 0.8236632347106934, "y": 0.5481318831443787}, {"x": 0.8236632347106934, "y": 0.5591208934783936}, {"x": 0.8048919439315796, "y": 0.5591208934783936}], "text": "(4)\n"}
{"page": 6, "bbox": [{"x": 0.31342434883117676, "y": 0.5481318831443787}, {"x": 0.37428897619247437, "y": 0.5507692098617554}, {"x": 0.3737201392650604, "y": 0.5600000023841858}, {"x": 0.3128555119037628, "y": 0.5573626160621643}], "text": "SCritique\n"}
{"page": 6, "bbox": [{"x": 0.3885096609592438, "y": 0.5525274872779846}, {"x": 0.3981797397136688, "y": 0.5525274872779846}, {"x": 0.3981797397136688, "y": 0.5556043982505798}, {"x": 0.3885096609592438, "y": 0.5556043982505798}], "text": "=\n"}
{"page": 6, "bbox": [{"x": 0.5494880676269531, "y": 0.5512087941169739}, {"x": 0.5796359777450562, "y": 0.5516483783721924}, {"x": 0.5796359777450562, "y": 0.5578022003173828}, {"x": 0.5494880676269531, "y": 0.5573626160621643}], "text": "ISREL\n"}
{"page": 6, "bbox": [{"x": 0.4601820111274719, "y": 0.5520879030227661}, {"x": 0.47212740778923035, "y": 0.5529670119285583}, {"x": 0.47098976373672485, "y": 0.5604395866394043}, {"x": 0.45904436707496643, "y": 0.5595604181289673}], "text": "St\n"}
{"page": 6, "bbox": [{"x": 0.4055745303630829, "y": 0.5657142996788025}, {"x": 0.4328782856464386, "y": 0.565274715423584}, {"x": 0.4328782856464386, "y": 0.5731868147850037}, {"x": 0.4055745303630829, "y": 0.5736263990402222}], "text": "GEG\n"}
{"page": 6, "bbox": [{"x": 0.28100115060806274, "y": 0.5806593298912048}, {"x": 0.3100113868713379, "y": 0.5802198052406311}, {"x": 0.3100113868713379, "y": 0.5885714292526245}, {"x": 0.2815699577331543, "y": 0.589011013507843}], "text": "Pt (ŕ)\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.5828571319580078}, {"x": 0.22810012102127075, "y": 0.5832967162132263}, {"x": 0.22810012102127075, "y": 0.5929670333862305}, {"x": 0.17519909143447876, "y": 0.592527449131012}], "text": "where s\n"}
{"page": 6, "bbox": [{"x": 0.2957906723022461, "y": 0.5920879244804382}, {"x": 0.3321956694126129, "y": 0.592527449131012}, {"x": 0.3321956694126129, "y": 0.6030769348144531}, {"x": 0.2957906723022461, "y": 0.6026373505592346}], "text": "Pt(ri)\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.5837362408638}, {"x": 0.8259385824203491, "y": 0.5837362408638}, {"x": 0.8259385824203491, "y": 0.7415384650230408}, {"x": 0.17519909143447876, "y": 0.7415384650230408}], "text": "stands for the generation probability of the most desirable reflection token\nŶ (e.g., ISREL] =Relevant) for the critique token type G with NG distinct tokens (that represent\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\ny is mostly supported by evidence, we can set a weight term for the ISSUP Score higher, while\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\nduring decoding using [Critique]. Instead of using a soft reward function in Eq. 4, we could explicitly\nfilter out a segment continuation when the model generates an undesirable Critique token (e.g.,\n|ISSUP]=No support). Balancing the trade-off between multiple preferences has been studied\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models'\nbehaviors. SELF-RAG tailors an LM with no additional training.\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.7643955945968628}, {"x": 0.3265073895454407, "y": 0.7648351788520813}, {"x": 0.3265073895454407, "y": 0.7758241891860962}, {"x": 0.17519909143447876, "y": 0.7753846049308777}], "text": "4 EXPERIMENTS\n"}
{"page": 6, "bbox": [{"x": 0.1757679134607315, "y": 0.7973626255989075}, {"x": 0.37144482135772705, "y": 0.797802209854126}, {"x": 0.37144482135772705, "y": 0.8057143092155457}, {"x": 0.1757679134607315, "y": 0.8052747249603271}], "text": "4.1 TASKS AND DATASETS\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.8219780325889587}, {"x": 0.8265073895454407, "y": 0.8219780325889587}, {"x": 0.8265073895454407, "y": 0.9243956208229065}, {"x": 0.17519909143447876, "y": 0.9243956208229065}], "text": "We conduct evaluations of our SELF-RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\nour experiments' settings, including test-time instructions, are available in the Appendix Section B.1.\nClosed-set tasks include two datasets, i.e., a fact verification dataset about public health (PubHealth;\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams (ARC-\n"}
{"page": 6, "bbox": [{"x": 0.49544936418533325, "y": 0.9507692456245422}, {"x": 0.5039817690849304, "y": 0.9507692456245422}, {"x": 0.5039817690849304, "y": 0.9586813449859619}, {"x": 0.49544936418533325, "y": 0.9586813449859619}], "text": "6\n"}
{"page": 7, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 7, "bbox": [{"x": 0.17519909143447876, "y": 0.10725274682044983}, {"x": 0.8253697156906128, "y": 0.10725274682044983}, {"x": 0.8253697156906128, "y": 0.3134065866470337}, {"x": 0.17519909143447876, "y": 0.3134065866470337}], "text": "Challenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\naggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\nShort-form generations tasks include two open-domain question answering (QA) datasets,\nPopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need\nto answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset,\nconsisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\nTriviaQA-unfiltered (open) test set is not publicly available, we follow prior work's validation and\ntest split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\nperformance based on whether gold answers are included in the model generations instead of strictly\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\nQA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\nMAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. 5\n"}
{"page": 7, "bbox": [{"x": 0.1757679134607315, "y": 0.3340659439563751}, {"x": 0.2935153543949127, "y": 0.33450549840927124}, {"x": 0.2935153543949127, "y": 0.34285715222358704}, {"x": 0.1757679134607315, "y": 0.34241756796836853}], "text": "4.2 BASELINES\n"}
{"page": 7, "bbox": [{"x": 0.1757679134607315, "y": 0.35956043004989624}, {"x": 0.8259385824203491, "y": 0.35956043004989624}, {"x": 0.8259385824203491, "y": 0.6136263608932495}, {"x": 0.1757679134607315, "y": 0.6136263608932495}], "text": "Baselines without retrievals. We evaluate strong publicly available pre-trained LLMs,\nLlama27B, 13B (Touvron et al., 2023), instruction-tuned models, Alpaca7B,13B (Dubois et al., 2023)\n(our replication based on Llama2); and models trained and reinforced using private data, Chat-\nGPT (Ouyang et al., 2022) and Llama2-chat13B. For instruction-tuned LMs, we use the official\nsystem prompt or instruction format used during training if publicly available. We also compare our\nmethod to concurrent work, CoVE65B (Dhuliawala et al., 2023), which introduces iterative prompt\nengineering to improve the factuality of LLM generations.\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training.\nThe first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\ngiven the query prepended with the top retrieved documents using the same retriever as in our system.\nIt also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\nreflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\nwith LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\naugmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\nsystem. The second category includes concurrent methods that are trained with retrieved text\npassages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\nto pre-train an LM with API calls (e.g., Wikipedia APIs).6\n"}
{"page": 7, "bbox": [{"x": 0.1757679134607315, "y": 0.6342856884002686}, {"x": 0.19852104783058167, "y": 0.6342856884002686}, {"x": 0.19852104783058167, "y": 0.6421977877616882}, {"x": 0.1757679134607315, "y": 0.6421977877616882}], "text": "4.3\n"}
{"page": 7, "bbox": [{"x": 0.2155858874320984, "y": 0.6338461637496948}, {"x": 0.39704209566116333, "y": 0.6351648569107056}, {"x": 0.39704209566116333, "y": 0.643516480922699}, {"x": 0.2155858874320984, "y": 0.6421977877616882}], "text": "EXPERIMENTAL SETTINGS\n"}
{"page": 7, "bbox": [{"x": 0.17519909143447876, "y": 0.6597802042961121}, {"x": 0.8242321014404297, "y": 0.6597802042961121}, {"x": 0.8242321014404297, "y": 0.7542856931686401}, {"x": 0.17519909143447876, "y": 0.7542856931686401}], "text": "Training data and settings. Our training data consists of diverse instruction-following input-output\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\nour generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\ndocuments for each input. More training details are in the Appendix Section B.1.\n"}
{"page": 7, "bbox": [{"x": 0.733219563961029, "y": 0.766153872013092}, {"x": 0.7747440338134766, "y": 0.767472505569458}, {"x": 0.774175226688385, "y": 0.7745054960250854}, {"x": 0.7326507568359375, "y": 0.7731868028640747}], "text": "ISSUP,\n"}
{"page": 7, "bbox": [{"x": 0.7895335555076599, "y": 0.7670329809188843}, {"x": 0.8185437917709351, "y": 0.767472505569458}, {"x": 0.8185437917709351, "y": 0.7736263871192932}, {"x": 0.7895335555076599, "y": 0.7731868028640747}], "text": "ISUSE\n"}
{"page": 7, "bbox": [{"x": 0.17519909143447876, "y": 0.7635164856910706}, {"x": 0.8248009085655212, "y": 0.7648351788520813}, {"x": 0.8242321014404297, "y": 0.9116483330726624}, {"x": 0.17463025450706482, "y": 0.9103296995162964}], "text": "Inference settings. As a default configuration, we assign the weight terms ISREL\nvalues of 1.0, 1.0 and 0.5, respectively. To encourage frequent retrieval, we set the retrieval threshold\nto 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed\nup inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2.\nFor a token-level generation, we use greedy decoding. By default, we use the top five documents\nfrom Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we\nuse additional top five documents retrieved by a web search engine, following Luo et al. (2023);\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\nbaselines for a fair comparison.\nShttps://github.com/princeton-nlp/ALCE\n"}
{"page": 7, "bbox": [{"x": 0.1962457299232483, "y": 0.9112088084220886}, {"x": 0.22127418220043182, "y": 0.9120879173278809}, {"x": 0.22070534527301788, "y": 0.921758234500885}, {"x": 0.19567690789699554, "y": 0.9208791255950928}], "text": "6We\n"}
{"page": 7, "bbox": [{"x": 0.2246871441602707, "y": 0.9138461351394653}, {"x": 0.7792946696281433, "y": 0.9138461351394653}, {"x": 0.7792946696281433, "y": 0.923956036567688}, {"x": 0.2246871441602707, "y": 0.923956036567688}], "text": "report numbers using the results reported in the paper as the implementations are not available.\n"}
{"page": 7, "bbox": [{"x": 0.49544936418533325, "y": 0.9507692456245422}, {"x": 0.5039817690849304, "y": 0.9507692456245422}, {"x": 0.5039817690849304, "y": 0.9586813449859619}, {"x": 0.49544936418533325, "y": 0.9586813449859619}], "text": "7\n"}
{"page": 8, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 8, "bbox": [{"x": 0.1757679134607315, "y": 0.10417582094669342}, {"x": 0.8248009085655212, "y": 0.10417582094669342}, {"x": 0.8248009085655212, "y": 0.1850549429655075}, {"x": 0.1757679134607315, "y": 0.1850549429655075}], "text": "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\nthey outperforms all non-proprietary models. * indicates concurrent or recent results reported by\nconcurrent work. – indicates numbers that are not reported by the original papers or are not applicable.\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n"}
{"page": 8, "bbox": [{"x": 0.34186574816703796, "y": 0.20483516156673431}, {"x": 0.40500569343566895, "y": 0.20483516156673431}, {"x": 0.40500569343566895, "y": 0.21230769157409668}, {"x": 0.34186574816703796, "y": 0.21230769157409668}], "text": "Short-form\n"}
{"page": 8, "bbox": [{"x": 0.44880545139312744, "y": 0.2043956071138382}, {"x": 0.5113765597343445, "y": 0.20483516156673431}, {"x": 0.5113765597343445, "y": 0.2127472460269928}, {"x": 0.44880545139312744, "y": 0.21230769157409668}], "text": "Closed-set\n"}
{"page": 8, "bbox": [{"x": 0.5591581463813782, "y": 0.20395603775978088}, {"x": 0.7872582674026489, "y": 0.20351648330688477}, {"x": 0.7872582674026489, "y": 0.22637362778186798}, {"x": 0.5591581463813782, "y": 0.2268131822347641}], "text": "Long-form generations (with citations)\nASQA\n"}
{"page": 8, "bbox": [{"x": 0.19055745005607605, "y": 0.23032967746257782}, {"x": 0.21160408854484558, "y": 0.23032967746257782}, {"x": 0.21160408854484558, "y": 0.237802192568779}, {"x": 0.19055745005607605, "y": 0.237802192568779}], "text": "LM\n"}
{"page": 8, "bbox": [{"x": 0.6348122954368591, "y": 0.2294505536556244}, {"x": 0.8083049058914185, "y": 0.23032967746257782}, {"x": 0.8083049058914185, "y": 0.2408791184425354}, {"x": 0.6348122954368591, "y": 0.23999999463558197}], "text": "(rg) (mau) (pre) (rec)\n"}
{"page": 8, "bbox": [{"x": 0.3265073895454407, "y": 0.2153846174478531}, {"x": 0.6120591759681702, "y": 0.21626374125480652}, {"x": 0.6114903092384338, "y": 0.3081318736076355}, {"x": 0.32593855261802673, "y": 0.3072527348995209}], "text": "PopQA TQA Pub ARC Bio\n(acc) (acc) (acc) (acc) (FS) (em)\nLMs with proprietary data\n20.0 59.3\n49.4 38.4 55.9\n51.8 59.8 52.1 37.9 79.9\n29.3 74.3 70.1 75.3 71.8\n50.8 65.7 54.7 75.3\n"}
{"page": 8, "bbox": [{"x": 0.19055745005607605, "y": 0.2606593370437622}, {"x": 0.26393628120422363, "y": 0.26241758465766907}, {"x": 0.2633674740791321, "y": 0.271208792924881}, {"x": 0.1899886280298233, "y": 0.2694505453109741}], "text": "Llama2-C13B\n"}
{"page": 8, "bbox": [{"x": 0.5858930349349976, "y": 0.2606593370437622}, {"x": 0.711604118347168, "y": 0.2606593370437622}, {"x": 0.711604118347168, "y": 0.2945055067539215}, {"x": 0.5858930349349976, "y": 0.2945055067539215}], "text": "22.4\n29.6 28.6\n32.8 34.8 43.8\n35.3 36.2 68.8\n"}
{"page": 8, "bbox": [{"x": 0.7832764387130737, "y": 0.2738461494445801}, {"x": 0.806598424911499, "y": 0.2738461494445801}, {"x": 0.806598424911499, "y": 0.28131869435310364}, {"x": 0.7832764387130737, "y": 0.28131869435310364}], "text": "36.1\n"}
{"page": 8, "bbox": [{"x": 0.7349260449409485, "y": 0.2738461494445801}, {"x": 0.7588168382644653, "y": 0.2742857038974762}, {"x": 0.7588168382644653, "y": 0.28175824880599976}, {"x": 0.7349260449409485, "y": 0.28131869435310364}], "text": "19.8\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.27340659499168396}, {"x": 0.28896471858024597, "y": 0.2742857038974762}, {"x": 0.28896471858024597, "y": 0.282637357711792}, {"x": 0.19112628698349, "y": 0.28175824880599976}], "text": "Ret-Llama2-C13B\n"}
{"page": 8, "bbox": [{"x": 0.19055745005607605, "y": 0.28703296184539795}, {"x": 0.24630261957645416, "y": 0.28703296184539795}, {"x": 0.24630261957645416, "y": 0.2945055067539215}, {"x": 0.19055745005607605, "y": 0.2945055067539215}], "text": "ChatGPT\n"}
{"page": 8, "bbox": [{"x": 0.6860068440437317, "y": 0.29890111088752747}, {"x": 0.8083049058914185, "y": 0.29890111088752747}, {"x": 0.8083049058914185, "y": 0.3072527348995209}, {"x": 0.6860068440437317, "y": 0.3072527348995209}], "text": "79.7 65.1 76.6\n"}
{"page": 8, "bbox": [{"x": 0.5858930349349976, "y": 0.29890111088752747}, {"x": 0.6564277410507202, "y": 0.2997802197933197}, {"x": 0.6564277410507202, "y": 0.3076923191547394}, {"x": 0.5858930349349976, "y": 0.30681318044662476}], "text": "40.7 39.9\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.2997802197933197}, {"x": 0.27189987897872925, "y": 0.2997802197933197}, {"x": 0.27189987897872925, "y": 0.3072527348995209}, {"x": 0.19112628698349, "y": 0.3072527348995209}], "text": "Ret-ChatGPT\n"}
{"page": 8, "bbox": [{"x": 0.5568827986717224, "y": 0.30373626947402954}, {"x": 0.5642775893211365, "y": 0.30373626947402954}, {"x": 0.5642775893211365, "y": 0.3054945170879364}, {"x": 0.5568827986717224, "y": 0.3054945170879364}], "text": "-\n"}
{"page": 8, "bbox": [{"x": 0.5392491221427917, "y": 0.31208792328834534}, {"x": 0.564846396446228, "y": 0.31208792328834534}, {"x": 0.564846396446228, "y": 0.3191208839416504}, {"x": 0.5392491221427917, "y": 0.3191208839416504}], "text": "71.2\n"}
{"page": 8, "bbox": [{"x": 0.19055745005607605, "y": 0.31208792328834534}, {"x": 0.26393628120422363, "y": 0.31208792328834534}, {"x": 0.26393628120422363, "y": 0.3217582404613495}, {"x": 0.19055745005607605, "y": 0.3217582404613495}], "text": "Perplexity.ai\n"}
{"page": 8, "bbox": [{"x": 0.4209328889846802, "y": 0.33142855763435364}, {"x": 0.5796359777450562, "y": 0.33142855763435364}, {"x": 0.5796359777450562, "y": 0.3389011025428772}, {"x": 0.4209328889846802, "y": 0.3389011025428772}], "text": "Baselines without retrieval\n"}
{"page": 8, "bbox": [{"x": 0.3953356146812439, "y": 0.34285715222358704}, {"x": 0.42036405205726624, "y": 0.34329670667648315}, {"x": 0.4197952151298523, "y": 0.35120880603790283}, {"x": 0.3953356146812439, "y": 0.3507692217826843}], "text": "30.5\n"}
{"page": 8, "bbox": [{"x": 0.6325369477272034, "y": 0.34285715222358704}, {"x": 0.6569966077804565, "y": 0.34329670667648315}, {"x": 0.6564277410507202, "y": 0.35120880603790283}, {"x": 0.6325369477272034, "y": 0.3507692217826843}], "text": "15.3\n"}
{"page": 8, "bbox": [{"x": 0.6871444582939148, "y": 0.34285715222358704}, {"x": 0.7110352516174316, "y": 0.34329670667648315}, {"x": 0.7104664444923401, "y": 0.35120880603790283}, {"x": 0.6865756511688232, "y": 0.3507692217826843}], "text": "19.0\n"}
{"page": 8, "bbox": [{"x": 0.3464163839817047, "y": 0.3437362611293793}, {"x": 0.37030717730522156, "y": 0.3437362611293793}, {"x": 0.37030717730522156, "y": 0.35120880603790283}, {"x": 0.3464163839817047, "y": 0.35120880603790283}], "text": "14.7\n"}
{"page": 8, "bbox": [{"x": 0.5938566327095032, "y": 0.3437362611293793}, {"x": 0.6114903092384338, "y": 0.3437362611293793}, {"x": 0.6114903092384338, "y": 0.35120880603790283}, {"x": 0.5938566327095032, "y": 0.35120880603790283}], "text": "7.9\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.34285715222358704}, {"x": 0.2468714416027069, "y": 0.3437362611293793}, {"x": 0.2468714416027069, "y": 0.3529670238494873}, {"x": 0.19112628698349, "y": 0.35208791494369507}], "text": "Llama27B\n"}
{"page": 8, "bbox": [{"x": 0.4431171715259552, "y": 0.34285715222358704}, {"x": 0.6103526949882507, "y": 0.3437362611293793}, {"x": 0.6103526949882507, "y": 0.3648351728916168}, {"x": 0.4431171715259552, "y": 0.3639560341835022}], "text": "34.2 21.8 44.5\n49.8 45.0 45.8 18.8\n"}
{"page": 8, "bbox": [{"x": 0.6860068440437317, "y": 0.356483519077301}, {"x": 0.7104664444923401, "y": 0.3556044101715088}, {"x": 0.7110352516174316, "y": 0.36307692527770996}, {"x": 0.6865756511688232, "y": 0.3639560341835022}], "text": "61.7\n"}
{"page": 8, "bbox": [{"x": 0.6313993334770203, "y": 0.3560439646244049}, {"x": 0.6569966077804565, "y": 0.356483519077301}, {"x": 0.6569966077804565, "y": 0.3639560341835022}, {"x": 0.6313993334770203, "y": 0.3635164797306061}], "text": "29.4\n"}
{"page": 8, "bbox": [{"x": 0.394197940826416, "y": 0.356483519077301}, {"x": 0.41922640800476074, "y": 0.3556044101715088}, {"x": 0.4197952151298523, "y": 0.3635164797306061}, {"x": 0.39476677775382996, "y": 0.3643956184387207}], "text": "54.5\n"}
{"page": 8, "bbox": [{"x": 0.3447099030017853, "y": 0.356483519077301}, {"x": 0.37144482135772705, "y": 0.356483519077301}, {"x": 0.37144482135772705, "y": 0.3639560341835022}, {"x": 0.3447099030017853, "y": 0.3639560341835022}], "text": "23.6\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.3560439646244049}, {"x": 0.2440273016691208, "y": 0.356483519077301}, {"x": 0.2440273016691208, "y": 0.3665934205055237}, {"x": 0.19112628698349, "y": 0.3661538362503052}], "text": "Alpaca7B\n"}
{"page": 8, "bbox": [{"x": 0.7508532404899597, "y": 0.34505495429039}, {"x": 0.7599545121192932, "y": 0.34505495429039}, {"x": 0.7593856453895569, "y": 0.39956045150756836}, {"x": 0.7502844333648682, "y": 0.39956045150756836}], "text": "| | | ││\n"}
{"page": 8, "bbox": [{"x": 0.3953356146812439, "y": 0.36879122257232666}, {"x": 0.41922640800476074, "y": 0.36835163831710815}, {"x": 0.41922640800476074, "y": 0.37626373767852783}, {"x": 0.3953356146812439, "y": 0.37670329213142395}], "text": "38.5\n"}
{"page": 8, "bbox": [{"x": 0.6871444582939148, "y": 0.3692307770252228}, {"x": 0.7104664444923401, "y": 0.3692307770252228}, {"x": 0.7104664444923401, "y": 0.3758241832256317}, {"x": 0.6871444582939148, "y": 0.3758241832256317}], "text": "16.0\n"}
{"page": 8, "bbox": [{"x": 0.44368600845336914, "y": 0.3692307770252228}, {"x": 0.4687144458293915, "y": 0.3692307770252228}, {"x": 0.4687144458293915, "y": 0.37626373767852783}, {"x": 0.44368600845336914, "y": 0.37626373767852783}], "text": "29.4\n"}
{"page": 8, "bbox": [{"x": 0.5392491221427917, "y": 0.3692307770252228}, {"x": 0.5642775893211365, "y": 0.3692307770252228}, {"x": 0.5642775893211365, "y": 0.37626373767852783}, {"x": 0.5392491221427917, "y": 0.37626373767852783}], "text": "53.4\n"}
{"page": 8, "bbox": [{"x": 0.6325369477272034, "y": 0.3692307770252228}, {"x": 0.6564277410507202, "y": 0.3692307770252228}, {"x": 0.6564277410507202, "y": 0.37626373767852783}, {"x": 0.6325369477272034, "y": 0.37626373767852783}], "text": "12.4\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.3674725294113159}, {"x": 0.25255972146987915, "y": 0.3692307770252228}, {"x": 0.2519908845424652, "y": 0.3784615397453308}, {"x": 0.19055745005607605, "y": 0.37670329213142395}], "text": "Llama213B\n"}
{"page": 8, "bbox": [{"x": 0.3464163839817047, "y": 0.3692307770252228}, {"x": 0.37030717730522156, "y": 0.3692307770252228}, {"x": 0.37030717730522156, "y": 0.37670329213142395}, {"x": 0.3464163839817047, "y": 0.37670329213142395}], "text": "14.7\n"}
{"page": 8, "bbox": [{"x": 0.5932878255844116, "y": 0.3692307770252228}, {"x": 0.6120591759681702, "y": 0.3692307770252228}, {"x": 0.6120591759681702, "y": 0.37670329213142395}, {"x": 0.5932878255844116, "y": 0.37670329213142395}], "text": "7.2\n"}
{"page": 8, "bbox": [{"x": 0.6313993334770203, "y": 0.3810988962650299}, {"x": 0.6558589339256287, "y": 0.3806593418121338}, {"x": 0.6558589339256287, "y": 0.38813185691833496}, {"x": 0.6313993334770203, "y": 0.38857144117355347}], "text": "32.0\n"}
{"page": 8, "bbox": [{"x": 0.3447099030017853, "y": 0.3810988962650299}, {"x": 0.3708759844303131, "y": 0.381538450717926}, {"x": 0.3708759844303131, "y": 0.3890109956264496}, {"x": 0.3447099030017853, "y": 0.38857144117355347}], "text": "24.4\n"}
{"page": 8, "bbox": [{"x": 0.44368600845336914, "y": 0.381538450717926}, {"x": 0.4692832827568054, "y": 0.381538450717926}, {"x": 0.4692832827568054, "y": 0.38857144117355347}, {"x": 0.44368600845336914, "y": 0.38857144117355347}], "text": "55.5\n"}
{"page": 8, "bbox": [{"x": 0.5858930349349976, "y": 0.381538450717926}, {"x": 0.6114903092384338, "y": 0.381538450717926}, {"x": 0.6114903092384338, "y": 0.38857144117355347}, {"x": 0.5858930349349976, "y": 0.38857144117355347}], "text": "22.9\n"}
{"page": 8, "bbox": [{"x": 0.6860068440437317, "y": 0.381538450717926}, {"x": 0.7121729254722595, "y": 0.381538450717926}, {"x": 0.7121729254722595, "y": 0.3890109956264496}, {"x": 0.6860068440437317, "y": 0.3890109956264496}], "text": "70.6\n"}
{"page": 8, "bbox": [{"x": 0.49317407608032227, "y": 0.36879122257232666}, {"x": 0.5642775893211365, "y": 0.36835163831710815}, {"x": 0.564846396446228, "y": 0.40175825357437134}, {"x": 0.4937428832054138, "y": 0.40219780802726746}], "text": "29.4\n54.9 50.2\n71.2\n"}
{"page": 8, "bbox": [{"x": 0.3953356146812439, "y": 0.38197803497314453}, {"x": 0.4197952151298523, "y": 0.38197803497314453}, {"x": 0.4197952151298523, "y": 0.3890109956264496}, {"x": 0.3953356146812439, "y": 0.3890109956264496}], "text": "61.3\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.3810988962650299}, {"x": 0.24914675951004028, "y": 0.38197803497314453}, {"x": 0.24857792258262634, "y": 0.3916483521461487}, {"x": 0.19112628698349, "y": 0.39076924324035645}], "text": "Alpaca13B\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.39340659976005554}, {"x": 0.2548350393772125, "y": 0.3947252631187439}, {"x": 0.2548350393772125, "y": 0.4026373624801636}, {"x": 0.19112628698349, "y": 0.40175825357437134}], "text": "COVE65B *\n"}
{"page": 8, "bbox": [{"x": 0.4306029677391052, "y": 0.41274726390838623}, {"x": 0.5705347061157227, "y": 0.41274726390838623}, {"x": 0.5705347061157227, "y": 0.4202197790145874}, {"x": 0.4306029677391052, "y": 0.4202197790145874}], "text": "Baselines with retrieval\n"}
{"page": 8, "bbox": [{"x": 0.394197940826416, "y": 0.4259340763092041}, {"x": 0.4186575710773468, "y": 0.4259340763092041}, {"x": 0.4186575710773468, "y": 0.43252748250961304}, {"x": 0.394197940826416, "y": 0.43252748250961304}], "text": "48.8\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.4254944920539856}, {"x": 0.26279863715171814, "y": 0.4254944920539856}, {"x": 0.26279863715171814, "y": 0.4334065914154053}, {"x": 0.19112628698349, "y": 0.4334065914154053}], "text": "Toolformer*\n"}
{"page": 8, "bbox": [{"x": 0.26564276218414307, "y": 0.4290109872817993}, {"x": 0.27588167786598206, "y": 0.4290109872817993}, {"x": 0.27588167786598206, "y": 0.434725284576416}, {"x": 0.26564276218414307, "y": 0.434725284576416}], "text": "6B\n"}
{"page": 8, "bbox": [{"x": 0.6865756511688232, "y": 0.43780219554901123}, {"x": 0.7110352516174316, "y": 0.43824175000190735}, {"x": 0.7110352516174316, "y": 0.4457142949104309}, {"x": 0.6865756511688232, "y": 0.4452747106552124}], "text": "32.0\n"}
{"page": 8, "bbox": [{"x": 0.7406143546104431, "y": 0.43868130445480347}, {"x": 0.7593856453895569, "y": 0.43868130445480347}, {"x": 0.7593856453895569, "y": 0.4457142949104309}, {"x": 0.7406143546104431, "y": 0.4457142949104309}], "text": "2.9\n"}
{"page": 8, "bbox": [{"x": 0.7895335555076599, "y": 0.43868130445480347}, {"x": 0.8077360391616821, "y": 0.43868130445480347}, {"x": 0.8077360391616821, "y": 0.4457142949104309}, {"x": 0.7895335555076599, "y": 0.4457142949104309}], "text": "4.0\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.43780219554901123}, {"x": 0.24744027853012085, "y": 0.439120888710022}, {"x": 0.2468714416027069, "y": 0.4474725127220154}, {"x": 0.19112628698349, "y": 0.446153849363327}], "text": "Llama27B\n"}
{"page": 8, "bbox": [{"x": 0.6860068440437317, "y": 0.450549453496933}, {"x": 0.7093287706375122, "y": 0.45010989904403687}, {"x": 0.7093287706375122, "y": 0.45758241415023804}, {"x": 0.6860068440437317, "y": 0.45802196860313416}], "text": "57.9\n"}
{"page": 8, "bbox": [{"x": 0.5864619016647339, "y": 0.4509890079498291}, {"x": 0.6564277410507202, "y": 0.4487912058830261}, {"x": 0.6569966077804565, "y": 0.45758241415023804}, {"x": 0.5870307087898254, "y": 0.459780216217041}], "text": "30.9 33.3\n"}
{"page": 8, "bbox": [{"x": 0.7406143546104431, "y": 0.4509890079498291}, {"x": 0.7599545121192932, "y": 0.4509890079498291}, {"x": 0.7599545121192932, "y": 0.45802196860313416}, {"x": 0.7406143546104431, "y": 0.45802196860313416}], "text": "5.5\n"}
{"page": 8, "bbox": [{"x": 0.7901023626327515, "y": 0.4509890079498291}, {"x": 0.80887371301651, "y": 0.4509890079498291}, {"x": 0.80887371301651, "y": 0.45802196860313416}, {"x": 0.7901023626327515, "y": 0.45802196860313416}], "text": "7.2\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.450549453496933}, {"x": 0.2440273016691208, "y": 0.4509890079498291}, {"x": 0.2440273016691208, "y": 0.46109890937805176}, {"x": 0.19112628698349, "y": 0.46065935492515564}], "text": "Alpaca7B\n"}
{"page": 8, "bbox": [{"x": 0.3447099030017853, "y": 0.43824175000190735}, {"x": 0.6558589339256287, "y": 0.43824175000190735}, {"x": 0.6558589339256287, "y": 0.4852747321128845}, {"x": 0.3447099030017853, "y": 0.4852747321128845}], "text": "38.2 42.5 30.0 48.0 78.0 15.2 22.1\n46.7 64.1 40.2 48.0 76.6\n57.3\n64.3 65.8 78.2\n69.2 48.4\n"}
{"page": 8, "bbox": [{"x": 0.6854379773139954, "y": 0.46373626589775085}, {"x": 0.7104664444923401, "y": 0.46373626589775085}, {"x": 0.7104664444923401, "y": 0.4707692265510559}, {"x": 0.6854379773139954, "y": 0.4707692265510559}], "text": "51.2\n"}
{"page": 8, "bbox": [{"x": 0.7411831617355347, "y": 0.46373626589775085}, {"x": 0.7599545121192932, "y": 0.46373626589775085}, {"x": 0.7599545121192932, "y": 0.4707692265510559}, {"x": 0.7411831617355347, "y": 0.4707692265510559}], "text": "5.0\n"}
{"page": 8, "bbox": [{"x": 0.3447099030017853, "y": 0.46373626589775085}, {"x": 0.3691695034503937, "y": 0.464175820350647}, {"x": 0.3691695034503937, "y": 0.471208781003952}, {"x": 0.3447099030017853, "y": 0.4707692265510559}], "text": "48.7\n"}
{"page": 8, "bbox": [{"x": 0.5864619016647339, "y": 0.464175820350647}, {"x": 0.6114903092384338, "y": 0.464175820350647}, {"x": 0.6114903092384338, "y": 0.4707692265510559}, {"x": 0.5864619016647339, "y": 0.4707692265510559}], "text": "31.0\n"}
{"page": 8, "bbox": [{"x": 0.6319681406021118, "y": 0.46373626589775085}, {"x": 0.6564277410507202, "y": 0.46373626589775085}, {"x": 0.6564277410507202, "y": 0.471208781003952}, {"x": 0.6319681406021118, "y": 0.471208781003952}], "text": "35.8\n"}
{"page": 8, "bbox": [{"x": 0.7901023626327515, "y": 0.46373626589775085}, {"x": 0.80887371301651, "y": 0.46373626589775085}, {"x": 0.80887371301651, "y": 0.471208781003952}, {"x": 0.7901023626327515, "y": 0.471208781003952}], "text": "7.5\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.4628571569919586}, {"x": 0.2696245610713959, "y": 0.46373626589775085}, {"x": 0.2696245610713959, "y": 0.47252747416496277}, {"x": 0.19112628698349, "y": 0.47164836525917053}], "text": "Llama2-FT7B\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.4764835238456726}, {"x": 0.24175198376178741, "y": 0.4764835238456726}, {"x": 0.24175198376178741, "y": 0.4848351776599884}, {"x": 0.19112628698349, "y": 0.4848351776599884}], "text": "SAIL*7B\n"}
{"page": 8, "bbox": [{"x": 0.394197940826416, "y": 0.48835164308547974}, {"x": 0.46814560890197754, "y": 0.48879119753837585}, {"x": 0.46814560890197754, "y": 0.4962637424468994}, {"x": 0.394197940826416, "y": 0.4958241879940033}], "text": "47.0 30.2\n"}
{"page": 8, "bbox": [{"x": 0.6860068440437317, "y": 0.48879119753837585}, {"x": 0.7110352516174316, "y": 0.48879119753837585}, {"x": 0.7110352516174316, "y": 0.4958241879940033}, {"x": 0.6860068440437317, "y": 0.4958241879940033}], "text": "24.7\n"}
{"page": 8, "bbox": [{"x": 0.7400454878807068, "y": 0.48879119753837585}, {"x": 0.7593856453895569, "y": 0.4879120886325836}, {"x": 0.7599545121192932, "y": 0.4958241879940033}, {"x": 0.7406143546104431, "y": 0.49670329689979553}], "text": "2.3\n"}
{"page": 8, "bbox": [{"x": 0.7895335555076599, "y": 0.48879119753837585}, {"x": 0.80887371301651, "y": 0.48879119753837585}, {"x": 0.80887371301651, "y": 0.4958241879940033}, {"x": 0.7895335555076599, "y": 0.4958241879940033}], "text": "3.6\n"}
{"page": 8, "bbox": [{"x": 0.3447099030017853, "y": 0.48879119753837585}, {"x": 0.37030717730522156, "y": 0.48879119753837585}, {"x": 0.37030717730522156, "y": 0.4962637424468994}, {"x": 0.3447099030017853, "y": 0.4962637424468994}], "text": "45.7\n"}
{"page": 8, "bbox": [{"x": 0.19112628698349, "y": 0.4874725341796875}, {"x": 0.25255972146987915, "y": 0.48923078179359436}, {"x": 0.2519908845424652, "y": 0.49758240580558777}, {"x": 0.19055745005607605, "y": 0.4958241879940033}], "text": "Llama213B\n"}
{"page": 8, "bbox": [{"x": 0.3447099030017853, "y": 0.5015384554862976}, {"x": 0.3697383403778076, "y": 0.5015384554862976}, {"x": 0.3697383403778076, "y": 0.508571445941925}, {"x": 0.3447099030017853, "y": 0.508571445941925}], "text": "46.1\n"}
{"page": 8, "bbox": [{"x": 0.3953356146812439, "y": 0.5010989308357239}, {"x": 0.4687144458293915, "y": 0.5010989308357239}, {"x": 0.4687144458293915, "y": 0.5090109705924988}, {"x": 0.3953356146812439, "y": 0.5090109705924988}], "text": "66.9 51.1\n"}
{"page": 8, "bbox": [{"x": 0.6854379773139954, "y": 0.5015384554862976}, {"x": 0.7121729254722595, "y": 0.5015384554862976}, {"x": 0.7121729254722595, "y": 0.508571445941925}, {"x": 0.6854379773139954, "y": 0.508571445941925}], "text": "56.6\n"}
{"page": 8, "bbox": [{"x": 0.7901023626327515, "y": 0.5015384554862976}, {"x": 0.8077360391616821, "y": 0.5015384554862976}, {"x": 0.8077360391616821, "y": 0.508571445941925}, {"x": 0.7901023626327515, "y": 0.508571445941925}], "text": "3.8\n"}
{"page": 8, "bbox": [{"x": 0.7411831617355347, "y": 0.5015384554862976}, {"x": 0.7605233192443848, "y": 0.5015384554862976}, {"x": 0.7605233192443848, "y": 0.5090109705924988}, {"x": 0.7411831617355347, "y": 0.5090109705924988}], "text": "2.0\n"}
{"page": 8, "bbox": [{"x": 0.19055745005607605, "y": 0.5006593465805054}, {"x": 0.24914675951004028, "y": 0.5015384554862976}, {"x": 0.24914675951004028, "y": 0.5116483569145203}, {"x": 0.19055745005607605, "y": 0.510769248008728}], "text": "Alpaca13B\n"}
{"page": 8, "bbox": [{"x": 0.49317407608032227, "y": 0.4879120886325836}, {"x": 0.6569966077804565, "y": 0.4874725341796875}, {"x": 0.6569966077804565, "y": 0.5349450707435608}, {"x": 0.49317407608032227, "y": 0.5353845953941345}], "text": "26.0 77.5 16.3 20.5\n57.6 77.7 34.8 36.7\n67.3 81.2 30.0 35.7\n73.1 80.2 31.7 37.0\n"}
{"page": 8, "bbox": [{"x": 0.7827076315879822, "y": 0.5142857432365417}, {"x": 0.8071672320365906, "y": 0.5134065747261047}, {"x": 0.8077360391616821, "y": 0.5213186740875244}, {"x": 0.7832764387130737, "y": 0.5221977829933167}], "text": "67.8\n"}
{"page": 8, "bbox": [{"x": 0.3452787399291992, "y": 0.5147252678871155}, {"x": 0.3697383403778076, "y": 0.5147252678871155}, {"x": 0.3697383403778076, "y": 0.5217582583427429}, {"x": 0.3452787399291992, "y": 0.5217582583427429}], "text": "54.9\n"}
{"page": 8, "bbox": [{"x": 0.19169510900974274, "y": 0.5134065747261047}, {"x": 0.3020477890968323, "y": 0.5147252678871155}, {"x": 0.3020477890968323, "y": 0.5235164761543274}, {"x": 0.19169510900974274, "y": 0.5221977829933167}], "text": "Our SELF-RAG 7B\n"}
{"page": 8, "bbox": [{"x": 0.39476677775382996, "y": 0.5134065747261047}, {"x": 0.4692832827568054, "y": 0.512967050075531}, {"x": 0.4692832827568054, "y": 0.5340659618377686}, {"x": 0.39476677775382996, "y": 0.5345054864883423}], "text": "66.4 72.4\n69.3 74.5\n"}
{"page": 8, "bbox": [{"x": 0.6860068440437317, "y": 0.512967050075531}, {"x": 0.7599545121192932, "y": 0.5125274658203125}, {"x": 0.7599545121192932, "y": 0.5345054864883423}, {"x": 0.6860068440437317, "y": 0.5349450707435608}], "text": "74.3 66.9\n71.6 70.3\n"}
{"page": 8, "bbox": [{"x": 0.3452787399291992, "y": 0.5270329713821411}, {"x": 0.37030717730522156, "y": 0.5270329713821411}, {"x": 0.37030717730522156, "y": 0.5345054864883423}, {"x": 0.3452787399291992, "y": 0.5345054864883423}], "text": "55.8\n"}
{"page": 8, "bbox": [{"x": 0.7827076315879822, "y": 0.5270329713821411}, {"x": 0.8083049058914185, "y": 0.5270329713821411}, {"x": 0.8083049058914185, "y": 0.5345054864883423}, {"x": 0.7827076315879822, "y": 0.5345054864883423}], "text": "71.3\n"}
{"page": 8, "bbox": [{"x": 0.19169510900974274, "y": 0.5257142782211304}, {"x": 0.3071672320365906, "y": 0.5274725556373596}, {"x": 0.3071672320365906, "y": 0.5362637639045715}, {"x": 0.19169510900974274, "y": 0.5345054864883423}], "text": "Our SELF-RAG 13B\n"}
{"page": 8, "bbox": [{"x": 0.17633675038814545, "y": 0.5701099038124084}, {"x": 0.4135380983352661, "y": 0.5705494284629822}, {"x": 0.4135380983352661, "y": 0.5810989141464233}, {"x": 0.17633675038814545, "y": 0.5806593298912048}], "text": "5 RESULTS AND ANALYSIS\n"}
{"page": 8, "bbox": [{"x": 0.17633675038814545, "y": 0.6026373505592346}, {"x": 0.32252559065818787, "y": 0.6030769348144531}, {"x": 0.32252559065818787, "y": 0.6114285588264465}, {"x": 0.17633675038814545, "y": 0.6109890341758728}], "text": "5.1 MAIN RESULTS\n"}
{"page": 8, "bbox": [{"x": 0.1757679134607315, "y": 0.6285714507102966}, {"x": 0.8259385824203491, "y": 0.6285714507102966}, {"x": 0.8259385824203491, "y": 0.9243956208229065}, {"x": 0.1757679134607315, "y": 0.9243956208229065}], "text": "Comparison against baselines without retrieval. Table 2 (top) presents the baselines without\nretrieval. Our SELF-RAG (bottom two rows) demonstrates a substantial performance advantage\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\ntask, our 7B and 13B models outperform the concurrent COVE (Dhuliawala et al., 2023), which\niteratively prompts Llama265 to refine output.\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAG also\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\nretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\naccuracy. On ASQA, our model shows significantly higher citation precision and recall than all\nmodels except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\nin this particular task, surpassing smaller LMs. Our SELF-RAG bridges this performance gap, even\noutperforming ChatGPT in citation precision, which measures whether the model-generated claim is\nfully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAG to often generate\n"}
{"page": 8, "bbox": [{"x": 0.49544936418533325, "y": 0.951208770275116}, {"x": 0.5028441548347473, "y": 0.951208770275116}, {"x": 0.5028441548347473, "y": 0.9582417607307434}, {"x": 0.49544936418533325, "y": 0.9582417607307434}], "text": "8\n"}
{"page": 9, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.04703296720981598}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 9, "bbox": [{"x": 0.6905574798583984, "y": 0.10681318491697311}, {"x": 0.7423208355903625, "y": 0.10681318491697311}, {"x": 0.7423208355903625, "y": 0.11296703666448593}, {"x": 0.6905574798583984, "y": 0.11296703666448593}], "text": "PubHealth\n"}
{"page": 9, "bbox": [{"x": 0.6274175047874451, "y": 0.11604395508766174}, {"x": 0.6456200480461121, "y": 0.11604395508766174}, {"x": 0.6456200480461121, "y": 0.1204395592212677}, {"x": 0.6274175047874451, "y": 0.1204395592212677}], "text": "1.00\n"}
{"page": 9, "bbox": [{"x": 0.7878270745277405, "y": 0.11604395508766174}, {"x": 0.8003413081169128, "y": 0.11604395508766174}, {"x": 0.8003413081169128, "y": 0.12087912112474442}, {"x": 0.7878270745277405, "y": 0.12087912112474442}], "text": "1.0\n"}
{"page": 9, "bbox": [{"x": 0.45676904916763306, "y": 0.11736264079809189}, {"x": 0.48122867941856384, "y": 0.11736264079809189}, {"x": 0.48122867941856384, "y": 0.12263736128807068}, {"x": 0.45676904916763306, "y": 0.12263736128807068}], "text": "70.5-\n"}
{"page": 9, "bbox": [{"x": 0.4084186553955078, "y": 0.11999999731779099}, {"x": 0.42718997597694397, "y": 0.11999999731779099}, {"x": 0.42718997597694397, "y": 0.12791208922863007}, {"x": 0.4084186553955078, "y": 0.12791208922863007}], "text": "AS\n"}
{"page": 9, "bbox": [{"x": 0.30659839510917664, "y": 0.11999999731779099}, {"x": 0.38282138109207153, "y": 0.11912088096141815}, {"x": 0.38282138109207153, "y": 0.1287912130355835}, {"x": 0.30659839510917664, "y": 0.12967033684253693}], "text": "PQA Med\n"}
{"page": 9, "bbox": [{"x": 0.6268486976623535, "y": 0.13274724781513214}, {"x": 0.6496018171310425, "y": 0.13274724781513214}, {"x": 0.6496018171310425, "y": 0.13802197575569153}, {"x": 0.6268486976623535, "y": 0.13802197575569153}], "text": "0.99-\n"}
{"page": 9, "bbox": [{"x": 0.3555176258087158, "y": 0.13230769336223602}, {"x": 0.3850966989994049, "y": 0.13274724781513214}, {"x": 0.3850966989994049, "y": 0.14153845608234406}, {"x": 0.3555176258087158, "y": 0.14109890162944794}], "text": "(acc)\n"}
{"page": 9, "bbox": [{"x": 0.30659839510917664, "y": 0.13274724781513214}, {"x": 0.3356086313724518, "y": 0.13274724781513214}, {"x": 0.3356086313724518, "y": 0.14153845608234406}, {"x": 0.30659839510917664, "y": 0.14153845608234406}], "text": "(acc)\n"}
{"page": 9, "bbox": [{"x": 0.404436856508255, "y": 0.13274724781513214}, {"x": 0.4311717748641968, "y": 0.13274724781513214}, {"x": 0.4311717748641968, "y": 0.14153845608234406}, {"x": 0.404436856508255, "y": 0.14153845608234406}], "text": "(em)\n"}
{"page": 9, "bbox": [{"x": 0.8020477890968323, "y": 0.16087912023067474}, {"x": 0.8043230772018433, "y": 0.12483516335487366}, {"x": 0.8128555417060852, "y": 0.12527473270893097}, {"x": 0.8105801939964294, "y": 0.16131867468357086}], "text": "Frequency\n"}
{"page": 9, "bbox": [{"x": 0.6160409450531006, "y": 0.15912087261676788}, {"x": 0.6166098117828369, "y": 0.12791208922863007}, {"x": 0.6240045428276062, "y": 0.12791208922863007}, {"x": 0.6234357357025146, "y": 0.15912087261676788}], "text": "Accuracy\n"}
{"page": 9, "bbox": [{"x": 0.7872582674026489, "y": 0.14153845608234406}, {"x": 0.7997724413871765, "y": 0.14153845608234406}, {"x": 0.7997724413871765, "y": 0.14593406021595}, {"x": 0.7872582674026489, "y": 0.14593406021595}], "text": "0.5\n"}
{"page": 9, "bbox": [{"x": 0.4442548453807831, "y": 0.15912087261676788}, {"x": 0.4442548453807831, "y": 0.1287912130355835}, {"x": 0.45164960622787476, "y": 0.1287912130355835}, {"x": 0.45164960622787476, "y": 0.15912087261676788}], "text": "Precision\n"}
{"page": 9, "bbox": [{"x": 0.45676904916763306, "y": 0.14989010989665985}, {"x": 0.48009100556373596, "y": 0.14989010989665985}, {"x": 0.48009100556373596, "y": 0.15472526848316193}, {"x": 0.45676904916763306, "y": 0.15472526848316193}], "text": "70.0-\n"}
{"page": 9, "bbox": [{"x": 0.6274175047874451, "y": 0.14989010989665985}, {"x": 0.6496018171310425, "y": 0.14989010989665985}, {"x": 0.6496018171310425, "y": 0.15472526848316193}, {"x": 0.6274175047874451, "y": 0.15472526848316193}], "text": "0.99-\n"}
{"page": 9, "bbox": [{"x": 0.3577929437160492, "y": 0.15208791196346283}, {"x": 0.38225257396698, "y": 0.15208791196346283}, {"x": 0.38225257396698, "y": 0.15912087261676788}, {"x": 0.3577929437160492, "y": 0.15912087261676788}], "text": "73.5\n"}
{"page": 9, "bbox": [{"x": 0.18657565116882324, "y": 0.1512087881565094}, {"x": 0.28725823760032654, "y": 0.1516483575105667}, {"x": 0.28725823760032654, "y": 0.16043956577777863}, {"x": 0.18657565116882324, "y": 0.1599999964237213}], "text": "SELF-RAG (50k)\n"}
{"page": 9, "bbox": [{"x": 0.30830490589141846, "y": 0.1516483575105667}, {"x": 0.3344709873199463, "y": 0.15208791196346283}, {"x": 0.33390215039253235, "y": 0.1599999964237213}, {"x": 0.3077360689640045, "y": 0.1595604419708252}], "text": "45.5\n"}
{"page": 9, "bbox": [{"x": 0.4055745303630829, "y": 0.15208791196346283}, {"x": 0.4306029677391052, "y": 0.15208791196346283}, {"x": 0.4306029677391052, "y": 0.1595604419708252}, {"x": 0.4055745303630829, "y": 0.1595604419708252}], "text": "32.1\n"}
{"page": 9, "bbox": [{"x": 0.7821387648582458, "y": 0.16571427881717682}, {"x": 0.7997724413871765, "y": 0.16571427881717682}, {"x": 0.7997724413871765, "y": 0.17142857611179352}, {"x": 0.7821387648582458, "y": 0.17142857611179352}], "text": "50.0\n"}
{"page": 9, "bbox": [{"x": 0.6274175047874451, "y": 0.16659340262413025}, {"x": 0.6456200480461121, "y": 0.16703297197818756}, {"x": 0.6456200480461121, "y": 0.17186813056468964}, {"x": 0.6274175047874451, "y": 0.17142857611179352}], "text": "0.98\n"}
{"page": 9, "bbox": [{"x": 0.18543799221515656, "y": 0.16483516991138458}, {"x": 0.2366325408220291, "y": 0.1652747243642807}, {"x": 0.2366325408220291, "y": 0.17406593263149261}, {"x": 0.18543799221515656, "y": 0.1736263781785965}], "text": "Training\n"}
{"page": 9, "bbox": [{"x": 0.6843003630638123, "y": 0.17494505643844604}, {"x": 0.6973834037780762, "y": 0.17494505643844604}, {"x": 0.6973834037780762, "y": 0.179340660572052}, {"x": 0.6843003630638123, "y": 0.179340660572052}], "text": "0.2\n"}
{"page": 9, "bbox": [{"x": 0.7184300422668457, "y": 0.17494505643844604}, {"x": 0.7315130829811096, "y": 0.17494505643844604}, {"x": 0.7315130829811096, "y": 0.17978021502494812}, {"x": 0.7184300422668457, "y": 0.17978021502494812}], "text": "0.4\n"}
{"page": 9, "bbox": [{"x": 0.7525597214698792, "y": 0.17494505643844604}, {"x": 0.766780436038971, "y": 0.17494505643844604}, {"x": 0.766780436038971, "y": 0.17978021502494812}, {"x": 0.7525597214698792, "y": 0.17978021502494812}], "text": "0.6\n"}
{"page": 9, "bbox": [{"x": 0.650170624256134, "y": 0.17538461089134216}, {"x": 0.6638225317001343, "y": 0.17538461089134216}, {"x": 0.6638225317001343, "y": 0.17978021502494812}, {"x": 0.650170624256134, "y": 0.17978021502494812}], "text": "0.0\n"}
{"page": 9, "bbox": [{"x": 0.5978384613990784, "y": 0.17494505643844604}, {"x": 0.6046643853187561, "y": 0.17494505643844604}, {"x": 0.6046643853187561, "y": 0.18021978437900543}, {"x": 0.5978384613990784, "y": 0.18021978437900543}], "text": "2\n"}
{"page": 9, "bbox": [{"x": 0.3077360689640045, "y": 0.17758241295814514}, {"x": 0.4306029677391052, "y": 0.17714285850524902}, {"x": 0.4306029677391052, "y": 0.1846153885126114}, {"x": 0.3077360689640045, "y": 0.1850549429655075}], "text": "43.6 67.8 31.0\n"}
{"page": 9, "bbox": [{"x": 0.18543799221515656, "y": 0.17758241295814514}, {"x": 0.27815699577331543, "y": 0.17758241295814514}, {"x": 0.27815699577331543, "y": 0.1850549429655075}, {"x": 0.18543799221515656, "y": 0.1850549429655075}], "text": "No Retriever R\n"}
{"page": 9, "bbox": [{"x": 0.7002275586128235, "y": 0.18109890818595886}, {"x": 0.7343572378158569, "y": 0.18153846263885498}, {"x": 0.7343572378158569, "y": 0.18945054709911346}, {"x": 0.7002275586128235, "y": 0.18901099264621735}], "text": "PopQA\n"}
{"page": 9, "bbox": [{"x": 0.6331058144569397, "y": 0.190769225358963}, {"x": 0.6461888551712036, "y": 0.190769225358963}, {"x": 0.6461888551712036, "y": 0.19560439884662628}, {"x": 0.6331058144569397, "y": 0.19560439884662628}], "text": "1.0\n"}
{"page": 9, "bbox": [{"x": 0.7878270745277405, "y": 0.190769225358963}, {"x": 0.8048919439315796, "y": 0.190769225358963}, {"x": 0.8048919439315796, "y": 0.19560439884662628}, {"x": 0.7878270745277405, "y": 0.19560439884662628}], "text": "1.00\n"}
{"page": 9, "bbox": [{"x": 0.1860068291425705, "y": 0.18945054709911346}, {"x": 0.2531285583972931, "y": 0.18901099264621735}, {"x": 0.2531285583972931, "y": 0.19736263155937195}, {"x": 0.1860068291425705, "y": 0.19780220091342926}], "text": "No Critic C\n"}
{"page": 9, "bbox": [{"x": 0.30830490589141846, "y": 0.18989011645317078}, {"x": 0.3333333432674408, "y": 0.18945054709911346}, {"x": 0.3333333432674408, "y": 0.19736263155937195}, {"x": 0.30830490589141846, "y": 0.19780220091342926}], "text": "42.6\n"}
{"page": 9, "bbox": [{"x": 0.3577929437160492, "y": 0.190769225358963}, {"x": 0.4300341308116913, "y": 0.190769225358963}, {"x": 0.4300341308116913, "y": 0.19780220091342926}, {"x": 0.3577929437160492, "y": 0.19780220091342926}], "text": "72.0 18.1\n"}
{"page": 9, "bbox": [{"x": 0.4653014838695526, "y": 0.19692307710647583}, {"x": 0.4755403995513916, "y": 0.19692307710647583}, {"x": 0.4755403995513916, "y": 0.2013186812400818}, {"x": 0.4653014838695526, "y": 0.2013186812400818}], "text": "95\n"}
{"page": 9, "bbox": [{"x": 0.18657565116882324, "y": 0.20351648330688477}, {"x": 0.20989760756492615, "y": 0.20351648330688477}, {"x": 0.20989760756492615, "y": 0.21054944396018982}, {"x": 0.18657565116882324, "y": 0.21054944396018982}], "text": "Test\n"}
{"page": 9, "bbox": [{"x": 0.7866894006729126, "y": 0.20659340918064117}, {"x": 0.8060295581817627, "y": 0.20659340918064117}, {"x": 0.8060295581817627, "y": 0.21142856776714325}, {"x": 0.7866894006729126, "y": 0.21142856776714325}], "text": "0.75\n"}
{"page": 9, "bbox": [{"x": 0.6325369477272034, "y": 0.20879121124744415}, {"x": 0.6461888551712036, "y": 0.20879121124744415}, {"x": 0.6461888551712036, "y": 0.21362636983394623}, {"x": 0.6325369477272034, "y": 0.21362636983394623}], "text": "0.8\n"}
{"page": 9, "bbox": [{"x": 0.45278725028038025, "y": 0.22901098430156708}, {"x": 0.45278725028038025, "y": 0.20571428537368774}, {"x": 0.4601820111274719, "y": 0.20571428537368774}, {"x": 0.4601820111274719, "y": 0.22901098430156708}], "text": "Mauve\n"}
{"page": 9, "bbox": [{"x": 0.8094425201416016, "y": 0.235604390501976}, {"x": 0.8094425201416016, "y": 0.20043955743312836}, {"x": 0.8168373107910156, "y": 0.20043955743312836}, {"x": 0.8168373107910156, "y": 0.235604390501976}], "text": "Frequency\n"}
{"page": 9, "bbox": [{"x": 0.6200227737426758, "y": 0.23384615778923035}, {"x": 0.6228668689727783, "y": 0.2017582356929779}, {"x": 0.6308304667472839, "y": 0.20219780504703522}, {"x": 0.6279863715171814, "y": 0.23428571224212646}], "text": "Accuracy\n"}
{"page": 9, "bbox": [{"x": 0.30830490589141846, "y": 0.21362636983394623}, {"x": 0.3833902180194855, "y": 0.2153846174478531}, {"x": 0.38282138109207153, "y": 0.22461538016796112}, {"x": 0.3077360689640045, "y": 0.22285714745521545}], "text": "24.7 73.0\n"}
{"page": 9, "bbox": [{"x": 0.1860068291425705, "y": 0.2158241719007492}, {"x": 0.25540387630462646, "y": 0.2158241719007492}, {"x": 0.25540387630462646, "y": 0.22285714745521545}, {"x": 0.1860068291425705, "y": 0.22285714745521545}], "text": "No retrieval\n"}
{"page": 9, "bbox": [{"x": 0.4653014838695526, "y": 0.2210988998413086}, {"x": 0.47610920667648315, "y": 0.2210988998413086}, {"x": 0.47610920667648315, "y": 0.22593407332897186}, {"x": 0.4653014838695526, "y": 0.22593407332897186}], "text": "90\n"}
{"page": 9, "bbox": [{"x": 0.7832764387130737, "y": 0.22197802364826202}, {"x": 0.806598424911499, "y": 0.22241757810115814}, {"x": 0.806598424911499, "y": 0.22769230604171753}, {"x": 0.7832764387130737, "y": 0.2272527515888214}], "text": "-0.50\n"}
{"page": 9, "bbox": [{"x": 0.6325369477272034, "y": 0.2272527515888214}, {"x": 0.6507394909858704, "y": 0.2272527515888214}, {"x": 0.6507394909858704, "y": 0.2320879101753235}, {"x": 0.6325369477272034, "y": 0.2320879101753235}], "text": "0.6-\n"}
{"page": 9, "bbox": [{"x": 0.1860068291425705, "y": 0.22857142984867096}, {"x": 0.2827076315879822, "y": 0.22857142984867096}, {"x": 0.2827076315879822, "y": 0.23604395985603333}, {"x": 0.1860068291425705, "y": 0.23604395985603333}], "text": "Hard constraints\n"}
{"page": 9, "bbox": [{"x": 0.3077360689640045, "y": 0.22813187539577484}, {"x": 0.3833902180194855, "y": 0.22813187539577484}, {"x": 0.3833902180194855, "y": 0.24835164844989777}, {"x": 0.3077360689640045, "y": 0.24835164844989777}], "text": "28.3 72.6\n41.8 73.1\n"}
{"page": 9, "bbox": [{"x": 0.7872582674026489, "y": 0.2382417619228363}, {"x": 0.8048919439315796, "y": 0.2382417619228363}, {"x": 0.8048919439315796, "y": 0.24307692050933838}, {"x": 0.7872582674026489, "y": 0.24307692050933838}], "text": "0.25\n"}
{"page": 9, "bbox": [{"x": 0.1860068291425705, "y": 0.23912088572978973}, {"x": 0.26393628120422363, "y": 0.2408791184425354}, {"x": 0.2633674740791321, "y": 0.25010988116264343}, {"x": 0.18543799221515656, "y": 0.24835164844989777}], "text": "Retrieve topl\n"}
{"page": 9, "bbox": [{"x": 0.40500569343566895, "y": 0.2408791184425354}, {"x": 0.4311717748641968, "y": 0.2408791184425354}, {"x": 0.4311717748641968, "y": 0.2606593370437622}, {"x": 0.40500569343566895, "y": 0.2606593370437622}], "text": "28.6\n30.6\n"}
{"page": 9, "bbox": [{"x": 0.5386803150177002, "y": 0.2492307722568512}, {"x": 0.5437997579574585, "y": 0.2492307722568512}, {"x": 0.5437997579574585, "y": 0.25362637639045715}, {"x": 0.5386803150177002, "y": 0.25362637639045715}], "text": "1\n"}
{"page": 9, "bbox": [{"x": 0.6496018171310425, "y": 0.25010988116264343}, {"x": 0.6638225317001343, "y": 0.25010988116264343}, {"x": 0.6638225317001343, "y": 0.2545054852962494}, {"x": 0.6496018171310425, "y": 0.2545054852962494}], "text": "0.0\n"}
{"page": 9, "bbox": [{"x": 0.7525597214698792, "y": 0.25010988116264343}, {"x": 0.7662116289138794, "y": 0.25010988116264343}, {"x": 0.7662116289138794, "y": 0.2545054852962494}, {"x": 0.7525597214698792, "y": 0.2545054852962494}], "text": "0.6\n"}
{"page": 9, "bbox": [{"x": 0.6746302843093872, "y": 0.24967032670974731}, {"x": 0.7588168382644653, "y": 0.24967032670974731}, {"x": 0.7588168382644653, "y": 0.2632966935634613}, {"x": 0.6746302843093872, "y": 0.2632966935634613}], "text": "0.2 0.4\nRetrieval Threshold\n"}
{"page": 9, "bbox": [{"x": 0.30830490589141846, "y": 0.25318682193756104}, {"x": 0.38225257396698, "y": 0.2527472674846649}, {"x": 0.38225257396698, "y": 0.2610988914966583}, {"x": 0.30830490589141846, "y": 0.26153847575187683}], "text": "44.1 73.2\n"}
{"page": 9, "bbox": [{"x": 0.1860068291425705, "y": 0.25318682193756104}, {"x": 0.27815699577331543, "y": 0.25406593084335327}, {"x": 0.27815699577331543, "y": 0.26197803020477295}, {"x": 0.1860068291425705, "y": 0.2610988914966583}], "text": "Remove ISSUP\n"}
{"page": 9, "bbox": [{"x": 0.4971558451652527, "y": 0.2580219805240631}, {"x": 0.5915813446044922, "y": 0.2580219805240631}, {"x": 0.5915813446044922, "y": 0.2641758322715759}, {"x": 0.4971558451652527, "y": 0.2641758322715759}], "text": "Weight for IsSupport\n"}
{"page": 9, "bbox": [{"x": 0.6797497272491455, "y": 0.27604395151138306}, {"x": 0.7519909143447876, "y": 0.2751648426055908}, {"x": 0.7519909143447876, "y": 0.28439560532569885}, {"x": 0.6797497272491455, "y": 0.2852747142314911}], "text": "(c) Retrieval\n"}
{"page": 9, "bbox": [{"x": 0.2633674740791321, "y": 0.2769230902194977}, {"x": 0.33276450634002686, "y": 0.2751648426055908}, {"x": 0.3333333432674408, "y": 0.28439560532569885}, {"x": 0.26393628120422363, "y": 0.2861538529396057}], "text": "(a) Ablation\n"}
{"page": 9, "bbox": [{"x": 0.4726962447166443, "y": 0.2764835059642792}, {"x": 0.5784983038902283, "y": 0.2764835059642792}, {"x": 0.5784983038902283, "y": 0.2852747142314911}, {"x": 0.4726962447166443, "y": 0.2852747142314911}], "text": "(b) Customization\n"}
{"page": 9, "bbox": [{"x": 0.1757679134607315, "y": 0.3019780218601227}, {"x": 0.8242321014404297, "y": 0.3019780218601227}, {"x": 0.8242321014404297, "y": 0.3410989046096802}, {"x": 0.1757679134607315, "y": 0.3410989046096802}], "text": "Figure 3: Analysis on SELF-RAG: (a) Ablation studies for key components of SELF-RAG training\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\nMauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\n"}
{"page": 9, "bbox": [{"x": 0.1757679134607315, "y": 0.37626373767852783}, {"x": 0.8242321014404297, "y": 0.37626373767852783}, {"x": 0.8242321014404297, "y": 0.4281318783760071}, {"x": 0.1757679134607315, "y": 0.4281318783760071}], "text": "precisely grounded yet shorter outputs. Llama2-FT7, which is the baseline LM trained on the same\ninstruction-output pairs as SELF-RAG without retrieval or self-reflection and is retrieval-augmented\nat test time only, lags behind SELF-RAG. This result indicates SELF-RAG gains are not solely from\ntraining data and demonstrate the effectiveness of SELF-RAG framework.\n"}
{"page": 9, "bbox": [{"x": 0.17633675038814545, "y": 0.4545055031776428}, {"x": 0.28612059354782104, "y": 0.45538461208343506}, {"x": 0.28612059354782104, "y": 0.46373626589775085}, {"x": 0.17633675038814545, "y": 0.4628571569919586}], "text": "5.2 ANALYSIS\n"}
{"page": 9, "bbox": [{"x": 0.17519909143447876, "y": 0.4821977913379669}, {"x": 0.8253697156906128, "y": 0.4821977913379669}, {"x": 0.8253697156906128, "y": 0.9243956208229065}, {"x": 0.17519909143447876, "y": 0.9243956208229065}], "text": "Ablation studies. We conduct a set of ablations of our framework to identify which factors play\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\n|Retrieve|=Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\ntop one document only, similar to standard RAG approaches; Remove [ISSUP] indicates the model\nperformance that removes ISSUP Score only during critique-guided beam search in Eq. 4. In this\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\nand the bottom part is for inference ablations. We see that all components play important roles. We\nalso observe a large performance gap between SELF-RAG and No Retriever or Critic baselines across\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\nSELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG'S\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\nEffects of inference-time customization. One key benefit of our proposed framework is that it\nenables us to control how much each critique type affects the final generation sampling. We analyze\nthe effects of different parameter weights on the top of our 7B model during inference time on\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\nthe weighting term for [ISSUP], which criticizes how supported the output is by the text passage. As\nthe figure shows, increasing the weight leads to positive effects on the models' citation precision\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\n"}
{"page": 9, "bbox": [{"x": 0.4948805570602417, "y": 0.9507692456245422}, {"x": 0.5039817690849304, "y": 0.9507692456245422}, {"x": 0.5039817690849304, "y": 0.9586813449859619}, {"x": 0.4948805570602417, "y": 0.9586813449859619}], "text": "9\n"}
{"page": 10, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 10, "bbox": [{"x": 0.19112628698349, "y": 0.10769230872392654}, {"x": 0.1990898698568344, "y": 0.10769230872392654}, {"x": 0.1990898698568344, "y": 0.11120878905057907}, {"x": 0.19112628698349, "y": 0.11120878905057907}], "text": "55\n"}
{"page": 10, "bbox": [{"x": 0.700796365737915, "y": 0.11692307889461517}, {"x": 0.7701933979988098, "y": 0.11736264079809189}, {"x": 0.7701933979988098, "y": 0.12703296542167664}, {"x": 0.700796365737915, "y": 0.12659341096878052}], "text": "Pop Bio.\n"}
{"page": 10, "bbox": [{"x": 0.33674630522727966, "y": 0.11956044286489487}, {"x": 0.3475540280342102, "y": 0.11999999731779099}, {"x": 0.3475540280342102, "y": 0.12439560145139694}, {"x": 0.33674630522727966, "y": 0.12439560145139694}], "text": "73\n"}
{"page": 10, "bbox": [{"x": 0.19112628698349, "y": 0.12087912112474442}, {"x": 0.2042093276977539, "y": 0.12087912112474442}, {"x": 0.2042093276977539, "y": 0.12483516335487366}, {"x": 0.19112628698349, "y": 0.12483516335487366}], "text": "50-\n"}
{"page": 10, "bbox": [{"x": 0.47212740778923035, "y": 0.12791208922863007}, {"x": 0.4829351603984833, "y": 0.12791208922863007}, {"x": 0.4829351603984833, "y": 0.13274724781513214}, {"x": 0.47212740778923035, "y": 0.13274724781513214}], "text": "60\n"}
{"page": 10, "bbox": [{"x": 0.19112628698349, "y": 0.134505495429039}, {"x": 0.1990898698568344, "y": 0.134505495429039}, {"x": 0.1990898698568344, "y": 0.13802197575569153}, {"x": 0.19112628698349, "y": 0.13802197575569153}], "text": "45\n"}
{"page": 10, "bbox": [{"x": 0.1820250302553177, "y": 0.1538461595773697}, {"x": 0.1820250302553177, "y": 0.12131868302822113}, {"x": 0.18771331012248993, "y": 0.12131868302822113}, {"x": 0.18771331012248993, "y": 0.1538461595773697}], "text": "Perfomance\n"}
{"page": 10, "bbox": [{"x": 0.6990898847579956, "y": 0.13670329749584198}, {"x": 0.7707622051239014, "y": 0.13670329749584198}, {"x": 0.7707622051239014, "y": 0.14373625814914703}, {"x": 0.6990898847579956, "y": 0.14373625814914703}], "text": "92.5 70.0\n"}
{"page": 10, "bbox": [{"x": 0.6376564502716064, "y": 0.13670329749584198}, {"x": 0.6734926104545593, "y": 0.1371428519487381}, {"x": 0.6734926104545593, "y": 0.14417582750320435}, {"x": 0.6376564502716064, "y": 0.14373625814914703}], "text": "S & P\n"}
{"page": 10, "bbox": [{"x": 0.3361774682998657, "y": 0.13846154510974884}, {"x": 0.34812286496162415, "y": 0.13890109956264496}, {"x": 0.34812286496162415, "y": 0.14417582750320435}, {"x": 0.3361774682998657, "y": 0.14373625814914703}], "text": "72\n"}
{"page": 10, "bbox": [{"x": 0.19112628698349, "y": 0.14769230782985687}, {"x": 0.1990898698568344, "y": 0.14769230782985687}, {"x": 0.1990898698568344, "y": 0.1512087881565094}, {"x": 0.19112628698349, "y": 0.1512087881565094}], "text": "40\n"}
{"page": 10, "bbox": [{"x": 0.7434584498405457, "y": 0.1485714316368103}, {"x": 0.7713310718536377, "y": 0.14769230782985687}, {"x": 0.7718998789787292, "y": 0.1564835160970688}, {"x": 0.744027316570282, "y": 0.15736263990402222}], "text": "90.0\n"}
{"page": 10, "bbox": [{"x": 0.6996586918830872, "y": 0.14901098608970642}, {"x": 0.7252559661865234, "y": 0.14989010989665985}, {"x": 0.7246871590614319, "y": 0.15736263990402222}, {"x": 0.6990898847579956, "y": 0.1564835160970688}], "text": "95.0\n"}
{"page": 10, "bbox": [{"x": 0.6456200480461121, "y": 0.1512087881565094}, {"x": 0.6757678985595703, "y": 0.15208791196346283}, {"x": 0.6757678985595703, "y": 0.15736263990402222}, {"x": 0.6456200480461121, "y": 0.1564835160970688}], "text": "ISREL\n"}
{"page": 10, "bbox": [{"x": 0.33674630522727966, "y": 0.15824176371097565}, {"x": 0.3475540280342102, "y": 0.15824176371097565}, {"x": 0.3475540280342102, "y": 0.1626373678445816}, {"x": 0.33674630522727966, "y": 0.1626373678445816}], "text": "71\n"}
{"page": 10, "bbox": [{"x": 0.19112628698349, "y": 0.16087912023067474}, {"x": 0.19965870678424835, "y": 0.16087912023067474}, {"x": 0.19965870678424835, "y": 0.16439560055732727}, {"x": 0.19112628698349, "y": 0.16439560055732727}], "text": "35\n"}
{"page": 10, "bbox": [{"x": 0.4715586006641388, "y": 0.16307692229747772}, {"x": 0.48350396752357483, "y": 0.16307692229747772}, {"x": 0.48350396752357483, "y": 0.1679120808839798}, {"x": 0.4715586006641388, "y": 0.1679120808839798}], "text": "40\n"}
{"page": 10, "bbox": [{"x": 0.6990898847579956, "y": 0.1621977984905243}, {"x": 0.7246871590614319, "y": 0.16131867468357086}, {"x": 0.7252559661865234, "y": 0.16879120469093323}, {"x": 0.6996586918830872, "y": 0.16967032849788666}], "text": "90.0\n"}
{"page": 10, "bbox": [{"x": 0.7445961236953735, "y": 0.1621977984905243}, {"x": 0.7707622051239014, "y": 0.1621977984905243}, {"x": 0.7707622051239014, "y": 0.16923077404499054}, {"x": 0.7445961236953735, "y": 0.16923077404499054}], "text": "85.0\n"}
{"page": 10, "bbox": [{"x": 0.6450511813163757, "y": 0.16351647675037384}, {"x": 0.6746302843093872, "y": 0.16395604610443115}, {"x": 0.6746302843093872, "y": 0.17010988295078278}, {"x": 0.6450511813163757, "y": 0.16967032849788666}], "text": "ISSUP\n"}
{"page": 10, "bbox": [{"x": 0.5506256818771362, "y": 0.16967032849788666}, {"x": 0.5671217441558838, "y": 0.16967032849788666}, {"x": 0.5671217441558838, "y": 0.17494505643844604}, {"x": 0.5506256818771362, "y": 0.17494505643844604}], "text": "100\n"}
{"page": 10, "bbox": [{"x": 0.351535826921463, "y": 0.17010988295078278}, {"x": 0.3577929437160492, "y": 0.17010988295078278}, {"x": 0.3577929437160492, "y": 0.17494505643844604}, {"x": 0.351535826921463, "y": 0.17494505643844604}], "text": "0\n"}
{"page": 10, "bbox": [{"x": 0.41524460911750793, "y": 0.17010988295078278}, {"x": 0.4317406117916107, "y": 0.17010988295078278}, {"x": 0.4317406117916107, "y": 0.17494505643844604}, {"x": 0.41524460911750793, "y": 0.17494505643844604}], "text": "100\n"}
{"page": 10, "bbox": [{"x": 0.4869169592857361, "y": 0.17010988295078278}, {"x": 0.49317407608032227, "y": 0.17010988295078278}, {"x": 0.49317407608032227, "y": 0.17494505643844604}, {"x": 0.4869169592857361, "y": 0.17494505643844604}], "text": "0\n"}
{"page": 10, "bbox": [{"x": 0.27588167786598206, "y": 0.17230768501758575}, {"x": 0.2883959114551544, "y": 0.17230768501758575}, {"x": 0.2883959114551544, "y": 0.1762637346982956}, {"x": 0.27588167786598206, "y": 0.1762637346982956}], "text": "100\n"}
{"page": 10, "bbox": [{"x": 0.20477814972400665, "y": 0.17274725437164307}, {"x": 0.2093287855386734, "y": 0.17274725437164307}, {"x": 0.2093287855386734, "y": 0.1762637346982956}, {"x": 0.20477814972400665, "y": 0.1762637346982956}], "text": "0\n"}
{"page": 10, "bbox": [{"x": 0.24004550278186798, "y": 0.17274725437164307}, {"x": 0.24914675951004028, "y": 0.17274725437164307}, {"x": 0.24914675951004028, "y": 0.1762637346982956}, {"x": 0.24004550278186798, "y": 0.1762637346982956}], "text": "50\n"}
{"page": 10, "bbox": [{"x": 0.3128555119037628, "y": 0.17274725437164307}, {"x": 0.32593855261802673, "y": 0.17274725437164307}, {"x": 0.32593855261802673, "y": 0.1767033040523529}, {"x": 0.3128555119037628, "y": 0.1767033040523529}], "text": "150\n"}
{"page": 10, "bbox": [{"x": 0.3640500605106354, "y": 0.17802198231220245}, {"x": 0.4510807693004608, "y": 0.17846153676509857}, {"x": 0.4510807693004608, "y": 0.1850549429655075}, {"x": 0.3640500605106354, "y": 0.1846153885126114}], "text": "Num of training (k)\n"}
{"page": 10, "bbox": [{"x": 0.5, "y": 0.17802198231220245}, {"x": 0.5870307087898254, "y": 0.17846153676509857}, {"x": 0.5870307087898254, "y": 0.18549451231956482}, {"x": 0.5, "y": 0.1850549429655075}], "text": "Num of training (k)\n"}
{"page": 10, "bbox": [{"x": 0.23151308298110962, "y": 0.17890110611915588}, {"x": 0.29920363426208496, "y": 0.17978021502494812}, {"x": 0.29920363426208496, "y": 0.1850549429655075}, {"x": 0.23151308298110962, "y": 0.18417581915855408}], "text": "Num of training (k)\n"}
{"page": 10, "bbox": [{"x": 0.6069397330284119, "y": 0.18285714089870453}, {"x": 0.8026165962219238, "y": 0.18549451231956482}, {"x": 0.8020477890968323, "y": 0.20967033505439758}, {"x": 0.6063708662986755, "y": 0.2070329636335373}], "text": "(d) Human evaluation on PopQA\nand Bio generation.\n"}
{"page": 10, "bbox": [{"x": 0.3577929437160492, "y": 0.19780220091342926}, {"x": 0.43970420956611633, "y": 0.19692307710647583}, {"x": 0.43970420956611633, "y": 0.20615383982658386}, {"x": 0.3577929437160492, "y": 0.2070329636335373}], "text": "(b) PubHealth\n"}
{"page": 10, "bbox": [{"x": 0.4857792854309082, "y": 0.1964835226535797}, {"x": 0.5847554206848145, "y": 0.19736263155937195}, {"x": 0.5847554206848145, "y": 0.20835164189338684}, {"x": 0.4857792854309082, "y": 0.2074725329875946}], "text": "(c) ASQA (prec)\n"}
{"page": 10, "bbox": [{"x": 0.22184300422668457, "y": 0.19692307710647583}, {"x": 0.28498294949531555, "y": 0.19780220091342926}, {"x": 0.2844141125679016, "y": 0.20835164189338684}, {"x": 0.22127418220043182, "y": 0.2074725329875946}], "text": "(a) PopQA\n"}
{"page": 10, "bbox": [{"x": 0.1757679134607315, "y": 0.22241757810115814}, {"x": 0.8248009085655212, "y": 0.22241757810115814}, {"x": 0.8248009085655212, "y": 0.26197803020477295}, {"x": 0.1757679134607315, "y": 0.26197803020477295}], "text": "Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\nHuman analysis on SELF-RAG outputs as well as reflection tokens.\n"}
{"page": 10, "bbox": [{"x": 0.17519909143447876, "y": 0.2945055067539215}, {"x": 0.8248009085655212, "y": 0.2945055067539215}, {"x": 0.8248009085655212, "y": 0.7714285850524902}, {"x": 0.17519909143447876, "y": 0.7714285850524902}], "text": "contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models' behaviors at\ntest time by adjusting such parameters without requiring additional training.\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\noverall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\nof threshold & (larger ♂ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\nthe model's retrieval frequencies dramatically change on both datasets. as ♪ varies. On one hand,\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\nEffects of training data size. We conduct an analysis of how the data scale affects the model's\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n150k training instances, and fine-tune four SELF-RAG 7B variants on those subsets. Then, we compare\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\nRAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models'\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\nnot observed such significant improvements on Llama2-FT7 when increasing the training data from\n50k to 150k. These results also indicate that further expanding the training data of SELF-RAG may\nlead to further improvements, although in this work we limit our training data size to 150k.\nHuman evaluations. We conduct small human evaluations on SELF-RAG outputs, as well as the\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\nresults. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\nverify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\nreflection tokens about ISREL] and [ISSUP] match their inspections (e.g., whether the fully supported\noutput is supported by the cited evidence). Human annotators find SELF-RAG answers are often\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\nconsistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\nexamples and explanations on assessments.\n"}
{"page": 10, "bbox": [{"x": 0.1757679134607315, "y": 0.7964835166931152}, {"x": 0.31740614771842957, "y": 0.7960439324378967}, {"x": 0.31740614771842957, "y": 0.8074725270271301}, {"x": 0.1757679134607315, "y": 0.8079121112823486}], "text": "6 CONCLUSION\n"}
{"page": 10, "bbox": [{"x": 0.17519909143447876, "y": 0.8298901319503784}, {"x": 0.8253697156906128, "y": 0.8298901319503784}, {"x": 0.8253697156906128, "y": 0.9243956208229065}, {"x": 0.17519909143447876, "y": 0.9243956208229065}], "text": "This work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\nthrough retrieval on demand and self-reflection. SELF-RAG trains an LM to learn to retrieve, generate,\nand critique text passages and its own generation by predicting the next tokens from its original\nvocabulary as well as newly added special tokens, called reflection tokens. SELF-RAG further enables\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\nsix tasks using multiple metrics demonstrate that SELF-RAG significantly outperforms LLMs with\nmore parameters or with conventional retrieval-augmented generation approaches.\n"}
{"page": 10, "bbox": [{"x": 0.4920364022254944, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9586813449859619}, {"x": 0.4920364022254944, "y": 0.9586813449859619}], "text": "10\n"}
{"page": 11, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 11, "bbox": [{"x": 0.1769055724143982, "y": 0.10593406856060028}, {"x": 0.34926050901412964, "y": 0.10549450665712357}, {"x": 0.34926050901412964, "y": 0.11648351699113846}, {"x": 0.1769055724143982, "y": 0.11692307889461517}], "text": "ETHICAL CONCERNS\n"}
{"page": 11, "bbox": [{"x": 0.1757679134607315, "y": 0.134505495429039}, {"x": 0.8259385824203491, "y": 0.1371428519487381}, {"x": 0.8253697156906128, "y": 0.21978022158145905}, {"x": 0.17519909143447876, "y": 0.21714285016059875}], "text": "This work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\nadvice). While our method shows significant improvements in terms of performance, factuality, and\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\nmodel outputs.\n"}
{"page": 11, "bbox": [{"x": 0.17633675038814545, "y": 0.23692308366298676}, {"x": 0.32593855261802673, "y": 0.237802192568779}, {"x": 0.32593855261802673, "y": 0.2461538463830948}, {"x": 0.17633675038814545, "y": 0.24527472257614136}], "text": "ACKNOWLEDGMENTS\n"}
{"page": 11, "bbox": [{"x": 0.17519909143447876, "y": 0.2602197825908661}, {"x": 0.8253697156906128, "y": 0.2602197825908661}, {"x": 0.8253697156906128, "y": 0.35472527146339417}, {"x": 0.17519909143447876, "y": 0.35472527146339417}], "text": "We thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\nAkari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\nto train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\nProgram for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\n"}
{"page": 11, "bbox": [{"x": 0.1769055724143982, "y": 0.37934064865112305}, {"x": 0.28612059354782104, "y": 0.37978023290634155}, {"x": 0.28612059354782104, "y": 0.3898901045322418}, {"x": 0.1769055724143982, "y": 0.3894505500793457}], "text": "REFERENCES\n"}
{"page": 11, "bbox": [{"x": 0.1757679134607315, "y": 0.4030769169330597}, {"x": 0.8276450634002686, "y": 0.4030769169330597}, {"x": 0.8276450634002686, "y": 0.639120876789093}, {"x": 0.1757679134607315, "y": 0.639120876789093}], "text": "Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learn-\ning to retrieve reasoning paths over wikipedia graph for question answering. In International\nConference on Learning Representations, 2020. URL https://openreview.net/forum?\nid=SJgVHkrYDH.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and appli-\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh\nHajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Associ-\nation for Computational Linguistics, 2023b. URL https://aclanthology.org/2023.\nfindings-ac1.225.\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:\nEvaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037,\n2022. URL https://arxiv.org/abs/2212.08037.\n"}
{"page": 11, "bbox": [{"x": 0.1757679134607315, "y": 0.6527472734451294}, {"x": 0.8265073895454407, "y": 0.6527472734451294}, {"x": 0.8265073895454407, "y": 0.9248351454734802}, {"x": 0.1757679134607315, "y": 0.9248351454734802}], "text": "Lingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt's behavior changing over time? arXiv\npreprint arXiv:2307.09009, 2023. URL https://arxiv.org/abs/2307.09009.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint\narXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of\nwikipedia: Knowledge-powered conversational agents. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=r1173iRqKm.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\n"}
{"page": 11, "bbox": [{"x": 0.4926052391529083, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9595604538917542}, {"x": 0.4926052391529083, "y": 0.9595604538917542}], "text": "11\n"}
{"page": 12, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 12, "bbox": [{"x": 0.19283276796340942, "y": 0.10725274682044983}, {"x": 0.8270761966705322, "y": 0.10505494475364685}, {"x": 0.8270761966705322, "y": 0.12923076748847961}, {"x": 0.19283276796340942, "y": 0.1314285695552826}], "text": "learn from human feedback. arXiv preprint arXiv:2305.14387, 2023. URL https://arxiv.\norg/abs/2305.14387.\n"}
{"page": 12, "bbox": [{"x": 0.1757679134607315, "y": 0.14461538195610046}, {"x": 0.8248009085655212, "y": 0.14461538195610046}, {"x": 0.8248009085655212, "y": 0.18153846263885498}, {"x": 0.1757679134607315, "y": 0.18153846263885498}], "text": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/abs/\n2305.14627.\n"}
{"page": 12, "bbox": [{"x": 0.1757679134607315, "y": 0.19560439884662628}, {"x": 0.8253697156906128, "y": 0.1960439532995224}, {"x": 0.8253697156906128, "y": 0.3643956184387207}, {"x": 0.1757679134607315, "y": 0.3639560341835022}], "text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International Conference on Machine Learning, 2020. URL\nhttps://dl.acm.org/doi/pdf/10.5555/3524938.3525306.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\nTransactions on Machine Learning Research, 2022a. URL https://openreview.net/\nforum?id=jKN1pXi7b0.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022b. URL https:\n//arxiv.org/abs/2208.03299.\n"}
{"page": 12, "bbox": [{"x": 0.1757679134607315, "y": 0.3775824308395386}, {"x": 0.8265073895454407, "y": 0.3775824308395386}, {"x": 0.8265073895454407, "y": 0.5841758251190186}, {"x": 0.1757679134607315, "y": 0.5841758251190186}], "text": "Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint\narXiv:2305.06983, 2023. URL https://arxiv.org/abs/2305.06983.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017. URL\nhttps://aclanthology.org/P17-1147.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\nCtrl: A conditional transformer language model for controllable generation. arXiv preprint\narXiv:1909.05858, 2019. URL https://arxiv.org/abs/1909.05858.\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason\nPhang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences.\nIn International Conference on Machine Learning, 2023. URL https://openreview.net/\n"}
{"page": 12, "bbox": [{"x": 0.1939704269170761, "y": 0.588131844997406}, {"x": 0.38168373703956604, "y": 0.588131844997406}, {"x": 0.38168373703956604, "y": 0.5960439443588257}, {"x": 0.1939704269170761, "y": 0.5960439443588257}], "text": "forum?id=AT8Iw8KOеC.\n"}
{"page": 12, "bbox": [{"x": 0.17633675038814545, "y": 0.6105494499206543}, {"x": 0.8270761966705322, "y": 0.6092307567596436}, {"x": 0.8276450634002686, "y": 0.6887912154197693}, {"x": 0.1769055724143982, "y": 0.69010990858078}], "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. Natural questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics, 2019. URL https://aclanthology.org/\nQ19-1026.\n"}
{"page": 12, "bbox": [{"x": 0.17519909143447876, "y": 0.7032967209815979}, {"x": 0.8265073895454407, "y": 0.7032967209815979}, {"x": 0.8265073895454407, "y": 0.9243956208229065}, {"x": 0.17519909143447876, "y": 0.9243956208229065}], "text": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023. URL https://arxiv.org/abs/2309.06180.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, 2020. URL https://proceedings. neurips.cc/paper/\n2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-\naugmented dual instruction tuning, 2023. URL https://arxiv.org/abs/2310.01352.\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848, 2023a. URL https://arxiv.org/abs/2304.09848.\n"}
{"page": 12, "bbox": [{"x": 0.49146756529808044, "y": 0.951208770275116}, {"x": 0.5068259239196777, "y": 0.951208770275116}, {"x": 0.5068259239196777, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "12\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.23208190500736237, "y": 0.036043956875801086}, {"x": 0.23208190500736237, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 13, "bbox": [{"x": 0.1769055724143982, "y": 0.10725274682044983}, {"x": 0.8259385824203491, "y": 0.10725274682044983}, {"x": 0.8259385824203491, "y": 0.1454945057630539}, {"x": 0.1769055724143982, "y": 0.1454945057630539}], "text": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.\nURL https://arxiv.org/abs/2303.16634.\n"}
{"page": 13, "bbox": [{"x": 0.17519909143447876, "y": 0.15912087261676788}, {"x": 0.8270761966705322, "y": 0.15912087261676788}, {"x": 0.8270761966705322, "y": 0.2101098895072937}, {"x": 0.17519909143447876, "y": 0.2101098895072937}], "text": "Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.\nIn Advances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id=5HaIds3ux50.\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.22505494952201843}, {"x": 0.8259385824203491, "y": 0.22505494952201843}, {"x": 0.8259385824203491, "y": 0.2632966935634613}, {"x": 0.17633675038814545, "y": 0.2632966935634613}], "text": "Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox,\nHelen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv preprint\narXiv:2305.15225, 2023. URL https://arxiv.org/abs/2305.15225.\n"}
{"page": 13, "bbox": [{"x": 0.1757679134607315, "y": 0.2773626446723938}, {"x": 0.8253697156906128, "y": 0.2773626446723938}, {"x": 0.8253697156906128, "y": 0.3437362611293793}, {"x": 0.1757679134607315, "y": 0.3437362611293793}], "text": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-\nrefine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. URL\nhttps://arxiv.org/abs/2303.17651.\n"}
{"page": 13, "bbox": [{"x": 0.1757679134607315, "y": 0.35736262798309326}, {"x": 0.8276450634002686, "y": 0.356483519077301}, {"x": 0.8276450634002686, "y": 0.4232966899871826}, {"x": 0.1757679134607315, "y": 0.42417582869529724}], "text": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric\nmemories. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2023. URL https://aclanthology.org/2023.\nacl-long.546.\n"}
{"page": 13, "bbox": [{"x": 0.1757679134607315, "y": 0.436923086643219}, {"x": 0.8259385824203491, "y": 0.436923086643219}, {"x": 0.8259385824203491, "y": 0.4896703362464905}, {"x": 0.1757679134607315, "y": 0.4896703362464905}], "text": "Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\nURL https://arxiv.org/abs/2203.11147.\n"}
{"page": 13, "bbox": [{"x": 0.1769055724143982, "y": 0.5028571486473083}, {"x": 0.8265073895454407, "y": 0.5019780397415161}, {"x": 0.8265073895454407, "y": 0.5542857050895691}, {"x": 0.1769055724143982, "y": 0.5551648139953613}], "text": "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, 2018. URL https://aclanthology.\norg/D18-1260.\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.5692307949066162}, {"x": 0.8259385824203491, "y": 0.5692307949066162}, {"x": 0.8259385824203491, "y": 0.6219780445098877}, {"x": 0.17633675038814545, "y": 0.6219780445098877}], "text": "Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard EM approach\nfor weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), 2019. URL https://aclanthology.org/\n"}
{"page": 13, "bbox": [{"x": 0.19283276796340942, "y": 0.6254944801330566}, {"x": 0.2753128409385681, "y": 0.6259340643882751}, {"x": 0.2753128409385681, "y": 0.6334065794944763}, {"x": 0.19283276796340942, "y": 0.6329670548439026}], "text": "D19 1284.\n"}
{"page": 13, "bbox": [{"x": 0.1757679134607315, "y": 0.6487911939620972}, {"x": 0.8259385824203491, "y": 0.6487911939620972}, {"x": 0.8259385824203491, "y": 0.7006593346595764}, {"x": 0.1757679134607315, "y": 0.7006593346595764}], "text": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. URL https:\n//arxiv.org/abs/2305.14251.\n"}
{"page": 13, "bbox": [{"x": 0.1757679134607315, "y": 0.7147252559661865}, {"x": 0.8265073895454407, "y": 0.7147252559661865}, {"x": 0.8265073895454407, "y": 0.9230769276618958}, {"x": 0.1757679134607315, "y": 0.9230769276618958}], "text": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https:\n//arxiv.org/abs/2112.09332.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.669.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.\norg/abs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\n"}
{"page": 13, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5051194429397583, "y": 0.9507692456245422}, {"x": 0.5051194429397583, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "13\n"}
{"page": 14, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 14, "bbox": [{"x": 0.19169510900974274, "y": 0.10681318491697311}, {"x": 0.8242321014404297, "y": 0.10725274682044983}, {"x": 0.8242321014404297, "y": 0.14461538195610046}, {"x": 0.19169510900974274, "y": 0.14417582750320435}], "text": "Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?\nid=TG8KACXEON.\n"}
{"page": 14, "bbox": [{"x": 0.1757679134607315, "y": 0.16043956577777863}, {"x": 0.8253697156906128, "y": 0.16043956577777863}, {"x": 0.8253697156906128, "y": 0.1986813247203827}, {"x": 0.1757679134607315, "y": 0.1986813247203827}], "text": "Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,\nand Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint\narXiv:2304.01904, 2023. URL https://arxiv.org/abs/2304.01904.\n"}
{"page": 14, "bbox": [{"x": 0.1757679134607315, "y": 0.2131868153810501}, {"x": 0.8253697156906128, "y": 0.2131868153810501}, {"x": 0.8253697156906128, "y": 0.2918681204319}, {"x": 0.1757679134607315, "y": 0.2918681204319}], "text": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel,\nand Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021. URL https://aclanthology.org/\n2021.naacl-main.200.\n"}
{"page": 14, "bbox": [{"x": 0.17633675038814545, "y": 0.3081318736076355}, {"x": 0.8248009085655212, "y": 0.3081318736076355}, {"x": 0.8248009085655212, "y": 0.36043956875801086}, {"x": 0.17633675038814545, "y": 0.36043956875801086}], "text": "Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using\ndivergence frontiers. In Advances in Neural Information Processing Systems, 2021. URL https:\n//openreview.net/forum?id=Tqx7nJp7PR.\n"}
{"page": 14, "bbox": [{"x": 0.1757679134607315, "y": 0.3749450445175171}, {"x": 0.8265073895454407, "y": 0.37406593561172485}, {"x": 0.8265073895454407, "y": 0.4263736307621002}, {"x": 0.1757679134607315, "y": 0.42725273966789246}], "text": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, 2020. URL https://dl.acm.\norg/doi/10.5555/3433701.3433727.\n"}
{"page": 14, "bbox": [{"x": 0.1757679134607315, "y": 0.44175824522972107}, {"x": 0.8276450634002686, "y": 0.44175824522972107}, {"x": 0.8276450634002686, "y": 0.6813187003135681}, {"x": 0.1757679134607315, "y": 0.6813187003135681}], "text": "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. Transactions of the Association\nfor Computational Linguistics, 2023. URL https://arxiv.org/abs/2302.00083.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\nenables zero-shot task generalization. In International Conference on Learning Representations,\n2022. URL https://openreview.net/forum?id=9Vrb9DOWI4.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.\n04761.\n"}
{"page": 14, "bbox": [{"x": 0.1757679134607315, "y": 0.6980219483375549}, {"x": 0.8248009085655212, "y": 0.6980219483375549}, {"x": 0.8248009085655212, "y": 0.7239560484886169}, {"x": 0.1757679134607315, "y": 0.7239560484886169}], "text": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. URL https:\ns:\n//arxiv.org/\n"}
{"page": 14, "bbox": [{"x": 0.19340158998966217, "y": 0.7265934348106384}, {"x": 0.3333333432674408, "y": 0.7265934348106384}, {"x": 0.3333333432674408, "y": 0.7349450588226318}, {"x": 0.19340158998966217, "y": 0.7349450588226318}], "text": "abs/1707.06347.\n"}
{"page": 14, "bbox": [{"x": 0.1757679134607315, "y": 0.7507692575454712}, {"x": 0.8265073895454407, "y": 0.7503296732902527}, {"x": 0.8265073895454407, "y": 0.9243956208229065}, {"x": 0.1757679134607315, "y": 0.9248351454734802}], "text": "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael\nSchärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\n//proceedings.mlr.press/v202/shi23a.html.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-\nform answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.566.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-\nscale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), 2018. URL https://aclanthology.org/N18-1074.\n"}
{"page": 14, "bbox": [{"x": 0.4920364022254944, "y": 0.9507692456245422}, {"x": 0.5056883096694946, "y": 0.9507692456245422}, {"x": 0.5056883096694946, "y": 0.9582417607307434}, {"x": 0.4920364022254944, "y": 0.9582417607307434}], "text": "14\n"}
{"page": 15, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 15, "bbox": [{"x": 0.17633675038814545, "y": 0.10725274682044983}, {"x": 0.8259385824203491, "y": 0.1063736230134964}, {"x": 0.8259385824203491, "y": 0.15824176371097565}, {"x": 0.17633675038814545, "y": 0.15912087261676788}], "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https://arxiv.\norg/abs/2307.09288.\n"}
{"page": 15, "bbox": [{"x": 0.1757679134607315, "y": 0.17230768501758575}, {"x": 0.8253697156906128, "y": 0.17274725437164307}, {"x": 0.8253697156906128, "y": 0.3556044101715088}, {"x": 0.1757679134607315, "y": 0.3551648259162903}], "text": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nexploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.\nURL https://arxiv.org/abs/2306.04751.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?\nid=gEZrGCozdqR.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. arXiv preprint arXiv:2306.01693, 2023. URL https:\n//arxiv.org/abs/2306.01693.\n"}
{"page": 15, "bbox": [{"x": 0.17519909143447876, "y": 0.36835163831710815}, {"x": 0.8276450634002686, "y": 0.36879122257232666}, {"x": 0.8276450634002686, "y": 0.45758241415023804}, {"x": 0.17519909143447876, "y": 0.4571428596973419}], "text": "Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decom-\nposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633,\n2023. URL https://arxiv.org/abs/2305.00633.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with\ncompression and selective augmentation, 2023. URL https://arxiv.org/abs/2310.\n04408.\n"}
{"page": 15, "bbox": [{"x": 0.17519909143447876, "y": 0.47208791971206665}, {"x": 0.8265073895454407, "y": 0.47208791971206665}, {"x": 0.8265073895454407, "y": 0.7032967209815979}, {"x": 0.17519909143447876, "y": 0.7032967209815979}], "text": "Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language\nmodels robust to irrelevant context, 2023. URL https://arxiv.org/abs/2310.01558.\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of\nattribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https:\n//arxiv.org/abs/2305.06311.\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,\nXixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.\narXiv preprint arXiv:2304.03728, 2023. URL https://arxiv.org/abs/2304.03728.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language\nagent tree search unifies reasoning acting and planning in language models, 2023. URL https:\n//arxiv.org/abs/2310.04406.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\n"}
{"page": 15, "bbox": [{"x": 0.4920364022254944, "y": 0.945494532585144}, {"x": 0.5056883096694946, "y": 0.945494532585144}, {"x": 0.5051194429397583, "y": 0.9635164737701416}, {"x": 0.49146756529808044, "y": 0.9635164737701416}], "text": "115\n"}
{"page": 15, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "15\n"}
{"page": 16, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 16, "bbox": [{"x": 0.17747440934181213, "y": 0.10593406856060028}, {"x": 0.26279863715171814, "y": 0.10681318491697311}, {"x": 0.26279863715171814, "y": 0.11648351699113846}, {"x": 0.17747440934181213, "y": 0.11560439318418503}], "text": "APPENDIX\n"}
{"page": 16, "bbox": [{"x": 0.8077360391616821, "y": 0.15032966434955597}, {"x": 0.8225256204605103, "y": 0.15032966434955597}, {"x": 0.8225256204605103, "y": 0.15780219435691833}, {"x": 0.8077360391616821, "y": 0.15780219435691833}], "text": "17\n"}
{"page": 16, "bbox": [{"x": 0.1757679134607315, "y": 0.14989010989665985}, {"x": 0.32992035150527954, "y": 0.14989010989665985}, {"x": 0.32992035150527954, "y": 0.15824176371097565}, {"x": 0.1757679134607315, "y": 0.15824176371097565}], "text": "A SELF-RAG Details\n"}
{"page": 16, "bbox": [{"x": 0.8077360391616821, "y": 0.17142857611179352}, {"x": 0.8225256204605103, "y": 0.17142857611179352}, {"x": 0.8225256204605103, "y": 0.179340660572052}, {"x": 0.8077360391616821, "y": 0.179340660572052}], "text": "17\n"}
{"page": 16, "bbox": [{"x": 0.2002275288105011, "y": 0.17142857611179352}, {"x": 0.35949942469596863, "y": 0.17142857611179352}, {"x": 0.35949942469596863, "y": 0.17978021502494812}, {"x": 0.2002275288105011, "y": 0.17978021502494812}], "text": "A.1 Reflection Tokens.\n"}
{"page": 16, "bbox": [{"x": 0.80887371301651, "y": 0.192967027425766}, {"x": 0.8225256204605103, "y": 0.192967027425766}, {"x": 0.8225256204605103, "y": 0.20087912678718567}, {"x": 0.80887371301651, "y": 0.20087912678718567}], "text": "17\n"}
{"page": 16, "bbox": [{"x": 0.2002275288105011, "y": 0.19120879471302032}, {"x": 0.38054606318473816, "y": 0.19252747297286987}, {"x": 0.38054606318473816, "y": 0.20351648330688477}, {"x": 0.2002275288105011, "y": 0.20219780504703522}], "text": "A.2 SELF-RAG Training.\n"}
{"page": 16, "bbox": [{"x": 0.8077360391616821, "y": 0.21406593918800354}, {"x": 0.8213879466056824, "y": 0.21406593918800354}, {"x": 0.8213879466056824, "y": 0.22197802364826202}, {"x": 0.8077360391616821, "y": 0.22197802364826202}], "text": "19\n"}
{"page": 16, "bbox": [{"x": 0.20079636573791504, "y": 0.21406593918800354}, {"x": 0.3771331012248993, "y": 0.21406593918800354}, {"x": 0.3771331012248993, "y": 0.22241757810115814}, {"x": 0.20079636573791504, "y": 0.22241757810115814}], "text": "A.3 SELF-RAG Inference\n"}
{"page": 16, "bbox": [{"x": 0.8071672320365906, "y": 0.24835164844989777}, {"x": 0.8213879466056824, "y": 0.24835164844989777}, {"x": 0.8213879466056824, "y": 0.25626373291015625}, {"x": 0.8071672320365906, "y": 0.25626373291015625}], "text": "19\n"}
{"page": 16, "bbox": [{"x": 0.17633675038814545, "y": 0.24747252464294434}, {"x": 0.3475540280342102, "y": 0.2465934008359909}, {"x": 0.3475540280342102, "y": 0.257582426071167}, {"x": 0.17633675038814545, "y": 0.25846153497695923}], "text": "B Experimental Details\n"}
{"page": 16, "bbox": [{"x": 0.8083049058914185, "y": 0.2694505453109741}, {"x": 0.8208191394805908, "y": 0.2694505453109741}, {"x": 0.8208191394805908, "y": 0.2773626446723938}, {"x": 0.8083049058914185, "y": 0.2773626446723938}], "text": "19\n"}
{"page": 16, "bbox": [{"x": 0.2002275288105011, "y": 0.26769229769706726}, {"x": 0.40216153860092163, "y": 0.269010990858078}, {"x": 0.40216153860092163, "y": 0.2795604467391968}, {"x": 0.2002275288105011, "y": 0.27824175357818604}], "text": "B.1 More Details of Training\n"}
{"page": 16, "bbox": [{"x": 0.806598424911499, "y": 0.2909890115261078}, {"x": 0.8225256204605103, "y": 0.2909890115261078}, {"x": 0.8225256204605103, "y": 0.29890111088752747}, {"x": 0.806598424911499, "y": 0.29890111088752747}], "text": "20\n"}
{"page": 16, "bbox": [{"x": 0.20079636573791504, "y": 0.29054945707321167}, {"x": 0.42150169610977173, "y": 0.29054945707321167}, {"x": 0.42150169610977173, "y": 0.2997802197933197}, {"x": 0.20079636573791504, "y": 0.2997802197933197}], "text": "B.2 More Details of Evaluations\n"}
{"page": 16, "bbox": [{"x": 0.806598424911499, "y": 0.3248351514339447}, {"x": 0.8236632347106934, "y": 0.3248351514339447}, {"x": 0.8236632347106934, "y": 0.3327472507953644}, {"x": 0.806598424911499, "y": 0.3327472507953644}], "text": "20\n"}
{"page": 16, "bbox": [{"x": 0.1769055724143982, "y": 0.3243955969810486}, {"x": 0.25142207741737366, "y": 0.3252747356891632}, {"x": 0.25142207741737366, "y": 0.3340659439563751}, {"x": 0.1769055724143982, "y": 0.3331868052482605}], "text": "C Results\n"}
{"page": 16, "bbox": [{"x": 0.806598424911499, "y": 0.3468131721019745}, {"x": 0.8236632347106934, "y": 0.3468131721019745}, {"x": 0.8236632347106934, "y": 0.35428571701049805}, {"x": 0.806598424911499, "y": 0.35428571701049805}], "text": "20\n"}
{"page": 16, "bbox": [{"x": 0.2002275288105011, "y": 0.3446153700351715}, {"x": 0.29522183537483215, "y": 0.34637361764907837}, {"x": 0.2946530282497406, "y": 0.35736262798309326}, {"x": 0.19965870678424835, "y": 0.3556044101715088}], "text": "C.1 Analysis\n"}
{"page": 16, "bbox": [{"x": 0.806598424911499, "y": 0.3226373493671417}, {"x": 0.8208191394805908, "y": 0.3226373493671417}, {"x": 0.8208191394805908, "y": 0.3978022038936615}, {"x": 0.806598424911499, "y": 0.3978022038936615}], "text": "2222\n"}
{"page": 16, "bbox": [{"x": 0.806598424911499, "y": 0.3674725294113159}, {"x": 0.8225256204605103, "y": 0.3674725294113159}, {"x": 0.8225256204605103, "y": 0.3753846287727356}, {"x": 0.806598424911499, "y": 0.3753846287727356}], "text": "21\n"}
{"page": 16, "bbox": [{"x": 0.2002275288105011, "y": 0.36527472734451294}, {"x": 0.4288964867591858, "y": 0.3665934205055237}, {"x": 0.42832764983177185, "y": 0.4004395604133606}, {"x": 0.19965870678424835, "y": 0.39912086725234985}], "text": "C.2 Human Evaluation Examples\nC.3 Qualitative Examples\n"}
{"page": 16, "bbox": [{"x": 0.806598424911499, "y": 0.3890109956264496}, {"x": 0.8219567537307739, "y": 0.3890109956264496}, {"x": 0.8219567537307739, "y": 0.3969230651855469}, {"x": 0.806598424911499, "y": 0.3969230651855469}], "text": "21\n"}
{"page": 16, "bbox": [{"x": 0.806598424911499, "y": 0.4228571355342865}, {"x": 0.8225256204605103, "y": 0.4228571355342865}, {"x": 0.8225256204605103, "y": 0.43032968044281006}, {"x": 0.806598424911499, "y": 0.43032968044281006}], "text": "21\n"}
{"page": 16, "bbox": [{"x": 0.17633675038814545, "y": 0.4224175810813904}, {"x": 0.5841865539550781, "y": 0.4224175810813904}, {"x": 0.5841865539550781, "y": 0.4312087893486023}, {"x": 0.17633675038814545, "y": 0.4312087893486023}], "text": "D Full List of Instructions and Demonstrations for GPT-4\n"}
{"page": 16, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "16\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.10593406856060028}, {"x": 0.3759954571723938, "y": 0.10593406856060028}, {"x": 0.3759954571723938, "y": 0.11604395508766174}, {"x": 0.17633675038814545, "y": 0.11604395508766174}], "text": "A SELF-RAG DETAILS\n"}
{"page": 17, "bbox": [{"x": 0.21899886429309845, "y": 0.13802197575569153}, {"x": 0.37428897619247437, "y": 0.13802197575569153}, {"x": 0.37428897619247437, "y": 0.14637362957000732}, {"x": 0.21899886429309845, "y": 0.14637362957000732}], "text": "REFLECTION TOKENS.\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.13846154510974884}, {"x": 0.20193400979042053, "y": 0.13846154510974884}, {"x": 0.20193400979042053, "y": 0.14637362957000732}, {"x": 0.17633675038814545, "y": 0.14637362957000732}], "text": "A.1\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.16307692229747772}, {"x": 0.8236632347106934, "y": 0.1621977984905243}, {"x": 0.8236632347106934, "y": 0.2017582356929779}, {"x": 0.17633675038814545, "y": 0.20263735949993134}], "text": "Definitions of reflection tokens. Below, we provide a detailed definition of reflection type and\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\nonly given at each output level.\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.2210988998413086}, {"x": 0.18145619332790375, "y": 0.2210988998413086}, {"x": 0.18145619332790375, "y": 0.22461538016796112}, {"x": 0.17633675038814545, "y": 0.22461538016796112}], "text": "•\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.21714285016059875}, {"x": 0.8259385824203491, "y": 0.21714285016059875}, {"x": 0.8259385824203491, "y": 0.4813186824321747}, {"x": 0.17633675038814545, "y": 0.4813186824321747}], "text": "Retrieval-on-demand (Retrieve]): Given an input and previous-step generation (if applicable),\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\nto use evidence, which indicates that a model can continue to use the evidence retrieved\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\ngenerates multiple segments based on the\npassage.\n• Relevant ( [ ISREL]): Retrieved knowledge may not be always relevant to the input. This aspect\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\nSupported ISSUP ): Attribution is the concept of whether the output is fully supported by\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\nsupported, Partially supported, and No support / Contradictory, follow-\ning Yue et al. (2023); Nakano et al. (2021).\n• Useful ( [ ISUSE] ): Following the definitions from Liu et al. (2023a), we define the perceived utility\nas whether the response is a helpful and informative answer to the query, independently from\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\n"}
{"page": 17, "bbox": [{"x": 0.1769055724143982, "y": 0.356483519077301}, {"x": 0.18145619332790375, "y": 0.356483519077301}, {"x": 0.18145619332790375, "y": 0.35956043004989624}, {"x": 0.1769055724143982, "y": 0.35956043004989624}], "text": "•\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.5002197623252869}, {"x": 0.4453924894332886, "y": 0.5002197623252869}, {"x": 0.4453924894332886, "y": 0.5094505548477173}, {"x": 0.17633675038814545, "y": 0.5094505548477173}], "text": "Details of GPT-4-based data collections.\n"}
{"page": 17, "bbox": [{"x": 0.1757679134607315, "y": 0.4962637424468994}, {"x": 0.8248009085655212, "y": 0.5006593465805054}, {"x": 0.8236632347106934, "y": 0.5841758251190186}, {"x": 0.17463025450706482, "y": 0.5797802209854126}], "text": "We use the instruction and demonstration pairs to prompt\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\noutputs with “##”. We use the temperature 1 and set the maximum output token counts to be 200. We\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\nthat do not match our expected category names. As a result, we collected 1,2594 for [Retrieve], 11,181\nfor ISSUP\n|, 19,317 for relevance, 3,831 for utility.\n"}
{"page": 17, "bbox": [{"x": 0.1757679134607315, "y": 0.6000000238418579}, {"x": 0.8253697156906128, "y": 0.5995604395866394}, {"x": 0.8253697156906128, "y": 0.6795604228973389}, {"x": 0.1757679134607315, "y": 0.6800000071525574}], "text": "Manual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\nthe same instruction, demonstrations, and test instances. We found our assessments show high\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\ndisagreement between 1 and 2 or 4 and 5.\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.7015384435653687}, {"x": 0.3668941855430603, "y": 0.7015384435653687}, {"x": 0.3668941855430603, "y": 0.7098901271820068}, {"x": 0.17633675038814545, "y": 0.7098901271820068}], "text": "A.2 SELF-RAG TRAINING\n"}
{"page": 17, "bbox": [{"x": 0.17633675038814545, "y": 0.7274725437164307}, {"x": 0.725824773311615, "y": 0.7274725437164307}, {"x": 0.725824773311615, "y": 0.7393406629562378}, {"x": 0.17633675038814545, "y": 0.7393406629562378}], "text": "Overview of training. Algorithm 2 provides a high-level overview of our training.\n"}
{"page": 17, "bbox": [{"x": 0.1757679134607315, "y": 0.7578021883964539}, {"x": 0.8259385824203491, "y": 0.7578021883964539}, {"x": 0.8259385824203491, "y": 0.8518681526184082}, {"x": 0.1757679134607315, "y": 0.8518681526184082}], "text": "Full list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\n2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n"}
{"page": 17, "bbox": [{"x": 0.1757679134607315, "y": 0.8712087869644165}, {"x": 0.8259385824203491, "y": 0.8712087869644165}, {"x": 0.8259385824203491, "y": 0.9243956208229065}, {"x": 0.1757679134607315, "y": 0.9243956208229065}], "text": "Performance of the Critic C. We evaluate the accuracy of reward predictions by splitting GPT-4\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\nas follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\n"}
{"page": 17, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "17\n"}
{"page": 18, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 18, "bbox": [{"x": 0.17519909143447876, "y": 0.10681318491697311}, {"x": 0.5716723799705505, "y": 0.1063736230134964}, {"x": 0.5716723799705505, "y": 0.18109890818595886}, {"x": 0.17519909143447876, "y": 0.18153846263885498}], "text": "Algorithm 2 SELF-RAG Training\n1: Input input-output data D = {X, Y}, generator M, C 0\n2: Initialize C with a pre-trained LM\n3: Sample data {X sample, Ysample}\n4: for (x, y) = (X sample, Y sample) do\n"}
{"page": 18, "bbox": [{"x": 0.447098970413208, "y": 0.1538461595773697}, {"x": 0.49544936418533325, "y": 0.15472526848316193}, {"x": 0.4948805570602417, "y": 0.16659340262413025}, {"x": 0.44653013348579407, "y": 0.16571427881717682}], "text": "{X,Y}\n"}
{"page": 18, "bbox": [{"x": 0.4288964867591858, "y": 0.15868131816387177}, {"x": 0.43970420956611633, "y": 0.15868131816387177}, {"x": 0.43970420956611633, "y": 0.16175824403762817}, {"x": 0.4288964867591858, "y": 0.16175824403762817}], "text": "~\n"}
{"page": 18, "bbox": [{"x": 0.5693970322608948, "y": 0.15472526848316193}, {"x": 0.8230944275856018, "y": 0.1542857140302658}, {"x": 0.8230944275856018, "y": 0.17714285850524902}, {"x": 0.5693970322608948, "y": 0.17758241295814514}], "text": "▷ Training Critic LM (Section 3.2.1)\n▷ Data collections for C\n"}
{"page": 18, "bbox": [{"x": 0.18373151123523712, "y": 0.18373626470565796}, {"x": 0.19567690789699554, "y": 0.18373626470565796}, {"x": 0.19567690789699554, "y": 0.2043956071138382}, {"x": 0.18373151123523712, "y": 0.2043956071138382}], "text": "5:\n6:\n"}
{"page": 18, "bbox": [{"x": 0.18373151123523712, "y": 0.179340660572052}, {"x": 0.5807735919952393, "y": 0.18065933883190155}, {"x": 0.5802047848701477, "y": 0.25362637639045715}, {"x": 0.18316268920898438, "y": 0.2523076832294464}], "text": "Prompt GPT-4 to collect a reflection token r for (x, y)\nAdd {(x, y, r)} to Dcritic\n7: Update C with next token prediction loss\n8: Initialize M with a pre-trained LM\n9: for (x, y) = (X, Y) do\n"}
{"page": 18, "bbox": [{"x": 0.5375426411628723, "y": 0.21142856776714325}, {"x": 0.8196814656257629, "y": 0.2127472460269928}, {"x": 0.8196814656257629, "y": 0.2523076832294464}, {"x": 0.5375426411628723, "y": 0.25098901987075806}], "text": "▷ Critic learning; Eq. 1\n▷ Training Generator LM (Section 3.2.2)\n▷ Data collection for M with Dcritic\n"}
{"page": 18, "bbox": [{"x": 0.17804323136806488, "y": 0.25626373291015625}, {"x": 0.19567690789699554, "y": 0.25626373291015625}, {"x": 0.19567690789699554, "y": 0.2632966935634613}, {"x": 0.17804323136806488, "y": 0.2632966935634613}], "text": "10:\n"}
{"page": 18, "bbox": [{"x": 0.22810012102127075, "y": 0.25010988116264343}, {"x": 0.4277588129043579, "y": 0.25406593084335327}, {"x": 0.4266211688518524, "y": 0.28395605087280273}, {"x": 0.22696246206760406, "y": 0.2800000011920929}], "text": "Run C to predict r given (x, y)\nAdd (x,y,r) to Dgen\n"}
{"page": 18, "bbox": [{"x": 0.17804323136806488, "y": 0.26989009976387024}, {"x": 0.19453924894332886, "y": 0.26989009976387024}, {"x": 0.19453924894332886, "y": 0.2773626446723938}, {"x": 0.17804323136806488, "y": 0.2773626446723938}], "text": "11:\n"}
{"page": 18, "bbox": [{"x": 0.6171786189079285, "y": 0.28439560532569885}, {"x": 0.8230944275856018, "y": 0.2861538529396057}, {"x": 0.8230944275856018, "y": 0.2971428632736206}, {"x": 0.6171786189079285, "y": 0.29538461565971375}], "text": "▷ Generator LM learning; Eq. 2\n"}
{"page": 18, "bbox": [{"x": 0.17804323136806488, "y": 0.2857142984867096}, {"x": 0.5369738340377808, "y": 0.2857142984867096}, {"x": 0.5369738340377808, "y": 0.2971428632736206}, {"x": 0.17804323136806488, "y": 0.2971428632736206}], "text": "12: Update M on D, with next token prediction loss\n"}
{"page": 18, "bbox": [{"x": 0.3117178678512573, "y": 0.29230770468711853}, {"x": 0.33162686228752136, "y": 0.2918681204319}, {"x": 0.33162686228752136, "y": 0.29802197217941284}, {"x": 0.3117178678512573, "y": 0.29846152663230896}], "text": "gen\n"}
{"page": 18, "bbox": [{"x": 0.6240045428276062, "y": 0.3257142901420593}, {"x": 0.7633674740791321, "y": 0.3257142901420593}, {"x": 0.7633674740791321, "y": 0.3327472507953644}, {"x": 0.6240045428276062, "y": 0.3327472507953644}], "text": "the number of instances\n"}
{"page": 18, "bbox": [{"x": 0.23606370389461517, "y": 0.3248351514339447}, {"x": 0.31569966673851013, "y": 0.3270329535007477}, {"x": 0.3151308298110962, "y": 0.33494505286216736}, {"x": 0.23549488186836243, "y": 0.3327472507953644}], "text": "Dataset name\n"}
{"page": 18, "bbox": [{"x": 0.5290102362632751, "y": 0.3252747356891632}, {"x": 0.5984072685241699, "y": 0.3274725377559662}, {"x": 0.5978384613990784, "y": 0.33450549840927124}, {"x": 0.5284414291381836, "y": 0.33230769634246826}], "text": "Data source\n"}
{"page": 18, "bbox": [{"x": 0.37940841913223267, "y": 0.3257142901420593}, {"x": 0.5051194429397583, "y": 0.3279120922088623}, {"x": 0.5045506358146667, "y": 0.35472527146339417}, {"x": 0.3788395822048187, "y": 0.3525274693965912}], "text": "category\nInstruction-following\n"}
{"page": 18, "bbox": [{"x": 0.6729238033294678, "y": 0.3441758155822754}, {"x": 0.7127417325973511, "y": 0.3437362611293793}, {"x": 0.7127417325973511, "y": 0.35208791494369507}, {"x": 0.6729238033294678, "y": 0.3525274693965912}], "text": "26,168\n"}
{"page": 18, "bbox": [{"x": 0.23606370389461517, "y": 0.34285715222358704}, {"x": 0.3202502727508545, "y": 0.34505495429039}, {"x": 0.31968146562576294, "y": 0.35428571701049805}, {"x": 0.23549488186836243, "y": 0.35208791494369507}], "text": "GPT-4 Alpaca\n"}
{"page": 18, "bbox": [{"x": 0.522753119468689, "y": 0.3441758155822754}, {"x": 0.6046643853187561, "y": 0.3437362611293793}, {"x": 0.6046643853187561, "y": 0.35384616255760193}, {"x": 0.522753119468689, "y": 0.35428571701049805}], "text": "Open-Instruct\n"}
{"page": 18, "bbox": [{"x": 0.23606370389461517, "y": 0.3551648259162903}, {"x": 0.33276450634002686, "y": 0.35736262798309326}, {"x": 0.3321956694126129, "y": 0.3674725294113159}, {"x": 0.23549488186836243, "y": 0.36527472734451294}], "text": "Stanford Alpaca\n"}
{"page": 18, "bbox": [{"x": 0.6734926104545593, "y": 0.35736262798309326}, {"x": 0.713879406452179, "y": 0.35736262798309326}, {"x": 0.713879406452179, "y": 0.36527472734451294}, {"x": 0.6734926104545593, "y": 0.36527472734451294}], "text": "25,153\n"}
{"page": 18, "bbox": [{"x": 0.23606370389461517, "y": 0.3696703314781189}, {"x": 0.29522183537483215, "y": 0.3692307770252228}, {"x": 0.29522183537483215, "y": 0.37670329213142395}, {"x": 0.23606370389461517, "y": 0.37714284658432007}], "text": "FLAN-V2\n"}
{"page": 18, "bbox": [{"x": 0.6746302843093872, "y": 0.3696703314781189}, {"x": 0.7127417325973511, "y": 0.3696703314781189}, {"x": 0.7127417325973511, "y": 0.3775824308395386}, {"x": 0.6746302843093872, "y": 0.3775824308395386}], "text": "17,817\n"}
{"page": 18, "bbox": [{"x": 0.23606370389461517, "y": 0.38197803497314453}, {"x": 0.2957906723022461, "y": 0.3810988962650299}, {"x": 0.2957906723022461, "y": 0.3890109956264496}, {"x": 0.23606370389461517, "y": 0.3898901045322418}], "text": "ShareGPT\n"}
{"page": 18, "bbox": [{"x": 0.3788395822048187, "y": 0.3551648259162903}, {"x": 0.6052331924438477, "y": 0.3560439646244049}, {"x": 0.6046643853187561, "y": 0.4171428680419922}, {"x": 0.3782707750797272, "y": 0.41626372933387756}], "text": "Instruction-following Open-Instruct\nInstruction-following Open-Instruct\nInstruction-following Open-Instruct\nInstruction-following Open-Instruct\nKnowledge-intensive\n"}
{"page": 18, "bbox": [{"x": 0.6751990914344788, "y": 0.38197803497314453}, {"x": 0.713879406452179, "y": 0.38241758942604065}, {"x": 0.713879406452179, "y": 0.39076924324035645}, {"x": 0.6751990914344788, "y": 0.39032965898513794}], "text": "13,406\n"}
{"page": 18, "bbox": [{"x": 0.23549488186836243, "y": 0.3942857086658478}, {"x": 0.3344709873199463, "y": 0.3925274610519409}, {"x": 0.33503982424736023, "y": 0.4030769169330597}, {"x": 0.23606370389461517, "y": 0.40483516454696655}], "text": "Open Assistant 1\n"}
{"page": 18, "bbox": [{"x": 0.6769055724143982, "y": 0.3956044018268585}, {"x": 0.7098976373672485, "y": 0.3956044018268585}, {"x": 0.7098976373672485, "y": 0.4030769169330597}, {"x": 0.6769055724143982, "y": 0.4030769169330597}], "text": "9,464\n"}
{"page": 18, "bbox": [{"x": 0.5483503937721252, "y": 0.40747252106666565}, {"x": 0.5779294371604919, "y": 0.40747252106666565}, {"x": 0.5779294371604919, "y": 0.4149450659751892}, {"x": 0.5483503937721252, "y": 0.4149450659751892}], "text": "KILT\n"}
{"page": 18, "bbox": [{"x": 0.2349260449409485, "y": 0.40527471899986267}, {"x": 0.3589306175708771, "y": 0.40703296661376953}, {"x": 0.3589306175708771, "y": 0.4180219769477844}, {"x": 0.2349260449409485, "y": 0.41626372933387756}], "text": "Wizard of Wikipedia\n"}
{"page": 18, "bbox": [{"x": 0.6746302843093872, "y": 0.40747252106666565}, {"x": 0.7133105993270874, "y": 0.40747252106666565}, {"x": 0.7133105993270874, "y": 0.41582417488098145}, {"x": 0.6746302843093872, "y": 0.41582417488098145}], "text": "17,367\n"}
{"page": 18, "bbox": [{"x": 0.5483503937721252, "y": 0.4197802245616913}, {"x": 0.5790671110153198, "y": 0.4202197790145874}, {"x": 0.5790671110153198, "y": 0.4276922941207886}, {"x": 0.5483503937721252, "y": 0.42725273966789246}], "text": "KILT\n"}
{"page": 18, "bbox": [{"x": 0.23549488186836243, "y": 0.41846153140068054}, {"x": 0.3424345850944519, "y": 0.41934067010879517}, {"x": 0.3424345850944519, "y": 0.42945054173469543}, {"x": 0.23549488186836243, "y": 0.4285714328289032}], "text": "Natural Questions\n"}
{"page": 18, "bbox": [{"x": 0.6746302843093872, "y": 0.4202197790145874}, {"x": 0.7133105993270874, "y": 0.4202197790145874}, {"x": 0.7133105993270874, "y": 0.4285714328289032}, {"x": 0.6746302843093872, "y": 0.4285714328289032}], "text": "15,535\n"}
{"page": 18, "bbox": [{"x": 0.3788395822048187, "y": 0.4197802245616913}, {"x": 0.5039817690849304, "y": 0.4197802245616913}, {"x": 0.5039817690849304, "y": 0.42945054173469543}, {"x": 0.3788395822048187, "y": 0.42945054173469543}], "text": "Knowledge-intensive\n"}
{"page": 18, "bbox": [{"x": 0.5483503937721252, "y": 0.43296703696250916}, {"x": 0.5784983038902283, "y": 0.43296703696250916}, {"x": 0.5784983038902283, "y": 0.4399999976158142}, {"x": 0.5483503937721252, "y": 0.4399999976158142}], "text": "KILT\n"}
{"page": 18, "bbox": [{"x": 0.23549488186836243, "y": 0.43296703696250916}, {"x": 0.28213879466056824, "y": 0.43296703696250916}, {"x": 0.28213879466056824, "y": 0.4404395520687103}, {"x": 0.23549488186836243, "y": 0.4404395520687103}], "text": "FEVER\n"}
{"page": 18, "bbox": [{"x": 0.6769055724143982, "y": 0.43296703696250916}, {"x": 0.7098976373672485, "y": 0.43296703696250916}, {"x": 0.7098976373672485, "y": 0.44087910652160645}, {"x": 0.6769055724143982, "y": 0.44087910652160645}], "text": "9,966\n"}
{"page": 18, "bbox": [{"x": 0.3788395822048187, "y": 0.43252748250961304}, {"x": 0.5034129619598389, "y": 0.43252748250961304}, {"x": 0.5034129619598389, "y": 0.4421977996826172}, {"x": 0.3788395822048187, "y": 0.4421977996826172}], "text": "Knowledge-intensive\n"}
{"page": 18, "bbox": [{"x": 0.6763367652893066, "y": 0.44439560174942017}, {"x": 0.7093287706375122, "y": 0.4448351562023163}, {"x": 0.7093287706375122, "y": 0.4531868100166321}, {"x": 0.6763367652893066, "y": 0.45274725556373596}], "text": "4,699\n"}
{"page": 18, "bbox": [{"x": 0.3788395822048187, "y": 0.4448351562023163}, {"x": 0.5034129619598389, "y": 0.44351649284362793}, {"x": 0.5034129619598389, "y": 0.4536263644695282}, {"x": 0.3788395822048187, "y": 0.45494505763053894}], "text": "Knowledge-intensive\n"}
{"page": 18, "bbox": [{"x": 0.5307167172431946, "y": 0.4452747106552124}, {"x": 0.5978384613990784, "y": 0.4457142949104309}, {"x": 0.5978384613990784, "y": 0.4531868100166321}, {"x": 0.5307167172431946, "y": 0.45274725556373596}], "text": "HF Dataset\n"}
{"page": 18, "bbox": [{"x": 0.23549488186836243, "y": 0.4448351562023163}, {"x": 0.3270762264728546, "y": 0.44395604729652405}, {"x": 0.3270762264728546, "y": 0.4540659487247467}, {"x": 0.23549488186836243, "y": 0.45494505763053894}], "text": "OpenBoookQA\n"}
{"page": 18, "bbox": [{"x": 0.6769055724143982, "y": 0.45802196860313416}, {"x": 0.7098976373672485, "y": 0.45802196860313416}, {"x": 0.7098976373672485, "y": 0.46593406796455383}, {"x": 0.6769055724143982, "y": 0.46593406796455383}], "text": "2,147\n"}
{"page": 18, "bbox": [{"x": 0.23549488186836243, "y": 0.45802196860313416}, {"x": 0.29010239243507385, "y": 0.4571428596973419}, {"x": 0.29010239243507385, "y": 0.46637362241744995}, {"x": 0.23549488186836243, "y": 0.4672527611255646}], "text": "Arc-Easy\n"}
{"page": 18, "bbox": [{"x": 0.3788395822048187, "y": 0.4567033052444458}, {"x": 0.5978384613990784, "y": 0.4567033052444458}, {"x": 0.5978384613990784, "y": 0.4676923155784607}, {"x": 0.3788395822048187, "y": 0.4676923155784607}], "text": "Knowledge-intensive HF Dataset\n"}
{"page": 18, "bbox": [{"x": 0.6774743795394897, "y": 0.46945056319236755}, {"x": 0.7098976373672485, "y": 0.46989011764526367}, {"x": 0.7098976373672485, "y": 0.47824177145957947}, {"x": 0.6774743795394897, "y": 0.47780218720436096}], "text": "3,897\n"}
{"page": 18, "bbox": [{"x": 0.5432309508323669, "y": 0.46989011764526367}, {"x": 0.583048939704895, "y": 0.46945056319236755}, {"x": 0.583048939704895, "y": 0.4791208803653717}, {"x": 0.5432309508323669, "y": 0.4795604348182678}], "text": "ASQA\n"}
{"page": 18, "bbox": [{"x": 0.3788395822048187, "y": 0.46989011764526367}, {"x": 0.5039817690849304, "y": 0.4703296720981598}, {"x": 0.5039817690849304, "y": 0.4795604348182678}, {"x": 0.3788395822048187, "y": 0.4791208803653717}], "text": "Knowledge-intensive\n"}
{"page": 18, "bbox": [{"x": 0.23549488186836243, "y": 0.46989011764526367}, {"x": 0.2753128409385681, "y": 0.4703296720981598}, {"x": 0.2753128409385681, "y": 0.47999998927116394}, {"x": 0.23549488186836243, "y": 0.4795604348182678}], "text": "ASQA\n"}
{"page": 18, "bbox": [{"x": 0.3242320716381073, "y": 0.4984615445137024}, {"x": 0.6740614175796509, "y": 0.4984615445137024}, {"x": 0.6740614175796509, "y": 0.5094505548477173}, {"x": 0.3242320716381073, "y": 0.5094505548477173}], "text": "Table 3: The generator LM M training data statistics.\n"}
{"page": 18, "bbox": [{"x": 0.5620023012161255, "y": 0.5340659618377686}, {"x": 0.5921501517295837, "y": 0.5349450707435608}, {"x": 0.5915813446044922, "y": 0.5410988926887512}, {"x": 0.5614334344863892, "y": 0.540219783782959}], "text": "ISREL\n"}
{"page": 18, "bbox": [{"x": 0.6234357357025146, "y": 0.5345054864883423}, {"x": 0.6524459719657898, "y": 0.5353845953941345}, {"x": 0.6518771052360535, "y": 0.5410988926887512}, {"x": 0.6228668689727783, "y": 0.540219783782959}], "text": "ISUSE\n"}
{"page": 18, "bbox": [{"x": 0.5028441548347473, "y": 0.5345054864883423}, {"x": 0.5312855243682861, "y": 0.5349450707435608}, {"x": 0.5312855243682861, "y": 0.5410988926887512}, {"x": 0.5028441548347473, "y": 0.5406593680381775}], "text": "ISSUP\n"}
{"page": 18, "bbox": [{"x": 0.43344709277153015, "y": 0.5340659618377686}, {"x": 0.47212740778923035, "y": 0.5349450707435608}, {"x": 0.4715586006641388, "y": 0.5415384769439697}, {"x": 0.4328782856464386, "y": 0.5406593680381775}], "text": "Retrieve\n"}
{"page": 18, "bbox": [{"x": 0.34186574816703796, "y": 0.5327472686767578}, {"x": 0.40784981846809387, "y": 0.5327472686767578}, {"x": 0.40784981846809387, "y": 0.5723077058792114}, {"x": 0.34186574816703796, "y": 0.5723077058792114}], "text": "base LM\nLlama2-7B\nFLAN-3B\n"}
{"page": 18, "bbox": [{"x": 0.5631399154663086, "y": 0.5520879030227661}, {"x": 0.5870307087898254, "y": 0.5520879030227661}, {"x": 0.5870307087898254, "y": 0.558681309223175}, {"x": 0.5631399154663086, "y": 0.558681309223175}], "text": "80.2\n"}
{"page": 18, "bbox": [{"x": 0.5017064809799194, "y": 0.5520879030227661}, {"x": 0.5284414291381836, "y": 0.5516483783721924}, {"x": 0.5284414291381836, "y": 0.5591208934783936}, {"x": 0.5017064809799194, "y": 0.5595604181289673}], "text": "93.5\n"}
{"page": 18, "bbox": [{"x": 0.6240045428276062, "y": 0.5520879030227661}, {"x": 0.6496018171310425, "y": 0.5520879030227661}, {"x": 0.6496018171310425, "y": 0.5591208934783936}, {"x": 0.6240045428276062, "y": 0.5591208934783936}], "text": "73.5\n"}
{"page": 18, "bbox": [{"x": 0.4391353726387024, "y": 0.5520879030227661}, {"x": 0.4653014838695526, "y": 0.5520879030227661}, {"x": 0.4653014838695526, "y": 0.5718681216239929}, {"x": 0.4391353726387024, "y": 0.5718681216239929}], "text": "93.8\n85.6\n"}
{"page": 18, "bbox": [{"x": 0.5631399154663086, "y": 0.5639560222625732}, {"x": 0.5887371897697449, "y": 0.5643956065177917}, {"x": 0.5887371897697449, "y": 0.5723077058792114}, {"x": 0.5631399154663086, "y": 0.5718681216239929}], "text": "82.0\n"}
{"page": 18, "bbox": [{"x": 0.6245733499526978, "y": 0.5648351907730103}, {"x": 0.647895336151123, "y": 0.5648351907730103}, {"x": 0.647895336151123, "y": 0.5718681216239929}, {"x": 0.6245733499526978, "y": 0.5718681216239929}], "text": "72.1\n"}
{"page": 18, "bbox": [{"x": 0.5028441548347473, "y": 0.5648351907730103}, {"x": 0.5261660814285278, "y": 0.5648351907730103}, {"x": 0.5261660814285278, "y": 0.5723077058792114}, {"x": 0.5028441548347473, "y": 0.5723077058792114}], "text": "73.1\n"}
{"page": 18, "bbox": [{"x": 0.20307166874408722, "y": 0.5929670333862305}, {"x": 0.7963594794273376, "y": 0.5929670333862305}, {"x": 0.7963594794273376, "y": 0.6043956279754639}, {"x": 0.20307166874408722, "y": 0.6043956279754639}], "text": "Figure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n"}
{"page": 18, "bbox": [{"x": 0.17519909143447876, "y": 0.6347252726554871}, {"x": 0.8248009085655212, "y": 0.6351648569107056}, {"x": 0.8248009085655212, "y": 0.7160439491271973}, {"x": 0.17519909143447876, "y": 0.7156044244766235}], "text": "While our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\nlower performance on ISUSE this is because both models often confuse between the two highest\ncases (5 and 4), where human annotators can also disagree.\n"}
{"page": 18, "bbox": [{"x": 0.37542662024497986, "y": 0.6975824236869812}, {"x": 0.3788395822048187, "y": 0.6975824236869812}, {"x": 0.3788395822048187, "y": 0.7006593346595764}, {"x": 0.37542662024497986, "y": 0.7006593346595764}], "text": ",\n"}
{"page": 18, "bbox": [{"x": 0.1757679134607315, "y": 0.7345054745674133}, {"x": 0.8242321014404297, "y": 0.7345054745674133}, {"x": 0.8242321014404297, "y": 0.8993406295776367}, {"x": 0.1757679134607315, "y": 0.8993406295776367}], "text": "Details of M data creation. Here, we provide detailed data creation procedures. Algorithm 3\nsummarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\nnot. For the instances where the critic predicts | Retrieve =No, we only predict the [ISUSE] given input\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\nthen split output sentences using Spacy. For each sentence, we run C to predict whether the retrieval\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. If C predicts\n|Retrieve|=No, then do not insert any paragraph at the tth segment. If C predicts [Retrieve|=Yes, then\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\nt-th segment. For each retrieved passage, we predict ISREL and ISSUP]. If there is any passage and\ncontinuation with ISREL =Relevant and | ISSUP] =Fully Supported/ |ISSUP] =Partially\n"}
{"page": 18, "bbox": [{"x": 0.19567690789699554, "y": 0.9112088084220886}, {"x": 0.35210466384887695, "y": 0.9120879173278809}, {"x": 0.35210466384887695, "y": 0.9248351454734802}, {"x": 0.19567690789699554, "y": 0.923956036567688}], "text": "7https://spacy.io/\n"}
{"page": 18, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9578021764755249}, {"x": 0.49146756529808044, "y": 0.9578021764755249}], "text": "18\n"}
{"page": 19, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 19, "bbox": [{"x": 0.17633675038814545, "y": 0.10725274682044983}, {"x": 0.8242321014404297, "y": 0.10725274682044983}, {"x": 0.8242321014404297, "y": 0.14681318402290344}, {"x": 0.17633675038814545, "y": 0.14681318402290344}], "text": "Supported, then we sample it as the continuation. If there is more than one passage satisfying this\ncriterion, we use the one with the highest retrieval score. If there are only | ISREL] =Irrelevant or\nISSUP =No Support passages, we randomly sample one passage.\n"}
{"page": 19, "bbox": [{"x": 0.17463025450706482, "y": 0.1621977984905243}, {"x": 0.4260523319244385, "y": 0.16131867468357086}, {"x": 0.4266211688518524, "y": 0.20835164189338684}, {"x": 0.17519909143447876, "y": 0.20923076570034027}], "text": "Algorithm 3 Mgen Data creation\n1: Input Input-output data D = X,Y\n2: for (x, y) = {X, Y} do\n"}
{"page": 19, "bbox": [{"x": 0.18430034816265106, "y": 0.21230769157409668}, {"x": 0.1951080709695816, "y": 0.21230769157409668}, {"x": 0.1951080709695816, "y": 0.21978022158145905}, {"x": 0.18430034816265106, "y": 0.21978022158145905}], "text": "3:\n"}
{"page": 19, "bbox": [{"x": 0.18373151123523712, "y": 0.22593407332897186}, {"x": 0.19567690789699554, "y": 0.22637362778186798}, {"x": 0.19567690789699554, "y": 0.23340658843517303}, {"x": 0.18373151123523712, "y": 0.23296703398227692}], "text": "4:\n"}
{"page": 19, "bbox": [{"x": 0.2286689430475235, "y": 0.20923076570034027}, {"x": 0.5802047848701477, "y": 0.2101098895072937}, {"x": 0.5802047848701477, "y": 0.2518681287765503}, {"x": 0.2286689430475235, "y": 0.25098901987075806}], "text": "Given (x, y) C predicts [Retrieve\nif Retrieve is predicted then\nRetrieve relevant passages D using R given (x, y)\n"}
{"page": 19, "bbox": [{"x": 0.18430034816265106, "y": 0.23999999463558197}, {"x": 0.1962457299232483, "y": 0.24043956398963928}, {"x": 0.19567690789699554, "y": 0.24835164844989777}, {"x": 0.18373151123523712, "y": 0.24791209399700165}], "text": "5:\n"}
{"page": 19, "bbox": [{"x": 0.6945392489433289, "y": 0.23912088572978973}, {"x": 0.8230944275856018, "y": 0.23736263811588287}, {"x": 0.8230944275856018, "y": 0.25010988116264343}, {"x": 0.6945392489433289, "y": 0.2518681287765503}], "text": "▷ Retrieve passages\n"}
{"page": 19, "bbox": [{"x": 0.25142207741737366, "y": 0.2523076832294464}, {"x": 0.34186574816703796, "y": 0.2523076832294464}, {"x": 0.34186574816703796, "y": 0.26153847575187683}, {"x": 0.25142207741737366, "y": 0.26153847575187683}], "text": "for d = D do\n"}
{"page": 19, "bbox": [{"x": 0.18430034816265106, "y": 0.25362637639045715}, {"x": 0.1962457299232483, "y": 0.25406593084335327}, {"x": 0.19567690789699554, "y": 0.26153847575187683}, {"x": 0.18373151123523712, "y": 0.2610988914966583}], "text": "6:\n"}
{"page": 19, "bbox": [{"x": 0.3555176258087158, "y": 0.26505494117736816}, {"x": 0.4624573290348053, "y": 0.2641758322715759}, {"x": 0.4624573290348053, "y": 0.27604395151138306}, {"x": 0.3555176258087158, "y": 0.2769230902194977}], "text": "ISREL for each d\n"}
{"page": 19, "bbox": [{"x": 0.18430034816265106, "y": 0.26725274324417114}, {"x": 0.1962457299232483, "y": 0.26769229769706726}, {"x": 0.19567690789699554, "y": 0.27604395151138306}, {"x": 0.18373151123523712, "y": 0.27560439705848694}], "text": "7:\n"}
{"page": 19, "bbox": [{"x": 0.276450514793396, "y": 0.2659340798854828}, {"x": 0.3412969410419464, "y": 0.2654944956302643}, {"x": 0.3412969410419464, "y": 0.2773626446723938}, {"x": 0.276450514793396, "y": 0.2778021991252899}], "text": "C predicts\n"}
{"page": 19, "bbox": [{"x": 0.6205915808677673, "y": 0.2654944956302643}, {"x": 0.8236632347106934, "y": 0.26725274324417114}, {"x": 0.8236632347106934, "y": 0.27824175357818604}, {"x": 0.6205915808677673, "y": 0.2764835059642792}], "text": "▷ Predict relevance of passages\n"}
{"page": 19, "bbox": [{"x": 0.35608646273612976, "y": 0.27868130803108215}, {"x": 0.48748576641082764, "y": 0.27824175357818604}, {"x": 0.48748576641082764, "y": 0.2909890115261078}, {"x": 0.35608646273612976, "y": 0.2914285659790039}], "text": "ISSUP] for each (y, d)\n"}
{"page": 19, "bbox": [{"x": 0.18430034816265106, "y": 0.28131869435310364}, {"x": 0.19567690789699554, "y": 0.28131869435310364}, {"x": 0.19567690789699554, "y": 0.2887912094593048}, {"x": 0.18430034816265106, "y": 0.2887912094593048}], "text": "8:\n"}
{"page": 19, "bbox": [{"x": 0.276450514793396, "y": 0.27912089228630066}, {"x": 0.3412969410419464, "y": 0.27868130803108215}, {"x": 0.3412969410419464, "y": 0.2914285659790039}, {"x": 0.276450514793396, "y": 0.2918681204319}], "text": "C predicts\n"}
{"page": 19, "bbox": [{"x": 0.6353811025619507, "y": 0.27824175357818604}, {"x": 0.8248009085655212, "y": 0.28131869435310364}, {"x": 0.8242321014404297, "y": 0.3081318736076355}, {"x": 0.6348122954368591, "y": 0.3050549328327179}], "text": "▷ Predict supports of outputs\nT only)\n"}
{"page": 19, "bbox": [{"x": 0.18145619332790375, "y": 0.23868131637573242}, {"x": 0.19681456685066223, "y": 0.23912088572978973}, {"x": 0.19226393103599548, "y": 0.34813186526298523}, {"x": 0.1769055724143982, "y": 0.3476923108100891}], "text": "16820=2\n"}
{"page": 19, "bbox": [{"x": 0.585324227809906, "y": 0.29582417011260986}, {"x": 0.7514220476150513, "y": 0.2971428632736206}, {"x": 0.7514220476150513, "y": 0.3081318736076355}, {"x": 0.585324227809906, "y": 0.30681318044662476}], "text": "▷ Predict overall utility (t\n"}
{"page": 19, "bbox": [{"x": 0.18373151123523712, "y": 0.29846152663230896}, {"x": 0.19567690789699554, "y": 0.29846152663230896}, {"x": 0.19567690789699554, "y": 0.3059340715408325}, {"x": 0.18373151123523712, "y": 0.3059340715408325}], "text": "9:\n"}
{"page": 19, "bbox": [{"x": 0.25142207741737366, "y": 0.2967033088207245}, {"x": 0.436860054731369, "y": 0.29538461565971375}, {"x": 0.436860054731369, "y": 0.3076923191547394}, {"x": 0.25142207741737366, "y": 0.30901098251342773}], "text": "C predicts ISUSE for each d\n"}
{"page": 19, "bbox": [{"x": 0.755972683429718, "y": 0.30109891295433044}, {"x": 0.7662116289138794, "y": 0.30109891295433044}, {"x": 0.7662116289138794, "y": 0.3050549328327179}, {"x": 0.755972683429718, "y": 0.3050549328327179}], "text": "=\n"}
{"page": 19, "bbox": [{"x": 0.25255972146987915, "y": 0.3107692301273346}, {"x": 0.3139931857585907, "y": 0.30989012122154236}, {"x": 0.31456199288368225, "y": 0.32131868600845337}, {"x": 0.2531285583972931, "y": 0.3221977949142456}], "text": "Sample d\n"}
{"page": 19, "bbox": [{"x": 0.17804323136806488, "y": 0.32615384459495544}, {"x": 0.1951080709695816, "y": 0.32615384459495544}, {"x": 0.1951080709695816, "y": 0.3331868052482605}, {"x": 0.17804323136806488, "y": 0.3331868052482605}], "text": "11:\n"}
{"page": 19, "bbox": [{"x": 0.2286689430475235, "y": 0.3248351514339447}, {"x": 0.45790672302246094, "y": 0.32351648807525635}, {"x": 0.45790672302246094, "y": 0.3358241617679596}, {"x": 0.2286689430475235, "y": 0.33714285492897034}], "text": "else if Retrieve is not predicted then\n"}
{"page": 19, "bbox": [{"x": 0.2519908845424652, "y": 0.3380219638347626}, {"x": 0.43629124760627747, "y": 0.3389011025428772}, {"x": 0.43629124760627747, "y": 0.3503296673297882}, {"x": 0.2519908845424652, "y": 0.34945055842399597}], "text": "C predicts ISUSE given x, y\n"}
{"page": 19, "bbox": [{"x": 0.22810012102127075, "y": 0.34945055842399597}, {"x": 0.45961320400238037, "y": 0.35164836049079895}, {"x": 0.45961320400238037, "y": 0.3643956184387207}, {"x": 0.22810012102127075, "y": 0.3621978163719177}], "text": "Add augmented (x, y, d, r) to Dgen\n"}
{"page": 19, "bbox": [{"x": 0.1769055724143982, "y": 0.39076924324035645}, {"x": 0.7189988493919373, "y": 0.39076924324035645}, {"x": 0.7189988493919373, "y": 0.4026373624801636}, {"x": 0.1769055724143982, "y": 0.4026373624801636}], "text": "Training examples. Table 4 show several training examples used for M training.\n"}
{"page": 19, "bbox": [{"x": 0.17633675038814545, "y": 0.4224175810813904}, {"x": 0.3759954571723938, "y": 0.4228571355342865}, {"x": 0.3759954571723938, "y": 0.4316483438014984}, {"x": 0.17633675038814545, "y": 0.4312087893486023}], "text": "A.3 SELF-RAG INFERENCE\n"}
{"page": 19, "bbox": [{"x": 0.17633675038814545, "y": 0.4479120969772339}, {"x": 0.8248009085655212, "y": 0.4479120969772339}, {"x": 0.8248009085655212, "y": 0.4729670286178589}, {"x": 0.17633675038814545, "y": 0.4729670286178589}], "text": "Details of beam-search score calculations. We first compute scores for each critique type by\ntaking the normalized probabilities of desirable tokens. For ISREL], we compute the score as follows:\n"}
{"page": 19, "bbox": [{"x": 0.5346985459327698, "y": 0.47824177145957947}, {"x": 0.6166098117828369, "y": 0.4786813259124756}, {"x": 0.6166098117828369, "y": 0.4896703362464905}, {"x": 0.5346985459327698, "y": 0.48923078179359436}], "text": "RELEVANT)\n"}
{"page": 19, "bbox": [{"x": 0.5176336765289307, "y": 0.4826373755931854}, {"x": 0.5273037552833557, "y": 0.4826373755931854}, {"x": 0.5273037552833557, "y": 0.48615384101867676}, {"x": 0.5176336765289307, "y": 0.48615384101867676}], "text": "=\n"}
{"page": 19, "bbox": [{"x": 0.29920363426208496, "y": 0.4901098906993866}, {"x": 0.329351544380188, "y": 0.4896703362464905}, {"x": 0.329351544380188, "y": 0.4958241879940033}, {"x": 0.29920363426208496, "y": 0.4962637424468994}], "text": "ISREL\n"}
{"page": 19, "bbox": [{"x": 0.36348122358322144, "y": 0.47824177145957947}, {"x": 0.5904436707496643, "y": 0.4791208803653717}, {"x": 0.5904436707496643, "y": 0.508571445941925}, {"x": 0.36348122358322144, "y": 0.5076923370361328}], "text": "P(ISREL\nPISREL= RELEVANT)p([ISREL\n"}
{"page": 19, "bbox": [{"x": 0.6166098117828369, "y": 0.49494504928588867}, {"x": 0.7121729254722595, "y": 0.4953846037387848}, {"x": 0.7121729254722595, "y": 0.5063736438751221}, {"x": 0.6166098117828369, "y": 0.5059340596199036}], "text": "IRRELEVANT)\n"}
{"page": 19, "bbox": [{"x": 0.17633675038814545, "y": 0.5120879411697388}, {"x": 0.2428896427154541, "y": 0.5138461589813232}, {"x": 0.24232082068920135, "y": 0.5221977829933167}, {"x": 0.1757679134607315, "y": 0.5204395651817322}], "text": "For ISSUP\n"}
{"page": 19, "bbox": [{"x": 0.2508532404899597, "y": 0.5134065747261047}, {"x": 0.4738338887691498, "y": 0.510769248008728}, {"x": 0.4738338887691498, "y": 0.5213186740875244}, {"x": 0.2508532404899597, "y": 0.5239560604095459}], "text": ", we compute the score as follows:\n"}
{"page": 19, "bbox": [{"x": 0.628555178642273, "y": 0.5287911891937256}, {"x": 0.711604118347168, "y": 0.5279120802879333}, {"x": 0.711604118347168, "y": 0.5389010906219482}, {"x": 0.628555178642273, "y": 0.5397801995277405}], "text": "PARTIALLY)\n"}
{"page": 19, "bbox": [{"x": 0.4379977285861969, "y": 0.5283516645431519}, {"x": 0.489761084318161, "y": 0.5287911891937256}, {"x": 0.489761084318161, "y": 0.5397801995277405}, {"x": 0.4379977285861969, "y": 0.5393406748771667}], "text": "FULLY)\n"}
{"page": 19, "bbox": [{"x": 0.5534698367118835, "y": 0.5318681597709656}, {"x": 0.6018202304840088, "y": 0.5305494666099548}, {"x": 0.6023890972137451, "y": 0.538021981716156}, {"x": 0.5540387034416199, "y": 0.5393406748771667}], "text": "PISSUP\n"}
{"page": 19, "bbox": [{"x": 0.36234357953071594, "y": 0.5318681597709656}, {"x": 0.4112628102302551, "y": 0.5305494666099548}, {"x": 0.4118316173553467, "y": 0.5384615659713745}, {"x": 0.3629123866558075, "y": 0.5397801995277405}], "text": "PISSUP\n"}
{"page": 19, "bbox": [{"x": 0.4971558451652527, "y": 0.5384615659713745}, {"x": 0.5466439127922058, "y": 0.5384615659713745}, {"x": 0.5466439127922058, "y": 0.5468131899833679}, {"x": 0.4971558451652527, "y": 0.5468131899833679}], "text": "+ 0.5 x\n"}
{"page": 19, "bbox": [{"x": 0.2815699577331543, "y": 0.5406593680381775}, {"x": 0.3282138705253601, "y": 0.5406593680381775}, {"x": 0.3282138705253601, "y": 0.5463736057281494}, {"x": 0.2815699577331543, "y": 0.5463736057281494}], "text": "SISREL\n"}
{"page": 19, "bbox": [{"x": 0.42207053303718567, "y": 0.5463736057281494}, {"x": 0.43230944871902466, "y": 0.5463736057281494}, {"x": 0.43230944871902466, "y": 0.5547252893447876}, {"x": 0.42207053303718567, "y": 0.5547252893447876}], "text": "S\n"}
{"page": 19, "bbox": [{"x": 0.628555178642273, "y": 0.5463736057281494}, {"x": 0.6393629312515259, "y": 0.5463736057281494}, {"x": 0.6393629312515259, "y": 0.5547252893447876}, {"x": 0.628555178642273, "y": 0.5547252893447876}], "text": "S\n"}
{"page": 19, "bbox": [{"x": 0.467576801776886, "y": 0.5639560222625732}, {"x": 0.47610920667648315, "y": 0.5639560222625732}, {"x": 0.47610920667648315, "y": 0.5657142996788025}, {"x": 0.467576801776886, "y": 0.5657142996788025}], "text": "-\n"}
{"page": 19, "bbox": [{"x": 0.2349260449409485, "y": 0.5635164976119995}, {"x": 0.2440273016691208, "y": 0.5635164976119995}, {"x": 0.2440273016691208, "y": 0.5670329928398132}, {"x": 0.2349260449409485, "y": 0.5670329928398132}], "text": "=\n"}
{"page": 19, "bbox": [{"x": 0.1757679134607315, "y": 0.5582417845726013}, {"x": 0.8236632347106934, "y": 0.5578022003173828}, {"x": 0.8236632347106934, "y": 0.6013186573982239}, {"x": 0.1757679134607315, "y": 0.6017582416534424}], "text": "where S Σte {FULLY, PARTIALLY, NO} P(ISSUP t). For ISUSE where we have a five-scale score, we\ncompute the weighted sum of the scores. We assigns weighted scores of w {-1,-0.5, 0, 0.5, 1}\nto the tokens ISUSE] ={1, 2, 3, 4, 5}, and compute the final scores as follows:\n"}
{"page": 19, "bbox": [{"x": 0.6695107817649841, "y": 0.5806593298912048}, {"x": 0.6786120533943176, "y": 0.5806593298912048}, {"x": 0.6786120533943176, "y": 0.5832967162132263}, {"x": 0.6695107817649841, "y": 0.5832967162132263}], "text": "-\n"}
{"page": 19, "bbox": [{"x": 0.47895336151123047, "y": 0.6074725389480591}, {"x": 0.48634812235832214, "y": 0.6074725389480591}, {"x": 0.48634812235832214, "y": 0.6131868362426758}, {"x": 0.47895336151123047, "y": 0.6131868362426758}], "text": "5\n"}
{"page": 19, "bbox": [{"x": 0.5910125374794006, "y": 0.6109890341758728}, {"x": 0.6001137495040894, "y": 0.6109890341758728}, {"x": 0.6001137495040894, "y": 0.6237362623214722}, {"x": 0.5910125374794006, "y": 0.6237362623214722}], "text": "i)\n"}
{"page": 19, "bbox": [{"x": 0.5147895216941833, "y": 0.614065945148468}, {"x": 0.5836177468299866, "y": 0.6136263608932495}, {"x": 0.5836177468299866, "y": 0.6373626589775085}, {"x": 0.5147895216941833, "y": 0.6378021836280823}], "text": "PISUSE =\nS\n"}
{"page": 19, "bbox": [{"x": 0.45392492413520813, "y": 0.6241758465766907}, {"x": 0.46302616596221924, "y": 0.6241758465766907}, {"x": 0.46302616596221924, "y": 0.6276922821998596}, {"x": 0.45392492413520813, "y": 0.6276922821998596}], "text": "=\n"}
{"page": 19, "bbox": [{"x": 0.3913538157939911, "y": 0.6232966780662537}, {"x": 0.436860054731369, "y": 0.6228571534156799}, {"x": 0.436860054731369, "y": 0.6290109753608704}, {"x": 0.3913538157939911, "y": 0.6294505596160889}], "text": "SISUSE\n"}
{"page": 19, "bbox": [{"x": 0.49658703804016113, "y": 0.6241758465766907}, {"x": 0.5136518478393555, "y": 0.6241758465766907}, {"x": 0.5136518478393555, "y": 0.6307692527770996}, {"x": 0.49658703804016113, "y": 0.6307692527770996}], "text": "Wi\n"}
{"page": 19, "bbox": [{"x": 0.4806598424911499, "y": 0.6382417678833008}, {"x": 0.48521047830581665, "y": 0.6382417678833008}, {"x": 0.48521047830581665, "y": 0.643516480922699}, {"x": 0.4806598424911499, "y": 0.643516480922699}], "text": "i\n"}
{"page": 19, "bbox": [{"x": 0.1757679134607315, "y": 0.6501098871231079}, {"x": 0.22980660200119019, "y": 0.6501098871231079}, {"x": 0.22980660200119019, "y": 0.6593406796455383}, {"x": 0.1757679134607315, "y": 0.6593406796455383}], "text": "where S\n"}
{"page": 19, "bbox": [{"x": 0.2349260449409485, "y": 0.6545054912567139}, {"x": 0.2440273016691208, "y": 0.6545054912567139}, {"x": 0.2440273016691208, "y": 0.6575824022293091}, {"x": 0.2349260449409485, "y": 0.6575824022293091}], "text": "=\n"}
{"page": 19, "bbox": [{"x": 0.25142207741737366, "y": 0.6501098871231079}, {"x": 0.436860054731369, "y": 0.6487911939620972}, {"x": 0.436860054731369, "y": 0.6637362837791443}, {"x": 0.25142207741737366, "y": 0.6650549173355103}], "text": "Σte{1,2,3,4,5} P(ISUSE | = t).\n"}
{"page": 19, "bbox": [{"x": 0.1757679134607315, "y": 0.6813187003135681}, {"x": 0.8230944275856018, "y": 0.6795604228973389}, {"x": 0.8230944275856018, "y": 0.7041758298873901}, {"x": 0.1757679134607315, "y": 0.7059340476989746}], "text": "Details of adaptive retrieval. For retrieval based on soft constraints, we trigger retrieval if the\nfollowing condition is satisfied:\n"}
{"page": 19, "bbox": [{"x": 0.6234357357025146, "y": 0.719560444355011}, {"x": 0.6496018171310425, "y": 0.7191208600997925}, {"x": 0.6496018171310425, "y": 0.7292307615280151}, {"x": 0.6234357357025146, "y": 0.7296703457832336}], "text": "> d.\n"}
{"page": 19, "bbox": [{"x": 0.42320817708969116, "y": 0.7112088203430176}, {"x": 0.5608646273612976, "y": 0.7107692360877991}, {"x": 0.5608646273612976, "y": 0.7410988807678223}, {"x": 0.42320817708969116, "y": 0.7415384650230408}], "text": "PRetrieve YES)\nYES) + P(p(Retrieve\n"}
{"page": 19, "bbox": [{"x": 0.5870307087898254, "y": 0.7283516526222229}, {"x": 0.6149032711982727, "y": 0.7283516526222229}, {"x": 0.6149032711982727, "y": 0.74021977186203}, {"x": 0.5870307087898254, "y": 0.74021977186203}], "text": "No)\n"}
{"page": 19, "bbox": [{"x": 0.34926050901412964, "y": 0.7314285635948181}, {"x": 0.4067121744155884, "y": 0.7309890389442444}, {"x": 0.4067121744155884, "y": 0.7393406629562378}, {"x": 0.34926050901412964, "y": 0.7397802472114563}], "text": "PRetrieve\n"}
{"page": 19, "bbox": [{"x": 0.1769055724143982, "y": 0.7604395747184753}, {"x": 0.18828213214874268, "y": 0.7604395747184753}, {"x": 0.18828213214874268, "y": 0.769670307636261}, {"x": 0.1769055724143982, "y": 0.769670307636261}], "text": "B\n"}
{"page": 19, "bbox": [{"x": 0.2104664444923401, "y": 0.7604395747184753}, {"x": 0.4197952151298523, "y": 0.7604395747184753}, {"x": 0.4197952151298523, "y": 0.770549476146698}, {"x": 0.2104664444923401, "y": 0.770549476146698}], "text": "EXPERIMENTAL DETAILS\n"}
{"page": 19, "bbox": [{"x": 0.1769055724143982, "y": 0.7894505262374878}, {"x": 0.1990898698568344, "y": 0.7898901104927063}, {"x": 0.19852104783058167, "y": 0.7982417345046997}, {"x": 0.17633675038814545, "y": 0.797802209854126}], "text": "B.1\n"}
{"page": 19, "bbox": [{"x": 0.2184300273656845, "y": 0.7903296947479248}, {"x": 0.4237770140171051, "y": 0.7903296947479248}, {"x": 0.4237770140171051, "y": 0.7991209030151367}, {"x": 0.2184300273656845, "y": 0.7991209030151367}], "text": "MORE DETAILS OF TRAINING\n"}
{"page": 19, "bbox": [{"x": 0.17519909143447876, "y": 0.8158241510391235}, {"x": 0.8242321014404297, "y": 0.8153846263885498}, {"x": 0.8242321014404297, "y": 0.9235165119171143}, {"x": 0.17519909143447876, "y": 0.923956036567688}], "text": "More details of training and computations. We use 4 Nvidia A100 with 80GB memory to train\nour models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5\nwith 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048\nfor the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed\nstage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision\nBfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more\nefficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB\nmemory.\n"}
{"page": 19, "bbox": [{"x": 0.4920364022254944, "y": 0.9507692456245422}, {"x": 0.5051194429397583, "y": 0.9507692456245422}, {"x": 0.5051194429397583, "y": 0.9586813449859619}, {"x": 0.4920364022254944, "y": 0.9586813449859619}], "text": "19\n"}
{"page": 20, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 20, "bbox": [{"x": 0.1769055724143982, "y": 0.10549450665712357}, {"x": 0.4510807693004608, "y": 0.10725274682044983}, {"x": 0.4510807693004608, "y": 0.11824175715446472}, {"x": 0.1769055724143982, "y": 0.11648351699113846}], "text": "B.2 MORE DETAILS OF EVALUATIONS\n"}
{"page": 20, "bbox": [{"x": 0.1757679134607315, "y": 0.13318681716918945}, {"x": 0.8259385824203491, "y": 0.13318681716918945}, {"x": 0.8259385824203491, "y": 0.3380219638347626}, {"x": 0.1757679134607315, "y": 0.3380219638347626}], "text": "Retrieval setup details. By default, we use Contriever-MS MARCO to retrieve the top five\ndocuments from Wikipedia, and use official Wikipedia embeddings based on 2018 English Wikipedia.\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\nadded to Wikipedia. Therefore, for PopQA, we used the December 2020 preprocessed Wikipedia\ncorpus provided by Izacard et al. (2022b) and generated document embeddings. The issues of\nperformance variance from different Wikipedia dumps have been reported by prior work (Asai et al.,\n2020; Izacard et al., 2022b). Yet, we observe limited effectiveness of such off-the-shelf retrieval\nmodels trained primarily on knowledge-intensive tasks for open-ended generation (e.g., instruction\nfollowing). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al.,\n2023b) or joint training of retrieval and LM components (Lin et al., 2023), while we leave exploring\nthe effectivess of such appraoches for future work. For bio generation and open-domain QA tasks,\nwe additionally retrieve five documents using Google Programmable Search and search documents\nfrom English Wikipedia. As this API only provides snippets, we retrieve Wikipedia introductory\nparagraphs for the corresponding entities.\n"}
{"page": 20, "bbox": [{"x": 0.1757679134607315, "y": 0.3556044101715088}, {"x": 0.8259385824203491, "y": 0.3556044101715088}, {"x": 0.8259385824203491, "y": 0.6017582416534424}, {"x": 0.1757679134607315, "y": 0.6017582416534424}], "text": "Detailed experimental settings for individual datasets. For OpenQA datasets, we set the max-\nimum new token number to 100 tokens. For closed-set tasks (PubHealth and ARC-C), we set the\nmaximum new token length to 50 for all baselines. For SELF-RAG inference on PubHealth and\nARC-C, instead of determining the output with the highest score 4 as in other tasks, we aggregate the\nscores for each option and select the answer option with the highest score. We found in zero-shot\nsettings of fact checking, some LLMs can generate capitalized class labels (e.g., True) while our\ngold labels are lower-cased. Therefore, across different LMs, for fact checking, we lowercase the\npredictions. In multiple choice tasks, we found some models generate answers in slightly different\nways (e.g., (A) instead of A). We slightly modify instructions for each LLM to avoid such format\nviolations, and further conduct string matching between each candidate and model predictions if\nformat violations still remain. After that processing, in closed set tasks, model predictions match\none of the gold classes in almost all cases. For ALCE, we found that Llama2-chat tend to generate\nsignificantly lower outputs than other models (e.g., on average, their output is nearly 100 token, while\nChatGPT generates 40 tokens on average), resulting in inflated str-em scores. We limit the maximum\ngeneration length to 100 tokens for all baselines to avoid this issue, rather than the original 300\ntokens in the ALCE paper. Consequently, all of the baseline output length is within 30-60 tokens.\nFor FactScore, we set the maximum new token length to 500 for baselines and 200 for SELF-RAG at\neach segment level.\n"}
{"page": 20, "bbox": [{"x": 0.17633675038814545, "y": 0.6202197670936584}, {"x": 0.8242321014404297, "y": 0.6193406581878662}, {"x": 0.8242321014404297, "y": 0.6452746987342834}, {"x": 0.17633675038814545, "y": 0.6461538672447205}], "text": "Task-specific instructions. Table 5 shows the list of the instructions used during evaluations. For\nOpen-domain QA, we do not provide explicit instructions.\n"}
{"page": 20, "bbox": [{"x": 0.17633675038814545, "y": 0.668571412563324}, {"x": 0.28498294949531555, "y": 0.668571412563324}, {"x": 0.28498294949531555, "y": 0.6782417297363281}, {"x": 0.17633675038814545, "y": 0.6782417297363281}], "text": "C RESULTS\n"}
{"page": 20, "bbox": [{"x": 0.17633675038814545, "y": 0.6989011168479919}, {"x": 0.2883959114551544, "y": 0.6997802257537842}, {"x": 0.2883959114551544, "y": 0.7090110182762146}, {"x": 0.17633675038814545, "y": 0.7081318497657776}], "text": "C.1 ANALYSIS\n"}
{"page": 20, "bbox": [{"x": 0.17519909143447876, "y": 0.7226373553276062}, {"x": 0.8253697156906128, "y": 0.7226373553276062}, {"x": 0.8253697156906128, "y": 0.923956036567688}, {"x": 0.17519909143447876, "y": 0.923956036567688}], "text": "Reliance on parametric- and non-parametric memories. We conduct analysis on how frequently\nmodel answers come from retrieved passages (non-parametric memories) or their own parametric\nmemories. On two open-domain QA datasets, TriviaQA and PopQA, we conduct the following\nanalysis: 1) sample query models successfully answer correctly, 2) for each query in this group,\ncheck whether the matched ground-truth answer is a sub-string of the retrieved passage or not. We\nevaluate SELF-RAG 7B, Alpaca 7B, Alpaca 13B, and Llama2-Chat-13B. We found that SELF-RAG\nsignificantly less frequently generates answers that are not included in the provided evidence; in\nparticular, in Alpaca 30B, 20% of the correct predictions are not included in the provided passages,\nfollowed by Llama2-chat 13B (18%) and Alpaca (15%), while it is only 2% in SELF-RAG. When\nretrieved passages are not relevant, SELF-RAG generates [ISREL] =Irrelevant, indicating that the\nfollowing answers may not be factually grounded, while those instruction-tuned models continue to\ngenerate plausible answers.\n8 https://github.com/facebookresearch/atlas\nhttps://programmablesearchengine.google.com/about/\n"}
{"page": 20, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5079635977745056, "y": 0.9507692456245422}, {"x": 0.5079635977745056, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "20\n"}
{"page": 20, "bbox": [{"x": 0.5062571167945862, "y": 0.944615364074707}, {"x": 0.5056883096694946, "y": 0.9692307710647583}, {"x": 0.49317407608032227, "y": 0.9692307710647583}, {"x": 0.4937428832054138, "y": 0.944615364074707}], "text": "20\n"}
{"page": 21, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 21, "bbox": [{"x": 0.17633675038814545, "y": 0.10769230872392654}, {"x": 0.2002275288105011, "y": 0.10769230872392654}, {"x": 0.2002275288105011, "y": 0.11604395508766174}, {"x": 0.17633675038814545, "y": 0.11604395508766174}], "text": "C.2\n"}
{"page": 21, "bbox": [{"x": 0.21899886429309845, "y": 0.10769230872392654}, {"x": 0.44880545139312744, "y": 0.10769230872392654}, {"x": 0.44880545139312744, "y": 0.11648351699113846}, {"x": 0.21899886429309845, "y": 0.11648351699113846}], "text": "HUMAN EVALUATION EXAMPLES\n"}
{"page": 21, "bbox": [{"x": 0.7087599635124207, "y": 0.134505495429039}, {"x": 0.8191125988960266, "y": 0.13230769336223602}, {"x": 0.8196814656257629, "y": 0.14197802543640137}, {"x": 0.7093287706375122, "y": 0.14417582750320435}], "text": "ISREL and ISSUP\n"}
{"page": 21, "bbox": [{"x": 0.1757679134607315, "y": 0.13318681716918945}, {"x": 0.6939704418182373, "y": 0.13318681716918945}, {"x": 0.6939704418182373, "y": 0.15516483783721924}, {"x": 0.1757679134607315, "y": 0.15516483783721924}], "text": "Table 6 shows examples with human evaluations on S&P and correctness of\nreflection tokens.\n"}
{"page": 21, "bbox": [{"x": 0.17633675038814545, "y": 0.17890110611915588}, {"x": 0.39362913370132446, "y": 0.17890110611915588}, {"x": 0.39362913370132446, "y": 0.18901099264621735}, {"x": 0.17633675038814545, "y": 0.18901099264621735}], "text": "C.3 QUALITative ExamplES\n"}
{"page": 21, "bbox": [{"x": 0.1757679134607315, "y": 0.2043956071138382}, {"x": 0.8259385824203491, "y": 0.2043956071138382}, {"x": 0.8259385824203491, "y": 0.32659339904785156}, {"x": 0.1757679134607315, "y": 0.32659339904785156}], "text": "Table 7 shows several examples predicted by our SELF-RAG (13B). The first example is the model\noutput to an ASQA question. The first reference states that Emperor Constantine made Sunday a\nday of rest from labor, and further the second citation supports the fact that the official adoption\nof Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts\nContradictory to the first output as the output says the person has served as the CEO since 2010,\nwhile the passage says he stepped down as CEO in 2015. Indicating those factual contradictions\nas reflection tokens enables to enforcement of hard control and also verification of model outputs\neasily. In the third example, while the generation is mostly correct, SELF-RAG predicts Partially\nSupport to the statement listing the name of the songs, as they were not explicitly mentioned.\n"}
{"page": 21, "bbox": [{"x": 0.17519909143447876, "y": 0.3503296673297882}, {"x": 0.8253697156906128, "y": 0.3503296673297882}, {"x": 0.8253697156906128, "y": 0.4896703362464905}, {"x": 0.17519909143447876, "y": 0.4896703362464905}], "text": "Ꭰ FULL LIST OF INSTRUCTIONS AND DEMONSTRATIONS FOR GPT-4\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens.\nTable 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows\nthe instruction and demonstrations used to collect the three-way output tokens for [Retrieve given\ninstruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration\nand test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations\nused to collect the three-way output tokens for ISREL]. Table 11 shows an instruction and demon-\nstrations used to collect the three-way output tokens for [ISREL]. Table 12 shows an instruction and\ndemonstrations used to collect the five-way output tokens for ISUSE\n"}
{"page": 21, "bbox": [{"x": 0.4908987581729889, "y": 0.951208770275116}, {"x": 0.5073947906494141, "y": 0.951208770275116}, {"x": 0.5073947906494141, "y": 0.9586813449859619}, {"x": 0.4908987581729889, "y": 0.9586813449859619}], "text": "21\n"}
{"page": 22, "bbox": [{"x": 0.17633675038814545, "y": 0.03560439497232437}, {"x": 0.2326507419347763, "y": 0.036043956875801086}, {"x": 0.2326507419347763, "y": 0.04703296720981598}, {"x": 0.17633675038814545, "y": 0.046593405306339264}], "text": "Preprint.\n"}
{"page": 22, "bbox": [{"x": 0.5904436707496643, "y": 0.13010989129543304}, {"x": 0.5984072685241699, "y": 0.13010989129543304}, {"x": 0.5984072685241699, "y": 0.1318681389093399}, {"x": 0.5904436707496643, "y": 0.1318681389093399}], "text": "-\n"}
{"page": 22, "bbox": [{"x": 0.1769055724143982, "y": 0.11076922714710236}, {"x": 0.8304892182350159, "y": 0.11076922714710236}, {"x": 0.8304892182350159, "y": 0.9125275015830994}, {"x": 0.1769055724143982, "y": 0.9125275015830994}], "text": "Input Identify the two types of computer memory. (from gpt4_alpaca)\nOutput [Retrieve =Yes]<p>Computer memory Computer memory In computing, memory refers to the\ncomputer hardware integrated circuits that store information for immediate use in a computer; it is synonymous\nwith the term \"primary storage\". Computer memory operates at a high speed, for example random-access memory\n(RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If\nneeded, contents of the computer memory can be transferred to secondary storage; a very common way of doing\nthis is through a memory management technique called \"virtual memory\". An archaic synonym for memory is\nstore. The term “memory”, meaning \"primary storage\" or \"main memory”, is<p>[| ISREL] =Relevant ]Primary\nmemory, also known as main memory, is the memory that the computer can directly access and manipulate.\n[ISSUP =Fully Supported] [[Retrieve|=Yes]<p>Computer memory-often associated with addressable\nsemiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as\nprimary storage but also other purposes in computers and other digital electronic devices. There are two main\nkinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory\n(used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware\nsuch as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access\nmemory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM)\nthat is fast but energy-consuming, offering lower memory<p>[| ISREL]=Relevant]Primary memory can be\naccessed quickly, but it is volatile, meaning that the data is lost if the power is turned off. [ ISSUP] =Partially\nSupported]...[ ISUSE :5]\nInput Give an example of a situation where you have to delegate tasks (from gpt4_alpaca)\nOutput [Retrieve =No]The manager realizes that there is an extensive list of tasks that need to be completed in\norder to make the event a success. [[Retrieve|=Yes]<p>Delegation-Delegation is the assignment of any responsi-\nbility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It\nis one of the core concepts of management leadership. However, the person who delegated the work remains\naccountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it\nis a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly\ndone, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too\nmuch input,<p>[ | ISREL] =Relevant] This includes tasks such as creating a guest list, designing and sending in-\nvitations, booking a venue, coordinating with vendors, planning the events program, and monitoring the budget.[\n|ISSUP =Partially Supported][[Retrieve =No] Since it is not possible for the manager to handle everything\nalone, they must delegate tasks to their team members. [ Retrieve |=No]The manager would then assign each\nteam member specific responsibilities based on their individual skillsets and strengths.[ Retrieve =Yes]<p>Joint\napplication design – Subject Matter Experts: These are the business users, the IS professionals, and the outside\nexperts that will be needed for a successful workshop. This group is the backbone of the meeting<p>[ [ISREL\n=Irrelevant] For instance, the teamś graphic designer might be tasked with designing the invitations, while\nthe event coordinator would be in charge of planning the program and working with vendors... [| Retrieve |=No]By\ndelegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also\nempowering their team members to take on responsibilities and contribute to the success of the event.[ ISUSE\n=5]\nInput Synthesize a poem based on this prompt: “The sun is setting\" (from stanford_alpaca)\nOutput [Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\n[Retrieve =NO] The stars begin to twinkle bright the trees, scattered throughout the night [|Retrieve|=No] They\nflicker like a silent dance, A peaceful moment in its trance. [Retrieve =NO] They flicker like a silent dance, A\npeaceful moment in its trance. [Retrieve =No] A chorus of crickets sing a lullaby, To every creature that roams\nthe sky [ISUSE =5]\nInput when did walking dead season 7 come out (from nq)\nOutput [Retrieve |=Yes]<p>The Walking Dead (season 7)-The Walking Dead (season 7) The seventh season of\n\"The Walking Dead\", an American post-apocalyptic horror television series on AMC, premiered on October 23,\n2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont,\nthe series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard.\n...<p>[ISREL=Relevant] October 23, 2016[ | ISSUP] =Fully Supported][ [ISUSE =5]\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging\nresponse: Hello, I love German shepherd dogs and you? - My favorite dog, they are officially known as German\nShepherd Dogs? - Yes, I have a German shepherd in my house and she is pregnant (from wow)\nOutput hey also are easily trained, obedient, and intelligent.[| Retrieve|=Yes]<p>German Shepherd – a simple\ndifference of opinion, it is the fundamental issue of the breeds essential conformation and movement. ... They\nare known for being easy to train and good for performing tasks and following instructions. They are especially\nwell known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and\nholding suspects. <p>[[ISREL] =Relevant ] Because of this, they are often used for disability assistance and\nfor police work. [ISSUP] =Partially Supported][ | ISUSE |=2]\n"}
{"page": 22, "bbox": [{"x": 0.3759954571723938, "y": 0.9296703338623047}, {"x": 0.6240045428276062, "y": 0.9314285516738892}, {"x": 0.6240045428276062, "y": 0.942417562007904}, {"x": 0.3759954571723938, "y": 0.9406593441963196}], "text": "Table 4: List of the training examples.\n"}
{"page": 22, "bbox": [{"x": 0.4908987581729889, "y": 0.9507692456245422}, {"x": 0.5085324048995972, "y": 0.9507692456245422}, {"x": 0.5085324048995972, "y": 0.9586813449859619}, {"x": 0.4908987581729889, "y": 0.9586813449859619}], "text": "22\n"}
{"page": 23, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 23, "bbox": [{"x": 0.28725823760032654, "y": 0.3973626494407654}, {"x": 0.3566552996635437, "y": 0.3973626494407654}, {"x": 0.3566552996635437, "y": 0.40483516454696655}, {"x": 0.28725823760032654, "y": 0.40483516454696655}], "text": "Instruction\n"}
{"page": 23, "bbox": [{"x": 0.17974971234798431, "y": 0.3969230651855469}, {"x": 0.22639362514019012, "y": 0.3969230651855469}, {"x": 0.22639362514019012, "y": 0.4263736307621002}, {"x": 0.17974971234798431, "y": 0.4263736307621002}], "text": "Dataset\nARC-C\n"}
{"page": 23, "bbox": [{"x": 0.286689430475235, "y": 0.41846153140068054}, {"x": 0.8441410660743713, "y": 0.4180219769477844}, {"x": 0.8441410660743713, "y": 0.46945056319236755}, {"x": 0.286689430475235, "y": 0.46989011764526367}], "text": "Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer\nwith the capitalized alphabet only, without adding any extra phrase or period.\nIs the following statement correct or not? Say true if it's correct; otherwise, say false. Don't\ncapitalize or add periods, just say \"true\" or \"false\".\n"}
{"page": 23, "bbox": [{"x": 0.17918089032173157, "y": 0.44659340381622314}, {"x": 0.24061433970928192, "y": 0.44659340381622314}, {"x": 0.24061433970928192, "y": 0.4540659487247467}, {"x": 0.17918089032173157, "y": 0.4540659487247467}], "text": "PubHealth\n"}
{"page": 23, "bbox": [{"x": 0.17974971234798431, "y": 0.47428572177886963}, {"x": 0.2696245610713959, "y": 0.47428572177886963}, {"x": 0.2696245610713959, "y": 0.4813186824321747}, {"x": 0.17974971234798431, "y": 0.4813186824321747}], "text": "Bio Generation\n"}
{"page": 23, "bbox": [{"x": 0.2878270745277405, "y": 0.473406583070755}, {"x": 0.5159271955490112, "y": 0.47428572177886963}, {"x": 0.5159271955490112, "y": 0.48351648449897766}, {"x": 0.2878270745277405, "y": 0.4826373755931854}], "text": "Tell me a bio about [Person Name]\n"}
{"page": 23, "bbox": [{"x": 0.17918089032173157, "y": 0.48835164308547974}, {"x": 0.8447098731994629, "y": 0.48835164308547974}, {"x": 0.8447098731994629, "y": 0.5595604181289673}, {"x": 0.17918089032173157, "y": 0.5595604181289673}], "text": "ASQA (baseline) Instruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results (some of which might be irrelevant) and cite them properly. Use\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of\nthe documents.\n"}
{"page": 23, "bbox": [{"x": 0.17974971234798431, "y": 0.5670329928398132}, {"x": 0.25767919421195984, "y": 0.567472517490387}, {"x": 0.25767919421195984, "y": 0.5767033100128174}, {"x": 0.17974971234798431, "y": 0.5762637257575989}], "text": "ASQA (ours)\n"}
{"page": 23, "bbox": [{"x": 0.286689430475235, "y": 0.5670329928398132}, {"x": 0.8430033922195435, "y": 0.5670329928398132}, {"x": 0.8430033922195435, "y": 0.5898901224136353}, {"x": 0.286689430475235, "y": 0.5898901224136353}], "text": "Answer the following question. The question may be ambiguous and have multiple correct\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\n"}
{"page": 23, "bbox": [{"x": 0.1757679134607315, "y": 0.6087912321090698}, {"x": 0.8242321014404297, "y": 0.6087912321090698}, {"x": 0.8242321014404297, "y": 0.6338461637496948}, {"x": 0.1757679134607315, "y": 0.6338461637496948}], "text": "Table 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don't\nuse any task specific instruction and simply use the original questions as input query.\n"}
{"page": 23, "bbox": [{"x": 0.4908987581729889, "y": 0.9503296613693237}, {"x": 0.5085324048995972, "y": 0.9503296613693237}, {"x": 0.5085324048995972, "y": 0.9582417607307434}, {"x": 0.4908987581729889, "y": 0.9582417607307434}], "text": "23\n"}
{"page": 23, "bbox": [{"x": 0.5056883096694946, "y": 0.945494532585144}, {"x": 0.5051194429397583, "y": 0.9635164737701416}, {"x": 0.489761084318161, "y": 0.9635164737701416}, {"x": 0.49032992124557495, "y": 0.945494532585144}], "text": "23\n"}
{"page": 24, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 24, "bbox": [{"x": 0.17918089032173157, "y": 0.148131862282753}, {"x": 0.8304892182350159, "y": 0.14769230782985687}, {"x": 0.8304892182350159, "y": 0.3380219638347626}, {"x": 0.17918089032173157, "y": 0.3384615480899811}], "text": "Input What genre is A Question and Answer Guide to Astronomy? (PopQA)\nOutput [Retrieve=Yes] <p>Reference [1]<p>[ | ISREL] =Relevant] A Question and Answer Guide to As-\ntronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ISSUP] =Fully\nSupported] | ISUSE =5]\nReference [1] A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\nof science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\nstudent, and that the answers are informative, well constructed, and thorough. The book covers information about\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\nand the Moon illusion.\n"}
{"page": 24, "bbox": [{"x": 0.17918089032173157, "y": 0.3441758155822754}, {"x": 0.4499431252479553, "y": 0.3441758155822754}, {"x": 0.4499431252479553, "y": 0.3692307770252228}, {"x": 0.17918089032173157, "y": 0.3692307770252228}], "text": "S&P Supported and Plausible.\nCorrectness of ISREL and ISSUP : Both\n"}
{"page": 24, "bbox": [{"x": 0.17861205339431763, "y": 0.35780221223831177}, {"x": 0.829351544380188, "y": 0.35736262798309326}, {"x": 0.829351544380188, "y": 0.4646153748035431}, {"x": 0.17861205339431763, "y": 0.4650549590587616}], "text": "ISREL and ISSUP are correct, as the Reference [1] provides\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\nInput Who is the author of The Lie? (PopQA)\nOutput [Retrieve|=Yes] <p>Reference [1]<p>[ [ ISREL] =Relevant] The author of The Lie is Sam Harris.[\nISSUP =Fully Supported] [ ISUSE] =5]\nReference [1] Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society by merely telling the truth in situations\nwhere others often lie.\n"}
{"page": 24, "bbox": [{"x": 0.17918089032173157, "y": 0.4707692265510559}, {"x": 0.37940841913223267, "y": 0.47164836525917053}, {"x": 0.37940841913223267, "y": 0.4821977913379669}, {"x": 0.17918089032173157, "y": 0.4813186824321747}], "text": "S&P Plausible but not supported.\n"}
{"page": 24, "bbox": [{"x": 0.17974971234798431, "y": 0.48615384101867676}, {"x": 0.8287827372550964, "y": 0.4817582368850708}, {"x": 0.8287827372550964, "y": 0.4962637424468994}, {"x": 0.17974971234798431, "y": 0.5006593465805054}], "text": "Correctness of ISREL and ISSUP Both ISREL and ISSUP are incorrect, as the Reference [1] is about a\n"}
{"page": 24, "bbox": [{"x": 0.17974971234798431, "y": 0.49978020787239075}, {"x": 0.4562002420425415, "y": 0.49978020787239075}, {"x": 0.4562002420425415, "y": 0.5081318616867065}, {"x": 0.17974971234798431, "y": 0.5081318616867065}], "text": "different book whose title is similar to The Lie.\n"}
{"page": 24, "bbox": [{"x": 0.17974971234798431, "y": 0.5213186740875244}, {"x": 0.44254836440086365, "y": 0.5213186740875244}, {"x": 0.44254836440086365, "y": 0.5314285755157471}, {"x": 0.17974971234798431, "y": 0.5314285755157471}], "text": "Input Tell me a bio about Ronaldinho (Bio)\n"}
{"page": 24, "bbox": [{"x": 0.17861205339431763, "y": 0.5367032885551453}, {"x": 0.829351544380188, "y": 0.5367032885551453}, {"x": 0.829351544380188, "y": 0.6632966995239258}, {"x": 0.17861205339431763, "y": 0.6632966995239258}], "text": "Output [Retrieve|=Yes] <p>Reference [1]<p>[ [ ISREL] =Relevant] Ronaldinho is a retired professional\nfootballer from Brazil.[ [ISSUP] =Fully Supported][ Retrieve |=Continue] He played as an attacking\nmidfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free\nkicks[Retrieve=Continue] He won two FIFA World Player of the Year awards and a Ballon d'Or.[ ISUSE =5]\nReference [1] Ronaldo de Assis Moreira (born 21 March 1980), commonly known as Ronaldinho Gaúcho or\nsimply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder,\nbut was also deployed as a winger. Considered one of the best players of his generation and regarded by many as\none of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d'Or. A\nglobal icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from\nfree-kicks, as well as his use of tricks, feints, no-look passes and overhead...\n"}
{"page": 24, "bbox": [{"x": 0.17918089032173157, "y": 0.6672527194023132}, {"x": 0.3873720169067383, "y": 0.6690109968185425}, {"x": 0.3873720169067383, "y": 0.6795604228973389}, {"x": 0.17918089032173157, "y": 0.6778022050857544}], "text": "S&P Plausible and fully supported.\n"}
{"page": 24, "bbox": [{"x": 0.17974971234798431, "y": 0.6835165023803711}, {"x": 0.6160409450531006, "y": 0.6813187003135681}, {"x": 0.6160409450531006, "y": 0.691428542137146}, {"x": 0.17974971234798431, "y": 0.693626344203949}], "text": "Correctness of ISREL and ISSUP Both ISREL and ISSUP are correct.\n"}
{"page": 24, "bbox": [{"x": 0.17974971234798431, "y": 0.7050549387931824}, {"x": 0.45904436707496643, "y": 0.7050549387931824}, {"x": 0.45904436707496643, "y": 0.715164840221405}, {"x": 0.17974971234798431, "y": 0.715164840221405}], "text": "Input Tell me a bio about Sergei Bodrov (Bio)\n"}
{"page": 24, "bbox": [{"x": 0.17861205339431763, "y": 0.7191208600997925}, {"x": 0.8299203515052795, "y": 0.719560444355011}, {"x": 0.8299203515052795, "y": 0.8536263704299927}, {"x": 0.17861205339431763, "y": 0.8531867861747742}], "text": "Output [Retrieve|=Yes] <p>Reference [1]<p>[ [ISREL] =Relevant] Sergei Vladimirovich Bodrov\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ ISSUP =Fully\nSupported] Retrieve|=Continue] He was born in Moscow, Russia. [[ Retrieve |=Continue] He is the presi-\ndent of the Russian Guild of Film Producers.[ ISUSE =5]\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\nproducer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\nS&P Plausible but only partially supported. The the Reference [1] does not explicitly mention the person is born\nin Moscow and there is no description about the Russian Guild of Film Producers.\nCorrectness of ISREL and ISSUP ISREL and ISSUP are correct as the reference document is highly relevant\nand the first sentence is fully supported.\n"}
{"page": 24, "bbox": [{"x": 0.3452787399291992, "y": 0.8712087869644165}, {"x": 0.6535836458206177, "y": 0.8712087869644165}, {"x": 0.6535836458206177, "y": 0.8817582130432129}, {"x": 0.3452787399291992, "y": 0.8817582130432129}], "text": "Table 6: Examples of human evaluation results.\n"}
{"page": 24, "bbox": [{"x": 0.49146756529808044, "y": 0.951208770275116}, {"x": 0.5079635977745056, "y": 0.951208770275116}, {"x": 0.5079635977745056, "y": 0.9582417607307434}, {"x": 0.49146756529808044, "y": 0.9582417607307434}], "text": "24\n"}
{"page": 24, "bbox": [{"x": 0.49032992124557495, "y": 0.945494532585144}, {"x": 0.5056883096694946, "y": 0.945494532585144}, {"x": 0.5056883096694946, "y": 0.9648351669311523}, {"x": 0.49032992124557495, "y": 0.9648351669311523}], "text": "24\n"}
{"page": 25, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 25, "bbox": [{"x": 0.17804323136806488, "y": 0.22857142984867096}, {"x": 0.8310580253601074, "y": 0.22857142984867096}, {"x": 0.8310580253601074, "y": 0.43296703696250916}, {"x": 0.17804323136806488, "y": 0.43296703696250916}], "text": "Input When did sunday became the seventh day of the week (asqa)\nOutput [Retrieve =Yes] <p>Reference [1]<p>[ | ISREL] =Relevant] In 321 AD, the Roman Emperor Con-\nstantine the Great made Sunday a day of rest from labor, stating: “On the venerable Day of the Sun let the\nmagistrates and people residing in cities rest, and let all workshops be closed\".[ | ISSUP|=Fully Supported]\n[Retrieve =Yes] <p>Reference [2]<p>[ |ISREL] =Relevant] The official adoption of Sunday as a day of\nrest by Constantine the Great in AD 321 made Sunday the seventh day of the week.[2][ |ISSUP] =Fully\nSupported][ISUSE] =5]\nReference [1] Emperor Constantine then enacted the first Sunday Laws, for “the venerable Day of the Sun\"\nin 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest\nfrom labor, stating: We find a good description of this in sources such as the book Rest Days which states:\nEarly Christian observance of both the spiritual seventh-day sabbath and a Lords Day assembly is evidenced in\nIgnatiusś letter to the Magnesians ca. 110.\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and\nthe nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The\nfact that the canon had to be issued at all is an indication that adoption of Constantine's decree of 321 was still\nnot universal...\n"}
{"page": 25, "bbox": [{"x": 0.17861205339431763, "y": 0.446153849363327}, {"x": 0.831626832485199, "y": 0.44659340381622314}, {"x": 0.8310580253601074, "y": 0.771868109703064}, {"x": 0.17804323136806488, "y": 0.7714285850524902}], "text": "Input Tell me a bio about Rory Tapner. (bio generation)\nOutput <p>Reference [1]<p>[ | ISREL] =Relevant] Rory Tapner is a British businessman who has served as\nthe Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.[ | ISSUP\n=Contradictory], Reference [2]<p>[ | ISREL =Relevant] Rory Tapner was born on 30 September 1959\nin London, England.[ |ISSUP] =Fully Supported]\nReference [1] Rory Tapner Rory Tapner is a UK businessman who, in September 2010 was appointed CEO of\nCoutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down\nas CEO when he was replaced in the role by Alison Rose\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017).\nRory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\nInput Tell me a bio about G. Venugopa. (bio generation)\nOutput (ranked 1) [[Retrieve |=Yes] <p>Reference [1]<p>[ [ISREL] =Relevant] G. Venugopal is a popu-\nlar playback singer in the Malayalam film industry. [Retrieve =Continue] He has sung over 300 songs in\nMalayalam, Tamil, and Hindi movies. [ | ISSUP] =Partially Supported] [[Retrieve|=Yes] <p>Reference\n[2]<p>G. Venugopal was born on December 10, 1960 in India.[ | ISSUP] =Fully Supported]\nReference [1] 'G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his\nwork in Malayalam films. He started his singing career in the film “Odaruthammaava Aalariyaam” (1984). Since\nthen he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has\nwon include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and\nthe Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004)\nthird time for the song \"aadedee..aadaadedee” from the film \"Ullam\". Venugopal'\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Profes-\nsional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal\n(born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\n"}
{"page": 25, "bbox": [{"x": 0.40273037552833557, "y": 0.7898901104927063}, {"x": 0.5978384613990784, "y": 0.7920879125595093}, {"x": 0.5978384613990784, "y": 0.8030769228935242}, {"x": 0.40273037552833557, "y": 0.8008791208267212}], "text": "Table 7: Examples of outputs.\n"}
{"page": 25, "bbox": [{"x": 0.4908987581729889, "y": 0.9630769491195679}, {"x": 0.49146756529808044, "y": 0.9428571462631226}, {"x": 0.5062571167945862, "y": 0.9428571462631226}, {"x": 0.5056883096694946, "y": 0.9630769491195679}], "text": "25\n"}
{"page": 25, "bbox": [{"x": 0.4908987581729889, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9591208696365356}, {"x": 0.4908987581729889, "y": 0.9591208696365356}], "text": "25\n"}
{"page": 26, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.19736263155937195}, {"x": 0.28612059354782104, "y": 0.19736263155937195}, {"x": 0.28612059354782104, "y": 0.20571428537368774}, {"x": 0.20193400979042053, "y": 0.20571428537368774}], "text": "Instructions\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.20967033505439758}, {"x": 0.7986348271369934, "y": 0.21054944396018982}, {"x": 0.7986348271369934, "y": 0.25010988116264343}, {"x": 0.20193400979042053, "y": 0.2492307722568512}], "text": "Given an instruction, please make a judgment on whether finding some external documents\nfrom the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or\n[No] and write an explanation.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.2716483473777771}, {"x": 0.3117178678512573, "y": 0.2716483473777771}, {"x": 0.3117178678512573, "y": 0.2800000011920929}, {"x": 0.20193400979042053, "y": 0.2800000011920929}], "text": "Demonstrations\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.2830769121646881}, {"x": 0.5073947906494141, "y": 0.2852747142314911}, {"x": 0.5073947906494141, "y": 0.2971428632736206}, {"x": 0.20136518776416779, "y": 0.2949450612068176}], "text": "Instruction Give three tips for staying healthy.\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.29802197217941284}, {"x": 0.34812286496162415, "y": 0.29890111088752747}, {"x": 0.34812286496162415, "y": 0.30989012122154236}, {"x": 0.20136518776416779, "y": 0.30901098251342773}], "text": "Need retrieval? [Yes]\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.31164833903312683}, {"x": 0.7986348271369934, "y": 0.31208792328834534}, {"x": 0.7986348271369934, "y": 0.3525274693965912}, {"x": 0.20136518776416779, "y": 0.35208791494369507}], "text": "Explanation There might be some online sources listing three tips for staying healthy or\nsome reliable sources to explain the effects of different behaviors on health. So retrieving\ndocuments is helpful to improve the response to this query.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.36791208386421204}, {"x": 0.6655290126800537, "y": 0.36791208386421204}, {"x": 0.6655290126800537, "y": 0.37890109419822693}, {"x": 0.20193400979042053, "y": 0.37890109419822693}], "text": "Instruction Describe a time when you had to make a difficult decision.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.3810988962650299}, {"x": 0.3435722291469574, "y": 0.38197803497314453}, {"x": 0.3435722291469574, "y": 0.3920879065990448}, {"x": 0.20193400979042053, "y": 0.39120879769325256}], "text": "Need retrieval? [No]\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.3956044018268585}, {"x": 0.799203634262085, "y": 0.3942857086658478}, {"x": 0.799203634262085, "y": 0.41934067010879517}, {"x": 0.20136518776416779, "y": 0.4206593334674835}], "text": "Explanation This instruction is asking about some personal experience and thus it does not\nrequire one to find some external documents.\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.4373626410961151}, {"x": 0.7980659604072571, "y": 0.4373626410961151}, {"x": 0.7980659604072571, "y": 0.4624175727367401}, {"x": 0.20136518776416779, "y": 0.4624175727367401}], "text": "Instruction Write a short story in third person narration about a protagonist who\nhas to make an important career decision.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.464175820350647}, {"x": 0.34414106607437134, "y": 0.4654945135116577}, {"x": 0.34414106607437134, "y": 0.475604385137558}, {"x": 0.20193400979042053, "y": 0.47428572177886963}], "text": "Need retrieval? [No]\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.4738461673259735}, {"x": 0.7974971532821655, "y": 0.47824177145957947}, {"x": 0.796928346157074, "y": 0.5072527527809143}, {"x": 0.20079636573791504, "y": 0.5028571486473083}], "text": "Explanation This instruction asks us to write a short story, which does not require external\nevidence to verify.\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.5191208720207214}, {"x": 0.47838452458381653, "y": 0.5204395651817322}, {"x": 0.47838452458381653, "y": 0.5450549721717834}, {"x": 0.20136518776416779, "y": 0.5437362790107727}], "text": "Instruction What is the capital of France?\nNeed retrieval? [Yes]\n"}
{"page": 26, "bbox": [{"x": 0.20079636573791504, "y": 0.5476922988891602}, {"x": 0.7986348271369934, "y": 0.5476922988891602}, {"x": 0.7986348271369934, "y": 0.5731868147850037}, {"x": 0.20079636573791504, "y": 0.5731868147850037}], "text": "Explanation While the instruction simply asks us to answer the capital of France, which is a\nwidely known fact, retrieving web documents for this question can still help.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.5894505381584167}, {"x": 0.6200227737426758, "y": 0.5894505381584167}, {"x": 0.6200227737426758, "y": 0.6008791327476501}, {"x": 0.20193400979042053, "y": 0.6008791327476501}], "text": "Instruction Find the area of a circle given its radius. Radius = 4\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.6008791327476501}, {"x": 0.34414106607437134, "y": 0.6021978259086609}, {"x": 0.34414106607437134, "y": 0.6136263608932495}, {"x": 0.20193400979042053, "y": 0.6123076677322388}], "text": "Need retrieval? [No]\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.6171428561210632}, {"x": 0.7974971532821655, "y": 0.6171428561210632}, {"x": 0.7974971532821655, "y": 0.6421977877616882}, {"x": 0.20136518776416779, "y": 0.6421977877616882}], "text": "Explanation This is a math question and although we may be able to find some documents\ndescribing a formula, it is unlikely to find a document exactly mentioning the answer.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.6584615111351013}, {"x": 0.8003413081169128, "y": 0.6584615111351013}, {"x": 0.8003413081169128, "y": 0.6839560270309448}, {"x": 0.20193400979042053, "y": 0.6839560270309448}], "text": "Instruction Arrange the words in the given sentence to form a grammatically cor-\nrect sentence. quickly the brown fox jumped\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.6852747201919556}, {"x": 0.34300342202186584, "y": 0.6861538290977478}, {"x": 0.34300342202186584, "y": 0.696703314781189}, {"x": 0.20193400979042053, "y": 0.695824146270752}], "text": "Need retrieval? [No]\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.6989011168479919}, {"x": 0.7974971532821655, "y": 0.7002198100090027}, {"x": 0.7974971532821655, "y": 0.7261538505554199}, {"x": 0.20193400979042053, "y": 0.7248351573944092}], "text": "Explanation This task doesn't require any external evidence, as it is a simple grammatical\nquestion.\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.7415384650230408}, {"x": 0.61774742603302, "y": 0.7415384650230408}, {"x": 0.61774742603302, "y": 0.7534065842628479}, {"x": 0.20193400979042053, "y": 0.7534065842628479}], "text": "Instruction Explain the process of cellular respiration in plants.\n"}
{"page": 26, "bbox": [{"x": 0.20136518776416779, "y": 0.7547252774238586}, {"x": 0.34698522090911865, "y": 0.7556043863296509}, {"x": 0.34698522090911865, "y": 0.766153872013092}, {"x": 0.20136518776416779, "y": 0.765274703502655}], "text": "Need retrieval? [Yes]\n"}
{"page": 26, "bbox": [{"x": 0.20193400979042053, "y": 0.7692307829856873}, {"x": 0.7974971532821655, "y": 0.7692307829856873}, {"x": 0.7974971532821655, "y": 0.7947252988815308}, {"x": 0.20193400979042053, "y": 0.7947252988815308}], "text": "Explanation This instruction asks for a detailed description of a scientific concept, and is\nhighly likely that we can find a reliable and useful document to support the response.\n"}
{"page": 26, "bbox": [{"x": 0.23720136284828186, "y": 0.8259340524673462}, {"x": 0.7616609930992126, "y": 0.8276923298835754}, {"x": 0.7616609930992126, "y": 0.8395604491233826}, {"x": 0.23720136284828186, "y": 0.8378021717071533}], "text": "Table 8: Instructions and demonstrations for Retrieve aspect given the input only.\n"}
{"page": 26, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5079635977745056, "y": 0.9507692456245422}, {"x": 0.5079635977745056, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "26\n"}
{"page": 26, "bbox": [{"x": 0.4908987581729889, "y": 0.9437362551689148}, {"x": 0.5045506358146667, "y": 0.9437362551689148}, {"x": 0.5039817690849304, "y": 0.9696703553199768}, {"x": 0.49032992124557495, "y": 0.9696703553199768}], "text": "26\n"}
{"page": 27, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 27, "bbox": [{"x": 0.20136518776416779, "y": 0.2800000011920929}, {"x": 0.2855517566204071, "y": 0.2795604467391968}, {"x": 0.2855517566204071, "y": 0.2887912094593048}, {"x": 0.20136518776416779, "y": 0.2892307639122009}], "text": "Instructions\n"}
{"page": 27, "bbox": [{"x": 0.19852104783058167, "y": 0.2914285659790039}, {"x": 0.8003413081169128, "y": 0.29274725914001465}, {"x": 0.7997724413871765, "y": 0.4448351562023163}, {"x": 0.19795222580432892, "y": 0.44351649284362793}], "text": "You will be provided with an instruction, evidence, output sentence, and preceding sentences\n(optional). If the preceding sentence is given, the output should be the sentence that follows\nthose preceding sentences. Your task is to determine whether the information in the output\nsentence can be fully verified by the evidence or if it requires further external verification.\nThere are three cases:\n- If the output sentence can be verified solely with the evidence, then respond with [Continue\nto Use Evidence].\n- If the sentence doesn't require any factual verification (e.g., a subjective sentence or a\nsentence about common sense), then respond with [No Retrieval].\n- If additional information is needed to verify the output sentence, respond with [Retrieval].\nPlease provide explanations for your judgments.\n"}
{"page": 27, "bbox": [{"x": 0.20079636573791504, "y": 0.4650549590587616}, {"x": 0.799203634262085, "y": 0.464175820350647}, {"x": 0.799203634262085, "y": 0.5287911891937256}, {"x": 0.20079636573791504, "y": 0.5296703577041626}], "text": "Instruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\n"}
{"page": 27, "bbox": [{"x": 0.20136518776416779, "y": 0.5327472686767578}, {"x": 0.3879408538341522, "y": 0.5345054864883423}, {"x": 0.3879408538341522, "y": 0.5450549721717834}, {"x": 0.20136518776416779, "y": 0.5432966947555542}], "text": "Evidence: Word embedding\n"}
{"page": 27, "bbox": [{"x": 0.20136518776416779, "y": 0.5476922988891602}, {"x": 0.799203634262085, "y": 0.5476922988891602}, {"x": 0.799203634262085, "y": 0.6417582631111145}, {"x": 0.20136518776416779, "y": 0.6417582631111145}], "text": "Word embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\nclassification, predicting the next word in a sequence, and understanding synonyms and\nanalogies.\n"}
{"page": 27, "bbox": [{"x": 0.20250284671783447, "y": 0.6448351740837097}, {"x": 0.3213879466056824, "y": 0.6439560651779175}, {"x": 0.3213879466056824, "y": 0.6549450755119324}, {"x": 0.20250284671783447, "y": 0.6558241844177246}], "text": "Rating [Retrieval]\n"}
{"page": 27, "bbox": [{"x": 0.20136518776416779, "y": 0.6575824022293091}, {"x": 0.7974971532821655, "y": 0.6580219864845276}, {"x": 0.7974971532821655, "y": 0.6980219483375549}, {"x": 0.20136518776416779, "y": 0.6975824236869812}], "text": "Explanation The output discusses the applications of word embeddings, while the evidence\nonly discusses the definitions of word embeddings and how they work. Therefore, we need to\nretrieve other evidence to verify whether the output is correct or not.\n"}
{"page": 27, "bbox": [{"x": 0.1757679134607315, "y": 0.7305494546890259}, {"x": 0.8248009085655212, "y": 0.7301098704338074}, {"x": 0.8248009085655212, "y": 0.7551648616790771}, {"x": 0.1757679134607315, "y": 0.7556043863296509}], "text": "Table 9: Instructions and demonstrations for Retrieve aspect given the input, preceding generations,\nand retrieved passages.\n"}
{"page": 27, "bbox": [{"x": 0.4908987581729889, "y": 0.9441758394241333}, {"x": 0.5056883096694946, "y": 0.9441758394241333}, {"x": 0.5051194429397583, "y": 0.9639560580253601}, {"x": 0.49032992124557495, "y": 0.9639560580253601}], "text": "27\n"}
{"page": 27, "bbox": [{"x": 0.4908987581729889, "y": 0.9503296613693237}, {"x": 0.5085324048995972, "y": 0.9503296613693237}, {"x": 0.5085324048995972, "y": 0.9591208696365356}, {"x": 0.4908987581729889, "y": 0.9591208696365356}], "text": "27\n"}
{"page": 28, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 28, "bbox": [{"x": 0.20193400979042053, "y": 0.30109891295433044}, {"x": 0.28612059354782104, "y": 0.30109891295433044}, {"x": 0.28612059354782104, "y": 0.30945053696632385}, {"x": 0.20193400979042053, "y": 0.30945053696632385}], "text": "Instructions\n"}
{"page": 28, "bbox": [{"x": 0.20079636573791504, "y": 0.3138461410999298}, {"x": 0.8003413081169128, "y": 0.3134065866470337}, {"x": 0.8003413081169128, "y": 0.3942857086658478}, {"x": 0.20079636573791504, "y": 0.3947252631187439}], "text": "You'll be provided with an instruction, along with evidence and possibly some preceding\nsentences. When there are preceding sentences, your focus should be on the sentence that\ncomes after them. Your job is to determine if the evidence is relevant to the initial instruction\nand the preceding context, and provides useful information to complete the task described in\nthe instruction. If the evidence meets this requirement, respond with [Relevant]; otherwise,\ngenerate [Irrelevant].\n"}
{"page": 28, "bbox": [{"x": 0.20136518776416779, "y": 0.41626372933387756}, {"x": 0.7224118113517761, "y": 0.41626372933387756}, {"x": 0.7224118113517761, "y": 0.4281318783760071}, {"x": 0.20136518776416779, "y": 0.4281318783760071}], "text": "Instruction Given four answer options, A, B, C, and D, choose the best answer.\n"}
{"page": 28, "bbox": [{"x": 0.2002275288105011, "y": 0.4307692348957062}, {"x": 0.43856656551361084, "y": 0.4307692348957062}, {"x": 0.43856656551361084, "y": 0.49670329689979553}, {"x": 0.2002275288105011, "y": 0.49670329689979553}], "text": "Input Earth's rotating causes\nA: the cycling of AM and PM\nB: the creation of volcanic eruptions\nC: the cycling of the tides\nD: the creation of gravity\n"}
{"page": 28, "bbox": [{"x": 0.20136518776416779, "y": 0.49934065341949463}, {"x": 0.7997724413871765, "y": 0.49934065341949463}, {"x": 0.7997724413871765, "y": 0.538021981716156}, {"x": 0.20136518776416779, "y": 0.538021981716156}], "text": "Evidence Rotation causes the day-night cycle which also creates a corresponding cycle of\ntemperature and humidity creates a corresponding cycle of temperature and humidity. Sea\nlevel rises and falls twice a day as the earth rotates.\n"}
{"page": 28, "bbox": [{"x": 0.20250284671783447, "y": 0.5410988926887512}, {"x": 0.3202502727508545, "y": 0.5406593680381775}, {"x": 0.3202502727508545, "y": 0.5512087941169739}, {"x": 0.20250284671783447, "y": 0.5516483783721924}], "text": "Rating [Relevant]\n"}
{"page": 28, "bbox": [{"x": 0.20193400979042053, "y": 0.5525274872779846}, {"x": 0.7986348271369934, "y": 0.5542857050895691}, {"x": 0.7986348271369934, "y": 0.5815384387969971}, {"x": 0.20193400979042053, "y": 0.5797802209854126}], "text": "Explanation The evidence explicitly mentions that the rotation causes a day-night cycle, as\ndescribed in the answer option A.\n"}
{"page": 28, "bbox": [{"x": 0.20136518776416779, "y": 0.5969230532646179}, {"x": 0.7986348271369934, "y": 0.5960439443588257}, {"x": 0.7986348271369934, "y": 0.6483516693115234}, {"x": 0.20136518776416779, "y": 0.6492307782173157}], "text": "Instruction age to run for US House of Representatives\nEvidence The Constitution sets three qualifications for service in the U.S. Senate: age (at\nleast thirty years of age); U.S. citizenship (at least nine years); and residency in the state a\nsenator represents at the time of election.\n"}
{"page": 28, "bbox": [{"x": 0.20250284671783447, "y": 0.6518681049346924}, {"x": 0.3265073895454407, "y": 0.6518681049346924}, {"x": 0.3265073895454407, "y": 0.6628571152687073}, {"x": 0.20250284671783447, "y": 0.6628571152687073}], "text": "Rating [Irrelevant]\n"}
{"page": 28, "bbox": [{"x": 0.20136518776416779, "y": 0.6624175906181335}, {"x": 0.7980659604072571, "y": 0.6654945015907288}, {"x": 0.7980659604072571, "y": 0.6940659284591675}, {"x": 0.20136518776416779, "y": 0.6909890174865723}], "text": "Explanation The evidence only discusses the ages to run for the US Senate, not for the\nHouse of Representatives.\n"}
{"page": 28, "bbox": [{"x": 0.23151308298110962, "y": 0.7226373553276062}, {"x": 0.7673492431640625, "y": 0.7243956327438354}, {"x": 0.7673492431640625, "y": 0.7362637519836426}, {"x": 0.23151308298110962, "y": 0.7345054745674133}], "text": "Table 10: Instructions and demonstrations for ISREL] aspect given the input only.\n"}
{"page": 28, "bbox": [{"x": 0.4908987581729889, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9586813449859619}, {"x": 0.4908987581729889, "y": 0.9586813449859619}], "text": "28\n"}
{"page": 28, "bbox": [{"x": 0.5051194429397583, "y": 0.9441758394241333}, {"x": 0.5051194429397583, "y": 0.9670329689979553}, {"x": 0.4926052391529083, "y": 0.9670329689979553}, {"x": 0.4926052391529083, "y": 0.9441758394241333}], "text": "28\n"}
{"page": 29, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 29, "bbox": [{"x": 0.20193400979042053, "y": 0.18989011645317078}, {"x": 0.286689430475235, "y": 0.18989011645317078}, {"x": 0.286689430475235, "y": 0.1986813247203827}, {"x": 0.20193400979042053, "y": 0.1986813247203827}], "text": "Instructions\n"}
{"page": 29, "bbox": [{"x": 0.19852104783058167, "y": 0.20307692885398865}, {"x": 0.7986348271369934, "y": 0.20351648330688477}, {"x": 0.7980659604072571, "y": 0.4065934121608734}, {"x": 0.19795222580432892, "y": 0.4061538577079773}], "text": "You will receive an instruction, evidence, and output, and optional preceding sentences. If the\npreceding sentence is given, the output should be the sentence that follows those preceding\nsentences. Your task is to evaluate if the output is fully supported by the information provided\nin the evidence.\nUse the following entailment scale to generate a score:\n- [Fully supported] - All information in output is supported by the evidence, or extractions\nfrom the evidence. This is only applicable when the output and part of the evidence are\nalmost identical.\n- [Partially supported] - The output is supported by the evidence to some extent, but there\nis major information in the output that is not discussed in the evidence. For example, if an\ninstruction asks about two concepts and the evidence only discusses either of them, it should\nbe considered a [Partially supported].\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the\nevidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the\ninstruction.\n"}
{"page": 29, "bbox": [{"x": 0.20079636573791504, "y": 0.4250549376010895}, {"x": 0.799203634262085, "y": 0.4250549376010895}, {"x": 0.799203634262085, "y": 0.464175820350647}, {"x": 0.20079636573791504, "y": 0.464175820350647}], "text": "Make sure to not use any external information/knowledge to judge whether the out-\nput is true or not. Only check whether the output is supported by the evidence, and not\nwhether the output follows the instructions or not.\n"}
{"page": 29, "bbox": [{"x": 0.20079636573791504, "y": 0.4848351776599884}, {"x": 0.7997724413871765, "y": 0.4852747321128845}, {"x": 0.7997724413871765, "y": 0.7327472567558289}, {"x": 0.20079636573791504, "y": 0.7323076725006104}], "text": "Instruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification,\npredicting the next word in a sequence, and understanding synonyms and analogies.\nEvidence Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Methods to generate this mapping include neural networks, dimensionality\nreduction on the word co-occurrence matrix, probabilistic models, explainable knowledge\nbase method, and explicit representation in terms of the context in which words appear. Word\nand phrase embeddings, when used as the underlying input representation, have been shown\nto boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next\ntoken predictions as well and analogy detection.\n"}
{"page": 29, "bbox": [{"x": 0.20136518776416779, "y": 0.7327472567558289}, {"x": 0.3577929437160492, "y": 0.7340659499168396}, {"x": 0.3577929437160492, "y": 0.7459340691566467}, {"x": 0.20136518776416779, "y": 0.744615375995636}], "text": "Score [Fully supported]\n"}
{"page": 29, "bbox": [{"x": 0.20136518776416779, "y": 0.7481318712234497}, {"x": 0.7986348271369934, "y": 0.7481318712234497}, {"x": 0.7986348271369934, "y": 0.8013187050819397}, {"x": 0.20136518776416779, "y": 0.8013187050819397}], "text": "Explanation The output sentence discusses the application of word embeddings, and the\nevidence mentions all of the applications syntactic parsing, sentiment analysis, next token\npredictions as well as analogy detection as the applications. Therefore, the score should be\n[Fully supported].\n"}
{"page": 29, "bbox": [{"x": 0.30034130811691284, "y": 0.8342857360839844}, {"x": 0.6018202304840088, "y": 0.8342857360839844}, {"x": 0.6018202304840088, "y": 0.84351646900177}, {"x": 0.30034130811691284, "y": 0.84351646900177}], "text": "Table 11: Instructions and demonstrations for\n"}
{"page": 29, "bbox": [{"x": 0.6149032711982727, "y": 0.8364835381507874}, {"x": 0.6979522109031677, "y": 0.8338461518287659}, {"x": 0.6985210180282593, "y": 0.8426373600959778}, {"x": 0.615472137928009, "y": 0.8452747464179993}], "text": "ISSUP tokens.\n"}
{"page": 29, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9507692456245422}, {"x": 0.5062571167945862, "y": 0.9586813449859619}, {"x": 0.49146756529808044, "y": 0.9586813449859619}], "text": "29\n"}
{"page": 29, "bbox": [{"x": 0.5062571167945862, "y": 0.944615364074707}, {"x": 0.5056883096694946, "y": 0.9661538600921631}, {"x": 0.4920364022254944, "y": 0.9661538600921631}, {"x": 0.4926052391529083, "y": 0.944615364074707}], "text": "29\n"}
{"page": 30, "bbox": [{"x": 0.1757679134607315, "y": 0.0364835150539875}, {"x": 0.23208190500736237, "y": 0.03692307695746422}, {"x": 0.23208190500736237, "y": 0.047472529113292694}, {"x": 0.1757679134607315, "y": 0.04703296720981598}], "text": "Preprint.\n"}
{"page": 30, "bbox": [{"x": 0.20193400979042053, "y": 0.3024175763130188}, {"x": 0.28612059354782104, "y": 0.3024175763130188}, {"x": 0.28612059354782104, "y": 0.3107692301273346}, {"x": 0.20193400979042053, "y": 0.3107692301273346}], "text": "Instructions\n"}
{"page": 30, "bbox": [{"x": 0.20079636573791504, "y": 0.3160439431667328}, {"x": 0.8009101152420044, "y": 0.3160439431667328}, {"x": 0.8009101152420044, "y": 0.4487912058830261}, {"x": 0.20079636573791504, "y": 0.4487912058830261}], "text": "Given an instruction and an output, rate whether the response appears to be a helpful and\ninformative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived\nutility. The detailed criterion is as follows: 5: The response provides a complete, highly\ndetailed, and informative response to the query, fully satisfying the information needs. 4: The\nresponse mostly fulfills the need in the query, while there can be some minor improvements\nsuch as discussing more detailed information, having better structure of the response, or im-\nproving coherence. 3: The response is acceptable, but some major additions or improvements\nare needed to satisfy users' needs. 2: The response still addresses the main request, but it is\nnot complete or not relevant to the query. 1: The response is barely on-topic or completely\nirrelevant.\n"}
{"page": 30, "bbox": [{"x": 0.20136518776416779, "y": 0.46945056319236755}, {"x": 0.799203634262085, "y": 0.4707692265510559}, {"x": 0.799203634262085, "y": 0.5516483783721924}, {"x": 0.20136518776416779, "y": 0.5503296852111816}], "text": "Instruction Who is the current prime minister of the UK as of 2023?\nOutput Boris Johnson was the prime minister of the UK from 2019-2022.\nPerceived utility 2\nExplanation While the output provides a factually correct statement about the UK prime\nminister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it\ndoesn't answer the instruction. Therefore, the utility is 2.\n"}
{"page": 30, "bbox": [{"x": 0.20136518776416779, "y": 0.567472517490387}, {"x": 0.7997724413871765, "y": 0.567472517490387}, {"x": 0.7997724413871765, "y": 0.6342856884002686}, {"x": 0.20136518776416779, "y": 0.6342856884002686}], "text": "Instruction Given a description of a travel destination, recommend 10 tourist attrac-\ntions with detailed descriptions of each. The travel destination is Tokyo, Japan.\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights\ninclude the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish\nMarket, Harajuku, and Shinjuku Gyoen.\n"}
{"page": 30, "bbox": [{"x": 0.20250284671783447, "y": 0.6342856884002686}, {"x": 0.3265073895454407, "y": 0.6360439658164978}, {"x": 0.32593855261802673, "y": 0.6470329761505127}, {"x": 0.20193400979042053, "y": 0.6452746987342834}], "text": "Perceived utility 3\n"}
{"page": 30, "bbox": [{"x": 0.20193400979042053, "y": 0.6492307782173157}, {"x": 0.7986348271369934, "y": 0.6501098871231079}, {"x": 0.7986348271369934, "y": 0.689230740070343}, {"x": 0.20193400979042053, "y": 0.6883516311645508}], "text": "Explanation This output doesn't provide descriptions of each attraction and the number of\nthe attractions is also less than 10. While this output partially answers the instructions, it\ndoesn't match the instructions strictly.\n"}
{"page": 30, "bbox": [{"x": 0.2997724711894989, "y": 0.7226373553276062}, {"x": 0.6996586918830872, "y": 0.7226373553276062}, {"x": 0.6996586918830872, "y": 0.7318681478500366}, {"x": 0.2997724711894989, "y": 0.7318681478500366}], "text": "Table 12: Instructions and demonstrations for ISUSE tokens.\n"}
{"page": 30, "bbox": [{"x": 0.4908987581729889, "y": 0.9639560580253601}, {"x": 0.4920364022254944, "y": 0.9384615421295166}, {"x": 0.5062571167945862, "y": 0.9389011263847351}, {"x": 0.5051194429397583, "y": 0.9643955826759338}], "text": "30\n"}
{"page": 30, "bbox": [{"x": 0.49146756529808044, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9507692456245422}, {"x": 0.5073947906494141, "y": 0.9582417607307434}, {"x": 0.49146756529808044, "y": 0.9582417607307434}], "text": "30\n"}
{"page": 1, "bbox": [{"x": 0.914675772190094, "y": 0.03340659290552139}, {"x": 0.9186575412750244, "y": 0.03340659290552139}, {"x": 0.9186575412750244, "y": 0.0391208790242672}, {"x": 0.914675772190094, "y": 0.0391208790242672}], "text": "1\n"}
{"page": 1, "bbox": [{"x": 0.10352673381567001, "y": 0.07604395598173141}, {"x": 0.8970420956611633, "y": 0.07604395598173141}, {"x": 0.8970420956611633, "y": 0.18813186883926392}, {"x": 0.10352673381567001, "y": 0.18813186883926392}], "text": "Retrieval-Augmented Generation for Large\nLanguage Models: A Survey\nYunfan Gaoª, Yun Xiong, Xinyu Gao³, Kangxiang Jia³, Jinliu Pan³, Yuxi Biº, Yi Daiª, Jiawei Suna, Meng\nWang, and Haofen Wang a,c\n"}
{"page": 1, "bbox": [{"x": 0.15984073281288147, "y": 0.20527473092079163}, {"x": 0.8378839492797852, "y": 0.20527473092079163}, {"x": 0.8378839492797852, "y": 0.2518681287765503}, {"x": 0.15984073281288147, "y": 0.2518681287765503}], "text": "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n\"College of Design and Innovation, Tongji University\n"}
{"page": 1, "bbox": [{"x": 0.07906711846590042, "y": 0.2993406653404236}, {"x": 0.4908987581729889, "y": 0.2993406653404236}, {"x": 0.4908987581729889, "y": 0.6070329546928406}, {"x": 0.07906711846590042, "y": 0.6070329546928406}], "text": "Abstract-Large Language Models (LLMs) showcase impres-\nsive capabilities but encounter challenges like hallucination,\noutdated knowledge, and non-transparent, untraceable reasoning\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\nas a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the\ngeneration, particularly for knowledge-intensive tasks, and allows\nfor continuous knowledge updates and integration of domain-\nspecific information. RAG synergistically merges LLMs' intrin-\nsic knowledge with the vast, dynamic repositories of external\ndatabases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\nIt meticulously scrutinizes the tripartite foundation of RAG\nframeworks, which includes the retrieval, the generation and the\naugmentation techniques. The paper highlights the state-of-the-\nart technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG\nsystems. Furthermore, this paper introduces up-to-date evalua-\ntion framework and benchmark. At the end, this article delineates\nthe challenges currently faced and points out prospective avenues\nfor research and development 1.\nIndex Terms-Large language model, retrieval-augmented gen-\neration, natural language processing, information retrieval\n"}
{"page": 1, "bbox": [{"x": 0.027872582897543907, "y": 0.7054945230484009}, {"x": 0.029010238125920296, "y": 0.26505494117736816}, {"x": 0.05915813520550728, "y": 0.26505494117736816}, {"x": 0.05802047625184059, "y": 0.7054945230484009}], "text": "arXiv:2312.10997v5 [cs.CL] 27 Mar 2024\n"}
{"page": 1, "bbox": [{"x": 0.5079635977745056, "y": 0.29802197217941284}, {"x": 0.9215016961097717, "y": 0.29802197217941284}, {"x": 0.9215016961097717, "y": 0.9450549483299255}, {"x": 0.5079635977745056, "y": 0.9450549483299255}], "text": "in Figure 1. The development trajectory of RAG in the era\nof large models exhibits several distinct stage characteristics.\nInitially, RAG's inception coincided with the rise of the\nTransformer architecture, focusing on enhancing language\nmodels by incorporating additional knowledge through Pre-\nTraining Models (PTM). This early stage was characterized\nby foundational work aimed at refining pre-training techniques\n[3]-[5]. The subsequent arrival of ChatGPT [6] marked a\npivotal moment, with LLM demonstrating powerful in context\nlearning (ICL) capabilities. RAG research shifted towards\nproviding better information for LLMs to answer more com-\nplex and knowledge-intensive tasks during the inference stage,\nleading to rapid development in RAG studies. As research\nprogressed, the enhancement of RAG was no longer limited\nto the inference stage but began to incorporate more with LLM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs. This paper considers both\ntechnical paradigms and research methods, summarizing three\nmain research paradigms from over 100 RAG studies, and\nanalyzing key technologies in the core stages of \"Retrieval,”\n\"Generation,\" and \"Augmentation.” On the other hand, current\nresearch tends to focus more on methods, lacking analysis and\nsummarization of how to evaluate RAG. This paper compre-\nhensively reviews the downstream tasks, datasets, benchmarks,\nand evaluation methods applicable to RAG. Overall, this\npaper sets out to meticulously compile and categorize the\nfoundational technical concepts, historical progression, and\nthe spectrum of RAG methodologies and applications that\nhave emerged post-LLMs. It is designed to equip readers and\nprofessionals with a detailed and structured understanding of\nboth large models and RAG. It aims to illuminate the evolution\nof retrieval augmentation techniques, assess the strengths and\nweaknesses of various approaches in their respective contexts,\nand speculate on upcoming trends and innovations.\nOur contributions are as follows:\n• In this survey, we present a thorough and systematic\nreview of the state-of-the-art RAG methods, delineating\nits evolution through paradigms including naive RAG,\n"}
{"page": 1, "bbox": [{"x": 0.22070534527301788, "y": 0.6347252726554871}, {"x": 0.34812286496162415, "y": 0.6356043815612793}, {"x": 0.34812286496162415, "y": 0.6443955898284912}, {"x": 0.22070534527301788, "y": 0.643516480922699}], "text": "I. INTRODUCTION\n"}
{"page": 1, "bbox": [{"x": 0.07963594794273376, "y": 0.6562637090682983}, {"x": 0.11092150211334229, "y": 0.6562637090682983}, {"x": 0.11092150211334229, "y": 0.6813187003135681}, {"x": 0.07963594794273376, "y": 0.6813187003135681}], "text": "L\n"}
{"page": 1, "bbox": [{"x": 0.07849829643964767, "y": 0.6558241844177246}, {"x": 0.49146756529808044, "y": 0.6558241844177246}, {"x": 0.49146756529808044, "y": 0.8953846096992493}, {"x": 0.07849829643964767, "y": 0.8953846096992493}], "text": "ARGE language models (LLMs) have achieved remark-\nable success, though they still face significant limitations,\nespecially in domain-specific or knowledge-intensive tasks [1],\nnotably producing “hallucinations” [2] when handling queries\nbeyond their training data or requiring current information. To\novercome challenges, Retrieval-Augmented Generation (RAG)\nenhances LLMs by retrieving relevant document chunks from\nexternal knowledge base through semantic similarity calcu-\nlation. By referencing external knowledge, RAG effectively\nreduces the problem of generating factually incorrect content.\nIts integration into LLMs has resulted in widespread adoption,\nestablishing RAG as a key technology in advancing chatbots\nand enhancing the suitability of LLMs for real-world applica-\ntions.\nRAG technology has rapidly developed in recent years, and\nthe technology tree summarizing related research is shown\n"}
{"page": 1, "bbox": [{"x": 0.08020477741956711, "y": 0.9090110063552856}, {"x": 0.489761084318161, "y": 0.9103296995162964}, {"x": 0.489761084318161, "y": 0.945494532585144}, {"x": 0.08020477741956711, "y": 0.9441758394241333}], "text": "Corresponding Author.Email:haofen.wang@tongji.edu.cn\n¹Resources\nRAG-Survey\nare available at https://github.com/Tongji-KGLLM/\n"}
{"page": 2, "bbox": [{"x": 0.9135380983352661, "y": 0.03384615480899811}, {"x": 0.9197952151298523, "y": 0.03384615480899811}, {"x": 0.9197952151298523, "y": 0.0391208790242672}, {"x": 0.9135380983352661, "y": 0.0391208790242672}], "text": "2\n"}
{"page": 2, "bbox": [{"x": 0.5989761352539062, "y": 0.08615384250879288}, {"x": 0.6473265290260315, "y": 0.08615384250879288}, {"x": 0.6473265290260315, "y": 0.09142857044935226}, {"x": 0.5989761352539062, "y": 0.09142857044935226}], "text": "Inference\n"}
{"page": 2, "bbox": [{"x": 0.36120590567588806, "y": 0.10241758078336716}, {"x": 0.4186575710773468, "y": 0.10285714268684387}, {"x": 0.4186575710773468, "y": 0.10901098698377609}, {"x": 0.36120590567588806, "y": 0.10857142508029938}], "text": "Fine-tuning\n"}
{"page": 2, "bbox": [{"x": 0.6387940645217896, "y": 0.10593406856060028}, {"x": 0.658703088760376, "y": 0.10593406856060028}, {"x": 0.658703088760376, "y": 0.10989011079072952}, {"x": 0.6387940645217896, "y": 0.10989011079072952}], "text": "RADA\n"}
{"page": 2, "bbox": [{"x": 0.5745164752006531, "y": 0.10901098698377609}, {"x": 0.615472137928009, "y": 0.10989011079072952}, {"x": 0.615472137928009, "y": 0.1147252768278122}, {"x": 0.5745164752006531, "y": 0.11384615302085876}], "text": "G-Retriever\n"}
{"page": 2, "bbox": [{"x": 0.4715586006641388, "y": 0.11560439318418503}, {"x": 0.532423198223114, "y": 0.11604395508766174}, {"x": 0.532423198223114, "y": 0.12219779938459396}, {"x": 0.4715586006641388, "y": 0.12175824493169785}], "text": "Pre-training\n"}
{"page": 2, "bbox": [{"x": 0.6467576622962952, "y": 0.11824175715446472}, {"x": 0.6757678985595703, "y": 0.11824175715446472}, {"x": 0.6757678985595703, "y": 0.1230769231915474}, {"x": 0.6467576622962952, "y": 0.1230769231915474}], "text": "RAPTOR\n"}
{"page": 2, "bbox": [{"x": 0.5631399154663086, "y": 0.1230769231915474}, {"x": 0.583048939704895, "y": 0.1230769231915474}, {"x": 0.583048939704895, "y": 0.12747253477573395}, {"x": 0.5631399154663086, "y": 0.12747253477573395}], "text": "CRAG\n"}
{"page": 2, "bbox": [{"x": 0.34072810411453247, "y": 0.1257142871618271}, {"x": 0.38168373703956604, "y": 0.1261538416147232}, {"x": 0.38168373703956604, "y": 0.1314285695552826}, {"x": 0.34072810411453247, "y": 0.13098901510238647}], "text": "UniMS-RAG\n"}
{"page": 2, "bbox": [{"x": 0.6922639608383179, "y": 0.12659341096878052}, {"x": 0.7081910967826843, "y": 0.12659341096878052}, {"x": 0.7081910967826843, "y": 0.13098901510238647}, {"x": 0.6922639608383179, "y": 0.13098901510238647}], "text": "BGM\n"}
{"page": 2, "bbox": [{"x": 0.180887371301651, "y": 0.1375824213027954}, {"x": 0.21217292547225952, "y": 0.1375824213027954}, {"x": 0.21217292547225952, "y": 0.14329670369625092}, {"x": 0.180887371301651, "y": 0.14329670369625092}], "text": "2024\n"}
{"page": 2, "bbox": [{"x": 0.6552901268005371, "y": 0.14505495131015778}, {"x": 0.6791808605194092, "y": 0.14461538195610046}, {"x": 0.6791808605194092, "y": 0.15032966434955597}, {"x": 0.6552901268005371, "y": 0.15076923370361328}], "text": "HYKGE\n"}
{"page": 2, "bbox": [{"x": 0.6029579043388367, "y": 0.14681318402290344}, {"x": 0.615472137928009, "y": 0.14681318402290344}, {"x": 0.615472137928009, "y": 0.1512087881565094}, {"x": 0.6029579043388367, "y": 0.1512087881565094}], "text": "IAG\n"}
{"page": 2, "bbox": [{"x": 0.38566553592681885, "y": 0.15076923370361328}, {"x": 0.41410693526268005, "y": 0.15076923370361328}, {"x": 0.41410693526268005, "y": 0.15516483783721924}, {"x": 0.38566553592681885, "y": 0.15516483783721924}], "text": "CT-RAG\n"}
{"page": 2, "bbox": [{"x": 0.6837314963340759, "y": 0.1538461595773697}, {"x": 0.725824773311615, "y": 0.1538461595773697}, {"x": 0.725824773311615, "y": 0.15824176371097565}, {"x": 0.6837314963340759, "y": 0.15824176371097565}], "text": "DRAGON-AI\n"}
{"page": 2, "bbox": [{"x": 0.5893060564994812, "y": 0.15824176371097565}, {"x": 0.6092150211334229, "y": 0.15824176371097565}, {"x": 0.6092150211334229, "y": 0.1626373678445816}, {"x": 0.5893060564994812, "y": 0.1626373678445816}], "text": "FILCO\n"}
{"page": 2, "bbox": [{"x": 0.6393629312515259, "y": 0.15780219435691833}, {"x": 0.6700796484947205, "y": 0.15824176371097565}, {"x": 0.6700796484947205, "y": 0.16351647675037384}, {"x": 0.6393629312515259, "y": 0.16307692229747772}], "text": "PaperQA\n"}
{"page": 2, "bbox": [{"x": 0.34414106607437134, "y": 0.15868131816387177}, {"x": 0.3589306175708771, "y": 0.15868131816387177}, {"x": 0.3589306175708771, "y": 0.16307692229747772}, {"x": 0.34414106607437134, "y": 0.16307692229747772}], "text": "CON\n"}
{"page": 2, "bbox": [{"x": 0.7042093276977539, "y": 0.16395604610443115}, {"x": 0.7383390069007874, "y": 0.16351647675037384}, {"x": 0.7383390069007874, "y": 0.1683516502380371}, {"x": 0.7042093276977539, "y": 0.16879120469093323}], "text": "CREA-ICL\n"}
{"page": 2, "bbox": [{"x": 0.3981797397136688, "y": 0.16483516991138458}, {"x": 0.4124004542827606, "y": 0.16483516991138458}, {"x": 0.4124004542827606, "y": 0.16923077404499054}, {"x": 0.3981797397136688, "y": 0.16923077404499054}], "text": "BEQ\n"}
{"page": 2, "bbox": [{"x": 0.3270762264728546, "y": 0.16967032849788666}, {"x": 0.34015926718711853, "y": 0.16967032849788666}, {"x": 0.34015926718711853, "y": 0.1736263781785965}, {"x": 0.3270762264728546, "y": 0.1736263781785965}], "text": "EAR\n"}
{"page": 2, "bbox": [{"x": 0.5608646273612976, "y": 0.16967032849788666}, {"x": 0.5961319804191589, "y": 0.16967032849788666}, {"x": 0.5961319804191589, "y": 0.1736263781785965}, {"x": 0.5608646273612976, "y": 0.1736263781785965}], "text": "ARM-RAG\n"}
{"page": 2, "bbox": [{"x": 0.38452786207199097, "y": 0.18681319057941437}, {"x": 0.40273037552833557, "y": 0.18681319057941437}, {"x": 0.40273037552833557, "y": 0.190769225358963}, {"x": 0.38452786207199097, "y": 0.190769225358963}], "text": "RAST\n"}
{"page": 2, "bbox": [{"x": 0.6592718958854675, "y": 0.18637362122535706}, {"x": 0.6905574798583984, "y": 0.18725274503231049}, {"x": 0.6905574798583984, "y": 0.19208791851997375}, {"x": 0.6592718958854675, "y": 0.19120879471302032}], "text": "1-PAGER\n"}
{"page": 2, "bbox": [{"x": 0.7150170803070068, "y": 0.18901099264621735}, {"x": 0.7292377948760986, "y": 0.18901099264621735}, {"x": 0.7292377948760986, "y": 0.1934065967798233}, {"x": 0.7150170803070068, "y": 0.1934065967798233}], "text": "ToC\n"}
{"page": 2, "bbox": [{"x": 0.5779294371604919, "y": 0.1876923143863678}, {"x": 0.640500545501709, "y": 0.1876923143863678}, {"x": 0.640500545501709, "y": 0.20219780504703522}, {"x": 0.5779294371604919, "y": 0.20219780504703522}], "text": "PRCA\nToken-Elimination\n"}
{"page": 2, "bbox": [{"x": 0.2946530282497406, "y": 0.19516482949256897}, {"x": 0.36518770456314087, "y": 0.19516482949256897}, {"x": 0.36518770456314087, "y": 0.20000000298023224}, {"x": 0.2946530282497406, "y": 0.20000000298023224}], "text": "Dual-Feedback-ToD\n"}
{"page": 2, "bbox": [{"x": 0.6678043007850647, "y": 0.19692307710647583}, {"x": 0.6973834037780762, "y": 0.19692307710647583}, {"x": 0.6973834037780762, "y": 0.2013186812400818}, {"x": 0.6678043007850647, "y": 0.2013186812400818}], "text": "FABULA\n"}
{"page": 2, "bbox": [{"x": 0.3833902180194855, "y": 0.19824175536632538}, {"x": 0.4158134162425995, "y": 0.19824175536632538}, {"x": 0.4158134162425995, "y": 0.20307692885398865}, {"x": 0.3833902180194855, "y": 0.20307692885398865}], "text": "Self-RAG\n"}
{"page": 2, "bbox": [{"x": 0.6905574798583984, "y": 0.20527473092079163}, {"x": 0.7548350691795349, "y": 0.20527473092079163}, {"x": 0.7548350691795349, "y": 0.2101098895072937}, {"x": 0.6905574798583984, "y": 0.2101098895072937}], "text": "QLM-Doc-ranking\n"}
{"page": 2, "bbox": [{"x": 0.4550625681877136, "y": 0.20659340918064117}, {"x": 0.5017064809799194, "y": 0.20659340918064117}, {"x": 0.5017064809799194, "y": 0.21098901331424713}, {"x": 0.4550625681877136, "y": 0.21098901331424713}], "text": "InstructRetro\n"}
{"page": 2, "bbox": [{"x": 0.5864619016647339, "y": 0.20791208744049072}, {"x": 0.6120591759681702, "y": 0.20835164189338684}, {"x": 0.6120591759681702, "y": 0.2127472460269928}, {"x": 0.5864619016647339, "y": 0.21230769157409668}], "text": "KALMV\n"}
{"page": 2, "bbox": [{"x": 0.3162684738636017, "y": 0.20923076570034027}, {"x": 0.3435722291469574, "y": 0.20923076570034027}, {"x": 0.3435722291469574, "y": 0.21362636983394623}, {"x": 0.3162684738636017, "y": 0.21362636983394623}], "text": "MK-TOD\n"}
{"page": 2, "bbox": [{"x": 0.3833902180194855, "y": 0.21054944396018982}, {"x": 0.42434585094451904, "y": 0.21098901331424713}, {"x": 0.42434585094451904, "y": 0.2153846174478531}, {"x": 0.3833902180194855, "y": 0.21494504809379578}], "text": "LM-Indexer\n"}
{"page": 2, "bbox": [{"x": 0.6786120533943176, "y": 0.21406593918800354}, {"x": 0.693401575088501, "y": 0.21406593918800354}, {"x": 0.693401575088501, "y": 0.21802197396755219}, {"x": 0.6786120533943176, "y": 0.21802197396755219}], "text": "SKR\n"}
{"page": 2, "bbox": [{"x": 0.7286689281463623, "y": 0.21670329570770264}, {"x": 0.7576791644096375, "y": 0.21714285016059875}, {"x": 0.7576791644096375, "y": 0.22197802364826202}, {"x": 0.7286689281463623, "y": 0.2215384542942047}], "text": "Recomp\n"}
{"page": 2, "bbox": [{"x": 0.5682593584060669, "y": 0.2184615433216095}, {"x": 0.5841865539550781, "y": 0.2184615433216095}, {"x": 0.5841865539550781, "y": 0.22241757810115814}, {"x": 0.5682593584060669, "y": 0.22241757810115814}], "text": "ITRG\n"}
{"page": 2, "bbox": [{"x": 0.29920363426208496, "y": 0.21890109777450562}, {"x": 0.34300342202186584, "y": 0.21934065222740173}, {"x": 0.34300342202186584, "y": 0.22461538016796112}, {"x": 0.29920363426208496, "y": 0.224175825715065}], "text": "RAG_Robust\n"}
{"page": 2, "bbox": [{"x": 0.404436856508255, "y": 0.22241757810115814}, {"x": 0.4288964867591858, "y": 0.22241757810115814}, {"x": 0.4288964867591858, "y": 0.22637362778186798}, {"x": 0.404436856508255, "y": 0.22637362778186798}], "text": "RA-DIT\n"}
{"page": 2, "bbox": [{"x": 0.27076223492622375, "y": 0.23076923191547394}, {"x": 0.3464163839817047, "y": 0.23076923191547394}, {"x": 0.3464163839817047, "y": 0.235604390501976}, {"x": 0.27076223492622375, "y": 0.235604390501976}], "text": "Retrieve-and-Sample\n"}
{"page": 2, "bbox": [{"x": 0.6205915808677673, "y": 0.23736263811588287}, {"x": 0.6348122954368591, "y": 0.23692308366298676}, {"x": 0.6348122954368591, "y": 0.2413186877965927}, {"x": 0.6205915808677673, "y": 0.24175824224948883}], "text": "KGP\n"}
{"page": 2, "bbox": [{"x": 0.44482365250587463, "y": 0.23912088572978973}, {"x": 0.4687144458293915, "y": 0.23956044018268585}, {"x": 0.4687144458293915, "y": 0.24439559876918793}, {"x": 0.44482365250587463, "y": 0.2439560443162918}], "text": "RAVEN\n"}
{"page": 2, "bbox": [{"x": 0.6643913388252258, "y": 0.24043956398963928}, {"x": 0.7093287706375122, "y": 0.23999999463558197}, {"x": 0.7093287706375122, "y": 0.24527472257614136}, {"x": 0.6643913388252258, "y": 0.24571429193019867}], "text": "KnowledGPT\n"}
{"page": 2, "bbox": [{"x": 0.1820250302553177, "y": 0.2435164898633957}, {"x": 0.2081911265850067, "y": 0.2435164898633957}, {"x": 0.2081911265850067, "y": 0.24791209399700165}, {"x": 0.1820250302553177, "y": 0.24791209399700165}], "text": "GPT-4\n"}
{"page": 2, "bbox": [{"x": 0.5978384613990784, "y": 0.2465934008359909}, {"x": 0.6217292547225952, "y": 0.2465934008359909}, {"x": 0.6217292547225952, "y": 0.25054946541786194}, {"x": 0.5978384613990784, "y": 0.25054946541786194}], "text": "LLM-R\n"}
{"page": 2, "bbox": [{"x": 0.5085324048995972, "y": 0.2465934008359909}, {"x": 0.5244596004486084, "y": 0.2461538463830948}, {"x": 0.5244596004486084, "y": 0.2514285743236542}, {"x": 0.5085324048995972, "y": 0.2518681287765503}], "text": "CoG\n"}
{"page": 2, "bbox": [{"x": 0.6985210180282593, "y": 0.24967032670974731}, {"x": 0.7207053303718567, "y": 0.24967032670974731}, {"x": 0.7207053303718567, "y": 0.25362637639045715}, {"x": 0.6985210180282593, "y": 0.25362637639045715}], "text": "IRCOT\n"}
{"page": 2, "bbox": [{"x": 0.3668941855430603, "y": 0.25582417845726013}, {"x": 0.3924914598464966, "y": 0.25670328736305237}, {"x": 0.39192265272140503, "y": 0.26197803020477295}, {"x": 0.36632537841796875, "y": 0.2610988914966583}], "text": "SUGRE\n"}
{"page": 2, "bbox": [{"x": 0.7286689281463623, "y": 0.25846153497695923}, {"x": 0.7485779523849487, "y": 0.25846153497695923}, {"x": 0.7485779523849487, "y": 0.2628571391105652}, {"x": 0.7286689281463623, "y": 0.2628571391105652}], "text": "PGRA\n"}
{"page": 2, "bbox": [{"x": 0.3162684738636017, "y": 0.25362637639045715}, {"x": 0.34072810411453247, "y": 0.25362637639045715}, {"x": 0.34072810411453247, "y": 0.2685714364051819}, {"x": 0.3162684738636017, "y": 0.2685714364051819}], "text": "SANTA\nRRR\n"}
{"page": 2, "bbox": [{"x": 0.5864619016647339, "y": 0.25890108942985535}, {"x": 0.6342434287071228, "y": 0.25934067368507385}, {"x": 0.6342434287071228, "y": 0.26461538672447205}, {"x": 0.5864619016647339, "y": 0.2641758322715759}], "text": "ITER-RETGEN\n"}
{"page": 2, "bbox": [{"x": 0.6985210180282593, "y": 0.26505494117736816}, {"x": 0.7127417325973511, "y": 0.26505494117736816}, {"x": 0.7127417325973511, "y": 0.2694505453109741}, {"x": 0.6985210180282593, "y": 0.2694505453109741}], "text": "PKG\n"}
{"page": 2, "bbox": [{"x": 0.38964733481407166, "y": 0.2659340798854828}, {"x": 0.404436856508255, "y": 0.2659340798854828}, {"x": 0.404436856508255, "y": 0.27032968401908875}, {"x": 0.38964733481407166, "y": 0.27032968401908875}], "text": "AAR\n"}
{"page": 2, "bbox": [{"x": 0.4431171715259552, "y": 0.26989009976387024}, {"x": 0.4635950028896332, "y": 0.26989009976387024}, {"x": 0.4635950028896332, "y": 0.2738461494445801}, {"x": 0.4431171715259552, "y": 0.2738461494445801}], "text": "TIGER\n"}
{"page": 2, "bbox": [{"x": 0.6131967902183533, "y": 0.2716483473777771}, {"x": 0.6296928524971008, "y": 0.271208792924881}, {"x": 0.6296928524971008, "y": 0.2764835059642792}, {"x": 0.6131967902183533, "y": 0.2769230902194977}], "text": "COQ\n"}
{"page": 2, "bbox": [{"x": 0.29522183537483215, "y": 0.27296704053878784}, {"x": 0.32878270745277405, "y": 0.2742857038974762}, {"x": 0.3282138705253601, "y": 0.27912089228630066}, {"x": 0.2946530282497406, "y": 0.2778021991252899}], "text": "Self-Mem\n"}
{"page": 2, "bbox": [{"x": 0.7025028467178345, "y": 0.2742857038974762}, {"x": 0.7434584498405457, "y": 0.2747252881526947}, {"x": 0.7434584498405457, "y": 0.2800000011920929}, {"x": 0.7025028467178345, "y": 0.2795604467391968}], "text": "SCM4LLMS\n"}
{"page": 2, "bbox": [{"x": 0.3981797397136688, "y": 0.27604395151138306}, {"x": 0.4209328889846802, "y": 0.27560439705848694}, {"x": 0.4209328889846802, "y": 0.2800000011920929}, {"x": 0.3981797397136688, "y": 0.280439555644989}], "text": "FLARE\n"}
{"page": 2, "bbox": [{"x": 0.5130830407142639, "y": 0.2769230902194977}, {"x": 0.5415244698524475, "y": 0.2773626446723938}, {"x": 0.5415244698524475, "y": 0.2821978032588959}, {"x": 0.5130830407142639, "y": 0.28175824880599976}], "text": "Retro++\n"}
{"page": 2, "bbox": [{"x": 0.6006826162338257, "y": 0.28175824880599976}, {"x": 0.6251422166824341, "y": 0.28175824880599976}, {"x": 0.6251422166824341, "y": 0.2857142984867096}, {"x": 0.6006826162338257, "y": 0.2857142984867096}], "text": "LLM-IE\n"}
{"page": 2, "bbox": [{"x": 0.28327643871307373, "y": 0.2835164964199066}, {"x": 0.30887371301651, "y": 0.28395605087280273}, {"x": 0.30887371301651, "y": 0.2887912094593048}, {"x": 0.28327643871307373, "y": 0.2883516550064087}], "text": "UPRISE\n"}
{"page": 2, "bbox": [{"x": 0.7161546945571899, "y": 0.28483515977859497}, {"x": 0.7679181098937988, "y": 0.28483515977859497}, {"x": 0.7679181098937988, "y": 0.29010990262031555}, {"x": 0.7161546945571899, "y": 0.29010990262031555}], "text": "Filter-Reranker\n"}
{"page": 2, "bbox": [{"x": 0.5807735919952393, "y": 0.29054945707321167}, {"x": 0.6075085401535034, "y": 0.2909890115261078}, {"x": 0.6075085401535034, "y": 0.29538461565971375}, {"x": 0.5807735919952393, "y": 0.2949450612068176}], "text": "ICRALM\n"}
{"page": 2, "bbox": [{"x": 0.7411831617355347, "y": 0.29890111088752747}, {"x": 0.7696245908737183, "y": 0.29890111088752747}, {"x": 0.7696245908737183, "y": 0.3028571307659149}, {"x": 0.7411831617355347, "y": 0.3028571307659149}], "text": "RePLUG\n"}
{"page": 2, "bbox": [{"x": 0.5642775893211365, "y": 0.29890111088752747}, {"x": 0.5784983038902283, "y": 0.29890111088752747}, {"x": 0.5784983038902283, "y": 0.3032967150211334}, {"x": 0.5642775893211365, "y": 0.3032967150211334}], "text": "DSP\n"}
{"page": 2, "bbox": [{"x": 0.17861205339431763, "y": 0.3050549328327179}, {"x": 0.21387940645217896, "y": 0.3046153783798218}, {"x": 0.21387940645217896, "y": 0.32043954730033875}, {"x": 0.17861205339431763, "y": 0.32087913155555725}], "text": "2023\nChatGPT\n"}
{"page": 2, "bbox": [{"x": 0.3111490309238434, "y": 0.31560438871383667}, {"x": 0.3356086313724518, "y": 0.31560438871383667}, {"x": 0.3356086313724518, "y": 0.32043954730033875}, {"x": 0.3111490309238434, "y": 0.32043954730033875}], "text": "R-GQA\n"}
{"page": 2, "bbox": [{"x": 0.3668941855430603, "y": 0.31780219078063965}, {"x": 0.3981797397136688, "y": 0.3191208839416504}, {"x": 0.39761093258857727, "y": 0.3243955969810486}, {"x": 0.36632537841796875, "y": 0.32307693362236023}], "text": "RAG-e2e\n"}
{"page": 2, "bbox": [{"x": 0.2935153543949127, "y": 0.3252747356891632}, {"x": 0.3100113868713379, "y": 0.3252747356891632}, {"x": 0.3100113868713379, "y": 0.32967033982276917}, {"x": 0.2935153543949127, "y": 0.32967033982276917}], "text": "Zemi\n"}
{"page": 2, "bbox": [{"x": 0.3680318593978882, "y": 0.33230769634246826}, {"x": 0.4288964867591858, "y": 0.33230769634246826}, {"x": 0.4288964867591858, "y": 0.3367033004760742}, {"x": 0.3680318593978882, "y": 0.3367033004760742}], "text": "PROMPTAGATOR\n"}
{"page": 2, "bbox": [{"x": 0.5784983038902283, "y": 0.33230769634246826}, {"x": 0.6103526949882507, "y": 0.33230769634246826}, {"x": 0.6103526949882507, "y": 0.33714285492897034}, {"x": 0.5784983038902283, "y": 0.33714285492897034}], "text": "GenRead\n"}
{"page": 2, "bbox": [{"x": 0.2713310718536377, "y": 0.33714285492897034}, {"x": 0.28896471858024597, "y": 0.33714285492897034}, {"x": 0.28896471858024597, "y": 0.3410989046096802}, {"x": 0.2713310718536377, "y": 0.3410989046096802}], "text": "Atlas\n"}
{"page": 2, "bbox": [{"x": 0.4197952151298523, "y": 0.3419780135154724}, {"x": 0.43458476662635803, "y": 0.34241756796836853}, {"x": 0.43458476662635803, "y": 0.347252756357193}, {"x": 0.4197952151298523, "y": 0.3468131721019745}], "text": "RAG\n"}
{"page": 2, "bbox": [{"x": 0.6820250153541565, "y": 0.34505495429039}, {"x": 0.7076222896575928, "y": 0.34549450874328613}, {"x": 0.7076222896575928, "y": 0.3498901128768921}, {"x": 0.6820250153541565, "y": 0.34945055842399597}], "text": "RECITE\n"}
{"page": 2, "bbox": [{"x": 0.18373151123523712, "y": 0.3639560341835022}, {"x": 0.20875994861125946, "y": 0.3635164797306061}, {"x": 0.20875994861125946, "y": 0.36879122257232666}, {"x": 0.18373151123523712, "y": 0.3692307770252228}], "text": "GPT-3\n"}
{"page": 2, "bbox": [{"x": 0.18145619332790375, "y": 0.3806593418121338}, {"x": 0.2133105844259262, "y": 0.3806593418121338}, {"x": 0.2133105844259262, "y": 0.3868131935596466}, {"x": 0.18145619332790375, "y": 0.3868131935596466}], "text": "2020\n"}
{"page": 2, "bbox": [{"x": 0.721274197101593, "y": 0.41274726390838623}, {"x": 0.7923777103424072, "y": 0.41274726390838623}, {"x": 0.7923777103424072, "y": 0.4175824224948883}, {"x": 0.721274197101593, "y": 0.4175824224948883}], "text": "Augmentation Stage\n"}
{"page": 2, "bbox": [{"x": 0.7423208355903625, "y": 0.4254944920539856}, {"x": 0.7718998789787292, "y": 0.4254944920539856}, {"x": 0.7718998789787292, "y": 0.4290109872817993}, {"x": 0.7423208355903625, "y": 0.4290109872817993}], "text": "Fine-tuning\n"}
{"page": 2, "bbox": [{"x": 0.7411831617355347, "y": 0.4373626410961151}, {"x": 0.7724686861038208, "y": 0.4373626410961151}, {"x": 0.7724686861038208, "y": 0.44087910652160645}, {"x": 0.7411831617355347, "y": 0.44087910652160645}], "text": "Pre-training\n"}
{"page": 2, "bbox": [{"x": 0.7445961236953735, "y": 0.44835165143013}, {"x": 0.7696245908737183, "y": 0.4487912058830261}, {"x": 0.7696245908737183, "y": 0.45230770111083984}, {"x": 0.7445961236953735, "y": 0.4518681466579437}], "text": "Inference\n"}
{"page": 2, "bbox": [{"x": 0.4004550576210022, "y": 0.4474725127220154}, {"x": 0.6126279830932617, "y": 0.44659340381622314}, {"x": 0.6126279830932617, "y": 0.4558241665363312}, {"x": 0.4004550576210022, "y": 0.4567033052444458}], "text": "Retrieval-Augmented Generation\n"}
{"page": 2, "bbox": [{"x": 0.07906711846590042, "y": 0.48307693004608154}, {"x": 0.9203640222549438, "y": 0.48307693004608154}, {"x": 0.9203640222549438, "y": 0.5261538624763489}, {"x": 0.07906711846590042, "y": 0.5261538624763489}], "text": "Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMS,\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\n"}
{"page": 2, "bbox": [{"x": 0.5091012716293335, "y": 0.5547252893447876}, {"x": 0.9209328889846802, "y": 0.5556043982505798}, {"x": 0.9209328889846802, "y": 0.5806593298912048}, {"x": 0.5091012716293335, "y": 0.5797802209854126}], "text": "faces and its future development directions. At last, the paper\nconcludes in Section VIII.\n"}
{"page": 2, "bbox": [{"x": 0.6325369477272034, "y": 0.6026373505592346}, {"x": 0.7952218651771545, "y": 0.6030769348144531}, {"x": 0.7952218651771545, "y": 0.6123076677322388}, {"x": 0.6325369477272034, "y": 0.611868143081665}], "text": "II. OVERVIEW OF RAG\n"}
{"page": 2, "bbox": [{"x": 0.095563143491745, "y": 0.5542857050895691}, {"x": 0.4920364022254944, "y": 0.5542857050895691}, {"x": 0.4920364022254944, "y": 0.7797802090644836}, {"x": 0.095563143491745, "y": 0.7797802090644836}], "text": "advanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n• We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof \"Retrieval”, “Generation\" and \"Augmentation”, and\ndelve into their synergies, elucidating how these com-\nponents intricately collaborate to form a cohesive and\neffective RAG framework.\n• We have summarized the current assessment methods of\nRAG, covering 26 tasks, nearly 50 datasets, outlining\nthe evaluation objectives and metrics, as well as the\ncurrent evaluation benchmarks and tools. Additionally,\nwe anticipate future directions for RAG, emphasizing\npotential enhancements to tackle current challenges.\n"}
{"page": 2, "bbox": [{"x": 0.5079635977745056, "y": 0.6215384602546692}, {"x": 0.9203640222549438, "y": 0.620659351348877}, {"x": 0.9209328889846802, "y": 0.8756043910980225}, {"x": 0.5085324048995972, "y": 0.8764835000038147}], "text": "A typical application of RAG is illustrated in Figure 2.\nHere, a user poses a question to ChatGPT about a recent,\nwidely discussed news. Given ChatGPT's reliance on pre-\ntraining data, it initially lacks the capacity to provide up-\ndates on recent developments. RAG bridges this information\ngap by sourcing and incorporating knowledge from external\ndatabases. In this case, it gathers relevant news articles related\nto the user's query. These articles, combined with the original\nquestion, form a comprehensive prompt that empowers LLMs\nto generate a well-informed answer.\nThe RAG research paradigm is continuously evolving, and\nwe categorize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\n"}
{"page": 2, "bbox": [{"x": 0.07906711846590042, "y": 0.795604407787323}, {"x": 0.49146756529808044, "y": 0.795604407787323}, {"x": 0.49146756529808044, "y": 0.944615364074707}, {"x": 0.07906711846590042, "y": 0.944615364074707}], "text": "The paper unfolds as follows: Section II introduces the\nmain concept and current paradigms of RAG. The following\nthree sections explore core components—“Retrieval”, “Gen-\neration\" and \"Augmentation”, respectively. Section III focuses\non optimization methods in retrieval, including indexing, query\nand embedding optimization. Section IV concentrates on post-\nretrieval process and LLM fine-tuning in generation. Section V\nanalyzes the three augmentation processes. Section VI focuses\non RAG's downstream tasks and evaluation system. Sec-\ntion VII mainly discusses the challenges that RAG currently\n"}
{"page": 2, "bbox": [{"x": 0.5085324048995972, "y": 0.8993406295776367}, {"x": 0.6063708662986755, "y": 0.8993406295776367}, {"x": 0.6063708662986755, "y": 0.9076923131942749}, {"x": 0.5085324048995972, "y": 0.9076923131942749}], "text": "A. Naive RAG\n"}
{"page": 2, "bbox": [{"x": 0.5085324048995972, "y": 0.9164835214614868}, {"x": 0.9197952151298523, "y": 0.917362630367279}, {"x": 0.9197952151298523, "y": 0.9459340572357178}, {"x": 0.5085324048995972, "y": 0.9450549483299255}], "text": "The Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n"}
{"page": 3, "bbox": [{"x": 0.9135380983352661, "y": 0.03384615480899811}, {"x": 0.9203640222549438, "y": 0.03384615480899811}, {"x": 0.9203640222549438, "y": 0.0391208790242672}, {"x": 0.9135380983352661, "y": 0.0391208790242672}], "text": "3\n"}
{"page": 3, "bbox": [{"x": 0.7656427621841431, "y": 0.08835165202617645}, {"x": 0.8236632347106934, "y": 0.08791209012269974}, {"x": 0.8236632347106934, "y": 0.0980219766497612}, {"x": 0.7656427621841431, "y": 0.09846153855323792}], "text": "Indexing\n"}
{"page": 3, "bbox": [{"x": 0.4084186553955078, "y": 0.09274725615978241}, {"x": 0.44482365250587463, "y": 0.09318681061267853}, {"x": 0.44482365250587463, "y": 0.10285714268684387}, {"x": 0.4084186553955078, "y": 0.10241758078336716}], "text": "Input\n"}
{"page": 3, "bbox": [{"x": 0.4857792854309082, "y": 0.10989011079072952}, {"x": 0.530147910118103, "y": 0.10989011079072952}, {"x": 0.530147910118103, "y": 0.11912088096141815}, {"x": 0.4857792854309082, "y": 0.11912088096141815}], "text": "Query\n"}
{"page": 3, "bbox": [{"x": 0.6518771052360535, "y": 0.12835164368152618}, {"x": 0.7013651728630066, "y": 0.12791208922863007}, {"x": 0.7013651728630066, "y": 0.13318681716918945}, {"x": 0.6518771052360535, "y": 0.13362637162208557}], "text": "Documents\n"}
{"page": 3, "bbox": [{"x": 0.2622298002243042, "y": 0.13582417368888855}, {"x": 0.29522183537483215, "y": 0.13626374304294586}, {"x": 0.29522183537483215, "y": 0.14373625814914703}, {"x": 0.2622298002243042, "y": 0.14329670369625092}], "text": "User\n"}
{"page": 3, "bbox": [{"x": 0.7389078736305237, "y": 0.14505495131015778}, {"x": 0.8100113868713379, "y": 0.1454945057630539}, {"x": 0.8100113868713379, "y": 0.1516483575105667}, {"x": 0.7389078736305237, "y": 0.1512087881565094}], "text": "Chunks Vectors\n"}
{"page": 3, "bbox": [{"x": 0.43230944871902466, "y": 0.13010989129543304}, {"x": 0.575654149055481, "y": 0.12967033684253693}, {"x": 0.5762229561805725, "y": 0.20659340918064117}, {"x": 0.4328782856464386, "y": 0.2070329636335373}], "text": "How do you evaluate the fact\nthat OpenAl's CEO, Sam Altman,\nwent through a sudden dismissal\nby the board in just three days,\nand then was rehired by the\ncompany, resembling a real-life\nversion of \"Game of Thrones\" in\nterms of power dynamics?\n"}
{"page": 3, "bbox": [{"x": 0.28725823760032654, "y": 0.16439560055732727}, {"x": 0.3356086313724518, "y": 0.16483516991138458}, {"x": 0.3356086313724518, "y": 0.17450548708438873}, {"x": 0.28725823760032654, "y": 0.17406593263149261}], "text": "Output\n"}
{"page": 3, "bbox": [{"x": 0.7281001210212708, "y": 0.17186813056468964}, {"x": 0.7923777103424072, "y": 0.17186813056468964}, {"x": 0.7923777103424072, "y": 0.17846153676509857}, {"x": 0.7281001210212708, "y": 0.17846153676509857}], "text": "embeddings\n"}
{"page": 3, "bbox": [{"x": 0.7650739550590515, "y": 0.19428572058677673}, {"x": 0.8248009085655212, "y": 0.19428572058677673}, {"x": 0.8248009085655212, "y": 0.20263735949993134}, {"x": 0.7650739550590515, "y": 0.20263735949993134}], "text": "Retrieval\n"}
{"page": 3, "bbox": [{"x": 0.20705346763134003, "y": 0.2153846174478531}, {"x": 0.2713310718536377, "y": 0.2153846174478531}, {"x": 0.2713310718536377, "y": 0.2215384542942047}, {"x": 0.20705346763134003, "y": 0.2215384542942047}], "text": "without RAG\n"}
{"page": 3, "bbox": [{"x": 0.658703088760376, "y": 0.21890109777450562}, {"x": 0.8054607510566711, "y": 0.21890109777450562}, {"x": 0.8054607510566711, "y": 0.2268131822347641}, {"x": 0.658703088760376, "y": 0.2268131822347641}], "text": "Relevant Documents\n"}
{"page": 3, "bbox": [{"x": 0.1962457299232483, "y": 0.22813187539577484}, {"x": 0.36006826162338257, "y": 0.22813187539577484}, {"x": 0.36006826162338257, "y": 0.25934067368507385}, {"x": 0.1962457299232483, "y": 0.25934067368507385}], "text": "...I am unable to provide comments on\nfuture events. Currently, I do not have\nany information regarding the dismissal\nand rehiring of OpenAl's CEO...\n"}
{"page": 3, "bbox": [{"x": 0.5307167172431946, "y": 0.24439559876918793}, {"x": 0.6063708662986755, "y": 0.24439559876918793}, {"x": 0.6063708662986755, "y": 0.2527472674846649}, {"x": 0.5307167172431946, "y": 0.2527472674846649}], "text": "Generation\n"}
{"page": 3, "bbox": [{"x": 0.4402730464935303, "y": 0.24571429193019867}, {"x": 0.46587032079696655, "y": 0.24571429193019867}, {"x": 0.46587032079696655, "y": 0.2527472674846649}, {"x": 0.4402730464935303, "y": 0.2527472674846649}], "text": "LLM\n"}
{"page": 3, "bbox": [{"x": 0.42548349499702454, "y": 0.2654944956302643}, {"x": 0.46700796484947205, "y": 0.26505494117736816}, {"x": 0.46700796484947205, "y": 0.27076923847198486}, {"x": 0.42548349499702454, "y": 0.271208792924881}], "text": "Question:\n"}
{"page": 3, "bbox": [{"x": 0.21387940645217896, "y": 0.2769230902194977}, {"x": 0.2610921561717987, "y": 0.2773626446723938}, {"x": 0.2610921561717987, "y": 0.2830769121646881}, {"x": 0.21387940645217896, "y": 0.282637357711792}], "text": "with RAG\n"}
{"page": 3, "bbox": [{"x": 0.6524459719657898, "y": 0.26989009976387024}, {"x": 0.806598424911499, "y": 0.26989009976387024}, {"x": 0.806598424911499, "y": 0.29230770468711853}, {"x": 0.6524459719657898, "y": 0.29230770468711853}], "text": "Chunk 1: \"Sam Altman Returns to\nOpenAl as CEO, Silicon Valley Drama\nResembles the 'Zhen Huan' Comedy\"\n"}
{"page": 3, "bbox": [{"x": 0.424914687871933, "y": 0.2738461494445801}, {"x": 0.5841865539550781, "y": 0.2738461494445801}, {"x": 0.5841865539550781, "y": 0.31560438871383667}, {"x": 0.424914687871933, "y": 0.31560438871383667}], "text": "How do you evaluate the fact that the\nOpenAl's CEO,..... dynamics?\nPlease answer the above questions\nbased on the following information:\nChunk 1 :\n"}
{"page": 3, "bbox": [{"x": 0.6524459719657898, "y": 0.30417582392692566}, {"x": 0.8122866749763489, "y": 0.30417582392692566}, {"x": 0.8122866749763489, "y": 0.3274725377559662}, {"x": 0.6524459719657898, "y": 0.3274725377559662}], "text": "Chunk 2: \"The Drama Concludes? Sam\nAltman to Return as CEO of OpenAI,\nBoard to Undergo Restructuring\"\n"}
{"page": 3, "bbox": [{"x": 0.19453924894332886, "y": 0.28967031836509705}, {"x": 0.361774742603302, "y": 0.2892307639122009}, {"x": 0.361774742603302, "y": 0.3468131721019745}, {"x": 0.19453924894332886, "y": 0.347252756357193}], "text": "This suggests significant internal\ndisagreements within OpenAl regarding\nthe company's future direction and\nstrategic decisions. All of these twists\nand turns reflect power struggles and\ncorporate governance issues within\nOpenAl...\n"}
{"page": 3, "bbox": [{"x": 0.424914687871933, "y": 0.3186813294887543}, {"x": 0.46700796484947205, "y": 0.3191208839416504}, {"x": 0.46700796484947205, "y": 0.3327472507953644}, {"x": 0.424914687871933, "y": 0.33230769634246826}], "text": "Chunk 2:\nChunk 3:\n"}
{"page": 3, "bbox": [{"x": 0.6530147790908813, "y": 0.3389011025428772}, {"x": 0.8009101152420044, "y": 0.3389011025428772}, {"x": 0.8009101152420044, "y": 0.360879123210907}, {"x": 0.6530147790908813, "y": 0.360879123210907}], "text": "Chunk 3: \"The Personnel Turmoil at\nOpenAI Comes to an End: Who Won\nand Who Lost?\"\n"}
{"page": 3, "bbox": [{"x": 0.447098970413208, "y": 0.34505495429039}, {"x": 0.5654152631759644, "y": 0.34637361764907837}, {"x": 0.564846396446228, "y": 0.36879122257232666}, {"x": 0.44653013348579407, "y": 0.3674725294113159}], "text": "Combine Context\nand Prompts\n"}
{"page": 3, "bbox": [{"x": 0.25142207741737366, "y": 0.35956043004989624}, {"x": 0.3060295879840851, "y": 0.35956043004989624}, {"x": 0.3060295879840851, "y": 0.3670329749584198}, {"x": 0.25142207741737366, "y": 0.3670329749584198}], "text": "Answer\n"}
{"page": 3, "bbox": [{"x": 0.07906711846590042, "y": 0.4123076796531677}, {"x": 0.9203640222549438, "y": 0.4123076796531677}, {"x": 0.9203640222549438, "y": 0.44395604729652405}, {"x": 0.07906711846590042, "y": 0.44395604729652405}], "text": "Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\n"}
{"page": 3, "bbox": [{"x": 0.5079635977745056, "y": 0.4738461673259735}, {"x": 0.9203640222549438, "y": 0.473406583070755}, {"x": 0.9209328889846802, "y": 0.7859340906143188}, {"x": 0.5085324048995972, "y": 0.7863736152648926}], "text": "Retrieval Challenges. The retrieval phase often struggles\nwith precision and recall, leading to the selection of misaligned\nor irrelevant chunks, and the missing of crucial information.\nGeneration Difficulties. In generating responses, the model\nmay face the issue of hallucination, where it produces con-\ntent not supported by the retrieved context. This phase can\nalso suffer from irrelevance, toxicity, or bias in the outputs,\ndetracting from the quality and reliability of the responses.\nAugmentation Hurdles. Integrating retrieved information\nwith the different task can be challenging, sometimes resulting\nin disjointed or incoherent outputs. The process may also\nencounter redundancy when similar information is retrieved\nfrom multiple sources, leading to repetitive responses. Deter-\nmining the significance and relevance of various passages and\nensuring stylistic and tonal consistency add further complexity.\nFacing complex issues, a single retrieval based on the original\nquery may not suffice to acquire adequate context information.\nMoreover, there's a concern that generation models might\noverly rely on augmented information, leading to outputs that\nsimply echo retrieved content without adding insightful or\nsynthesized information.\n"}
{"page": 3, "bbox": [{"x": 0.07906711846590042, "y": 0.47164836525917053}, {"x": 0.49146756529808044, "y": 0.47164836525917053}, {"x": 0.4908987581729889, "y": 0.9428571462631226}, {"x": 0.07849829643964767, "y": 0.9428571462631226}], "text": "widespread adoption of ChatGPT. The Naive RAG follows\na traditional process that includes indexing, retrieval, and\ngeneration, which is also characterized as a \"Retrieve-Read\"\nframework [7].\nIndexing starts with the cleaning and extraction of raw data\nin diverse formats like PDF, HTML, Word, and Markdown,\nwhich is then converted into a uniform plain text format. To\naccommodate the context limitations of language models, text\nis segmented into smaller, digestible chunks. Chunks are then\nencoded into vector representations using an embedding model\nand stored in vector database. This step is crucial for enabling\nefficient similarity searches in the subsequent retrieval phase.\nRetrieval. Upon receipt of a user query, the RAG system\nemploys the same encoding model utilized during the indexing\nphase to transform the query into a vector representation.\nIt then computes the similarity scores between the query\nvector and the vector of chunks within the indexed corpus.\nThe system prioritizes and retrieves the top K chunks that\ndemonstrate the greatest similarity to the query. These chunks\nare subsequently used as the expanded context in prompt.\nGeneration. The posed query and selected documents are\nsynthesized into a coherent prompt to which a large language\nmodel is tasked with formulating a response. The model's\napproach to answering may vary depending on task-specific\ncriteria, allowing it to either draw upon its inherent parametric\nknowledge or restrict its responses to the information con-\ntained within the provided documents. In cases of ongoing\ndialogues, any existing conversational history can be integrated\ninto the prompt, enabling the model to engage in multi-turn\ndialogue interactions effectively.\nHowever, Naive RAG encounters notable drawbacks:\n"}
{"page": 3, "bbox": [{"x": 0.509670078754425, "y": 0.8079121112823486}, {"x": 0.6325369477272034, "y": 0.8083516359329224}, {"x": 0.6325369477272034, "y": 0.8175824284553528}, {"x": 0.509670078754425, "y": 0.8171428442001343}], "text": "B. Advanced RAG\n"}
{"page": 3, "bbox": [{"x": 0.5091012716293335, "y": 0.8254945278167725}, {"x": 0.9220705628395081, "y": 0.8263736367225647}, {"x": 0.9215016961097717, "y": 0.9459340572357178}, {"x": 0.5085324048995972, "y": 0.9450549483299255}], "text": "Advanced RAG introduces specific improvements to over-\ncome the limitations of Naive RAG. Focusing on enhancing re-\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\ngies. To tackle the indexing issues, Advanced RAG refines\nits indexing techniques through the use of a sliding window\napproach, fine-grained segmentation, and the incorporation of\nmetadata. Additionally, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n"}
{"page": 4, "bbox": [{"x": 0.9135380983352661, "y": 0.03384615480899811}, {"x": 0.9197952151298523, "y": 0.03384615480899811}, {"x": 0.9197952151298523, "y": 0.038241758942604065}, {"x": 0.9135380983352661, "y": 0.038241758942604065}], "text": "4\n"}
{"page": 4, "bbox": [{"x": 0.5494880676269531, "y": 0.09054945409297943}, {"x": 0.5887371897697449, "y": 0.09054945409297943}, {"x": 0.5887371897697449, "y": 0.09626373648643494}, {"x": 0.5494880676269531, "y": 0.09626373648643494}], "text": "Modules\n"}
{"page": 4, "bbox": [{"x": 0.457337886095047, "y": 0.1037362664937973}, {"x": 0.4692832827568054, "y": 0.1037362664937973}, {"x": 0.4692832827568054, "y": 0.1147252768278122}, {"x": 0.457337886095047, "y": 0.1147252768278122}], "text": "M\n"}
{"page": 4, "bbox": [{"x": 0.6877133250236511, "y": 0.10813187062740326}, {"x": 0.7189988493919373, "y": 0.10813187062740326}, {"x": 0.7189988493919373, "y": 0.11340659111738205}, {"x": 0.6877133250236511, "y": 0.11340659111738205}], "text": "Search\n"}
{"page": 4, "bbox": [{"x": 0.14960181713104248, "y": 0.10285714268684387}, {"x": 0.2081911265850067, "y": 0.10329670459032059}, {"x": 0.20762230455875397, "y": 0.12923076748847961}, {"x": 0.14903299510478973, "y": 0.1287912130355835}], "text": "8 园\nUser Query\n"}
{"page": 4, "bbox": [{"x": 0.3543799817562103, "y": 0.10285714268684387}, {"x": 0.4124004542827606, "y": 0.10329670459032059}, {"x": 0.4124004542827606, "y": 0.13010989129543304}, {"x": 0.3543799817562103, "y": 0.12967033684253693}], "text": "8 园\nUser Query\n"}
{"page": 4, "bbox": [{"x": 0.4431171715259552, "y": 0.12263736128807068}, {"x": 0.49317407608032227, "y": 0.12219779938459396}, {"x": 0.49317407608032227, "y": 0.12791208922863007}, {"x": 0.4431171715259552, "y": 0.12835164368152618}], "text": "Documents\n"}
{"page": 4, "bbox": [{"x": 0.23435722291469574, "y": 0.1230769231915474}, {"x": 0.2855517566204071, "y": 0.1230769231915474}, {"x": 0.2855517566204071, "y": 0.12835164368152618}, {"x": 0.23435722291469574, "y": 0.12835164368152618}], "text": "Documents\n"}
{"page": 4, "bbox": [{"x": 0.788395881652832, "y": 0.12263736128807068}, {"x": 0.8208191394805908, "y": 0.12351648509502411}, {"x": 0.8202502727508545, "y": 0.12967033684253693}, {"x": 0.788395881652832, "y": 0.1287912130355835}], "text": "Predict\n"}
{"page": 4, "bbox": [{"x": 0.585324227809906, "y": 0.1230769231915474}, {"x": 0.6205915808677673, "y": 0.12395604699850082}, {"x": 0.6200227737426758, "y": 0.13054944574832916}, {"x": 0.5847554206848145, "y": 0.12967033684253693}], "text": "Routing\n"}
{"page": 4, "bbox": [{"x": 0.6854379773139954, "y": 0.14329670369625092}, {"x": 0.7224118113517761, "y": 0.14329670369625092}, {"x": 0.7224118113517761, "y": 0.14901098608970642}, {"x": 0.6854379773139954, "y": 0.14901098608970642}], "text": "Retrieve\n"}
{"page": 4, "bbox": [{"x": 0.3543799817562103, "y": 0.15780219435691833}, {"x": 0.4106939733028412, "y": 0.15780219435691833}, {"x": 0.4106939733028412, "y": 0.17230768501758575}, {"x": 0.3543799817562103, "y": 0.17230768501758575}], "text": "Pre-Retrieval\n• Query Routing\n"}
{"page": 4, "bbox": [{"x": 0.2377701997756958, "y": 0.1709890067577362}, {"x": 0.27588167786598206, "y": 0.1709890067577362}, {"x": 0.27588167786598206, "y": 0.17714285850524902}, {"x": 0.2377701997756958, "y": 0.17714285850524902}], "text": "Indexing\n"}
{"page": 4, "bbox": [{"x": 0.447098970413208, "y": 0.1709890067577362}, {"x": 0.4846416413784027, "y": 0.1709890067577362}, {"x": 0.4846416413784027, "y": 0.17758241295814514}, {"x": 0.447098970413208, "y": 0.17758241295814514}], "text": "Indexing\n"}
{"page": 4, "bbox": [{"x": 0.6916950941085815, "y": 0.17318680882453918}, {"x": 0.7150170803070068, "y": 0.17318680882453918}, {"x": 0.7150170803070068, "y": 0.179340660572052}, {"x": 0.6916950941085815, "y": 0.179340660572052}], "text": "RAG\n"}
{"page": 4, "bbox": [{"x": 0.6279863715171814, "y": 0.1736263781785965}, {"x": 0.6615471839904785, "y": 0.1736263781785965}, {"x": 0.6615471839904785, "y": 0.17978021502494812}, {"x": 0.6279863715171814, "y": 0.17978021502494812}], "text": "Rewrite\n"}
{"page": 4, "bbox": [{"x": 0.7480090856552124, "y": 0.17406593263149261}, {"x": 0.7798634767532349, "y": 0.17406593263149261}, {"x": 0.7798634767532349, "y": 0.179340660572052}, {"x": 0.7480090856552124, "y": 0.179340660572052}], "text": "Rerank\n"}
{"page": 4, "bbox": [{"x": 0.36348122358322144, "y": 0.17494505643844604}, {"x": 0.4004550576210022, "y": 0.17538461089134216}, {"x": 0.4004550576210022, "y": 0.179340660572052}, {"x": 0.36348122358322144, "y": 0.17890110611915588}], "text": "Query Rewriting\n"}
{"page": 4, "bbox": [{"x": 0.3577929437160492, "y": 0.18109890818595886}, {"x": 0.4015927314758301, "y": 0.18065933883190155}, {"x": 0.4015927314758301, "y": 0.1850549429655075}, {"x": 0.3577929437160492, "y": 0.18549451231956482}], "text": "⚫ Query Expansion\n"}
{"page": 4, "bbox": [{"x": 0.6922639608383179, "y": 0.2043956071138382}, {"x": 0.713879406452179, "y": 0.2043956071138382}, {"x": 0.713879406452179, "y": 0.20967033505439758}, {"x": 0.6922639608383179, "y": 0.20967033505439758}], "text": "Read\n"}
{"page": 4, "bbox": [{"x": 0.19738338887691498, "y": 0.2215384542942047}, {"x": 0.23720136284828186, "y": 0.22197802364826202}, {"x": 0.23720136284828186, "y": 0.22813187539577484}, {"x": 0.19738338887691498, "y": 0.22769230604171753}], "text": "Retrieval\n"}
{"page": 4, "bbox": [{"x": 0.40216153860092163, "y": 0.22197802364826202}, {"x": 0.4408418536186218, "y": 0.22197802364826202}, {"x": 0.4408418536186218, "y": 0.22769230604171753}, {"x": 0.40216153860092163, "y": 0.22769230604171753}], "text": "Retrieval\n"}
{"page": 4, "bbox": [{"x": 0.5728099942207336, "y": 0.2237362563610077}, {"x": 0.6325369477272034, "y": 0.224175825715065}, {"x": 0.6325369477272034, "y": 0.2298901081085205}, {"x": 0.5728099942207336, "y": 0.2294505536556244}], "text": "Demonstrate\n"}
{"page": 4, "bbox": [{"x": 0.7901023626327515, "y": 0.224175825715065}, {"x": 0.8191125988960266, "y": 0.224175825715065}, {"x": 0.8191125988960266, "y": 0.2294505536556244}, {"x": 0.7901023626327515, "y": 0.2294505536556244}], "text": "Fusion\n"}
{"page": 4, "bbox": [{"x": 0.6848691701889038, "y": 0.23956044018268585}, {"x": 0.723549485206604, "y": 0.2408791184425354}, {"x": 0.7229806780815125, "y": 0.2465934008359909}, {"x": 0.6843003630638123, "y": 0.24527472257614136}], "text": "Memory\n"}
{"page": 4, "bbox": [{"x": 0.3924914598464966, "y": 0.2641758322715759}, {"x": 0.45392492413520813, "y": 0.2641758322715759}, {"x": 0.45392492413520813, "y": 0.269010990858078}, {"x": 0.3924914598464966, "y": 0.269010990858078}], "text": "Post-Retrieval\n"}
{"page": 4, "bbox": [{"x": 0.5494880676269531, "y": 0.26725274324417114}, {"x": 0.5927190184593201, "y": 0.26725274324417114}, {"x": 0.5927190184593201, "y": 0.2725274860858917}, {"x": 0.5494880676269531, "y": 0.2725274860858917}], "text": "Patterns\n"}
{"page": 4, "bbox": [{"x": 0.3686006963253021, "y": 0.2742857038974762}, {"x": 0.3850966989994049, "y": 0.2742857038974762}, {"x": 0.3850966989994049, "y": 0.2821978032588959}, {"x": 0.3686006963253021, "y": 0.2821978032588959}], "text": "冷\n"}
{"page": 4, "bbox": [{"x": 0.36746302247047424, "y": 0.2857142984867096}, {"x": 0.3885096609592438, "y": 0.2857142984867096}, {"x": 0.3885096609592438, "y": 0.28967031836509705}, {"x": 0.36746302247047424, "y": 0.28967031836509705}], "text": "Rerank\n"}
{"page": 4, "bbox": [{"x": 0.4584755301475525, "y": 0.2857142984867096}, {"x": 0.47838452458381653, "y": 0.2857142984867096}, {"x": 0.47838452458381653, "y": 0.28967031836509705}, {"x": 0.4584755301475525, "y": 0.28967031836509705}], "text": "Fusion\n"}
{"page": 4, "bbox": [{"x": 0.40898749232292175, "y": 0.2852747142314911}, {"x": 0.43742889165878296, "y": 0.2861538529396057}, {"x": 0.43742889165878296, "y": 0.2909890115261078}, {"x": 0.40898749232292175, "y": 0.29010990262031555}], "text": "Summary\n"}
{"page": 4, "bbox": [{"x": 0.6490330100059509, "y": 0.2914285659790039}, {"x": 0.6786120533943176, "y": 0.2914285659790039}, {"x": 0.6786120533943176, "y": 0.296263724565506}, {"x": 0.6490330100059509, "y": 0.296263724565506}], "text": "Rewrite\n"}
{"page": 4, "bbox": [{"x": 0.7167235612869263, "y": 0.2918681204319}, {"x": 0.7588168382644653, "y": 0.2918681204319}, {"x": 0.7588168382644653, "y": 0.296263724565506}, {"x": 0.7167235612869263, "y": 0.296263724565506}], "text": "Demonstrate\n"}
{"page": 4, "bbox": [{"x": 0.7957906723022461, "y": 0.2918681204319}, {"x": 0.8270761966705322, "y": 0.2918681204319}, {"x": 0.8270761966705322, "y": 0.2967033088207245}, {"x": 0.7957906723022461, "y": 0.2967033088207245}], "text": "Retrieve\n"}
{"page": 4, "bbox": [{"x": 0.17918089032173157, "y": 0.29274725914001465}, {"x": 0.19055745005607605, "y": 0.29274725914001465}, {"x": 0.1899886280298233, "y": 0.30637362599372864}, {"x": 0.17861205339431763, "y": 0.30637362599372864}], "text": "All\n"}
{"page": 4, "bbox": [{"x": 0.647895336151123, "y": 0.3134065866470337}, {"x": 0.6797497272491455, "y": 0.3138461410999298}, {"x": 0.6797497272491455, "y": 0.3186813294887543}, {"x": 0.647895336151123, "y": 0.31824174523353577}], "text": "Retrieve\n"}
{"page": 4, "bbox": [{"x": 0.22980660200119019, "y": 0.3134065866470337}, {"x": 0.2827076315879822, "y": 0.3134065866470337}, {"x": 0.2827076315879822, "y": 0.3191208839416504}, {"x": 0.22980660200119019, "y": 0.3191208839416504}], "text": "Frozen LLM\n"}
{"page": 4, "bbox": [{"x": 0.8020477890968323, "y": 0.3138461410999298}, {"x": 0.8202502727508545, "y": 0.3138461410999298}, {"x": 0.8202502727508545, "y": 0.3186813294887543}, {"x": 0.8020477890968323, "y": 0.3186813294887543}], "text": "Read\n"}
{"page": 4, "bbox": [{"x": 0.57337886095047, "y": 0.3138461410999298}, {"x": 0.6046643853187561, "y": 0.3138461410999298}, {"x": 0.6046643853187561, "y": 0.3191208839416504}, {"x": 0.57337886095047, "y": 0.3191208839416504}], "text": "Retrieve\n"}
{"page": 4, "bbox": [{"x": 0.16837315261363983, "y": 0.3138461410999298}, {"x": 0.20193400979042053, "y": 0.3138461410999298}, {"x": 0.20193400979042053, "y": 0.3199999928474426}, {"x": 0.16837315261363983, "y": 0.3199999928474426}], "text": "Prompt\n"}
{"page": 4, "bbox": [{"x": 0.7241182923316956, "y": 0.3243955969810486}, {"x": 0.7508532404899597, "y": 0.3243955969810486}, {"x": 0.7508532404899597, "y": 0.32923075556755066}, {"x": 0.7241182923316956, "y": 0.32923075556755066}], "text": "Search\n"}
{"page": 4, "bbox": [{"x": 0.6490330100059509, "y": 0.3353846073150635}, {"x": 0.6769055724143982, "y": 0.3353846073150635}, {"x": 0.6769055724143982, "y": 0.34021976590156555}, {"x": 0.6490330100059509, "y": 0.34021976590156555}], "text": "Rerank\n"}
{"page": 4, "bbox": [{"x": 0.7957906723022461, "y": 0.33494505286216736}, {"x": 0.8276450634002686, "y": 0.3353846073150635}, {"x": 0.8276450634002686, "y": 0.34065935015678406}, {"x": 0.7957906723022461, "y": 0.34021976590156555}], "text": "Retrieve\n"}
{"page": 4, "bbox": [{"x": 0.4357224106788635, "y": 0.34065935015678406}, {"x": 0.48748576641082764, "y": 0.34021976590156555}, {"x": 0.48748576641082764, "y": 0.34637361764907837}, {"x": 0.4357224106788635, "y": 0.3468131721019745}], "text": "Frozen LLM\n"}
{"page": 4, "bbox": [{"x": 0.37428897619247437, "y": 0.3415384590625763}, {"x": 0.4072810113430023, "y": 0.3415384590625763}, {"x": 0.4072810113430023, "y": 0.3476923108100891}, {"x": 0.37428897619247437, "y": 0.3476923108100891}], "text": "Prompt\n"}
{"page": 4, "bbox": [{"x": 0.5796359777450562, "y": 0.35736262798309326}, {"x": 0.5984072685241699, "y": 0.35736262798309326}, {"x": 0.5984072685241699, "y": 0.3617582321166992}, {"x": 0.5796359777450562, "y": 0.3617582321166992}], "text": "Read\n"}
{"page": 4, "bbox": [{"x": 0.7241182923316956, "y": 0.35692307353019714}, {"x": 0.7519909143447876, "y": 0.35736262798309326}, {"x": 0.7519909143447876, "y": 0.36263737082481384}, {"x": 0.7241182923316956, "y": 0.3621978163719177}], "text": "Predict\n"}
{"page": 4, "bbox": [{"x": 0.8020477890968323, "y": 0.35736262798309326}, {"x": 0.8208191394805908, "y": 0.35736262798309326}, {"x": 0.8208191394805908, "y": 0.3621978163719177}, {"x": 0.8020477890968323, "y": 0.3621978163719177}], "text": "Read\n"}
{"page": 4, "bbox": [{"x": 0.6541524529457092, "y": 0.35780221223831177}, {"x": 0.6723549365997314, "y": 0.35780221223831177}, {"x": 0.6723549365997314, "y": 0.3621978163719177}, {"x": 0.6541524529457092, "y": 0.3621978163719177}], "text": "Read\n"}
{"page": 4, "bbox": [{"x": 0.7303754091262817, "y": 0.37450549006462097}, {"x": 0.7451649308204651, "y": 0.37450549006462097}, {"x": 0.7451649308204651, "y": 0.3784615397453308}, {"x": 0.7303754091262817, "y": 0.3784615397453308}], "text": "DSP\n"}
{"page": 4, "bbox": [{"x": 0.7906712293624878, "y": 0.37450549006462097}, {"x": 0.8384528160095215, "y": 0.37450549006462097}, {"x": 0.8384528160095215, "y": 0.3841758370399475}, {"x": 0.7906712293624878, "y": 0.3841758370399475}], "text": "ITER-RETGEN\n[shao et al., 2023]\n"}
{"page": 4, "bbox": [{"x": 0.635949969291687, "y": 0.3775824308395386}, {"x": 0.6916950941085815, "y": 0.3780219852924347}, {"x": 0.6916950941085815, "y": 0.3832966983318329}, {"x": 0.635949969291687, "y": 0.38285714387893677}], "text": "Advanced RAG\n"}
{"page": 4, "bbox": [{"x": 0.5699658989906311, "y": 0.3784615397453308}, {"x": 0.608077347278595, "y": 0.3784615397453308}, {"x": 0.608077347278595, "y": 0.38285714387893677}, {"x": 0.5699658989906311, "y": 0.38285714387893677}], "text": "Naive RAG\n"}
{"page": 4, "bbox": [{"x": 0.7104664444923401, "y": 0.37978023290634155}, {"x": 0.7650739550590515, "y": 0.37978023290634155}, {"x": 0.7650739550590515, "y": 0.3841758370399475}, {"x": 0.7104664444923401, "y": 0.3841758370399475}], "text": "[Khattab et al, 2022]\n"}
{"page": 4, "bbox": [{"x": 0.4067121744155884, "y": 0.38021978735923767}, {"x": 0.43856656551361084, "y": 0.38021978735923767}, {"x": 0.43856656551361084, "y": 0.3868131935596466}, {"x": 0.4067121744155884, "y": 0.3868131935596466}], "text": "Output\n"}
{"page": 4, "bbox": [{"x": 0.20136518776416779, "y": 0.3806593418121338}, {"x": 0.23321956396102905, "y": 0.3810988962650299}, {"x": 0.23321956396102905, "y": 0.3872527480125427}, {"x": 0.20136518776416779, "y": 0.3868131935596466}], "text": "Output\n"}
{"page": 4, "bbox": [{"x": 0.17747440934181213, "y": 0.40703296661376953}, {"x": 0.255972683429718, "y": 0.4065934121608734}, {"x": 0.255972683429718, "y": 0.4153846204280853}, {"x": 0.17747440934181213, "y": 0.41582417488098145}], "text": "Naive RAG\n"}
{"page": 4, "bbox": [{"x": 0.3640500605106354, "y": 0.40791207551956177}, {"x": 0.47838452458381653, "y": 0.40791207551956177}, {"x": 0.47838452458381653, "y": 0.41582417488098145}, {"x": 0.3640500605106354, "y": 0.41582417488098145}], "text": "Advanced RAG\n"}
{"page": 4, "bbox": [{"x": 0.6547212600708008, "y": 0.40791207551956177}, {"x": 0.7525597214698792, "y": 0.40791207551956177}, {"x": 0.7525597214698792, "y": 0.41582417488098145}, {"x": 0.6547212600708008, "y": 0.41582417488098145}], "text": "Modular RAG\n"}
{"page": 4, "bbox": [{"x": 0.07906711846590042, "y": 0.45494505763053894}, {"x": 0.9203640222549438, "y": 0.45494505763053894}, {"x": 0.9203640222549438, "y": 0.5094505548477173}, {"x": 0.07906711846590042, "y": 0.5094505548477173}], "text": "Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\ngeneration; it includes methods such as iterative and adaptive retrieval.\n"}
{"page": 4, "bbox": [{"x": 0.5102388858795166, "y": 0.5397801995277405}, {"x": 0.626279890537262, "y": 0.5397801995277405}, {"x": 0.626279890537262, "y": 0.5481318831443787}, {"x": 0.5102388858795166, "y": 0.5481318831443787}], "text": "C. Modular RAG\n"}
{"page": 4, "bbox": [{"x": 0.07906711846590042, "y": 0.5393406748771667}, {"x": 0.49146756529808044, "y": 0.5393406748771667}, {"x": 0.49146756529808044, "y": 0.8756043910980225}, {"x": 0.07906711846590042, "y": 0.8756043910980225}], "text": "Pre-retrieval process. In this stage, the primary focus is\non optimizing the indexing structure and the original query.\nThe goal of optimizing indexing is to enhance the quality of\nthe content being indexed. This involves strategies: enhancing\ndata granularity, optimizing index structures, adding metadata,\nalignment optimization, and mixed retrieval. While the goal\nof query optimization is to make the user's original question\nclearer and more suitable for the retrieval task. Common\nmethods include query rewriting query transformation, query\nexpansion and other techniques [7], [9]–[11].\nPost-Retrieval Process. Once relevant context is retrieved,\nit's crucial to integrate it effectively with the query. The main\nmethods in post-retrieval process include rerank chunks and\ncontext compressing. Re-ranking the retrieved information to\nrelocate the most relevant content to the edges of the prompt is\na key strategy. This concept has been implemented in frame-\nworks such as LlamaIndex², LangChain³, and HayStack [12].\nFeeding all relevant documents directly into LLMs can lead\nto information overload, diluting the focus on key details with\nirrelevant content.To mitigate this, post-retrieval efforts con-\ncentrate on selecting the essential information, emphasizing\ncritical sections, and shortening the context to be processed.\n"}
{"page": 4, "bbox": [{"x": 0.5085324048995972, "y": 0.5670329928398132}, {"x": 0.9209328889846802, "y": 0.5665934085845947}, {"x": 0.9215016961097717, "y": 0.9450549483299255}, {"x": 0.5091012716293335, "y": 0.945494532585144}], "text": "The modular RAG architecture advances beyond the for-\nmer two RAG paradigms, offering enhanced adaptability and\nversatility. It incorporates diverse strategies for improving its\ncomponents, such as adding a search module for similarity\nsearches and refining the retriever through fine-tuning. Inno-\nvations like restructured RAG modules [13] and rearranged\nRAG pipelines [14] have been introduced to tackle specific\nchallenges. The shift towards a modular RAG approach is\nbecoming prevalent, supporting both sequential processing and\nintegrated end-to-end training across its components. Despite\nits distinctiveness, Modular RAG builds upon the foundational\nprinciples of Advanced and Naive RAG, illustrating a progres-\nsion and refinement within the RAG family.\n1) New Modules: The Modular RAG framework introduces\nadditional specialized components to enhance retrieval and\nprocessing capabilities. The Search module adapts to spe-\ncific scenarios, enabling direct searches across various data\nsources like search engines, databases, and knowledge graphs,\nusing LLM-generated code and query languages [15]. RAG-\nFusion addresses traditional search limitations by employing\na multi-query strategy that expands user queries into diverse\nperspectives, utilizing parallel vector searches and intelligent\nre-ranking to uncover both explicit and transformative knowl-\nedge [16]. The Memory module leverages the LLM's memory\nto guide retrieval, creating an unbounded memory pool that\n"}
{"page": 4, "bbox": [{"x": 0.09215017408132553, "y": 0.919560432434082}, {"x": 0.24630261957645416, "y": 0.919560432434082}, {"x": 0.24630261957645416, "y": 0.944615364074707}, {"x": 0.09215017408132553, "y": 0.944615364074707}], "text": "2https://www.llamaindex.ai\nhttps://www.langchain.com/\n"}
{"page": 5, "bbox": [{"x": 0.9141069650650024, "y": 0.03384615480899811}, {"x": 0.9197952151298523, "y": 0.03384615480899811}, {"x": 0.9197952151298523, "y": 0.0391208790242672}, {"x": 0.9141069650650024, "y": 0.0391208790242672}], "text": "5\n"}
{"page": 5, "bbox": [{"x": 0.5091012716293335, "y": 0.07208791375160217}, {"x": 0.9220705628395081, "y": 0.07252747565507889}, {"x": 0.9215016961097717, "y": 0.6430768966674805}, {"x": 0.5085324048995972, "y": 0.6426373720169067}], "text": "methods for LLMS, RAG is often compared with Fine-tuning\n(FT) and prompt engineering. Each method has distinct charac-\nteristics as illustrated in Figure 4. We used a quadrant chart to\nillustrate the differences among three methods in two dimen-\nsions: external knowledge requirements and model adaption\nrequirements. Prompt engineering leverages a model's inherent\ncapabilities with minimum necessity for external knowledge\nand model adaption. RAG can be likened to providing a model\nwith a tailored textbook for information retrieval, ideal for pre-\ncise information retrieval tasks. In contrast, FT is comparable\nto a student internalizing knowledge over time, suitable for\nscenarios requiring replication of specific structures, styles, or\nformats.\nRAG excels in dynamic environments by offering real-\ntime knowledge updates and effective utilization of external\nknowledge sources with high interpretability. However, it\ncomes with higher latency and ethical considerations regarding\ndata retrieval. On the other hand, FT is more static, requiring\nretraining for updates but enabling deep customization of the\nmodel's behavior and style. It demands significant compu-\ntational resources for dataset preparation and training, and\nwhile it can reduce hallucinations, it may face challenges with\nunfamiliar data.\nIn multiple evaluations of their performance on various\nknowledge-intensive tasks across different topics, [28] re-\nvealed that while unsupervised fine-tuning shows some im-\nprovement, RAG consistently outperforms it, for both exist-\ning knowledge encountered during training and entirely new\nknowledge. Additionally, it was found that LLMs struggle\nto learn new factual information through unsupervised fine-\ntuning. The choice between RAG and FT depends on the\nspecific needs for data dynamics, customization, and com-\nputational capabilities in the application context. RAG and\nFT are not mutually exclusive and can complement each\nother, enhancing a model's capabilities at different levels.\nIn some instances, their combined use may lead to optimal\nperformance. The optimization process involving RAG and FT\nmay require multiple iterations to achieve satisfactory results.\n"}
{"page": 5, "bbox": [{"x": 0.07849829643964767, "y": 0.07208791375160217}, {"x": 0.49146756529808044, "y": 0.07208791375160217}, {"x": 0.49146756529808044, "y": 0.8826373815536499}, {"x": 0.07849829643964767, "y": 0.8826373815536499}], "text": "aligns the text more closely with data distribution through iter-\native self-enhancement [17], [18]. Routing in the RAG system\nnavigates through diverse data sources, selecting the optimal\npathway for a query, whether it involves summarization,\nspecific database searches, or merging different information\nstreams [19]. The Predict module aims to reduce redundancy\nand noise by generating context directly through the LLM,\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\nmodule tailors RAG to various downstream tasks, automating\nprompt retrieval for zero-shot inputs and creating task-specific\nretrievers through few-shot query generation [20], [21] .This\ncomprehensive approach not only streamlines the retrieval pro-\ncess but also significantly improves the quality and relevance\nof the information retrieved, catering to a wide array of tasks\nand queries with enhanced precision and flexibility.\n2) New Patterns: Modular RAG offers remarkable adapt-\nability by allowing module substitution or reconfiguration\nto address specific challenges. This goes beyond the fixed\nstructures of Naive and Advanced RAG, characterized by a\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM's capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model., improving task performance.\nSimilarly, approaches like Generate-Read [13] replace tradi-\ntional retrieval with LLM-generated content, while Recite-\nRead [22] emphasizes retrieval from model weights, enhanc-\ning the model's ability to handle knowledge-intensive tasks.\nHybrid retrieval strategies integrate keyword, semantic, and\nvector searches to cater to diverse queries. Additionally, em-\nploying sub-queries and hypothetical document embeddings\n(HYDE) [11] seeks to improve retrieval relevance by focusing\non embedding similarities between generated answers and real\ndocuments.\nAdjustments in module arrangement and interaction, such\nas the Demonstrate-Search-Predict (DSP) [23] framework\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\nRETGEN [14], showcase the dynamic use of module out-\nputs to bolster another module's functionality, illustrating a\nsophisticated understanding of enhancing module synergy.\nThe flexible orchestration of Modular RAG Flow showcases\nthe benefits of adaptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\nof retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\n"}
{"page": 5, "bbox": [{"x": 0.658703088760376, "y": 0.6632966995239258}, {"x": 0.7701933979988098, "y": 0.664175808429718}, {"x": 0.7701933979988098, "y": 0.6734066009521484}, {"x": 0.658703088760376, "y": 0.6725274920463562}], "text": "III. RETRIEVAL\n"}
{"page": 5, "bbox": [{"x": 0.5091012716293335, "y": 0.6826373338699341}, {"x": 0.9215016961097717, "y": 0.6817582249641418}, {"x": 0.9215016961097717, "y": 0.7538461685180664}, {"x": 0.5091012716293335, "y": 0.7547252774238586}], "text": "In the context of RAG, it is crucial to efficiently retrieve\nrelevant documents from the data source. There are several\nkey issues involved, such as the retrieval source, retrieval\ngranularity, pre-processing of the retrieval, and selection of\nthe corresponding embedding model.\n"}
{"page": 5, "bbox": [{"x": 0.5085324048995972, "y": 0.7784615159034729}, {"x": 0.6410694122314453, "y": 0.7784615159034729}, {"x": 0.6410694122314453, "y": 0.7868131995201111}, {"x": 0.5085324048995972, "y": 0.7868131995201111}], "text": "A. Retrieval Source\n"}
{"page": 5, "bbox": [{"x": 0.5091012716293335, "y": 0.7960439324378967}, {"x": 0.9197952151298523, "y": 0.7964835166931152}, {"x": 0.9192264080047607, "y": 0.9450549483299255}, {"x": 0.5085324048995972, "y": 0.944615364074707}], "text": "RAG relies on external knowledge to enhance LLMs, while\nthe type of retrieval source and the granularity of retrieval\nunits both affect the final generation results.\n1) Data Structure: Initially, text is s the mainstream source\nof retrieval. Subsequently, the retrieval source expanded to in-\nclude semi-structured data (PDF) and structured data (Knowl-\nedge Graph, KG) for enhancement. In addition to retrieving\nfrom original external sources, there is also a growing trend in\nrecent researches towards utilizing content generated by LLMs\nthemselves for retrieval and enhancement purposes.\n"}
{"page": 5, "bbox": [{"x": 0.07963594794273376, "y": 0.8980219960212708}, {"x": 0.23606370389461517, "y": 0.8997802138328552}, {"x": 0.23606370389461517, "y": 0.9103296995162964}, {"x": 0.07963594794273376, "y": 0.9085714221000671}], "text": "D. RAG vs Fine-tuning\n"}
{"page": 5, "bbox": [{"x": 0.07849829643964767, "y": 0.9178022146224976}, {"x": 0.489761084318161, "y": 0.9178022146224976}, {"x": 0.489761084318161, "y": 0.9450549483299255}, {"x": 0.07849829643964767, "y": 0.9450549483299255}], "text": "The augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\n"}
{"page": 6, "bbox": [{"x": 0.9135380983352661, "y": 0.03384615480899811}, {"x": 0.9203640222549438, "y": 0.03384615480899811}, {"x": 0.9203640222549438, "y": 0.0391208790242672}, {"x": 0.9135380983352661, "y": 0.0391208790242672}], "text": "6\n"}
{"page": 6, "bbox": [{"x": 0.4755403995513916, "y": 0.07472527772188187}, {"x": 0.5250284671783447, "y": 0.07472527772188187}, {"x": 0.5250284671783447, "y": 0.0813186839222908}, {"x": 0.4755403995513916, "y": 0.0813186839222908}], "text": "TABLE I\n"}
{"page": 6, "bbox": [{"x": 0.41695109009742737, "y": 0.08483516424894333}, {"x": 0.583048939704895, "y": 0.08571428805589676}, {"x": 0.583048939704895, "y": 0.09318681061267853}, {"x": 0.41695109009742737, "y": 0.0923076942563057}], "text": "SUMMARY OF RAG METHODS\n"}
{"page": 6, "bbox": [{"x": 0.20364050567150116, "y": 0.11736264079809189}, {"x": 0.24345847964286804, "y": 0.11692307889461517}, {"x": 0.24345847964286804, "y": 0.12395604699850082}, {"x": 0.20364050567150116, "y": 0.12439560145139694}], "text": "Method\n"}
{"page": 6, "bbox": [{"x": 0.36632537841796875, "y": 0.11692307889461517}, {"x": 0.4544937312602997, "y": 0.11736264079809189}, {"x": 0.4544937312602997, "y": 0.12483516335487366}, {"x": 0.36632537841796875, "y": 0.12439560145139694}], "text": "Retrieval Source\n"}
{"page": 6, "bbox": [{"x": 0.6302616596221924, "y": 0.11076922714710236}, {"x": 0.6894198060035706, "y": 0.11032967269420624}, {"x": 0.6899886131286621, "y": 0.1314285695552826}, {"x": 0.6308304667472839, "y": 0.1318681389093399}], "text": "Retrieval\nGranularity\n"}
{"page": 6, "bbox": [{"x": 0.5284414291381836, "y": 0.11120878905057907}, {"x": 0.5836177468299866, "y": 0.1120879128575325}, {"x": 0.583048939704895, "y": 0.13230769336223602}, {"x": 0.5278725624084473, "y": 0.1314285695552826}], "text": "Retrieval\nData Type\n"}
{"page": 6, "bbox": [{"x": 0.7189988493919373, "y": 0.11076922714710236}, {"x": 0.8583617806434631, "y": 0.10989011079072952}, {"x": 0.8589305877685547, "y": 0.14901098608970642}, {"x": 0.7195677161216736, "y": 0.14989010989665985}], "text": "Augmentation Retrieval\nStage\nprocess\nPre-training Iterative\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.14109890162944794}, {"x": 0.5671217441558838, "y": 0.14109890162944794}, {"x": 0.5671217441558838, "y": 0.14725275337696075}, {"x": 0.5443686246871948, "y": 0.14725275337696075}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.37770193815231323, "y": 0.1402197778224945}, {"x": 0.44141069054603577, "y": 0.1402197778224945}, {"x": 0.44141069054603577, "y": 0.1595604419708252}, {"x": 0.37770193815231323, "y": 0.1595604419708252}], "text": "Wikipedia\nFactoid Wiki\n"}
{"page": 6, "bbox": [{"x": 0.6302616596221924, "y": 0.14109890162944794}, {"x": 0.6899886131286621, "y": 0.14109890162944794}, {"x": 0.6899886131286621, "y": 0.16131867468357086}, {"x": 0.6302616596221924, "y": 0.16131867468357086}], "text": "Phrase\nProposition\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.15252746641635895}, {"x": 0.7804322838783264, "y": 0.15252746641635895}, {"x": 0.7804322838783264, "y": 0.1595604419708252}, {"x": 0.7320818901062012, "y": 0.1595604419708252}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.15296703577041626}, {"x": 0.8492605090141296, "y": 0.15296703577041626}, {"x": 0.8492605090141296, "y": 0.1595604419708252}, {"x": 0.8219567537307739, "y": 0.1595604419708252}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.15340659022331238}, {"x": 0.5676905512809753, "y": 0.15340659022331238}, {"x": 0.5676905512809753, "y": 0.1595604419708252}, {"x": 0.5449374318122864, "y": 0.1595604419708252}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.1899886280298233, "y": 0.13934065401554108}, {"x": 0.26052331924438477, "y": 0.14065934717655182}, {"x": 0.25881683826446533, "y": 0.1850549429655075}, {"x": 0.18828213214874268, "y": 0.18373626470565796}], "text": "CoG [29]\nDenseX [30]\nEAR [31]\nUPRISE [20]\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.16483516991138458}, {"x": 0.44254836440086365, "y": 0.16439560055732727}, {"x": 0.44254836440086365, "y": 0.17142857611179352}, {"x": 0.3771331012248993, "y": 0.17186813056468964}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.16483516991138458}, {"x": 0.6825938820838928, "y": 0.16439560055732727}, {"x": 0.6825938820838928, "y": 0.17142857611179352}, {"x": 0.6370875835418701, "y": 0.17186813056468964}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.1652747243642807}, {"x": 0.8492605090141296, "y": 0.1652747243642807}, {"x": 0.8492605090141296, "y": 0.17186813056468964}, {"x": 0.8219567537307739, "y": 0.17186813056468964}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.16483516991138458}, {"x": 0.5676905512809753, "y": 0.16659340262413025}, {"x": 0.5665528774261475, "y": 0.17318680882453918}, {"x": 0.5443686246871948, "y": 0.17142857611179352}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.16483516991138458}, {"x": 0.7747440338134766, "y": 0.1652747243642807}, {"x": 0.7747440338134766, "y": 0.1736263781785965}, {"x": 0.7383390069007874, "y": 0.17318680882453918}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.1762637346982956}, {"x": 0.6831626892089844, "y": 0.17582418024539948}, {"x": 0.6831626892089844, "y": 0.18329671025276184}, {"x": 0.6370875835418701, "y": 0.18373626470565796}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.17714285850524902}, {"x": 0.44368600845336914, "y": 0.17714285850524902}, {"x": 0.44368600845336914, "y": 0.18373626470565796}, {"x": 0.3771331012248993, "y": 0.18373626470565796}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.17714285850524902}, {"x": 0.5676905512809753, "y": 0.17714285850524902}, {"x": 0.5676905512809753, "y": 0.18373626470565796}, {"x": 0.5443686246871948, "y": 0.18373626470565796}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.17714285850524902}, {"x": 0.8492605090141296, "y": 0.17714285850524902}, {"x": 0.8492605090141296, "y": 0.18373626470565796}, {"x": 0.8219567537307739, "y": 0.18373626470565796}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.1762637346982956}, {"x": 0.7747440338134766, "y": 0.1767033040523529}, {"x": 0.7747440338134766, "y": 0.1850549429655075}, {"x": 0.7383390069007874, "y": 0.1846153885126114}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.18901099264621735}, {"x": 0.6831626892089844, "y": 0.18901099264621735}, {"x": 0.6831626892089844, "y": 0.1960439532995224}, {"x": 0.6370875835418701, "y": 0.1960439532995224}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.37656426429748535, "y": 0.18945054709911346}, {"x": 0.4442548453807831, "y": 0.18945054709911346}, {"x": 0.4442548453807831, "y": 0.1960439532995224}, {"x": 0.37656426429748535, "y": 0.1960439532995224}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.18989011645317078}, {"x": 0.5671217441558838, "y": 0.18989011645317078}, {"x": 0.5671217441558838, "y": 0.1960439532995224}, {"x": 0.5449374318122864, "y": 0.1960439532995224}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8225256204605103, "y": 0.18989011645317078}, {"x": 0.8492605090141296, "y": 0.18989011645317078}, {"x": 0.8492605090141296, "y": 0.1960439532995224}, {"x": 0.8225256204605103, "y": 0.1960439532995224}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.19453924894332886, "y": 0.18813186883926392}, {"x": 0.25369739532470703, "y": 0.18901099264621735}, {"x": 0.25369739532470703, "y": 0.19824175536632538}, {"x": 0.19453924894332886, "y": 0.19736263155937195}], "text": "RAST [32]\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.18901099264621735}, {"x": 0.7747440338134766, "y": 0.18945054709911346}, {"x": 0.7747440338134766, "y": 0.19780220091342926}, {"x": 0.7383390069007874, "y": 0.19736263155937195}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.20043955743312836}, {"x": 0.6825938820838928, "y": 0.20000000298023224}, {"x": 0.6825938820838928, "y": 0.2074725329875946}, {"x": 0.6370875835418701, "y": 0.20791208744049072}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.20087912678718567}, {"x": 0.8566552996635437, "y": 0.20087912678718567}, {"x": 0.8566552996635437, "y": 0.20791208744049072}, {"x": 0.8145620226860046, "y": 0.20791208744049072}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.1848691701889038, "y": 0.19956043362617493}, {"x": 0.26393628120422363, "y": 0.2013186812400818}, {"x": 0.2633674740791321, "y": 0.20967033505439758}, {"x": 0.18430034816265106, "y": 0.20791208744049072}], "text": "Self-Mem [17]\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.2013186812400818}, {"x": 0.44368600845336914, "y": 0.2013186812400818}, {"x": 0.44368600845336914, "y": 0.20791208744049072}, {"x": 0.3771331012248993, "y": 0.20791208744049072}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.2017582356929779}, {"x": 0.5676905512809753, "y": 0.2017582356929779}, {"x": 0.5676905512809753, "y": 0.20791208744049072}, {"x": 0.5449374318122864, "y": 0.20791208744049072}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.20043955743312836}, {"x": 0.7753128409385681, "y": 0.20087912678718567}, {"x": 0.7753128409385681, "y": 0.20967033505439758}, {"x": 0.7383390069007874, "y": 0.20923076570034027}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.2131868153810501}, {"x": 0.6831626892089844, "y": 0.2131868153810501}, {"x": 0.6831626892089844, "y": 0.22021977603435516}, {"x": 0.6370875835418701, "y": 0.22021977603435516}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.3435722291469574, "y": 0.21186813712120056}, {"x": 0.4778156876564026, "y": 0.2127472460269928}, {"x": 0.4778156876564026, "y": 0.22241757810115814}, {"x": 0.3435722291469574, "y": 0.2215384542942047}], "text": "Search Engine,Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.21406593918800354}, {"x": 0.5671217441558838, "y": 0.21406593918800354}, {"x": 0.5671217441558838, "y": 0.22021977603435516}, {"x": 0.5443686246871948, "y": 0.22021977603435516}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.19055745005607605, "y": 0.2127472460269928}, {"x": 0.25767919421195984, "y": 0.21362636983394623}, {"x": 0.25767919421195984, "y": 0.22197802364826202}, {"x": 0.19055745005607605, "y": 0.2210988998413086}], "text": "FLARE [24]\n"}
{"page": 6, "bbox": [{"x": 0.8122866749763489, "y": 0.2127472460269928}, {"x": 0.8589305877685547, "y": 0.2131868153810501}, {"x": 0.8589305877685547, "y": 0.22197802364826202}, {"x": 0.8122866749763489, "y": 0.2215384542942047}], "text": "Adaptive\n"}
{"page": 6, "bbox": [{"x": 0.7377701997756958, "y": 0.21362636983394623}, {"x": 0.7747440338134766, "y": 0.21362636983394623}, {"x": 0.7747440338134766, "y": 0.22197802364826202}, {"x": 0.7377701997756958, "y": 0.22197802364826202}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.22461538016796112}, {"x": 0.6825938820838928, "y": 0.22461538016796112}, {"x": 0.6825938820838928, "y": 0.2320879101753235}, {"x": 0.6370875835418701, "y": 0.2320879101753235}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.1939704269170761, "y": 0.2237362563610077}, {"x": 0.2548350393772125, "y": 0.22461538016796112}, {"x": 0.2548350393772125, "y": 0.23340658843517303}, {"x": 0.1939704269170761, "y": 0.2325274795293808}], "text": "PGRA [33]\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.22549450397491455}, {"x": 0.5676905512809753, "y": 0.22549450397491455}, {"x": 0.5676905512809753, "y": 0.23164835572242737}, {"x": 0.5449374318122864, "y": 0.23164835572242737}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.22549450397491455}, {"x": 0.849829375743866, "y": 0.22549450397491455}, {"x": 0.849829375743866, "y": 0.2320879101753235}, {"x": 0.8219567537307739, "y": 0.2320879101753235}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.22549450397491455}, {"x": 0.7804322838783264, "y": 0.22593407332897186}, {"x": 0.7804322838783264, "y": 0.2325274795293808}, {"x": 0.7320818901062012, "y": 0.2320879101753235}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.22505494952201843}, {"x": 0.43742889165878296, "y": 0.22505494952201843}, {"x": 0.43742889165878296, "y": 0.23384615778923035}, {"x": 0.3833902180194855, "y": 0.23384615778923035}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.23692308366298676}, {"x": 0.6831626892089844, "y": 0.23648351430892944}, {"x": 0.6831626892089844, "y": 0.2439560443162918}, {"x": 0.6370875835418701, "y": 0.24439559876918793}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.23736263811588287}, {"x": 0.7798634767532349, "y": 0.23692308366298676}, {"x": 0.7798634767532349, "y": 0.24439559876918793}, {"x": 0.7315130829811096, "y": 0.24483516812324524}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.237802192568779}, {"x": 0.8492605090141296, "y": 0.237802192568779}, {"x": 0.8492605090141296, "y": 0.24439559876918793}, {"x": 0.8219567537307739, "y": 0.24439559876918793}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.2382417619228363}, {"x": 0.5671217441558838, "y": 0.2382417619228363}, {"x": 0.5671217441558838, "y": 0.24439559876918793}, {"x": 0.5443686246871948, "y": 0.24439559876918793}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.19283276796340942, "y": 0.23604395985603333}, {"x": 0.2548350393772125, "y": 0.23692308366298676}, {"x": 0.2548350393772125, "y": 0.2465934008359909}, {"x": 0.19283276796340942, "y": 0.24571429193019867}], "text": "FILCO [34]\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.23692308366298676}, {"x": 0.43742889165878296, "y": 0.23736263811588287}, {"x": 0.43742889165878296, "y": 0.2465934008359909}, {"x": 0.3833902180194855, "y": 0.2461538463830948}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.2492307722568512}, {"x": 0.6825938820838928, "y": 0.24879120290279388}, {"x": 0.6825938820838928, "y": 0.25626373291015625}, {"x": 0.6370875835418701, "y": 0.25670328736305237}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.2492307722568512}, {"x": 0.5682593584060669, "y": 0.24967032670974731}, {"x": 0.5682593584060669, "y": 0.25626373291015625}, {"x": 0.5449374318122864, "y": 0.25582417845726013}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.2492307722568512}, {"x": 0.4431171715259552, "y": 0.24967032670974731}, {"x": 0.4431171715259552, "y": 0.25670328736305237}, {"x": 0.3771331012248993, "y": 0.25626373291015625}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.24967032670974731}, {"x": 0.7810011506080627, "y": 0.24967032670974731}, {"x": 0.7810011506080627, "y": 0.25626373291015625}, {"x": 0.7315130829811096, "y": 0.25626373291015625}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.24967032670974731}, {"x": 0.8492605090141296, "y": 0.24967032670974731}, {"x": 0.8492605090141296, "y": 0.25626373291015625}, {"x": 0.8219567537307739, "y": 0.25626373291015625}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.19283276796340942, "y": 0.24879120290279388}, {"x": 0.2542662024497986, "y": 0.2492307722568512}, {"x": 0.2542662024497986, "y": 0.257582426071167}, {"x": 0.19283276796340942, "y": 0.2571428716182709}], "text": "RADA [35]\n"}
{"page": 6, "bbox": [{"x": 0.3577929437160492, "y": 0.2610988914966583}, {"x": 0.46188852190971375, "y": 0.25978022813796997}, {"x": 0.46188852190971375, "y": 0.269010990858078}, {"x": 0.3577929437160492, "y": 0.27032968401908875}], "text": "Synthesized dataset\n"}
{"page": 6, "bbox": [{"x": 0.6370875835418701, "y": 0.26153847575187683}, {"x": 0.6831626892089844, "y": 0.2610988914966583}, {"x": 0.6831626892089844, "y": 0.2685714364051819}, {"x": 0.6370875835418701, "y": 0.269010990858078}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.26153847575187683}, {"x": 0.8486917018890381, "y": 0.26197803020477295}, {"x": 0.8486917018890381, "y": 0.2685714364051819}, {"x": 0.8219567537307739, "y": 0.26813188195228577}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.17861205339431763, "y": 0.2602197825908661}, {"x": 0.2701933979988098, "y": 0.26153847575187683}, {"x": 0.2701933979988098, "y": 0.27032968401908875}, {"x": 0.17861205339431763, "y": 0.269010990858078}], "text": "Filter-rerank [36]\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.26197803020477295}, {"x": 0.7804322838783264, "y": 0.26197803020477295}, {"x": 0.7804322838783264, "y": 0.2685714364051819}, {"x": 0.7320818901062012, "y": 0.2685714364051819}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.26241758465766907}, {"x": 0.5671217441558838, "y": 0.26241758465766907}, {"x": 0.5671217441558838, "y": 0.2685714364051819}, {"x": 0.5449374318122864, "y": 0.2685714364051819}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6245733499526978, "y": 0.27340659499168396}, {"x": 0.6973834037780762, "y": 0.27340659499168396}, {"x": 0.6973834037780762, "y": 0.280439555644989}, {"x": 0.6245733499526978, "y": 0.280439555644989}], "text": "Sentence Pair\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.27340659499168396}, {"x": 0.8486917018890381, "y": 0.2738461494445801}, {"x": 0.8486917018890381, "y": 0.280439555644989}, {"x": 0.8219567537307739, "y": 0.2800000011920929}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.2738461494445801}, {"x": 0.4431171715259552, "y": 0.2738461494445801}, {"x": 0.4431171715259552, "y": 0.280439555644989}, {"x": 0.3771331012248993, "y": 0.280439555644989}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.2742857038974762}, {"x": 0.5671217441558838, "y": 0.2742857038974762}, {"x": 0.5671217441558838, "y": 0.280439555644989}, {"x": 0.5449374318122864, "y": 0.280439555644989}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.19055745005607605, "y": 0.2738461494445801}, {"x": 0.25767919421195984, "y": 0.2738461494445801}, {"x": 0.25767919421195984, "y": 0.28175824880599976}, {"x": 0.19055745005607605, "y": 0.28175824880599976}], "text": "R-GQA [37]\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.2738461494445801}, {"x": 0.7747440338134766, "y": 0.2738461494445801}, {"x": 0.7747440338134766, "y": 0.2821978032588959}, {"x": 0.7383390069007874, "y": 0.2821978032588959}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.2857142984867096}, {"x": 0.7798634767532349, "y": 0.28483515977859497}, {"x": 0.7798634767532349, "y": 0.29230770468711853}, {"x": 0.7315130829811096, "y": 0.29318681359291077}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.2861538529396057}, {"x": 0.5676905512809753, "y": 0.2861538529396057}, {"x": 0.5676905512809753, "y": 0.29230770468711853}, {"x": 0.5443686246871948, "y": 0.29230770468711853}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6240045428276062, "y": 0.2857142984867096}, {"x": 0.6973834037780762, "y": 0.2857142984867096}, {"x": 0.6973834037780762, "y": 0.29274725914001465}, {"x": 0.6240045428276062, "y": 0.29274725914001465}], "text": "Sentence Pair\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.2857142984867096}, {"x": 0.8560864329338074, "y": 0.2857142984867096}, {"x": 0.8560864329338074, "y": 0.29274725914001465}, {"x": 0.8145620226860046, "y": 0.29274725914001465}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.37656426429748535, "y": 0.2861538529396057}, {"x": 0.4442548453807831, "y": 0.2861538529396057}, {"x": 0.4442548453807831, "y": 0.29274725914001465}, {"x": 0.37656426429748535, "y": 0.29274725914001465}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.19112628698349, "y": 0.28483515977859497}, {"x": 0.25654152035713196, "y": 0.2857142984867096}, {"x": 0.25654152035713196, "y": 0.2949450612068176}, {"x": 0.19112628698349, "y": 0.294065922498703}], "text": "LLM-R [38]\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.29802197217941284}, {"x": 0.44368600845336914, "y": 0.29802197217941284}, {"x": 0.44368600845336914, "y": 0.3046153783798218}, {"x": 0.3771331012248993, "y": 0.3046153783798218}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.6342434287071228, "y": 0.29802197217941284}, {"x": 0.6860068440437317, "y": 0.29802197217941284}, {"x": 0.6860068440437317, "y": 0.3046153783798218}, {"x": 0.6342434287071228, "y": 0.3046153783798218}], "text": "Item-base\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.29802197217941284}, {"x": 0.8492605090141296, "y": 0.29802197217941284}, {"x": 0.8492605090141296, "y": 0.3046153783798218}, {"x": 0.8219567537307739, "y": 0.3046153783798218}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.29846152663230896}, {"x": 0.5676905512809753, "y": 0.29846152663230896}, {"x": 0.5676905512809753, "y": 0.3046153783798218}, {"x": 0.5443686246871948, "y": 0.3046153783798218}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.725824773311615, "y": 0.2971428632736206}, {"x": 0.7878270745277405, "y": 0.29802197217941284}, {"x": 0.7878270745277405, "y": 0.3059340715408325}, {"x": 0.725824773311615, "y": 0.3050549328327179}], "text": "Pre-training\n"}
{"page": 6, "bbox": [{"x": 0.19226393103599548, "y": 0.2971428632736206}, {"x": 0.255972683429718, "y": 0.29802197217941284}, {"x": 0.255972683429718, "y": 0.30637362599372864}, {"x": 0.19226393103599548, "y": 0.3054945170879364}], "text": "TIGER [39]\n"}
{"page": 6, "bbox": [{"x": 0.6348122954368591, "y": 0.30945053696632385}, {"x": 0.6854379773139954, "y": 0.30945053696632385}, {"x": 0.6854379773139954, "y": 0.3164835274219513}, {"x": 0.6348122954368591, "y": 0.3164835274219513}], "text": "Item-base\n"}
{"page": 6, "bbox": [{"x": 0.17974971234798431, "y": 0.3085714280605316}, {"x": 0.2696245610713959, "y": 0.30901098251342773}, {"x": 0.2696245610713959, "y": 0.31780219078063965}, {"x": 0.17974971234798431, "y": 0.31736263632774353}], "text": "LM-Indexer [40]\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.30989012122154236}, {"x": 0.4442548453807831, "y": 0.30989012122154236}, {"x": 0.4442548453807831, "y": 0.3164835274219513}, {"x": 0.3771331012248993, "y": 0.3164835274219513}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.30989012122154236}, {"x": 0.8492605090141296, "y": 0.30989012122154236}, {"x": 0.8492605090141296, "y": 0.3164835274219513}, {"x": 0.8219567537307739, "y": 0.3164835274219513}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.30901098251342773}, {"x": 0.7753128409385681, "y": 0.30945053696632385}, {"x": 0.7753128409385681, "y": 0.31780219078063965}, {"x": 0.7383390069007874, "y": 0.31736263632774353}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.30989012122154236}, {"x": 0.5676905512809753, "y": 0.3103296756744385}, {"x": 0.5676905512809753, "y": 0.3169230818748474}, {"x": 0.5449374318122864, "y": 0.3164835274219513}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6348122954368591, "y": 0.3217582404613495}, {"x": 0.6848691701889038, "y": 0.32131868600845337}, {"x": 0.6848691701889038, "y": 0.3283516466617584}, {"x": 0.6348122954368591, "y": 0.32879120111465454}], "text": "Item-base\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.3221977949142456}, {"x": 0.5671217441558838, "y": 0.3221977949142456}, {"x": 0.5671217441558838, "y": 0.3283516466617584}, {"x": 0.5443686246871948, "y": 0.3283516466617584}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.3221977949142456}, {"x": 0.44368600845336914, "y": 0.3221977949142456}, {"x": 0.44368600845336914, "y": 0.32879120111465454}, {"x": 0.3771331012248993, "y": 0.32879120111465454}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.3221977949142456}, {"x": 0.8492605090141296, "y": 0.3221977949142456}, {"x": 0.8492605090141296, "y": 0.32879120111465454}, {"x": 0.8219567537307739, "y": 0.32879120111465454}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.32131868600845337}, {"x": 0.7747440338134766, "y": 0.3221977949142456}, {"x": 0.7747440338134766, "y": 0.3301098942756653}, {"x": 0.7383390069007874, "y": 0.32923075556755066}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.19283276796340942, "y": 0.3221977949142456}, {"x": 0.2548350393772125, "y": 0.3221977949142456}, {"x": 0.2548350393772125, "y": 0.32967033982276917}, {"x": 0.19283276796340942, "y": 0.32967033982276917}], "text": "BEQUE [9]\n"}
{"page": 6, "bbox": [{"x": 0.6348122954368591, "y": 0.3336263597011566}, {"x": 0.6854379773139954, "y": 0.3331868052482605}, {"x": 0.6854379773139954, "y": 0.34021976590156555}, {"x": 0.6348122954368591, "y": 0.34065935015678406}], "text": "Item-base\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.3336263597011566}, {"x": 0.5682593584060669, "y": 0.3340659439563751}, {"x": 0.5682593584060669, "y": 0.34065935015678406}, {"x": 0.5449374318122864, "y": 0.34021976590156555}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8225256204605103, "y": 0.3336263597011566}, {"x": 0.8486917018890381, "y": 0.3340659439563751}, {"x": 0.8486917018890381, "y": 0.34065935015678406}, {"x": 0.8225256204605103, "y": 0.34021976590156555}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.35836178064346313, "y": 0.3331868052482605}, {"x": 0.4624573290348053, "y": 0.33230769634246826}, {"x": 0.4624573290348053, "y": 0.3415384590625763}, {"x": 0.35836178064346313, "y": 0.34241756796836853}], "text": "Synthesized dataset\n"}
{"page": 6, "bbox": [{"x": 0.18771331012248993, "y": 0.3327472507953644}, {"x": 0.26052331924438477, "y": 0.3340659439563751}, {"x": 0.26052331924438477, "y": 0.3419780135154724}, {"x": 0.18771331012248993, "y": 0.34065935015678406}], "text": "CT-RAG [41]\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.3331868052482605}, {"x": 0.7747440338134766, "y": 0.3340659439563751}, {"x": 0.774175226688385, "y": 0.3419780135154724}, {"x": 0.7377701997756958, "y": 0.3410989046096802}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.33674630522727966, "y": 0.34549450874328613}, {"x": 0.48407280445098877, "y": 0.3441758155822754}, {"x": 0.48407280445098877, "y": 0.3529670238494873}, {"x": 0.33674630522727966, "y": 0.35428571701049805}], "text": "Wikipedia, Common Crawl\n"}
{"page": 6, "bbox": [{"x": 0.19738338887691498, "y": 0.3446153700351715}, {"x": 0.2508532404899597, "y": 0.34593406319618225}, {"x": 0.2502844035625458, "y": 0.35428571701049805}, {"x": 0.19681456685066223, "y": 0.3529670238494873}], "text": "Atlas [42]\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.34637361764907837}, {"x": 0.6780432462692261, "y": 0.34637361764907837}, {"x": 0.6780432462692261, "y": 0.3529670238494873}, {"x": 0.6427758932113647, "y": 0.3529670238494873}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.3468131721019745}, {"x": 0.5671217441558838, "y": 0.3468131721019745}, {"x": 0.5671217441558838, "y": 0.3529670238494873}, {"x": 0.5449374318122864, "y": 0.3529670238494873}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7252559661865234, "y": 0.3446153700351715}, {"x": 0.8572241067886353, "y": 0.34637361764907837}, {"x": 0.8572241067886353, "y": 0.3551648259162903}, {"x": 0.7252559661865234, "y": 0.3534066081047058}], "text": "Pre-training Iterative\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.3582417666912079}, {"x": 0.6780432462692261, "y": 0.3582417666912079}, {"x": 0.6780432462692261, "y": 0.3648351728916168}, {"x": 0.6427758932113647, "y": 0.3648351728916168}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.3582417666912079}, {"x": 0.8486917018890381, "y": 0.358681321144104}, {"x": 0.8486917018890381, "y": 0.3648351728916168}, {"x": 0.8219567537307739, "y": 0.3643956184387207}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.358681321144104}, {"x": 0.5671217441558838, "y": 0.358681321144104}, {"x": 0.5671217441558838, "y": 0.3648351728916168}, {"x": 0.5449374318122864, "y": 0.3648351728916168}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.35736262798309326}, {"x": 0.43742889165878296, "y": 0.35780221223831177}, {"x": 0.43742889165878296, "y": 0.3670329749584198}, {"x": 0.3833902180194855, "y": 0.3665934205055237}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.7252559661865234, "y": 0.35780221223831177}, {"x": 0.7878270745277405, "y": 0.3582417666912079}, {"x": 0.7878270745277405, "y": 0.3665934205055237}, {"x": 0.7252559661865234, "y": 0.3661538362503052}], "text": "Pre-training\n"}
{"page": 6, "bbox": [{"x": 0.18373151123523712, "y": 0.356483519077301}, {"x": 0.266211599111557, "y": 0.35780221223831177}, {"x": 0.26564276218414307, "y": 0.37890109419822693}, {"x": 0.18316268920898438, "y": 0.3780219852924347}], "text": "RAVEN [43]\nRETRO++ [44]\n"}
{"page": 6, "bbox": [{"x": 0.8139931559562683, "y": 0.370109885931015}, {"x": 0.8566552996635437, "y": 0.3696703314781189}, {"x": 0.8566552996635437, "y": 0.37714284658432007}, {"x": 0.8139931559562683, "y": 0.3775824308395386}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.37054944038391113}, {"x": 0.5676905512809753, "y": 0.37054944038391113}, {"x": 0.5676905512809753, "y": 0.37714284658432007}, {"x": 0.5443686246871948, "y": 0.37714284658432007}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.37054944038391113}, {"x": 0.6780432462692261, "y": 0.37054944038391113}, {"x": 0.6780432462692261, "y": 0.37714284658432007}, {"x": 0.6427758932113647, "y": 0.37714284658432007}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.7246871590614319, "y": 0.37054944038391113}, {"x": 0.7878270745277405, "y": 0.37054944038391113}, {"x": 0.7878270745277405, "y": 0.3784615397453308}, {"x": 0.7246871590614319, "y": 0.3784615397453308}], "text": "Pre-training\n"}
{"page": 6, "bbox": [{"x": 0.3577929437160492, "y": 0.370109885931015}, {"x": 0.4624573290348053, "y": 0.37054944038391113}, {"x": 0.4624573290348053, "y": 0.37978023290634155}, {"x": 0.3577929437160492, "y": 0.37934064865112305}], "text": "Pre-training Corpus\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.38197803497314453}, {"x": 0.6774743795394897, "y": 0.381538450717926}, {"x": 0.6774743795394897, "y": 0.38857144117355347}, {"x": 0.6427758932113647, "y": 0.3890109956264496}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8139931559562683, "y": 0.38197803497314453}, {"x": 0.8566552996635437, "y": 0.38197803497314453}, {"x": 0.8566552996635437, "y": 0.3890109956264496}, {"x": 0.8139931559562683, "y": 0.3890109956264496}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.725824773311615, "y": 0.381538450717926}, {"x": 0.7878270745277405, "y": 0.38197803497314453}, {"x": 0.7878270745277405, "y": 0.39032965898513794}, {"x": 0.725824773311615, "y": 0.3898901045322418}], "text": "Pre-training\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.38285714387893677}, {"x": 0.5671217441558838, "y": 0.38285714387893677}, {"x": 0.5671217441558838, "y": 0.3890109956264496}, {"x": 0.5443686246871948, "y": 0.3890109956264496}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.15813423693180084, "y": 0.381538450717926}, {"x": 0.2895335555076599, "y": 0.38197803497314453}, {"x": 0.2895335555076599, "y": 0.39076924324035645}, {"x": 0.15813423693180084, "y": 0.39032965898513794}], "text": "INSTRUCTRETRO [45]\n"}
{"page": 6, "bbox": [{"x": 0.35949942469596863, "y": 0.3810988962650299}, {"x": 0.4613196849822998, "y": 0.3832966983318329}, {"x": 0.46075084805488586, "y": 0.3916483521461487}, {"x": 0.3589306175708771, "y": 0.3894505500793457}], "text": "Pre-training corpus\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.3942857086658478}, {"x": 0.6774743795394897, "y": 0.3942857086658478}, {"x": 0.6774743795394897, "y": 0.40131866931915283}, {"x": 0.6427758932113647, "y": 0.40131866931915283}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.3947252631187439}, {"x": 0.5671217441558838, "y": 0.3947252631187439}, {"x": 0.5671217441558838, "y": 0.40131866931915283}, {"x": 0.5443686246871948, "y": 0.40131866931915283}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.3947252631187439}, {"x": 0.8492605090141296, "y": 0.3947252631187439}, {"x": 0.8492605090141296, "y": 0.40131866931915283}, {"x": 0.8219567537307739, "y": 0.40131866931915283}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.37144482135772705, "y": 0.3929670453071594}, {"x": 0.4493742883205414, "y": 0.3947252631187439}, {"x": 0.44880545139312744, "y": 0.4030769169330597}, {"x": 0.3708759844303131, "y": 0.40131866931915283}], "text": "Search Engine\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.3942857086658478}, {"x": 0.7747440338134766, "y": 0.3947252631187439}, {"x": 0.7747440338134766, "y": 0.4030769169330597}, {"x": 0.7383390069007874, "y": 0.4026373624801636}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.19169510900974274, "y": 0.39384615421295166}, {"x": 0.2582480013370514, "y": 0.3951648473739624}, {"x": 0.25767919421195984, "y": 0.4145054817199707}, {"x": 0.19112628698349, "y": 0.41318681836128235}], "text": "RRR [7]\nRA-e2e [46]\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.4061538577079773}, {"x": 0.4431171715259552, "y": 0.4061538577079773}, {"x": 0.4431171715259552, "y": 0.41318681836128235}, {"x": 0.3771331012248993, "y": 0.41318681836128235}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.4065934121608734}, {"x": 0.6780432462692261, "y": 0.4065934121608734}, {"x": 0.6780432462692261, "y": 0.41318681836128235}, {"x": 0.6427758932113647, "y": 0.41318681836128235}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.4065934121608734}, {"x": 0.8492605090141296, "y": 0.4065934121608734}, {"x": 0.8492605090141296, "y": 0.41318681836128235}, {"x": 0.8219567537307739, "y": 0.41318681836128235}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.40703296661376953}, {"x": 0.5671217441558838, "y": 0.40703296661376953}, {"x": 0.5671217441558838, "y": 0.41318681836128235}, {"x": 0.5449374318122864, "y": 0.41318681836128235}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.4065934121608734}, {"x": 0.7753128409385681, "y": 0.4065934121608734}, {"x": 0.7753128409385681, "y": 0.4149450659751892}, {"x": 0.7383390069007874, "y": 0.4149450659751892}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.39590445160865784, "y": 0.41846153140068054}, {"x": 0.42434585094451904, "y": 0.41846153140068054}, {"x": 0.42434585094451904, "y": 0.4250549376010895}, {"x": 0.39590445160865784, "y": 0.4250549376010895}], "text": "BEIR\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.41890108585357666}, {"x": 0.5671217441558838, "y": 0.41890108585357666}, {"x": 0.5671217441558838, "y": 0.4250549376010895}, {"x": 0.5443686246871948, "y": 0.4250549376010895}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.41890108585357666}, {"x": 0.6774743795394897, "y": 0.41890108585357666}, {"x": 0.6774743795394897, "y": 0.4254944920539856}, {"x": 0.6427758932113647, "y": 0.4254944920539856}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.41890108585357666}, {"x": 0.8492605090141296, "y": 0.41890108585357666}, {"x": 0.8492605090141296, "y": 0.4254944920539856}, {"x": 0.8219567537307739, "y": 0.4254944920539856}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.15984073281288147, "y": 0.4175824224948883}, {"x": 0.2883959114551544, "y": 0.41846153140068054}, {"x": 0.2883959114551544, "y": 0.42725273966789246}, {"x": 0.15984073281288147, "y": 0.4263736307621002}], "text": "PROMPTAGATOR [21]\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.41846153140068054}, {"x": 0.7747440338134766, "y": 0.41890108585357666}, {"x": 0.7747440338134766, "y": 0.42681318521499634}, {"x": 0.7383390069007874, "y": 0.4263736307621002}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.43032968044281006}, {"x": 0.8486917018890381, "y": 0.4307692348957062}, {"x": 0.8486917018890381, "y": 0.4373626410961151}, {"x": 0.8219567537307739, "y": 0.436923086643219}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.3486917018890381, "y": 0.4290109872817993}, {"x": 0.4726962447166443, "y": 0.43032968044281006}, {"x": 0.4726962447166443, "y": 0.439120888710022}, {"x": 0.3486917018890381, "y": 0.43780219554901123}], "text": "MSMARCO,Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.4307692348957062}, {"x": 0.5671217441558838, "y": 0.4307692348957062}, {"x": 0.5671217441558838, "y": 0.4373626410961151}, {"x": 0.5443686246871948, "y": 0.4373626410961151}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.4307692348957062}, {"x": 0.6780432462692261, "y": 0.4307692348957062}, {"x": 0.6780432462692261, "y": 0.4373626410961151}, {"x": 0.6427758932113647, "y": 0.4373626410961151}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.4307692348957062}, {"x": 0.7747440338134766, "y": 0.4307692348957062}, {"x": 0.7747440338134766, "y": 0.43868130445480347}, {"x": 0.7383390069007874, "y": 0.43868130445480347}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.18885096907615662, "y": 0.42989009618759155}, {"x": 0.25881683826446533, "y": 0.4307692348957062}, {"x": 0.2582480013370514, "y": 0.4514285624027252}, {"x": 0.18828213214874268, "y": 0.450549453496933}], "text": "AAR [47]\nRA-DIT [27]\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.4421977996826172}, {"x": 0.6780432462692261, "y": 0.4426373541355133}, {"x": 0.6780432462692261, "y": 0.45010989904403687}, {"x": 0.6427758932113647, "y": 0.44967031478881836}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.4426373541355133}, {"x": 0.8492605090141296, "y": 0.4430769085884094}, {"x": 0.8492605090141296, "y": 0.45010989904403687}, {"x": 0.8219567537307739, "y": 0.44967031478881836}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.33902162313461304, "y": 0.44087910652160645}, {"x": 0.4817974865436554, "y": 0.4426373541355133}, {"x": 0.4817974865436554, "y": 0.4518681466579437}, {"x": 0.33902162313461304, "y": 0.45010989904403687}], "text": "Common Crawl, Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.7377701997756958, "y": 0.4426373541355133}, {"x": 0.7753128409385681, "y": 0.44351649284362793}, {"x": 0.7747440338134766, "y": 0.4509890079498291}, {"x": 0.7372013926506042, "y": 0.45010989904403687}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.44351649284362793}, {"x": 0.5676905512809753, "y": 0.44351649284362793}, {"x": 0.5676905512809753, "y": 0.45010989904403687}, {"x": 0.5443686246871948, "y": 0.45010989904403687}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.45494505763053894}, {"x": 0.5676905512809753, "y": 0.45494505763053894}, {"x": 0.5676905512809753, "y": 0.46109890937805176}, {"x": 0.5443686246871948, "y": 0.46109890937805176}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.45494505763053894}, {"x": 0.6780432462692261, "y": 0.45494505763053894}, {"x": 0.6780432462692261, "y": 0.4615384638309479}, {"x": 0.6427758932113647, "y": 0.4615384638309479}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.45494505763053894}, {"x": 0.8492605090141296, "y": 0.45494505763053894}, {"x": 0.8492605090141296, "y": 0.4615384638309479}, {"x": 0.8219567537307739, "y": 0.4615384638309479}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.4540659487247467}, {"x": 0.7753128409385681, "y": 0.4545055031776428}, {"x": 0.7753128409385681, "y": 0.4628571569919586}, {"x": 0.7383390069007874, "y": 0.4624175727367401}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.17747440934181213, "y": 0.4540659487247467}, {"x": 0.2713310718536377, "y": 0.45494505763053894}, {"x": 0.2713310718536377, "y": 0.46329671144485474}, {"x": 0.17747440934181213, "y": 0.4624175727367401}], "text": "RAG-Robust [48]\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.4540659487247467}, {"x": 0.43742889165878296, "y": 0.4545055031776428}, {"x": 0.43742889165878296, "y": 0.46373626589775085}, {"x": 0.3833902180194855, "y": 0.46329671144485474}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.46681317687034607}, {"x": 0.4431171715259552, "y": 0.46637362241744995}, {"x": 0.4431171715259552, "y": 0.473406583070755}, {"x": 0.3771331012248993, "y": 0.4738461673259735}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.4672527611255646}, {"x": 0.5676905512809753, "y": 0.4672527611255646}, {"x": 0.5676905512809753, "y": 0.473406583070755}, {"x": 0.5443686246871948, "y": 0.473406583070755}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.46681317687034607}, {"x": 0.8492605090141296, "y": 0.4672527611255646}, {"x": 0.8492605090141296, "y": 0.4738461673259735}, {"x": 0.8219567537307739, "y": 0.473406583070755}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.4672527611255646}, {"x": 0.6780432462692261, "y": 0.4672527611255646}, {"x": 0.6780432462692261, "y": 0.4738461673259735}, {"x": 0.6427758932113647, "y": 0.4738461673259735}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.17007963359355927, "y": 0.46681317687034607}, {"x": 0.2775881588459015, "y": 0.46637362241744995}, {"x": 0.2775881588459015, "y": 0.47516483068466187}, {"x": 0.17007963359355927, "y": 0.475604385137558}], "text": "RA-Long-Form [49]\n"}
{"page": 6, "bbox": [{"x": 0.7377701997756958, "y": 0.4672527611255646}, {"x": 0.774175226688385, "y": 0.4676923155784607}, {"x": 0.774175226688385, "y": 0.475604385137558}, {"x": 0.7377701997756958, "y": 0.47516483068466187}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.19852104783058167, "y": 0.4769230782985687}, {"x": 0.24971558153629303, "y": 0.4786813259124756}, {"x": 0.24914675951004028, "y": 0.4874725341796875}, {"x": 0.19795222580432892, "y": 0.48571428656578064}], "text": "CON [50]\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.4786813259124756}, {"x": 0.6774743795394897, "y": 0.4786813259124756}, {"x": 0.6774743795394897, "y": 0.48571428656578064}, {"x": 0.6427758932113647, "y": 0.48571428656578064}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.4791208803653717}, {"x": 0.5671217441558838, "y": 0.4791208803653717}, {"x": 0.5671217441558838, "y": 0.48571428656578064}, {"x": 0.5443686246871948, "y": 0.48571428656578064}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.4791208803653717}, {"x": 0.8486917018890381, "y": 0.4791208803653717}, {"x": 0.8486917018890381, "y": 0.48571428656578064}, {"x": 0.8219567537307739, "y": 0.48571428656578064}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7377701997756958, "y": 0.47824177145957947}, {"x": 0.774175226688385, "y": 0.4791208803653717}, {"x": 0.774175226688385, "y": 0.4870329797267914}, {"x": 0.7377701997756958, "y": 0.48615384101867676}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.47824177145957947}, {"x": 0.43742889165878296, "y": 0.4791208803653717}, {"x": 0.43742889165878296, "y": 0.4879120886325836}, {"x": 0.3833902180194855, "y": 0.4870329797267914}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.49142858386039734}, {"x": 0.5671217441558838, "y": 0.49142858386039734}, {"x": 0.5671217441558838, "y": 0.49758240580558777}, {"x": 0.5443686246871948, "y": 0.49758240580558777}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.1848691701889038, "y": 0.4901098906993866}, {"x": 0.26279863715171814, "y": 0.49098899960517883}, {"x": 0.26279863715171814, "y": 0.49934065341949463}, {"x": 0.1848691701889038, "y": 0.4984615445137024}], "text": "Self-RAG [25]\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.49142858386039734}, {"x": 0.6780432462692261, "y": 0.49142858386039734}, {"x": 0.6780432462692261, "y": 0.4980219900608063}, {"x": 0.6427758932113647, "y": 0.4980219900608063}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.4905494451522827}, {"x": 0.7747440338134766, "y": 0.49142858386039734}, {"x": 0.774175226688385, "y": 0.49934065341949463}, {"x": 0.7377701997756958, "y": 0.4984615445137024}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.8122866749763489, "y": 0.49098899960517883}, {"x": 0.8577929735183716, "y": 0.49098899960517883}, {"x": 0.8577929735183716, "y": 0.49978020787239075}, {"x": 0.8122866749763489, "y": 0.49978020787239075}], "text": "Adaptive\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.49098899960517883}, {"x": 0.43742889165878296, "y": 0.49098899960517883}, {"x": 0.43742889165878296, "y": 0.5002197623252869}, {"x": 0.3833902180194855, "y": 0.5002197623252869}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.1962457299232483, "y": 0.5015384554862976}, {"x": 0.25142207741737366, "y": 0.5028571486473083}, {"x": 0.2508532404899597, "y": 0.5112087726593018}, {"x": 0.19567690789699554, "y": 0.5098901391029358}], "text": "BGM [26]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.5028571486473083}, {"x": 0.5676905512809753, "y": 0.5032967329025269}, {"x": 0.5676905512809753, "y": 0.5098901391029358}, {"x": 0.5443686246871948, "y": 0.5094505548477173}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.5028571486473083}, {"x": 0.8492605090141296, "y": 0.5032967329025269}, {"x": 0.8492605090141296, "y": 0.5098901391029358}, {"x": 0.8219567537307739, "y": 0.5094505548477173}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.5032967329025269}, {"x": 0.6774743795394897, "y": 0.5032967329025269}, {"x": 0.6774743795394897, "y": 0.5098901391029358}, {"x": 0.6427758932113647, "y": 0.5098901391029358}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.5032967329025269}, {"x": 0.7804322838783264, "y": 0.5032967329025269}, {"x": 0.7804322838783264, "y": 0.5098901391029358}, {"x": 0.7320818901062012, "y": 0.5098901391029358}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.5028571486473083}, {"x": 0.436860054731369, "y": 0.5024175643920898}, {"x": 0.436860054731369, "y": 0.5112087726593018}, {"x": 0.3833902180194855, "y": 0.5116483569145203}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.515164852142334}, {"x": 0.8566552996635437, "y": 0.515164852142334}, {"x": 0.8566552996635437, "y": 0.5221977829933167}, {"x": 0.8145620226860046, "y": 0.5221977829933167}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.5156043767929077}, {"x": 0.6780432462692261, "y": 0.5156043767929077}, {"x": 0.6780432462692261, "y": 0.5221977829933167}, {"x": 0.6427758932113647, "y": 0.5221977829933167}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.5156043767929077}, {"x": 0.7810011506080627, "y": 0.5156043767929077}, {"x": 0.7810011506080627, "y": 0.5221977829933167}, {"x": 0.7315130829811096, "y": 0.5221977829933167}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.19852104783058167, "y": 0.5147252678871155}, {"x": 0.24857792258262634, "y": 0.5142857432365417}, {"x": 0.24857792258262634, "y": 0.5230769515037537}, {"x": 0.19852104783058167, "y": 0.5235164761543274}], "text": "CoQ [51]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.5160439610481262}, {"x": 0.5671217441558838, "y": 0.5160439610481262}, {"x": 0.5671217441558838, "y": 0.5221977829933167}, {"x": 0.5443686246871948, "y": 0.5221977829933167}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.515164852142334}, {"x": 0.43742889165878296, "y": 0.515164852142334}, {"x": 0.43742889165878296, "y": 0.5239560604095459}, {"x": 0.3833902180194855, "y": 0.5239560604095459}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.5265933871269226}, {"x": 0.6774743795394897, "y": 0.5274725556373596}, {"x": 0.6774743795394897, "y": 0.5345054864883423}, {"x": 0.6427758932113647, "y": 0.53362637758255}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8225256204605103, "y": 0.5270329713821411}, {"x": 0.8492605090141296, "y": 0.5274725556373596}, {"x": 0.8492605090141296, "y": 0.5340659618377686}, {"x": 0.8225256204605103, "y": 0.53362637758255}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.1626848727464676, "y": 0.5257142782211304}, {"x": 0.286689430475235, "y": 0.5265933871269226}, {"x": 0.286689430475235, "y": 0.535824179649353}, {"x": 0.1626848727464676, "y": 0.5349450707435608}], "text": "Token-Elimination [52]\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.5270329713821411}, {"x": 0.7804322838783264, "y": 0.5274725556373596}, {"x": 0.7804322838783264, "y": 0.5345054864883423}, {"x": 0.7320818901062012, "y": 0.5340659618377686}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.5274725556373596}, {"x": 0.5676905512809753, "y": 0.5274725556373596}, {"x": 0.5676905512809753, "y": 0.5340659618377686}, {"x": 0.5443686246871948, "y": 0.5340659618377686}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.5265933871269226}, {"x": 0.4379977285861969, "y": 0.5270329713821411}, {"x": 0.4379977285861969, "y": 0.5362637639045715}, {"x": 0.3833902180194855, "y": 0.535824179649353}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.5389010906219482}, {"x": 0.7798634767532349, "y": 0.5384615659713745}, {"x": 0.7798634767532349, "y": 0.5454944968223572}, {"x": 0.7315130829811096, "y": 0.5459340810775757}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.5393406748771667}, {"x": 0.5676905512809753, "y": 0.5393406748771667}, {"x": 0.5676905512809753, "y": 0.5454944968223572}, {"x": 0.5443686246871948, "y": 0.5454944968223572}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.5389010906219482}, {"x": 0.8566552996635437, "y": 0.5389010906219482}, {"x": 0.8566552996635437, "y": 0.5459340810775757}, {"x": 0.8145620226860046, "y": 0.5459340810775757}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.5393406748771667}, {"x": 0.6780432462692261, "y": 0.5393406748771667}, {"x": 0.6780432462692261, "y": 0.5459340810775757}, {"x": 0.6427758932113647, "y": 0.5459340810775757}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.3265073895454407, "y": 0.5389010906219482}, {"x": 0.49431172013282776, "y": 0.5389010906219482}, {"x": 0.49431172013282776, "y": 0.5468131899833679}, {"x": 0.3265073895454407, "y": 0.5468131899833679}], "text": "Arxiv, Online Database, PubMed\n"}
{"page": 6, "bbox": [{"x": 0.18657565116882324, "y": 0.5393406748771667}, {"x": 0.2610921561717987, "y": 0.5393406748771667}, {"x": 0.2610921561717987, "y": 0.5476922988891602}, {"x": 0.18657565116882324, "y": 0.5476922988891602}], "text": "PaperQA [53]\n"}
{"page": 6, "bbox": [{"x": 0.37770193815231323, "y": 0.5512087941169739}, {"x": 0.4419795274734497, "y": 0.5512087941169739}, {"x": 0.4419795274734497, "y": 0.5582417845726013}, {"x": 0.37770193815231323, "y": 0.5582417845726013}], "text": "Factoid Wiki\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.5516483783721924}, {"x": 0.5671217441558838, "y": 0.5516483783721924}, {"x": 0.5671217441558838, "y": 0.5578022003173828}, {"x": 0.5449374318122864, "y": 0.5578022003173828}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.5516483783721924}, {"x": 0.7810011506080627, "y": 0.5516483783721924}, {"x": 0.7810011506080627, "y": 0.5578022003173828}, {"x": 0.7320818901062012, "y": 0.5578022003173828}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.5512087941169739}, {"x": 0.8492605090141296, "y": 0.5516483783721924}, {"x": 0.8492605090141296, "y": 0.5582417845726013}, {"x": 0.8219567537307739, "y": 0.5578022003173828}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.18259385228157043, "y": 0.5503296852111816}, {"x": 0.26678043603897095, "y": 0.5516483783721924}, {"x": 0.26678043603897095, "y": 0.5595604181289673}, {"x": 0.18259385228157043, "y": 0.5582417845726013}], "text": "NoiseRAG [54]\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.5516483783721924}, {"x": 0.6780432462692261, "y": 0.5516483783721924}, {"x": 0.6780432462692261, "y": 0.5582417845726013}, {"x": 0.6427758932113647, "y": 0.5582417845726013}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.563076913356781}, {"x": 0.6774743795394897, "y": 0.563076913356781}, {"x": 0.6774743795394897, "y": 0.5701099038124084}, {"x": 0.6427758932113647, "y": 0.5701099038124084}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.563076913356781}, {"x": 0.7810011506080627, "y": 0.563076913356781}, {"x": 0.7810011506080627, "y": 0.5701099038124084}, {"x": 0.7315130829811096, "y": 0.5701099038124084}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.19965870678424835, "y": 0.5621978044509888}, {"x": 0.2480091005563736, "y": 0.563076913356781}, {"x": 0.24744027853012085, "y": 0.5714285969734192}, {"x": 0.1990898698568344, "y": 0.5705494284629822}], "text": "IAG [55]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.5635164976119995}, {"x": 0.5671217441558838, "y": 0.5635164976119995}, {"x": 0.5671217441558838, "y": 0.5701099038124084}, {"x": 0.5443686246871948, "y": 0.5701099038124084}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.5635164976119995}, {"x": 0.8492605090141296, "y": 0.5635164976119995}, {"x": 0.8492605090141296, "y": 0.5701099038124084}, {"x": 0.8219567537307739, "y": 0.5701099038124084}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.3435722291469574, "y": 0.5617582201957703}, {"x": 0.4778156876564026, "y": 0.563076913356781}, {"x": 0.4778156876564026, "y": 0.5723077058792114}, {"x": 0.3435722291469574, "y": 0.5709890127182007}], "text": "Search Engine, Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.5758242011070251}, {"x": 0.5671217441558838, "y": 0.5758242011070251}, {"x": 0.5671217441558838, "y": 0.5819780230522156}, {"x": 0.5443686246871948, "y": 0.5819780230522156}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.17804323136806488, "y": 0.5749450325965881}, {"x": 0.2696245610713959, "y": 0.5753846168518066}, {"x": 0.2696245610713959, "y": 0.5832967162132263}, {"x": 0.17804323136806488, "y": 0.5828571319580078}], "text": "NOMIRACL [56]\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.5753846168518066}, {"x": 0.7798634767532349, "y": 0.5753846168518066}, {"x": 0.7798634767532349, "y": 0.5828571319580078}, {"x": 0.7315130829811096, "y": 0.5828571319580078}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8225256204605103, "y": 0.5753846168518066}, {"x": 0.8486917018890381, "y": 0.5762637257575989}, {"x": 0.8481228947639465, "y": 0.5828571319580078}, {"x": 0.8219567537307739, "y": 0.5819780230522156}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.5758242011070251}, {"x": 0.6774743795394897, "y": 0.5762637257575989}, {"x": 0.6774743795394897, "y": 0.5828571319580078}, {"x": 0.6427758932113647, "y": 0.5824176073074341}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.38282138109207153, "y": 0.5745055079460144}, {"x": 0.43742889165878296, "y": 0.5749450325965881}, {"x": 0.43742889165878296, "y": 0.5846154093742371}, {"x": 0.38282138109207153, "y": 0.5841758251190186}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.8100113868713379, "y": 0.5872527360916138}, {"x": 0.8600682616233826, "y": 0.58681321144104}, {"x": 0.8600682616233826, "y": 0.5938461422920227}, {"x": 0.8100113868713379, "y": 0.5942857265472412}], "text": "Recursive\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.5876923203468323}, {"x": 0.5676905512809753, "y": 0.5876923203468323}, {"x": 0.5676905512809753, "y": 0.5938461422920227}, {"x": 0.5443686246871948, "y": 0.5938461422920227}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.5872527360916138}, {"x": 0.7804322838783264, "y": 0.5876923203468323}, {"x": 0.7804322838783264, "y": 0.5942857265472412}, {"x": 0.7320818901062012, "y": 0.5938461422920227}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.2002275288105011, "y": 0.585934042930603}, {"x": 0.24857792258262634, "y": 0.5876923203468323}, {"x": 0.2480091005563736, "y": 0.5960439443588257}, {"x": 0.19965870678424835, "y": 0.5942857265472412}], "text": "ToC [57]\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.5876923203468323}, {"x": 0.6774743795394897, "y": 0.5876923203468323}, {"x": 0.6774743795394897, "y": 0.5942857265472412}, {"x": 0.6427758932113647, "y": 0.5942857265472412}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.3435722291469574, "y": 0.585934042930603}, {"x": 0.4778156876564026, "y": 0.5872527360916138}, {"x": 0.4778156876564026, "y": 0.5964835286140442}, {"x": 0.3435722291469574, "y": 0.5951648354530334}], "text": "Search Engine, Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.6000000238418579}, {"x": 0.7804322838783264, "y": 0.6000000238418579}, {"x": 0.7804322838783264, "y": 0.6061538457870483}, {"x": 0.7315130829811096, "y": 0.6061538457870483}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.3486917018890381, "y": 0.5982417464256287}, {"x": 0.4726962447166443, "y": 0.6000000238418579}, {"x": 0.4726962447166443, "y": 0.6083516478538513}, {"x": 0.3486917018890381, "y": 0.6065934300422668}], "text": "Dataset-base, Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.6000000238418579}, {"x": 0.6780432462692261, "y": 0.6000000238418579}, {"x": 0.6780432462692261, "y": 0.6065934300422668}, {"x": 0.6427758932113647, "y": 0.6065934300422668}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.1990898698568344, "y": 0.5986813306808472}, {"x": 0.24971558153629303, "y": 0.5995604395866394}, {"x": 0.24914675951004028, "y": 0.6083516478538513}, {"x": 0.19852104783058167, "y": 0.6074725389480591}], "text": "SKR [58]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.6004395484924316}, {"x": 0.5671217441558838, "y": 0.6004395484924316}, {"x": 0.5671217441558838, "y": 0.6065934300422668}, {"x": 0.5443686246871948, "y": 0.6065934300422668}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8122866749763489, "y": 0.5991208553314209}, {"x": 0.8589305877685547, "y": 0.5995604395866394}, {"x": 0.8589305877685547, "y": 0.6083516478538513}, {"x": 0.8122866749763489, "y": 0.6079120635986328}], "text": "Adaptive\n"}
{"page": 6, "bbox": [{"x": 0.1962457299232483, "y": 0.6101098656654358}, {"x": 0.25255972146987915, "y": 0.6114285588264465}, {"x": 0.2519908845424652, "y": 0.6197802424430847}, {"x": 0.1962457299232483, "y": 0.618461549282074}], "text": "ITRG [59]\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.6114285588264465}, {"x": 0.7804322838783264, "y": 0.611868143081665}, {"x": 0.7804322838783264, "y": 0.618461549282074}, {"x": 0.7320818901062012, "y": 0.6180219650268555}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.611868143081665}, {"x": 0.6780432462692261, "y": 0.611868143081665}, {"x": 0.6780432462692261, "y": 0.618461549282074}, {"x": 0.6427758932113647, "y": 0.618461549282074}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8139931559562683, "y": 0.611868143081665}, {"x": 0.8572241067886353, "y": 0.611868143081665}, {"x": 0.8572241067886353, "y": 0.618461549282074}, {"x": 0.8139931559562683, "y": 0.618461549282074}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.6123076677322388}, {"x": 0.5676905512809753, "y": 0.6123076677322388}, {"x": 0.5676905512809753, "y": 0.618461549282074}, {"x": 0.5449374318122864, "y": 0.618461549282074}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.3839590549468994, "y": 0.6105494499206543}, {"x": 0.43742889165878296, "y": 0.6114285588264465}, {"x": 0.436860054731369, "y": 0.620659351348877}, {"x": 0.3839590549468994, "y": 0.6197802424430847}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.6237362623214722}, {"x": 0.8481228947639465, "y": 0.6232966780662537}, {"x": 0.8481228947639465, "y": 0.6303296685218811}, {"x": 0.8219567537307739, "y": 0.6307692527770996}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.6237362623214722}, {"x": 0.4431171715259552, "y": 0.6232966780662537}, {"x": 0.4431171715259552, "y": 0.6307692527770996}, {"x": 0.3771331012248993, "y": 0.6312087774276733}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.6237362623214722}, {"x": 0.6774743795394897, "y": 0.6237362623214722}, {"x": 0.6774743795394897, "y": 0.6307692527770996}, {"x": 0.6427758932113647, "y": 0.6307692527770996}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.6241758465766907}, {"x": 0.5671217441558838, "y": 0.6241758465766907}, {"x": 0.5671217441558838, "y": 0.6307692527770996}, {"x": 0.5443686246871948, "y": 0.6307692527770996}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.6241758465766907}, {"x": 0.7810011506080627, "y": 0.6241758465766907}, {"x": 0.7810011506080627, "y": 0.6307692527770996}, {"x": 0.7315130829811096, "y": 0.6307692527770996}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.1615472137928009, "y": 0.6228571534156799}, {"x": 0.286689430475235, "y": 0.6237362623214722}, {"x": 0.286689430475235, "y": 0.6329670548439026}, {"x": 0.1615472137928009, "y": 0.6320878863334656}], "text": "RAG-LongContext [60]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.6360439658164978}, {"x": 0.5671217441558838, "y": 0.6360439658164978}, {"x": 0.5671217441558838, "y": 0.6421977877616882}, {"x": 0.5443686246871948, "y": 0.6421977877616882}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.6360439658164978}, {"x": 0.7810011506080627, "y": 0.6360439658164978}, {"x": 0.7810011506080627, "y": 0.6421977877616882}, {"x": 0.7320818901062012, "y": 0.6421977877616882}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.6360439658164978}, {"x": 0.6780432462692261, "y": 0.6360439658164978}, {"x": 0.6780432462692261, "y": 0.6426373720169067}, {"x": 0.6427758932113647, "y": 0.6426373720169067}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.8139931559562683, "y": 0.6360439658164978}, {"x": 0.8572241067886353, "y": 0.6360439658164978}, {"x": 0.8572241067886353, "y": 0.6426373720169067}, {"x": 0.8139931559562683, "y": 0.6426373720169067}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.6356043815612793}, {"x": 0.436860054731369, "y": 0.6356043815612793}, {"x": 0.436860054731369, "y": 0.6443955898284912}, {"x": 0.3833902180194855, "y": 0.6443955898284912}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.16894197463989258, "y": 0.6351648569107056}, {"x": 0.27986347675323486, "y": 0.6360439658164978}, {"x": 0.27986347675323486, "y": 0.6448351740837097}, {"x": 0.16894197463989258, "y": 0.6439560651779175}], "text": "ITER-RETGEN [14]\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.6479120850563049}, {"x": 0.6774743795394897, "y": 0.6479120850563049}, {"x": 0.6774743795394897, "y": 0.6549450755119324}, {"x": 0.6427758932113647, "y": 0.6549450755119324}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.19283276796340942, "y": 0.6465933918952942}, {"x": 0.25540387630462646, "y": 0.6479120850563049}, {"x": 0.25540387630462646, "y": 0.6567032933235168}, {"x": 0.19283276796340942, "y": 0.6553846001625061}], "text": "IRCOT [61]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.6483516693115234}, {"x": 0.5671217441558838, "y": 0.6483516693115234}, {"x": 0.5671217441558838, "y": 0.6549450755119324}, {"x": 0.5443686246871948, "y": 0.6549450755119324}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.6483516693115234}, {"x": 0.7810011506080627, "y": 0.6483516693115234}, {"x": 0.7810011506080627, "y": 0.6549450755119324}, {"x": 0.7320818901062012, "y": 0.6549450755119324}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8100113868713379, "y": 0.6483516693115234}, {"x": 0.861774742603302, "y": 0.6483516693115234}, {"x": 0.861774742603302, "y": 0.6549450755119324}, {"x": 0.8100113868713379, "y": 0.6549450755119324}], "text": "Recursive\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.6483516693115234}, {"x": 0.43742889165878296, "y": 0.6483516693115234}, {"x": 0.43742889165878296, "y": 0.6567032933235168}, {"x": 0.3833902180194855, "y": 0.6567032933235168}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.6602197885513306}, {"x": 0.5671217441558838, "y": 0.6602197885513306}, {"x": 0.5671217441558838, "y": 0.666373610496521}, {"x": 0.5443686246871948, "y": 0.666373610496521}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.6602197885513306}, {"x": 0.6780432462692261, "y": 0.6602197885513306}, {"x": 0.6780432462692261, "y": 0.6668131947517395}, {"x": 0.6427758932113647, "y": 0.6668131947517395}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.13765642046928406, "y": 0.6593406796455383}, {"x": 0.3100113868713379, "y": 0.6597802042961121}, {"x": 0.3100113868713379, "y": 0.6681318879127502}, {"x": 0.13765642046928406, "y": 0.6676923036575317}], "text": "LLM-Knowledge-Boundary [62]\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.6606593132019043}, {"x": 0.7810011506080627, "y": 0.6606593132019043}, {"x": 0.7810011506080627, "y": 0.6672527194023132}, {"x": 0.7320818901062012, "y": 0.6672527194023132}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.6589010953903198}, {"x": 0.43742889165878296, "y": 0.6597802042961121}, {"x": 0.43742889165878296, "y": 0.6690109968185425}, {"x": 0.3833902180194855, "y": 0.6681318879127502}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.6720879077911377}, {"x": 0.4431171715259552, "y": 0.6716483235359192}, {"x": 0.4431171715259552, "y": 0.6786813139915466}, {"x": 0.3771331012248993, "y": 0.6791208982467651}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.6720879077911377}, {"x": 0.7792946696281433, "y": 0.6716483235359192}, {"x": 0.7792946696281433, "y": 0.6791208982467651}, {"x": 0.7315130829811096, "y": 0.6795604228973389}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.6720879077911377}, {"x": 0.6774743795394897, "y": 0.6720879077911377}, {"x": 0.6774743795394897, "y": 0.6791208982467651}, {"x": 0.6427758932113647, "y": 0.6791208982467651}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.6725274920463562}, {"x": 0.5671217441558838, "y": 0.6725274920463562}, {"x": 0.5671217441558838, "y": 0.6791208982467651}, {"x": 0.5443686246871948, "y": 0.6791208982467651}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.1860068291425705, "y": 0.670769214630127}, {"x": 0.26279863715171814, "y": 0.6716483235359192}, {"x": 0.2622298002243042, "y": 0.6927472352981567}, {"x": 0.18543799221515656, "y": 0.6918681263923645}], "text": "RAPTOR [63]\nRECITE [22]\n"}
{"page": 6, "bbox": [{"x": 0.8094425201416016, "y": 0.6602197885513306}, {"x": 0.8612059354782104, "y": 0.6602197885513306}, {"x": 0.8612059354782104, "y": 0.7032967209815979}, {"x": 0.8094425201416016, "y": 0.7032967209815979}], "text": "Once\nRecursive\nOnce\nIterative\n"}
{"page": 6, "bbox": [{"x": 0.6433447003364563, "y": 0.6835165023803711}, {"x": 0.6769055724143982, "y": 0.6839560270309448}, {"x": 0.6769055724143982, "y": 0.6909890174865723}, {"x": 0.6433447003364563, "y": 0.6905494332313538}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.6843956112861633}, {"x": 0.7810011506080627, "y": 0.6843956112861633}, {"x": 0.7810011506080627, "y": 0.6905494332313538}, {"x": 0.7320818901062012, "y": 0.6905494332313538}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.39362913370132446, "y": 0.6843956112861633}, {"x": 0.42718997597694397, "y": 0.6848351359367371}, {"x": 0.42718997597694397, "y": 0.6909890174865723}, {"x": 0.39362913370132446, "y": 0.6905494332313538}], "text": "LLMs\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.6843956112861633}, {"x": 0.5671217441558838, "y": 0.6843956112861633}, {"x": 0.5671217441558838, "y": 0.6909890174865723}, {"x": 0.5443686246871948, "y": 0.6909890174865723}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.6962637305259705}, {"x": 0.7798634767532349, "y": 0.695824146270752}, {"x": 0.7798634767532349, "y": 0.7028571367263794}, {"x": 0.7320818901062012, "y": 0.7032967209815979}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.37144482135772705, "y": 0.6953846216201782}, {"x": 0.4493742883205414, "y": 0.6962637305259705}, {"x": 0.4493742883205414, "y": 0.7041758298873901}, {"x": 0.37144482135772705, "y": 0.7032967209815979}], "text": "Pile, Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.696703314781189}, {"x": 0.6780432462692261, "y": 0.696703314781189}, {"x": 0.6780432462692261, "y": 0.7032967209815979}, {"x": 0.6427758932113647, "y": 0.7032967209815979}], "text": "Chunk\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.696703314781189}, {"x": 0.5671217441558838, "y": 0.6971428394317627}, {"x": 0.5671217441558838, "y": 0.7037362456321716}, {"x": 0.5443686246871948, "y": 0.7032967209815979}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.18543799221515656, "y": 0.6953846216201782}, {"x": 0.26279863715171814, "y": 0.6962637305259705}, {"x": 0.26279863715171814, "y": 0.7054945230484009}, {"x": 0.18543799221515656, "y": 0.7046154141426086}], "text": "ICRALM [64]\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.7085714340209961}, {"x": 0.5676905512809753, "y": 0.7085714340209961}, {"x": 0.5676905512809753, "y": 0.7147252559661865}, {"x": 0.5449374318122864, "y": 0.7147252559661865}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.37656426429748535, "y": 0.7085714340209961}, {"x": 0.4442548453807831, "y": 0.7085714340209961}, {"x": 0.4442548453807831, "y": 0.715164840221405}, {"x": 0.37656426429748535, "y": 0.715164840221405}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.7085714340209961}, {"x": 0.8486917018890381, "y": 0.7085714340209961}, {"x": 0.8486917018890381, "y": 0.715164840221405}, {"x": 0.8219567537307739, "y": 0.715164840221405}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.15585893392562866, "y": 0.7072527408599854}, {"x": 0.2918088734149933, "y": 0.7081318497657776}, {"x": 0.2918088734149933, "y": 0.7169230580329895}, {"x": 0.15585893392562866, "y": 0.7160439491271973}], "text": "Retrieve-and-Sample [65]\n"}
{"page": 6, "bbox": [{"x": 0.6496018171310425, "y": 0.7090110182762146}, {"x": 0.6712172627449036, "y": 0.7090110182762146}, {"x": 0.6712172627449036, "y": 0.715164840221405}, {"x": 0.6496018171310425, "y": 0.715164840221405}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.7377701997756958, "y": 0.7085714340209961}, {"x": 0.7747440338134766, "y": 0.7085714340209961}, {"x": 0.7747440338134766, "y": 0.7164835333824158}, {"x": 0.7377701997756958, "y": 0.7164835333824158}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.19738338887691498, "y": 0.7186813354492188}, {"x": 0.25142207741737366, "y": 0.7204395532608032}, {"x": 0.2508532404899597, "y": 0.7292307615280151}, {"x": 0.19681456685066223, "y": 0.7274725437164307}], "text": "Zemi [66]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.7200000286102295}, {"x": 0.5682593584060669, "y": 0.7213186621665955}, {"x": 0.5676905512809753, "y": 0.7279120683670044}, {"x": 0.5437997579574585, "y": 0.7265934348106384}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6496018171310425, "y": 0.7204395532608032}, {"x": 0.6717861294746399, "y": 0.7208791375160217}, {"x": 0.6717861294746399, "y": 0.7274725437164307}, {"x": 0.6496018171310425, "y": 0.7270329594612122}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.40273037552833557, "y": 0.7208791375160217}, {"x": 0.4186575710773468, "y": 0.7208791375160217}, {"x": 0.4186575710773468, "y": 0.7274725437164307}, {"x": 0.40273037552833557, "y": 0.7274725437164307}], "text": "C4\n"}
{"page": 6, "bbox": [{"x": 0.8225256204605103, "y": 0.7208791375160217}, {"x": 0.8492605090141296, "y": 0.7208791375160217}, {"x": 0.8492605090141296, "y": 0.7274725437164307}, {"x": 0.8225256204605103, "y": 0.7274725437164307}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.7208791375160217}, {"x": 0.7747440338134766, "y": 0.7208791375160217}, {"x": 0.7747440338134766, "y": 0.7287912368774414}, {"x": 0.7383390069007874, "y": 0.7287912368774414}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.3953356146812439, "y": 0.7327472567558289}, {"x": 0.42548349499702454, "y": 0.7327472567558289}, {"x": 0.42548349499702454, "y": 0.7389010787010193}, {"x": 0.3953356146812439, "y": 0.7389010787010193}], "text": "Arxiv\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.7323076725006104}, {"x": 0.7810011506080627, "y": 0.7327472567558289}, {"x": 0.7810011506080627, "y": 0.7393406629562378}, {"x": 0.7320818901062012, "y": 0.7389010787010193}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.19340158998966217, "y": 0.7314285635948181}, {"x": 0.25540387630462646, "y": 0.7327472567558289}, {"x": 0.2548350393772125, "y": 0.7406593561172485}, {"x": 0.19283276796340942, "y": 0.7393406629562378}], "text": "CRAG [67]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.7327472567558289}, {"x": 0.5671217441558838, "y": 0.7327472567558289}, {"x": 0.5671217441558838, "y": 0.7393406629562378}, {"x": 0.5443686246871948, "y": 0.7393406629562378}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.7327472567558289}, {"x": 0.8492605090141296, "y": 0.7327472567558289}, {"x": 0.8492605090141296, "y": 0.7393406629562378}, {"x": 0.8219567537307739, "y": 0.7393406629562378}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.6496018171310425, "y": 0.7331868410110474}, {"x": 0.6717861294746399, "y": 0.7331868410110474}, {"x": 0.6717861294746399, "y": 0.7393406629562378}, {"x": 0.6496018171310425, "y": 0.7393406629562378}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.744615375995636}, {"x": 0.5671217441558838, "y": 0.744615375995636}, {"x": 0.5671217441558838, "y": 0.7507692575454712}, {"x": 0.5443686246871948, "y": 0.7507692575454712}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.7441758513450623}, {"x": 0.7804322838783264, "y": 0.7437362670898438}, {"x": 0.7804322838783264, "y": 0.7512087821960449}, {"x": 0.7315130829811096, "y": 0.7516483664512634}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.744615375995636}, {"x": 0.8572241067886353, "y": 0.744615375995636}, {"x": 0.8572241067886353, "y": 0.7512087821960449}, {"x": 0.8145620226860046, "y": 0.7512087821960449}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.6496018171310425, "y": 0.7450549602508545}, {"x": 0.6717861294746399, "y": 0.7450549602508545}, {"x": 0.6717861294746399, "y": 0.7512087821960449}, {"x": 0.6496018171310425, "y": 0.7512087821960449}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.1860068291425705, "y": 0.7437362670898438}, {"x": 0.26279863715171814, "y": 0.744615375995636}, {"x": 0.26279863715171814, "y": 0.7529670596122742}, {"x": 0.1860068291425705, "y": 0.7520878911018372}], "text": "1-PAGER [68]\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.7437362670898438}, {"x": 0.43742889165878296, "y": 0.7441758513450623}, {"x": 0.43742889165878296, "y": 0.7534065842628479}, {"x": 0.3833902180194855, "y": 0.7529670596122742}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.7564834952354431}, {"x": 0.4431171715259552, "y": 0.7556043863296509}, {"x": 0.4431171715259552, "y": 0.763076901435852}, {"x": 0.3771331012248993, "y": 0.7639560699462891}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.7564834952354431}, {"x": 0.8481228947639465, "y": 0.7560439705848694}, {"x": 0.8481228947639465, "y": 0.763076901435852}, {"x": 0.8219567537307739, "y": 0.7635164856910706}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.7564834952354431}, {"x": 0.7792946696281433, "y": 0.7560439705848694}, {"x": 0.7792946696281433, "y": 0.7635164856910706}, {"x": 0.7315130829811096, "y": 0.7639560699462891}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.7569230794906616}, {"x": 0.5671217441558838, "y": 0.7569230794906616}, {"x": 0.5671217441558838, "y": 0.7635164856910706}, {"x": 0.5443686246871948, "y": 0.7635164856910706}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.19453924894332886, "y": 0.7556043863296509}, {"x": 0.25369739532470703, "y": 0.7564834952354431}, {"x": 0.25369739532470703, "y": 0.765274703502655}, {"x": 0.19453924894332886, "y": 0.7643955945968628}], "text": "PRCA [69]\n"}
{"page": 6, "bbox": [{"x": 0.6490330100059509, "y": 0.7573626637458801}, {"x": 0.6712172627449036, "y": 0.7573626637458801}, {"x": 0.6712172627449036, "y": 0.7635164856910706}, {"x": 0.6490330100059509, "y": 0.7635164856910706}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.7687911987304688}, {"x": 0.7810011506080627, "y": 0.7687911987304688}, {"x": 0.7810011506080627, "y": 0.774945080280304}, {"x": 0.7320818901062012, "y": 0.774945080280304}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.7687911987304688}, {"x": 0.4442548453807831, "y": 0.7687911987304688}, {"x": 0.4442548453807831, "y": 0.7753846049308777}, {"x": 0.3771331012248993, "y": 0.7753846049308777}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.7687911987304688}, {"x": 0.5671217441558838, "y": 0.7687911987304688}, {"x": 0.5671217441558838, "y": 0.7753846049308777}, {"x": 0.5443686246871948, "y": 0.7753846049308777}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.7687911987304688}, {"x": 0.8492605090141296, "y": 0.7687911987304688}, {"x": 0.8492605090141296, "y": 0.7753846049308777}, {"x": 0.8219567537307739, "y": 0.7753846049308777}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.1615472137928009, "y": 0.767472505569458}, {"x": 0.286689430475235, "y": 0.7679120898246765}, {"x": 0.286689430475235, "y": 0.7771428823471069}, {"x": 0.1615472137928009, "y": 0.7767032980918884}], "text": "QLM-Doc-ranking [70]\n"}
{"page": 6, "bbox": [{"x": 0.6490330100059509, "y": 0.7692307829856873}, {"x": 0.6717861294746399, "y": 0.7692307829856873}, {"x": 0.6717861294746399, "y": 0.7753846049308777}, {"x": 0.6490330100059509, "y": 0.7753846049308777}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.7806593179702759}, {"x": 0.7798634767532349, "y": 0.7806593179702759}, {"x": 0.7798634767532349, "y": 0.7876923084259033}, {"x": 0.7320818901062012, "y": 0.7876923084259033}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.7810989022254944}, {"x": 0.8486917018890381, "y": 0.7810989022254944}, {"x": 0.8486917018890381, "y": 0.7872527241706848}, {"x": 0.8219567537307739, "y": 0.7872527241706848}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.6490330100059509, "y": 0.7810989022254944}, {"x": 0.670648455619812, "y": 0.7810989022254944}, {"x": 0.670648455619812, "y": 0.7876923084259033}, {"x": 0.6490330100059509, "y": 0.7876923084259033}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.7810989022254944}, {"x": 0.5671217441558838, "y": 0.7815384864807129}, {"x": 0.5671217441558838, "y": 0.7881318926811218}, {"x": 0.5443686246871948, "y": 0.7876923084259033}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.18885096907615662, "y": 0.7810989022254944}, {"x": 0.25881683826446533, "y": 0.7806593179702759}, {"x": 0.25881683826446533, "y": 0.7890110015869141}, {"x": 0.18885096907615662, "y": 0.7894505262374878}], "text": "Recomp [71]\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.7810989022254944}, {"x": 0.436860054731369, "y": 0.7810989022254944}, {"x": 0.436860054731369, "y": 0.7898901104927063}, {"x": 0.3833902180194855, "y": 0.7898901104927063}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.7929670214653015}, {"x": 0.5676905512809753, "y": 0.7929670214653015}, {"x": 0.5676905512809753, "y": 0.7991209030151367}, {"x": 0.5449374318122864, "y": 0.7991209030151367}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.7925274968147278}, {"x": 0.7798634767532349, "y": 0.7925274968147278}, {"x": 0.7798634767532349, "y": 0.7995604276657104}, {"x": 0.7320818901062012, "y": 0.7995604276657104}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.7925274968147278}, {"x": 0.8572241067886353, "y": 0.7925274968147278}, {"x": 0.8572241067886353, "y": 0.7995604276657104}, {"x": 0.8145620226860046, "y": 0.7995604276657104}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.1990898698568344, "y": 0.7920879125595093}, {"x": 0.2480091005563736, "y": 0.7925274968147278}, {"x": 0.2480091005563736, "y": 0.8004395365715027}, {"x": 0.1990898698568344, "y": 0.800000011920929}], "text": "DSP [23]\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.7916483283042908}, {"x": 0.43742889165878296, "y": 0.7925274968147278}, {"x": 0.43742889165878296, "y": 0.8013187050819397}, {"x": 0.3833902180194855, "y": 0.8004395365715027}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.6496018171310425, "y": 0.79340660572052}, {"x": 0.6712172627449036, "y": 0.79340660572052}, {"x": 0.6712172627449036, "y": 0.7995604276657104}, {"x": 0.6496018171310425, "y": 0.7995604276657104}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.8048351407051086}, {"x": 0.7798634767532349, "y": 0.8043956160545349}, {"x": 0.7798634767532349, "y": 0.8114285469055176}, {"x": 0.7315130829811096, "y": 0.8118681311607361}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.4004550576210022, "y": 0.8039560317993164}, {"x": 0.42036405205726624, "y": 0.8052747249603271}, {"x": 0.41922640800476074, "y": 0.8127472400665283}, {"x": 0.3993174135684967, "y": 0.8114285469055176}], "text": "Pile\n"}
{"page": 6, "bbox": [{"x": 0.6496018171310425, "y": 0.8048351407051086}, {"x": 0.6712172627449036, "y": 0.8052747249603271}, {"x": 0.670648455619812, "y": 0.8118681311607361}, {"x": 0.6490330100059509, "y": 0.8114285469055176}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.18657565116882324, "y": 0.8035165071487427}, {"x": 0.26166099309921265, "y": 0.8043956160545349}, {"x": 0.26166099309921265, "y": 0.8131868243217468}, {"x": 0.18657565116882324, "y": 0.8123077154159546}], "text": "RePLUG [72]\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.8043956160545349}, {"x": 0.5676905512809753, "y": 0.8057143092155457}, {"x": 0.5671217441558838, "y": 0.8123077154159546}, {"x": 0.5443686246871948, "y": 0.8109890222549438}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.8052747249603271}, {"x": 0.8492605090141296, "y": 0.8052747249603271}, {"x": 0.8492605090141296, "y": 0.8118681311607361}, {"x": 0.8219567537307739, "y": 0.8118681311607361}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.8167033195495605}, {"x": 0.4431171715259552, "y": 0.816263735294342}, {"x": 0.4431171715259552, "y": 0.8232967257499695}, {"x": 0.3771331012248993, "y": 0.8237362504005432}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.8139931559562683, "y": 0.8167033195495605}, {"x": 0.8560864329338074, "y": 0.8158241510391235}, {"x": 0.8560864329338074, "y": 0.8232967257499695}, {"x": 0.8139931559562683, "y": 0.8241758346557617}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.18031854927539825, "y": 0.8158241510391235}, {"x": 0.26791808009147644, "y": 0.8167033195495605}, {"x": 0.26791808009147644, "y": 0.8246153593063354}, {"x": 0.18031854927539825, "y": 0.8237362504005432}], "text": "ARM-RAG [73]\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.8167033195495605}, {"x": 0.7810011506080627, "y": 0.8171428442001343}, {"x": 0.7810011506080627, "y": 0.8237362504005432}, {"x": 0.7320818901062012, "y": 0.8232967257499695}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.8171428442001343}, {"x": 0.5671217441558838, "y": 0.8171428442001343}, {"x": 0.5671217441558838, "y": 0.8237362504005432}, {"x": 0.5443686246871948, "y": 0.8237362504005432}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6490330100059509, "y": 0.8171428442001343}, {"x": 0.6717861294746399, "y": 0.8171428442001343}, {"x": 0.6717861294746399, "y": 0.8237362504005432}, {"x": 0.6490330100059509, "y": 0.8237362504005432}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.8290109634399414}, {"x": 0.8566552996635437, "y": 0.8285714387893677}, {"x": 0.8566552996635437, "y": 0.8356043696403503}, {"x": 0.8145620226860046, "y": 0.8360439538955688}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.18714448809623718, "y": 0.8281318545341492}, {"x": 0.26166099309921265, "y": 0.8294505476951599}, {"x": 0.26166099309921265, "y": 0.8373626470565796}, {"x": 0.18714448809623718, "y": 0.8360439538955688}], "text": "GenRead [13]\n"}
{"page": 6, "bbox": [{"x": 0.5443686246871948, "y": 0.8294505476951599}, {"x": 0.5671217441558838, "y": 0.8294505476951599}, {"x": 0.5671217441558838, "y": 0.8360439538955688}, {"x": 0.5443686246871948, "y": 0.8360439538955688}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.6496018171310425, "y": 0.8298901319503784}, {"x": 0.6717861294746399, "y": 0.8298901319503784}, {"x": 0.6717861294746399, "y": 0.8360439538955688}, {"x": 0.6496018171310425, "y": 0.8360439538955688}], "text": "Doc\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.8298901319503784}, {"x": 0.7804322838783264, "y": 0.8298901319503784}, {"x": 0.7804322838783264, "y": 0.8360439538955688}, {"x": 0.7320818901062012, "y": 0.8360439538955688}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.8294505476951599}, {"x": 0.4431171715259552, "y": 0.8290109634399414}, {"x": 0.4431171715259552, "y": 0.847912073135376}, {"x": 0.3771331012248993, "y": 0.8483516573905945}], "text": "LLMs\nDataset-base\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.8391208648681641}, {"x": 0.2724687159061432, "y": 0.8404395580291748}, {"x": 0.2724687159061432, "y": 0.8492307662963867}, {"x": 0.1769055724143982, "y": 0.847912073135376}], "text": "UniMS-RAG [74]\n"}
{"page": 6, "bbox": [{"x": 0.6456200480461121, "y": 0.8404395580291748}, {"x": 0.6734926104545593, "y": 0.8404395580291748}, {"x": 0.6734926104545593, "y": 0.847912073135376}, {"x": 0.6456200480461121, "y": 0.847912073135376}], "text": "Multi\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.8408791422843933}, {"x": 0.8486917018890381, "y": 0.8404395580291748}, {"x": 0.8486917018890381, "y": 0.8474725484848022}, {"x": 0.8219567537307739, "y": 0.847912073135376}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.841318666934967}, {"x": 0.5676905512809753, "y": 0.841318666934967}, {"x": 0.5676905512809753, "y": 0.847912073135376}, {"x": 0.5449374318122864, "y": 0.847912073135376}], "text": "Text\n"}
{"page": 6, "bbox": [{"x": 0.7377701997756958, "y": 0.841318666934967}, {"x": 0.7747440338134766, "y": 0.841318666934967}, {"x": 0.7747440338134766, "y": 0.8492307662963867}, {"x": 0.7377701997756958, "y": 0.8492307662963867}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.8531867861747742}, {"x": 0.7804322838783264, "y": 0.8536263704299927}, {"x": 0.7804322838783264, "y": 0.8602197766304016}, {"x": 0.7320818901062012, "y": 0.8597801923751831}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.6376564502716064, "y": 0.8536263704299927}, {"x": 0.6837314963340759, "y": 0.8536263704299927}, {"x": 0.6837314963340759, "y": 0.8602197766304016}, {"x": 0.6376564502716064, "y": 0.8602197766304016}], "text": "Sentence\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.8536263704299927}, {"x": 0.8486917018890381, "y": 0.8536263704299927}, {"x": 0.8486917018890381, "y": 0.8602197766304016}, {"x": 0.8219567537307739, "y": 0.8602197766304016}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.180887371301651, "y": 0.8527472615242004}, {"x": 0.2673492729663849, "y": 0.8531867861747742}, {"x": 0.2673492729663849, "y": 0.8619779944419861}, {"x": 0.180887371301651, "y": 0.8615384697914124}], "text": "CREA-ICL [19]\n"}
{"page": 6, "bbox": [{"x": 0.509670078754425, "y": 0.8527472615242004}, {"x": 0.6018202304840088, "y": 0.8527472615242004}, {"x": 0.6018202304840088, "y": 0.872967004776001}, {"x": 0.509670078754425, "y": 0.872967004776001}], "text": "Crosslingual, Text\nTabular, Text\n"}
{"page": 6, "bbox": [{"x": 0.19795222580432892, "y": 0.8637362718582153}, {"x": 0.2502844035625458, "y": 0.8650549650192261}, {"x": 0.24971558153629303, "y": 0.8734065890312195}, {"x": 0.19738338887691498, "y": 0.8720878958702087}], "text": "PKG [75]\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.8650549650192261}, {"x": 0.7798634767532349, "y": 0.8646153807640076}, {"x": 0.7798634767532349, "y": 0.8720878958702087}, {"x": 0.7315130829811096, "y": 0.8725274801254272}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.8654944896697998}, {"x": 0.8492605090141296, "y": 0.8654944896697998}, {"x": 0.8492605090141296, "y": 0.8720878958702087}, {"x": 0.8219567537307739, "y": 0.8720878958702087}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.8536263704299927}, {"x": 0.4431171715259552, "y": 0.8536263704299927}, {"x": 0.4431171715259552, "y": 0.8848351836204529}, {"x": 0.3771331012248993, "y": 0.8848351836204529}], "text": "Dataset-base\nLLM\nDataset-base\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.8650549650192261}, {"x": 0.6774743795394897, "y": 0.8650549650192261}, {"x": 0.6774743795394897, "y": 0.8843955993652344}, {"x": 0.6427758932113647, "y": 0.8843955993652344}], "text": "Chunk\nItem\n"}
{"page": 6, "bbox": [{"x": 0.8225256204605103, "y": 0.8778021931648254}, {"x": 0.8486917018890381, "y": 0.8778021931648254}, {"x": 0.8486917018890381, "y": 0.8843955993652344}, {"x": 0.8225256204605103, "y": 0.8843955993652344}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.5284414291381836, "y": 0.8778021931648254}, {"x": 0.583048939704895, "y": 0.8778021931648254}, {"x": 0.583048939704895, "y": 0.8848351836204529}, {"x": 0.5284414291381836, "y": 0.8848351836204529}], "text": "Code,Text\n"}
{"page": 6, "bbox": [{"x": 0.7252559661865234, "y": 0.8778021931648254}, {"x": 0.7878270745277405, "y": 0.8778021931648254}, {"x": 0.7878270745277405, "y": 0.8861538171768188}, {"x": 0.7252559661865234, "y": 0.8861538171768188}], "text": "Pre-training\n"}
{"page": 6, "bbox": [{"x": 0.1899886280298233, "y": 0.8764835000038147}, {"x": 0.2593856751918793, "y": 0.8773626089096069}, {"x": 0.25881683826446533, "y": 0.898901104927063}, {"x": 0.18941979110240936, "y": 0.8980219960212708}], "text": "SANTA [76]\nSURGE [77]\n"}
{"page": 6, "bbox": [{"x": 0.5466439127922058, "y": 0.8892307877540588}, {"x": 0.5642775893211365, "y": 0.8892307877540588}, {"x": 0.5642775893211365, "y": 0.8962637186050415}, {"x": 0.5466439127922058, "y": 0.8962637186050415}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.3873720169067383, "y": 0.8896703124046326}, {"x": 0.43344709277153015, "y": 0.8896703124046326}, {"x": 0.43344709277153015, "y": 0.8962637186050415}, {"x": 0.3873720169067383, "y": 0.8962637186050415}], "text": "Freebase\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.8896703124046326}, {"x": 0.8492605090141296, "y": 0.8896703124046326}, {"x": 0.8492605090141296, "y": 0.8962637186050415}, {"x": 0.8219567537307739, "y": 0.8962637186050415}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.8887912034988403}, {"x": 0.7747440338134766, "y": 0.8892307877540588}, {"x": 0.7747440338134766, "y": 0.8975824117660522}, {"x": 0.7383390069007874, "y": 0.8971428275108337}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.6319681406021118, "y": 0.8883516192436218}, {"x": 0.6894198060035706, "y": 0.8892307877540588}, {"x": 0.6894198060035706, "y": 0.8984615206718445}, {"x": 0.6319681406021118, "y": 0.8975824117660522}], "text": "Sub-Graph\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.9015384912490845}, {"x": 0.4431171715259552, "y": 0.9015384912490845}, {"x": 0.4431171715259552, "y": 0.9085714221000671}, {"x": 0.3771331012248993, "y": 0.9085714221000671}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.9015384912490845}, {"x": 0.8492605090141296, "y": 0.9019780158996582}, {"x": 0.8492605090141296, "y": 0.9085714221000671}, {"x": 0.8219567537307739, "y": 0.9081318974494934}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.18714448809623718, "y": 0.9006593227386475}, {"x": 0.26166099309921265, "y": 0.9019780158996582}, {"x": 0.26166099309921265, "y": 0.9098901152610779}, {"x": 0.18714448809623718, "y": 0.9085714221000671}], "text": "MK-TOD [78]\n"}
{"page": 6, "bbox": [{"x": 0.5460751056671143, "y": 0.9019780158996582}, {"x": 0.564846396446228, "y": 0.9019780158996582}, {"x": 0.564846396446228, "y": 0.9085714221000671}, {"x": 0.5460751056671143, "y": 0.9085714221000671}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.6444823741912842, "y": 0.9006593227386475}, {"x": 0.6763367652893066, "y": 0.9019780158996582}, {"x": 0.6757678985595703, "y": 0.9098901152610779}, {"x": 0.6439135670661926, "y": 0.9085714221000671}], "text": "Entity\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.901098906993866}, {"x": 0.7747440338134766, "y": 0.9015384912490845}, {"x": 0.7747440338134766, "y": 0.9098901152610779}, {"x": 0.7383390069007874, "y": 0.9094505310058594}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.3771331012248993, "y": 0.9134066104888916}, {"x": 0.4431171715259552, "y": 0.9129670262336731}, {"x": 0.4431171715259552, "y": 0.9200000166893005}, {"x": 0.3771331012248993, "y": 0.9204395413398743}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5460751056671143, "y": 0.9138461351394653}, {"x": 0.564846396446228, "y": 0.9138461351394653}, {"x": 0.564846396446228, "y": 0.9204395413398743}, {"x": 0.5460751056671143, "y": 0.9204395413398743}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.9138461351394653}, {"x": 0.8492605090141296, "y": 0.9138461351394653}, {"x": 0.8492605090141296, "y": 0.9204395413398743}, {"x": 0.8219567537307739, "y": 0.9204395413398743}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.1575654149055481, "y": 0.9125275015830994}, {"x": 0.29010239243507385, "y": 0.9134066104888916}, {"x": 0.29010239243507385, "y": 0.921758234500885}, {"x": 0.1575654149055481, "y": 0.9208791255950928}], "text": "Dual-Feedback-ToD [79]\n"}
{"page": 6, "bbox": [{"x": 0.6171786189079285, "y": 0.9138461351394653}, {"x": 0.7042093276977539, "y": 0.9138461351394653}, {"x": 0.7042093276977539, "y": 0.921758234500885}, {"x": 0.6171786189079285, "y": 0.921758234500885}], "text": "Entity Sequence\n"}
{"page": 6, "bbox": [{"x": 0.7383390069007874, "y": 0.9138461351394653}, {"x": 0.7747440338134766, "y": 0.9138461351394653}, {"x": 0.7747440338134766, "y": 0.921758234500885}, {"x": 0.7383390069007874, "y": 0.921758234500885}], "text": "Tuning\n"}
{"page": 6, "bbox": [{"x": 0.80887371301651, "y": 0.9257143139839172}, {"x": 0.8612059354782104, "y": 0.9252747297286987}, {"x": 0.8612059354782104, "y": 0.9323077201843262}, {"x": 0.80887371301651, "y": 0.9327472448348999}], "text": "Muti-time\n"}
{"page": 6, "bbox": [{"x": 0.37656426429748535, "y": 0.926153838634491}, {"x": 0.4442548453807831, "y": 0.926153838634491}, {"x": 0.4442548453807831, "y": 0.9327472448348999}, {"x": 0.37656426429748535, "y": 0.9327472448348999}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.5460751056671143, "y": 0.926153838634491}, {"x": 0.5642775893211365, "y": 0.926153838634491}, {"x": 0.5642775893211365, "y": 0.9327472448348999}, {"x": 0.5460751056671143, "y": 0.9327472448348999}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.926153838634491}, {"x": 0.7810011506080627, "y": 0.926153838634491}, {"x": 0.7810011506080627, "y": 0.9327472448348999}, {"x": 0.7315130829811096, "y": 0.9327472448348999}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.6433447003364563, "y": 0.9252747297286987}, {"x": 0.6786120533943176, "y": 0.9257143139839172}, {"x": 0.6786120533943176, "y": 0.9340659379959106}, {"x": 0.6433447003364563, "y": 0.9336263537406921}], "text": "Triplet\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.9252747297286987}, {"x": 0.27303755283355713, "y": 0.926153838634491}, {"x": 0.27303755283355713, "y": 0.9345055222511292}, {"x": 0.17519909143447876, "y": 0.9336263537406921}], "text": "KnowledGPT [15]\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.9375824332237244}, {"x": 0.7792946696281433, "y": 0.9371428489685059}, {"x": 0.7792946696281433, "y": 0.944615364074707}, {"x": 0.7315130829811096, "y": 0.9450549483299255}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5460751056671143, "y": 0.9380219578742981}, {"x": 0.564846396446228, "y": 0.9380219578742981}, {"x": 0.564846396446228, "y": 0.944615364074707}, {"x": 0.5460751056671143, "y": 0.944615364074707}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.9380219578742981}, {"x": 0.8492605090141296, "y": 0.9380219578742981}, {"x": 0.8492605090141296, "y": 0.944615364074707}, {"x": 0.8219567537307739, "y": 0.944615364074707}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.3589306175708771, "y": 0.9367033243179321}, {"x": 0.46188852190971375, "y": 0.9375824332237244}, {"x": 0.46188852190971375, "y": 0.9463736414909363}, {"x": 0.3589306175708771, "y": 0.945494532585144}], "text": "Dataset-base,Graph\n"}
{"page": 6, "bbox": [{"x": 0.6444823741912842, "y": 0.9371428489685059}, {"x": 0.6757678985595703, "y": 0.9380219578742981}, {"x": 0.6751990914344788, "y": 0.9463736414909363}, {"x": 0.6439135670661926, "y": 0.945494532585144}], "text": "Entity\n"}
{"page": 6, "bbox": [{"x": 0.3879408538341522, "y": 0.94989013671875}, {"x": 0.4328782856464386, "y": 0.9494505524635315}, {"x": 0.4328782856464386, "y": 0.9564835429191589}, {"x": 0.3879408538341522, "y": 0.9569230675697327}], "text": "CMeKG\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.94989013671875}, {"x": 0.8481228947639465, "y": 0.9494505524635315}, {"x": 0.8481228947639465, "y": 0.9564835429191589}, {"x": 0.8219567537307739, "y": 0.9569230675697327}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.18543799221515656, "y": 0.9362637400627136}, {"x": 0.26279863715171814, "y": 0.9371428489685059}, {"x": 0.2622298002243042, "y": 0.970549464225769}, {"x": 0.1848691701889038, "y": 0.9696703553199768}], "text": "FABULA [80]\nHyKGE [81]\nKALMV [82]\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.9503296613693237}, {"x": 0.7804322838783264, "y": 0.9503296613693237}, {"x": 0.7804322838783264, "y": 0.9569230675697327}, {"x": 0.7320818901062012, "y": 0.9569230675697327}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5460751056671143, "y": 0.9503296613693237}, {"x": 0.5642775893211365, "y": 0.94989013671875}, {"x": 0.564846396446228, "y": 0.9569230675697327}, {"x": 0.5466439127922058, "y": 0.9573626518249512}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.6444823741912842, "y": 0.9494505524635315}, {"x": 0.6757678985595703, "y": 0.9503296613693237}, {"x": 0.6751990914344788, "y": 0.9582417607307434}, {"x": 0.6439135670661926, "y": 0.9573626518249512}], "text": "Entity\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.9617582559585571}, {"x": 0.7798634767532349, "y": 0.9613186717033386}, {"x": 0.7798634767532349, "y": 0.9687911868095398}, {"x": 0.7315130829811096, "y": 0.9692307710647583}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.5460751056671143, "y": 0.9621977806091309}, {"x": 0.5654152631759644, "y": 0.9621977806091309}, {"x": 0.5654152631759644, "y": 0.9687911868095398}, {"x": 0.5460751056671143, "y": 0.9687911868095398}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.9621977806091309}, {"x": 0.8566552996635437, "y": 0.9621977806091309}, {"x": 0.8566552996635437, "y": 0.9687911868095398}, {"x": 0.8145620226860046, "y": 0.9687911868095398}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.3833902180194855, "y": 0.9613186717033386}, {"x": 0.43742889165878296, "y": 0.9621977806091309}, {"x": 0.43742889165878296, "y": 0.970549464225769}, {"x": 0.3833902180194855, "y": 0.9696703553199768}], "text": "Wikipedia\n"}
{"page": 6, "bbox": [{"x": 0.6427758932113647, "y": 0.9613186717033386}, {"x": 0.6780432462692261, "y": 0.9621977806091309}, {"x": 0.6774743795394897, "y": 0.9709889888763428}, {"x": 0.6422070264816284, "y": 0.9701098799705505}], "text": "Triplet\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.9740659594535828}, {"x": 0.7810011506080627, "y": 0.9740659594535828}, {"x": 0.7810011506080627, "y": 0.9802197813987732}, {"x": 0.7315130829811096, "y": 0.9802197813987732}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8145620226860046, "y": 0.9736263751983643}, {"x": 0.8566552996635437, "y": 0.9736263751983643}, {"x": 0.8566552996635437, "y": 0.9806593656539917}, {"x": 0.8145620226860046, "y": 0.9806593656539917}], "text": "Iterative\n"}
{"page": 6, "bbox": [{"x": 0.1990898698568344, "y": 0.9723076820373535}, {"x": 0.24971558153629303, "y": 0.9736263751983643}, {"x": 0.24914675951004028, "y": 0.9824175834655762}, {"x": 0.19852104783058167, "y": 0.9810988903045654}], "text": "RoG [83]\n"}
{"page": 6, "bbox": [{"x": 0.3873720169067383, "y": 0.9740659594535828}, {"x": 0.4328782856464386, "y": 0.9740659594535828}, {"x": 0.4328782856464386, "y": 0.9806593656539917}, {"x": 0.3873720169067383, "y": 0.9806593656539917}], "text": "Freebase\n"}
{"page": 6, "bbox": [{"x": 0.5460751056671143, "y": 0.9740659594535828}, {"x": 0.564846396446228, "y": 0.9740659594535828}, {"x": 0.564846396446228, "y": 0.9806593656539917}, {"x": 0.5460751056671143, "y": 0.9806593656539917}], "text": "KG\n"}
{"page": 6, "bbox": [{"x": 0.6433447003364563, "y": 0.9731867909431458}, {"x": 0.6786120533943176, "y": 0.9736263751983643}, {"x": 0.6786120533943176, "y": 0.9824175834655762}, {"x": 0.6433447003364563, "y": 0.9819779992103577}], "text": "Triplet\n"}
{"page": 6, "bbox": [{"x": 0.37656426429748535, "y": 0.9854944944381714}, {"x": 0.4431171715259552, "y": 0.9859340786933899}, {"x": 0.4431171715259552, "y": 0.9929670095443726}, {"x": 0.37656426429748535, "y": 0.9925274848937988}], "text": "Dataset-base\n"}
{"page": 6, "bbox": [{"x": 0.7320818901062012, "y": 0.9863736033439636}, {"x": 0.7810011506080627, "y": 0.9863736033439636}, {"x": 0.7810011506080627, "y": 0.9929670095443726}, {"x": 0.7320818901062012, "y": 0.9929670095443726}], "text": "Inference\n"}
{"page": 6, "bbox": [{"x": 0.8219567537307739, "y": 0.9863736033439636}, {"x": 0.8492605090141296, "y": 0.9863736033439636}, {"x": 0.8492605090141296, "y": 0.9929670095443726}, {"x": 0.8219567537307739, "y": 0.9929670095443726}], "text": "Once\n"}
{"page": 6, "bbox": [{"x": 0.18031854927539825, "y": 0.9854944944381714}, {"x": 0.2684869170188904, "y": 0.9863736033439636}, {"x": 0.2684869170188904, "y": 0.9947252869606018}, {"x": 0.18031854927539825, "y": 0.9938461780548096}], "text": "G-Retriever [84]\n"}
{"page": 6, "bbox": [{"x": 0.5284414291381836, "y": 0.9854944944381714}, {"x": 0.583048939704895, "y": 0.9859340786933899}, {"x": 0.583048939704895, "y": 0.9947252869606018}, {"x": 0.5284414291381836, "y": 0.9942857027053833}], "text": "TextGraph\n"}
{"page": 6, "bbox": [{"x": 0.6325369477272034, "y": 0.9859340786933899}, {"x": 0.6888509392738342, "y": 0.9863736033439636}, {"x": 0.6888509392738342, "y": 0.9947252869606018}, {"x": 0.6325369477272034, "y": 0.9942857027053833}], "text": "Sub-Graph\n"}
{"page": 7, "bbox": [{"x": 0.9135380983352661, "y": 0.03340659290552139}, {"x": 0.9197952151298523, "y": 0.03340659290552139}, {"x": 0.9197952151298523, "y": 0.0391208790242672}, {"x": 0.9135380983352661, "y": 0.0391208790242672}], "text": "7\n"}
{"page": 7, "bbox": [{"x": 0.1717861145734787, "y": 0.07780219614505768}, {"x": 0.2974971532821655, "y": 0.07912088185548782}, {"x": 0.2969283163547516, "y": 0.1006593406200409}, {"x": 0.17121729254722595, "y": 0.09934066236019135}], "text": "External Knowledge\nRequired\n"}
{"page": 7, "bbox": [{"x": 0.36746302247047424, "y": 0.1204395592212677}, {"x": 0.4277588129043579, "y": 0.1204395592212677}, {"x": 0.4277588129043579, "y": 0.1261538416147232}, {"x": 0.36746302247047424, "y": 0.1261538416147232}], "text": "Modular RAG\n"}
{"page": 7, "bbox": [{"x": 0.1990898698568344, "y": 0.13010989129543304}, {"x": 0.22696246206760406, "y": 0.13098901510238647}, {"x": 0.22639362514019012, "y": 0.1402197778224945}, {"x": 0.19852104783058167, "y": 0.13934065401554108}], "text": "High\n"}
{"page": 7, "bbox": [{"x": 0.351535826921463, "y": 0.12967033684253693}, {"x": 0.44368600845336914, "y": 0.12923076748847961}, {"x": 0.44368600845336914, "y": 0.14109890162944794}, {"x": 0.351535826921463, "y": 0.14153845608234406}], "text": "Organic combination of\nmultiple modules\n"}
{"page": 7, "bbox": [{"x": 0.6194539070129395, "y": 0.1375824213027954}, {"x": 0.7184300422668457, "y": 0.13846154510974884}, {"x": 0.7184300422668457, "y": 0.14461538195610046}, {"x": 0.6194539070129395, "y": 0.14373625814914703}], "text": "Retriever Fine-tuning\n"}
{"page": 7, "bbox": [{"x": 0.6581342220306396, "y": 0.16175824403762817}, {"x": 0.7770193219184875, "y": 0.1626373678445816}, {"x": 0.7770193219184875, "y": 0.16967032849788666}, {"x": 0.6581342220306396, "y": 0.16879120469093323}], "text": "Collaborative Fine-tuning\n"}
{"page": 7, "bbox": [{"x": 0.266211599111557, "y": 0.16439560055732727}, {"x": 0.39704209566116333, "y": 0.16439560055732727}, {"x": 0.39704209566116333, "y": 0.18593406677246094}, {"x": 0.266211599111557, "y": 0.18593406677246094}], "text": "Advanced RAG\nIndex/pre-retrieval/post-retrieval\noptimization\n"}
{"page": 7, "bbox": [{"x": 0.530147910118103, "y": 0.1846153885126114}, {"x": 0.628555178642273, "y": 0.1846153885126114}, {"x": 0.628555178642273, "y": 0.19164834916591644}, {"x": 0.530147910118103, "y": 0.19164834916591644}], "text": "All of the above\n"}
{"page": 7, "bbox": [{"x": 0.45676904916763306, "y": 0.20395603775978088}, {"x": 0.4829351603984833, "y": 0.20395603775978088}, {"x": 0.4829351603984833, "y": 0.21098901331424713}, {"x": 0.45676904916763306, "y": 0.21098901331424713}], "text": "RAG\n"}
{"page": 7, "bbox": [{"x": 0.6387940645217896, "y": 0.20791208744049072}, {"x": 0.7428896427154541, "y": 0.20923076570034027}, {"x": 0.7428896427154541, "y": 0.2158241719007492}, {"x": 0.6387940645217896, "y": 0.21450549364089966}], "text": "Generator Fine-tuning\n"}
{"page": 7, "bbox": [{"x": 0.3122866749763489, "y": 0.20967033505439758}, {"x": 0.3606370985507965, "y": 0.2101098895072937}, {"x": 0.3606370985507965, "y": 0.21626374125480652}, {"x": 0.3122866749763489, "y": 0.2158241719007492}], "text": "Naive RAG\n"}
{"page": 7, "bbox": [{"x": 0.2906712293624878, "y": 0.21978022158145905}, {"x": 0.38225257396698, "y": 0.21978022158145905}, {"x": 0.38225257396698, "y": 0.224175825715065}, {"x": 0.2906712293624878, "y": 0.224175825715065}], "text": "Add relevant contextual\n"}
{"page": 7, "bbox": [{"x": 0.3151308298110962, "y": 0.22593407332897186}, {"x": 0.35836178064346313, "y": 0.22549450397491455}, {"x": 0.35836178064346313, "y": 0.23120878636837006}, {"x": 0.3151308298110962, "y": 0.23164835572242737}], "text": "paragraphs.\n"}
{"page": 7, "bbox": [{"x": 0.5472127199172974, "y": 0.2527472674846649}, {"x": 0.6205915808677673, "y": 0.2527472674846649}, {"x": 0.6205915808677673, "y": 0.26197803020477295}, {"x": 0.5472127199172974, "y": 0.26197803020477295}], "text": "Fine-tuning\n"}
{"page": 7, "bbox": [{"x": 0.3833902180194855, "y": 0.26153847575187683}, {"x": 0.5073947906494141, "y": 0.26153847575187683}, {"x": 0.5073947906494141, "y": 0.27032968401908875}, {"x": 0.3833902180194855, "y": 0.27032968401908875}], "text": "Prompt Engineering\n"}
{"page": 7, "bbox": [{"x": 0.2923777103424072, "y": 0.26505494117736816}, {"x": 0.3555176258087158, "y": 0.26505494117736816}, {"x": 0.3555176258087158, "y": 0.2835164964199066}, {"x": 0.2923777103424072, "y": 0.2835164964199066}], "text": "XoT Prompt\ne.g. CoT, TOT\n"}
{"page": 7, "bbox": [{"x": 0.28725823760032654, "y": 0.30417582392692566}, {"x": 0.3686006963253021, "y": 0.3050549328327179}, {"x": 0.3686006963253021, "y": 0.31164833903312683}, {"x": 0.28725823760032654, "y": 0.3107692301273346}], "text": "Few-shot Prompt\n"}
{"page": 7, "bbox": [{"x": 0.3930602967739105, "y": 0.32351648807525635}, {"x": 0.47212740778923035, "y": 0.3248351514339447}, {"x": 0.47212740778923035, "y": 0.33142855763435364}, {"x": 0.3930602967739105, "y": 0.3301098942756653}], "text": "Standard Prompt\n"}
{"page": 7, "bbox": [{"x": 0.20193400979042053, "y": 0.3257142901420593}, {"x": 0.22582480311393738, "y": 0.3257142901420593}, {"x": 0.22582480311393738, "y": 0.33230769634246826}, {"x": 0.20193400979042053, "y": 0.33230769634246826}], "text": "Low\n"}
{"page": 7, "bbox": [{"x": 0.7064846158027649, "y": 0.33714285492897034}, {"x": 0.8179749846458435, "y": 0.3380219638347626}, {"x": 0.8179749846458435, "y": 0.360879123210907}, {"x": 0.7064846158027649, "y": 0.36000001430511475}], "text": "Model Adaptation\nRequired\n"}
{"page": 7, "bbox": [{"x": 0.26678043603897095, "y": 0.36043956875801086}, {"x": 0.2906712293624878, "y": 0.35956043004989624}, {"x": 0.29124003648757935, "y": 0.3670329749584198}, {"x": 0.2673492729663849, "y": 0.36791208386421204}], "text": "Low\n"}
{"page": 7, "bbox": [{"x": 0.6376564502716064, "y": 0.360879123210907}, {"x": 0.6649602055549622, "y": 0.360879123210907}, {"x": 0.6649602055549622, "y": 0.36879122257232666}, {"x": 0.6376564502716064, "y": 0.36879122257232666}], "text": "High\n"}
{"page": 7, "bbox": [{"x": 0.07906711846590042, "y": 0.3898901045322418}, {"x": 0.9209328889846802, "y": 0.3898901045322418}, {"x": 0.9209328889846802, "y": 0.43296703696250916}, {"x": 0.07906711846590042, "y": 0.43296703696250916}], "text": "Fig. 4. RAG compared with other model optimization methods in the aspects of \"External Knowledge Required” and “Model Adaption Required\". Prompt\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\n"}
{"page": 7, "bbox": [{"x": 0.07906711846590042, "y": 0.461978018283844}, {"x": 0.49146756529808044, "y": 0.461978018283844}, {"x": 0.49146756529808044, "y": 0.8993406295776367}, {"x": 0.07906711846590042, "y": 0.8993406295776367}], "text": "Unstructured Data, such as text, is the most widely used\nretrieval source, which are mainly gathered from corpus. For\nopen-domain question-answering (ODQA) tasks, the primary\nretrieval sources are Wikipedia Dump with the current major\nversions including HotpotQA 4 (1st October, 2017), DPR³ (20\nDecember, 2018). In addition to encyclopedic data, common\nunstructured data includes cross-lingual text [19] and domain-\nspecific data (such as medical [67] and legal domains [29]).\nSemi-structured data. typically refers to data that contains a\ncombination of text and table information, such as PDF. Han-\ndling semi-structured data poses challenges for conventional\nRAG systems due to two main reasons. Firstly, text splitting\nprocesses may inadvertently separate tables, leading to data\ncorruption during retrieval. Secondly, incorporating tables into\nthe data can complicate semantic similarity searches. When\ndealing with semi-structured data, one approach involves lever-\naging the code capabilities of LLMs to execute Text-2-SQL\nqueries on tables within databases, such as TableGPT [85].\nAlternatively, tables can be transformed into text format for\nfurther analysis using text-based methods [75]. However, both\nof these methods are not optimal solutions, indicating substan-\ntial research opportunities in this area.\nStructured data, such as knowledge graphs (KGs) [86],\nwhich are typically verified and can provide more precise in-\nformation. KnowledGPT [15] generates KB search queries and\nstores knowledge in a personalized base, enhancing the RAG\nmodel's knowledge richness. In response to the limitations of\nLLMs in understanding and answering questions about textual\ngraphs, G-Retriever [84] integrates Graph Neural Networks\n"}
{"page": 7, "bbox": [{"x": 0.5085324048995972, "y": 0.4624175727367401}, {"x": 0.9215016961097717, "y": 0.4624175727367401}, {"x": 0.9209328889846802, "y": 0.944615364074707}, {"x": 0.5079635977745056, "y": 0.944615364074707}], "text": "(GNNs), LLMs and RAG, enhancing graph comprehension\nand question-answering capabilities through soft prompting\nof the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort to build, validate,\nand maintain structured databases. On the contrary, it requires\nadditional effort to build, validate, and maintain structured\ndatabases.\nLLMs-Generated Content. Addressing the limitations of\nexternal auxiliary information in RAG, some research has\nfocused on exploiting LLMs' internal knowledge. SKR [58]\nclassifies questions as known or unknown, applying retrieval\nenhancement selectively. GenRead [13] replaces the retriever\nwith an LLM generator, finding that LLM-generated contexts\noften contain more accurate answers due to better alignment\nwith the pre-training objectives of causal language modeling.\nSelfmem [17] iteratively creates an unbounded memory pool\nwith a retrieval-enhanced generator, using a memory selec-\ntor to choose outputs that serve as dual problems to the\noriginal question, thus self-enhancing the generative model.\nThese methodologies underscore the breadth of innovative\ndata source utilization in RAG, striving to improve model\nperformance and task effectiveness.\n2) Retrieval Granularity: Another important factor besides\nthe data format of the retrieval source is the granularity of\nthe retrieved data. Coarse-grained retrieval units theoretically\ncan provide more relevant information for the problem, but\nthey may also contain redundant content, which could distract\nthe retriever and language models in downstream tasks [50],\n[87]. On the other hand, fine-grained retrieval unit granularity\nincreases the burden of retrieval and does not guarantee seman-\ntic integrity and meeting the required knowledge. Choosing\n"}
{"page": 7, "bbox": [{"x": 0.0904436856508255, "y": 0.9208791255950928}, {"x": 0.3265073895454407, "y": 0.9200000166893005}, {"x": 0.3265073895454407, "y": 0.9428571462631226}, {"x": 0.0904436856508255, "y": 0.9437362551689148}], "text": "4https://hotpotqa.github.io/wiki-readme.html\nShttps://github.com/facebookresearch/DPR\n"}
{"page": 8, "bbox": [{"x": 0.9141069650650024, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.03868131712079048}, {"x": 0.9141069650650024, "y": 0.03868131712079048}], "text": "8\n"}
{"page": 8, "bbox": [{"x": 0.07906711846590042, "y": 0.07252747565507889}, {"x": 0.4920364022254944, "y": 0.07252747565507889}, {"x": 0.4920364022254944, "y": 0.3081318736076355}, {"x": 0.07906711846590042, "y": 0.3081318736076355}], "text": "the appropriate retrieval granularity during inference can be\na simple and effective strategy to improve the retrieval and\ndownstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse,\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\nument. Among them, DenseX [30] proposed the concept of\nusing propositions as retrieval units. Propositions are defined\nas atomic expressions in the text, each encapsulating a unique\nfactual segment and presented in a concise, self-contained nat-\nural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph.\nThe granularity of retrieval can also be adapted to downstream\ntasks, such as retrieving Item IDs [40]in recommendation tasks\nand Sentence pairs [38]. Detailed information is illustrated in\nTable I.\n"}
{"page": 8, "bbox": [{"x": 0.5085324048995972, "y": 0.07120878994464874}, {"x": 0.9209328889846802, "y": 0.07120878994464874}, {"x": 0.9209328889846802, "y": 0.42989009618759155}, {"x": 0.5085324048995972, "y": 0.42989009618759155}], "text": "Hierarchical index structure. File are arranged in parent-\nchild relationships, with chunks linked to them. Data sum-\nmaries are stored at each node, aiding in the swift traversal\nof data and assisting the RAG system in determining which\nchunks to extract. This approach can also mitigate the illusion\ncaused by block extraction issues.\nKnowledge Graph index. Utilize KG in constructing the\nhierarchical structure of documents contributes to maintaining\nconsistency. It delineates the connections between different\nconcepts and entities, markedly reducing the potential for\nillusions. Another advantage is the transformation of the\ninformation retrieval process into instructions that LLM can\ncomprehend, thereby enhancing the accuracy of knowledge\nretrieval and enabling LLM to generate contextually coherent\nresponses, thus improving the overall efficiency of the RAG\nsystem. To capture the logical relationship between document\ncontent and structure, KGP [91] proposed a method of building\nan index between multiple documents using KG. This KG\nconsists of nodes (representing paragraphs or structures in the\ndocuments, such as pages and tables) and edges (indicating\nsemantic/lexical similarity between paragraphs or relationships\nwithin the document structure), effectively addressing knowl-\nedge retrieval and reasoning problems in a multi-document\nenvironment.\n"}
{"page": 8, "bbox": [{"x": 0.07906711846590042, "y": 0.3283516466617584}, {"x": 0.2502844035625458, "y": 0.3283516466617584}, {"x": 0.2502844035625458, "y": 0.3389011025428772}, {"x": 0.07906711846590042, "y": 0.3389011025428772}], "text": "B. Indexing Optimization\n"}
{"page": 8, "bbox": [{"x": 0.509670078754425, "y": 0.46637362241744995}, {"x": 0.6649602055549622, "y": 0.46637362241744995}, {"x": 0.6649602055549622, "y": 0.4786813259124756}, {"x": 0.509670078754425, "y": 0.4786813259124756}], "text": "C. Query Optimization\n"}
{"page": 8, "bbox": [{"x": 0.07906711846590042, "y": 0.34549450874328613}, {"x": 0.49146756529808044, "y": 0.34549450874328613}, {"x": 0.4908987581729889, "y": 0.945494532585144}, {"x": 0.07849829643964767, "y": 0.945494532585144}], "text": "In the Indexing phase, documents will be processed, seg-\nmented, and transformed into Embeddings to be stored in a\nvector database. The quality of index construction determines\nwhether the correct context can be obtained in the retrieval\nphase.\n1) Chunking Strategy: The most common method is to split\nthe document into chunks on a fixed number of tokens (e.g.,\n100, 256, 512) [88]. Larger chunks can capture more context,\nbut they also generate more noise, requiring longer processing\ntime and higher costs. While smaller chunks may not fully\nconvey the necessary context, they do have less noise. How-\never, chunks leads to truncation within sentences, prompting\nthe optimization of a recursive splits and sliding window meth-\nods, enabling layered retrieval by merging globally related\ninformation across multiple retrieval processes [89]. Never-\ntheless, these approaches still cannot strike a balance between\nsemantic completeness and context length. Therefore, methods\nlike Small2Big have been proposed, where sentences (small)\nare used as the retrieval unit, and the preceding and following\nsentences are provided as (big) context to LLMs [90].\n2) Metadata Attachments: Chunks can be enriched with\nmetadata information such as page number, file name, au-\nthor,category timestamp. Subsequently, retrieval can be filtered\nbased on this metadata, limiting the scope of the retrieval.\nAssigning different weights to document timestamps during\nretrieval can achieve time-aware RAG, ensuring the freshness\nof knowledge and avoiding outdated information.\nIn addition to extracting metadata from the original doc-\numents, metadata can also be artificially constructed. For\nexample, adding summaries of paragraph, as well as intro-\nducing hypothetical questions. This method is also known as\nReverse HyDE. Specifically, using LLM to generate questions\nthat can be answered by the document, then calculating the\nsimilarity between the original question and the hypothetical\nquestion during retrieval to reduce the semantic gap between\nthe question and the answer.\n3) Structural Index: One effective method for enhancing\ninformation retrieval is to establish a hierarchical structure for\nthe documents. By constructing In structure, RAG system can\nexpedite the retrieval and processing of pertinent data.\n"}
{"page": 8, "bbox": [{"x": 0.5085324048995972, "y": 0.48835164308547974}, {"x": 0.9215016961097717, "y": 0.48835164308547974}, {"x": 0.9215016961097717, "y": 0.9441758394241333}, {"x": 0.5085324048995972, "y": 0.9441758394241333}], "text": "One of the primary challenges with Naive RAG is its\ndirect reliance on the user's original query as the basis for\nretrieval. Formulating a precise and clear question is difficult,\nand imprudent queries result in subpar retrieval effectiveness.\nSometimes, the question itself is complex, and the language\nis not well-organized. Another difficulty lies in language\ncomplexity ambiguity. Language models often struggle when\ndealing with specialized vocabulary or ambiguous abbrevi-\nations with multiple meanings. For instance, they may not\ndiscern whether \"LLM\" refers to large language model or a\nMaster of Laws in a legal context.\n1) Query Expansion: Expanding a single query into mul-\ntiple queries enriches the content of the query, providing\nfurther context to address any lack of specific nuances, thereby\nensuring the optimal relevance of the generated answers.\nMulti-Query. By employing prompt engineering to expand\nqueries via LLMs, these queries can then be executed in\nparallel. The expansion of queries is not random, but rather\nmeticulously designed.\nSub-Query. The process of sub-question planning represents\nthe generation of the necessary sub-questions to contextualize\nand fully answer the original question when combined. This\nprocess of adding relevant context is, in principle, similar\nto query expansion. Specifically, a complex question can be\ndecomposed into a series of simpler sub-questions using the\nleast-to-most prompting method [92].\nChain-of-Verification(CoVe). The expanded queries undergo\nvalidation by LLM to achieve the effect of reducing halluci-\nnations. Validated expanded queries typically exhibit higher\nreliability [93].\n"}
{"page": 9, "bbox": [{"x": 0.9135380983352661, "y": 0.03340659290552139}, {"x": 0.9203640222549438, "y": 0.03340659290552139}, {"x": 0.9203640222549438, "y": 0.0391208790242672}, {"x": 0.9135380983352661, "y": 0.0391208790242672}], "text": "9\n"}
{"page": 9, "bbox": [{"x": 0.07849829643964767, "y": 0.0698901116847992}, {"x": 0.4920364022254944, "y": 0.07076922804117203}, {"x": 0.49032992124557495, "y": 0.5978022217750549}, {"x": 0.07679180800914764, "y": 0.5969230532646179}], "text": "2) Query Transformation: The core concept is to retrieve\nchunks based on a transformed query instead of the user's\noriginal query.\nQuery Rewrite. The original queries are not always optimal\nfor LLM retrieval, especially in real-world scenarios. There-\nfore, we can prompt LLM to rewrite the queries. In addition to\nusing LLM for query rewriting, specialized smaller language\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\nmentation of the query rewrite method in the Taobao, known\nas BEQUE [9] has notably enhanced recall effectiveness for\nlong-tail queries, resulting in a rise in GMV.\nAnother query transformation method is to use prompt\nengineering to let LLM generate a query based on the original\nquery for subsequent retrieval. HyDE [11] construct hypothet-\nical documents (assumed answers to the original query). It\nfocuses on embedding similarity from answer to answer rather\nthan seeking embedding similarity for the problem or query.\nUsing the Step-back Prompting method [10], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n3) Query Routing: Based on varying queries, routing to\ndistinct RAG pipeline, which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter. The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and\nmetadata-based methods for enhanced query routing.\n"}
{"page": 9, "bbox": [{"x": 0.5085324048995972, "y": 0.07208791375160217}, {"x": 0.9209328889846802, "y": 0.07252747565507889}, {"x": 0.9203640222549438, "y": 0.6136263608932495}, {"x": 0.5079635977745056, "y": 0.6131868362426758}], "text": "to provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse\nretrieval. Specifically, it also demonstrates that sparse retrieval\nmodels can enhance the zero-shot retrieval capability of dense\nretrieval models and assist dense retrievers in handling queries\ncontaining rare entities, thereby improving robustness.\n2) Fine-tuning Embedding Model: In instances where the\ncontext significantly deviates from pre-training corpus, partic-\nularly within highly specialized disciplines such as healthcare,\nlegal practice, and other sectors replete with proprietary jargon,\nfine-tuning the embedding model on your own domain dataset\nbecomes essential to mitigate such discrepancies.\nIn addition to supplementing domain knowledge, another\npurpose of fine-tuning is to align the retriever and generator,\nfor example, using the results of LLM as the supervision signal\nfor fine-tuning, known as LSR (LM-supervised Retriever).\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\ngenerator to create task-specific retrievers, addressing chal-\nlenges in supervised fine-tuning, particularly in data-scarce\ndomains. Another approach, LLM-Embedder [97], exploits\nLLMs to generate reward signals across multiple downstream\ntasks. The retriever is fine-tuned with two types of supervised\nsignals: hard labels for the dataset and soft rewards from\nthe LLMs. This dual-signal approach fosters a more effective\nfine-tuning process, tailoring the embedding model to diverse\ndownstream applications. REPLUG [72] utilizes a retriever\nand an LLM to calculate the probability distributions of the\nretrieved documents and then performs supervised training\nby computing the KL divergence. This straightforward and\neffective training method enhances the performance of the\nretrieval model by using an LM as the supervisory signal,\neliminating the need for specific cross-attention mechanisms.\nMoreover, inspired by RLHF (Reinforcement Learning from\nHuman Feedback), utilizing LM-based feedback to reinforce\nthe retriever through reinforcement learning.\n"}
{"page": 9, "bbox": [{"x": 0.07906711846590042, "y": 0.6193406581878662}, {"x": 0.17804323136806488, "y": 0.6197802424430847}, {"x": 0.17804323136806488, "y": 0.6298900842666626}, {"x": 0.07906711846590042, "y": 0.6294505596160889}], "text": "D. Embedding\n"}
{"page": 9, "bbox": [{"x": 0.5091012716293335, "y": 0.6399999856948853}, {"x": 0.585324227809906, "y": 0.641318678855896}, {"x": 0.585324227809906, "y": 0.6518681049346924}, {"x": 0.5091012716293335, "y": 0.6505494713783264}], "text": "E. Adapter\n"}
{"page": 9, "bbox": [{"x": 0.07792946696281433, "y": 0.63692307472229}, {"x": 0.4908987581729889, "y": 0.6356043815612793}, {"x": 0.4920364022254944, "y": 0.8470329642295837}, {"x": 0.07906711846590042, "y": 0.8483516573905945}], "text": "In RAG, retrieval is achieved by calculating the similarity\n(e.g. cosine similarity) between the embeddings of the ques-\ntion and document chunks, where the semantic representation\ncapability of embedding models plays a key role. This mainly\nincludes a sparse encoder (BM25) and a dense retriever (BERT\narchitecture Pre-training language models). Recent research\nhas introduced prominent embedding models such as AngIE,\nVoyage, BGE,etc [94]-[96], which are benefit from multi-task\ninstruct tuning. Hugging Face's MTEB leaderboard 7 evaluates\nembedding models across 8 tasks, covering 58 datasests. Ad-\nditionally, C-MTEB focuses on Chinese capability, covering\n6 tasks and 35 datasets. There is no one-size-fits-all answer\nto \"which embedding model to use.” However, some specific\nmodels are better suited for particular use cases.\n"}
{"page": 9, "bbox": [{"x": 0.5073947906494141, "y": 0.6606593132019043}, {"x": 0.9203640222549438, "y": 0.6597802042961121}, {"x": 0.9209328889846802, "y": 0.8540659546852112}, {"x": 0.5079635977745056, "y": 0.8549450635910034}], "text": "Fine-tuning models may present challenges, such as in-\ntegrating functionality through an API or addressing con-\nstraints arising from limited local computational resources.\nConsequently, some approaches opt to incorporate an external\nadapter to aid in alignment.\nTo optimize the multi-task capabilities of LLM, UP-\nRISE [20] trained a lightweight prompt retriever that can\nautomatically retrieve prompts from a pre-built prompt pool\nthat are suitable for a given zero-shot task input. AAR\n(Augmentation-Adapted Retriver) [47] introduces a universal\nadapter designed to accommodate multiple downstream tasks.\nWhile PRCA [69] add a pluggable reward-driven contextual\nadapter to enhance performance on specific tasks. BGM [26]\n"}
{"page": 9, "bbox": [{"x": 0.07906711846590042, "y": 0.8443955779075623}, {"x": 0.9197952151298523, "y": 0.8465933799743652}, {"x": 0.9197952151298523, "y": 0.9090110063552856}, {"x": 0.07906711846590042, "y": 0.9068132042884827}], "text": "1) Mix/hybrid Retrieval: Sparse and dense embedding keeps the retriever and LLM fixed,and trains a bridge Seq2Seq\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n"}
{"page": 9, "bbox": [{"x": 0.5091012716293335, "y": 0.8725274801254272}, {"x": 0.9209328889846802, "y": 0.8725274801254272}, {"x": 0.9209328889846802, "y": 0.944615364074707}, {"x": 0.5091012716293335, "y": 0.944615364074707}], "text": "model in between. The bridge model aims to transform the\nretrieved information into a format that LLMs can work with\neffectively, allowing it to not only rerank but also dynami-\ncally select passages for each query, and potentially employ\nmore advanced strategies like repetition. Furthermore, PKG\n"}
{"page": 9, "bbox": [{"x": 0.09215017408132553, "y": 0.9208791255950928}, {"x": 0.3475540280342102, "y": 0.9208791255950928}, {"x": 0.3475540280342102, "y": 0.9441758394241333}, {"x": 0.09215017408132553, "y": 0.9441758394241333}], "text": "6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\n"}
{"page": 10, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.0391208790242672}, {"x": 0.9089874625205994, "y": 0.0391208790242672}], "text": "10\n"}
{"page": 10, "bbox": [{"x": 0.07906711846590042, "y": 0.07120878994464874}, {"x": 0.49032992124557495, "y": 0.07164835184812546}, {"x": 0.49032992124557495, "y": 0.1599999964237213}, {"x": 0.07906711846590042, "y": 0.1595604419708252}], "text": "introduces an innovative method for integrating knowledge\ninto white-box models via directive fine-tuning [75]. In this\napproach, the retriever module is directly substituted to gen-\nerate relevant documents according to a query. This method\nassists in addressing the difficulties encountered during the\nfine-tuning process and enhances model performance.\n"}
{"page": 10, "bbox": [{"x": 0.5091012716293335, "y": 0.07252747565507889}, {"x": 0.9209328889846802, "y": 0.07252747565507889}, {"x": 0.9209328889846802, "y": 0.23296703398227692}, {"x": 0.5091012716293335, "y": 0.23296703398227692}], "text": "In this paradigm, SLMs serve as filters, while LLMs function\nas reordering agents. The research shows that instructing\nLLMs to rearrange challenging samples identified by SLMs\nleads to significant improvements in various Information\nExtraction (IE) tasks. Another straightforward and effective\napproach involves having the LLM evaluate the retrieved\ncontent before generating the final answer. This allows the\nLLM to filter out documents with poor relevance through LLM\ncritique. For instance, in Chatlaw [104], the LLM is prompted\nto self-suggestion on the referenced legal provisions to assess\ntheir relevance.\n"}
{"page": 10, "bbox": [{"x": 0.22411832213401794, "y": 0.18065933883190155}, {"x": 0.3452787399291992, "y": 0.18065933883190155}, {"x": 0.3452787399291992, "y": 0.18901099264621735}, {"x": 0.22411832213401794, "y": 0.18901099264621735}], "text": "IV. GENERATION\n"}
{"page": 10, "bbox": [{"x": 0.07906711846590042, "y": 0.1991208791732788}, {"x": 0.49032992124557495, "y": 0.19956043362617493}, {"x": 0.49032992124557495, "y": 0.25670328736305237}, {"x": 0.07906711846590042, "y": 0.25626373291015625}], "text": "After retrieval, it is not a good practice to directly input all\nthe retrieved information to the LLM for answering questions.\nFollowing will introduce adjustments from two perspectives:\nadjusting the retrieved content and adjusting the LLM.\n"}
{"page": 10, "bbox": [{"x": 0.5091012716293335, "y": 0.25846153497695923}, {"x": 0.6461888551712036, "y": 0.2602197825908661}, {"x": 0.6461888551712036, "y": 0.27076923847198486}, {"x": 0.5091012716293335, "y": 0.269010990858078}], "text": "B. LLM Fine-tuning\n"}
{"page": 10, "bbox": [{"x": 0.07792946696281433, "y": 0.280439555644989}, {"x": 0.21672354638576508, "y": 0.280439555644989}, {"x": 0.21672354638576508, "y": 0.2887912094593048}, {"x": 0.07792946696281433, "y": 0.2887912094593048}], "text": "A. Context Curation\n"}
{"page": 10, "bbox": [{"x": 0.07906711846590042, "y": 0.2967033088207245}, {"x": 0.489761084318161, "y": 0.2967033088207245}, {"x": 0.489761084318161, "y": 0.3868131935596466}, {"x": 0.07906711846590042, "y": 0.3868131935596466}], "text": "Redundant information can interfere with the final gener-\nation of LLM, and overly long contexts can also lead LLM\nto the \"Lost in the middle\" problem [98]. Like humans, LLM\ntends to only focus on the beginning and end of long texts,\nwhile forgetting the middle portion. Therefore, in the RAG\nsystem, we typically need to further process the retrieved\n"}
{"page": 10, "bbox": [{"x": 0.07963594794273376, "y": 0.39120879769325256}, {"x": 0.13083049654960632, "y": 0.39120879769325256}, {"x": 0.13083049654960632, "y": 0.3982417583465576}, {"x": 0.07963594794273376, "y": 0.3982417583465576}], "text": "content.\n"}
{"page": 10, "bbox": [{"x": 0.5085324048995972, "y": 0.27824175357818604}, {"x": 0.9209328889846802, "y": 0.27824175357818604}, {"x": 0.9209328889846802, "y": 0.7164835333824158}, {"x": 0.5085324048995972, "y": 0.7164835333824158}], "text": "Targeted fine-tuning based on the scenario and data char-\nacteristics on LLMs can yield better results. This is also one\nof the greatest advantages of using on-premise LLMs. When\nLLMs lack data in a specific domain, additional knowledge can\nbe provided to the LLM through fine-tuning. Huggingface's\nfine-tuning data can also be used as an initial step.\nAnother benefit of fine-tuning is the ability to adjust the\nmodel's input and output. For example, it can enable LLM to\nadapt to specific data formats and generate responses in a par-\nticular style as instructed [37]. For retrieval tasks that engage\nwith structured data, the SANTA framework [76] implements\na tripartite training regimen to effectively encapsulate both\nstructural and semantic nuances. The initial phase focuses on\nthe retriever, where contrastive learning is harnessed to refine\nthe query and document embeddings.\nAligning LLM outputs with human or retriever preferences\nthrough reinforcement learning is a potential approach. For\ninstance, manually annotating the final generated answers\nand then providing feedback through reinforcement learning.\nIn addition to aligning with human preferences, it is also\npossible to align with the preferences of fine-tuned models\nand retrievers [79]. When circumstances prevent access to\npowerful proprietary models or larger parameter open-source\nmodels, a simple and effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align pref-\nerences. A typical approach, such as RA-DIT [27], aligns the\nscoring functions between Retriever and Generator using KL\ndivergence.\n"}
{"page": 10, "bbox": [{"x": 0.07849829643964767, "y": 0.4026373624801636}, {"x": 0.49146756529808044, "y": 0.40219780802726746}, {"x": 0.4920364022254944, "y": 0.944615364074707}, {"x": 0.07906711846590042, "y": 0.9450549483299255}], "text": "1) Reranking: Reranking fundamentally reorders document\nchunks to highlight the most pertinent results first, effectively\nreducing the overall document pool, severing a dual purpose\nin information retrieval, acting as both an enhancer and a\nfilter, delivering refined inputs for more precise language\nmodel processing [70]. Reranking can be performed using\nrule-based methods that depend on predefined metrics like\nDiversity, Relevance, and MRR, or model-based approaches\nlike Encoder-Decoder models from the BERT series (e.g.,\nSpanBERT), specialized reranking models such as Cohere\nrerank or bge-raranker-large, and general large language mod-\nels like GPT [12], [99].\n2) Context Selection/Compression: A common misconcep-\ntion in the RAG process is the belief that retrieving as many\nrelevant documents as possible and concatenating them to form\na lengthy retrieval prompt is beneficial. However, excessive\ncontext can introduce more noise, diminishing the LLM's\nperception of key information.\n(Long) LLMLingua [100], [101] utilize small language\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\ndetect and remove unimportant tokens, transforming it into\na form that is challenging for humans to comprehend but\nwell understood by LLMs. This approach presents a direct\nand practical method for prompt compression, eliminating the\nneed for additional training of LLMs while balancing language\nintegrity and compression ratio. PRCA tackled this issue by\ntraining an information extractor [69]. Similarly, RECOMP\nadopts a comparable approach by training an information\ncondenser using contrastive learning [71]. Each training data\npoint consists of one positive sample and five negative sam-\nples, and the encoder undergoes training using contrastive loss\nthroughout this process [102] .\nIn addition to compressing the context, reducing the num-\nber of documents aslo helps improve the accuracy of the\nmodel's answers. Ma et al. [103] propose the \"Filter-Reranker\"\nparadigm, which combines the strengths of LLMS and SLMs.\n"}
{"page": 10, "bbox": [{"x": 0.5091012716293335, "y": 0.7367032766342163}, {"x": 0.9203640222549438, "y": 0.7367032766342163}, {"x": 0.9203640222549438, "y": 0.8575823903083801}, {"x": 0.5091012716293335, "y": 0.8575823903083801}], "text": "V. AUGMENTATION PROCESS IN RAG\nIn the domain of RAG, the standard practice often involves\na singular (once) retrieval step followed by generation, which\ncan lead to inefficiencies and sometimes is typically insuffi-\ncient for complex problems demanding multi-step reasoning,\nas it provides a limited scope of information [105]. Many\nstudies have optimized the retrieval process in response to this\nissue, and we have summarised them in Figure 5.\n"}
{"page": 10, "bbox": [{"x": 0.5085324048995972, "y": 0.8835164904594421}, {"x": 0.6530147790908813, "y": 0.8835164904594421}, {"x": 0.6530147790908813, "y": 0.8918681144714355}, {"x": 0.5085324048995972, "y": 0.8918681144714355}], "text": "A. Iterative Retrieval\n"}
{"page": 10, "bbox": [{"x": 0.5091012716293335, "y": 0.9028571248054504}, {"x": 0.9209328889846802, "y": 0.9028571248054504}, {"x": 0.9209328889846802, "y": 0.9450549483299255}, {"x": 0.5091012716293335, "y": 0.9450549483299255}], "text": "Iterative retrieval is a process where the knowledge base\nis repeatedly searched based on the initial query and the text\ngenerated so far, providing a more comprehensive knowledge\n"}
{"page": 11, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9180887341499329, "y": 0.03384615480899811}, {"x": 0.9180887341499329, "y": 0.0391208790242672}, {"x": 0.9089874625205994, "y": 0.0391208790242672}], "text": "11\n"}
{"page": 11, "bbox": [{"x": 0.1678043156862259, "y": 0.08527472615242004}, {"x": 0.24004550278186798, "y": 0.08571428805589676}, {"x": 0.24004550278186798, "y": 0.09362637251615524}, {"x": 0.1678043156862259, "y": 0.09318681061267853}], "text": "ITERATIVE\n"}
{"page": 11, "bbox": [{"x": 0.41524460911750793, "y": 0.08483516424894333}, {"x": 0.4960182011127472, "y": 0.08527472615242004}, {"x": 0.4960182011127472, "y": 0.09406593441963196}, {"x": 0.41524460911750793, "y": 0.09362637251615524}], "text": "RECURSIVE\n"}
{"page": 11, "bbox": [{"x": 0.7167235612869263, "y": 0.08571428805589676}, {"x": 0.788395881652832, "y": 0.08571428805589676}, {"x": 0.788395881652832, "y": 0.09362637251615524}, {"x": 0.7167235612869263, "y": 0.09362637251615524}], "text": "ADAPTIVE\n"}
{"page": 11, "bbox": [{"x": 0.14106939733028412, "y": 0.10153846442699432}, {"x": 0.26678043603897095, "y": 0.10153846442699432}, {"x": 0.26678043603897095, "y": 0.1063736230134964}, {"x": 0.14106939733028412, "y": 0.1063736230134964}], "text": "Provide more context information\n"}
{"page": 11, "bbox": [{"x": 0.6513082981109619, "y": 0.1006593406200409}, {"x": 0.8543799519538879, "y": 0.10153846442699432}, {"x": 0.8543799519538879, "y": 0.10769230872392654}, {"x": 0.6513082981109619, "y": 0.10681318491697311}], "text": "Flexible and active control of retrieval and generation\n"}
{"page": 11, "bbox": [{"x": 0.37258246541023254, "y": 0.1006593406200409}, {"x": 0.5375426411628723, "y": 0.10109890252351761}, {"x": 0.5375426411628723, "y": 0.10813187062740326}, {"x": 0.37258246541023254, "y": 0.10769230872392654}], "text": "Break down complex problems step by step\n"}
{"page": 11, "bbox": [{"x": 0.7076222896575928, "y": 0.12791208922863007}, {"x": 0.7406143546104431, "y": 0.12791208922863007}, {"x": 0.7406143546104431, "y": 0.13494504988193512}, {"x": 0.7076222896575928, "y": 0.13494504988193512}], "text": "Query\n"}
{"page": 11, "bbox": [{"x": 0.1535836160182953, "y": 0.1371428519487381}, {"x": 0.18657565116882324, "y": 0.13802197575569153}, {"x": 0.1860068291425705, "y": 0.14505495131015778}, {"x": 0.15301479399204254, "y": 0.14417582750320435}], "text": "Query\n"}
{"page": 11, "bbox": [{"x": 0.3885096609592438, "y": 0.13846154510974884}, {"x": 0.42150169610977173, "y": 0.13890109956264496}, {"x": 0.42150169610977173, "y": 0.14637362957000732}, {"x": 0.3885096609592438, "y": 0.14593406021595}], "text": "Query\n"}
{"page": 11, "bbox": [{"x": 0.7827076315879822, "y": 0.1709890067577362}, {"x": 0.871444821357727, "y": 0.1709890067577362}, {"x": 0.871444821357727, "y": 0.1762637346982956}, {"x": 0.7827076315879822, "y": 0.1762637346982956}], "text": "Retrieve On Demand\n"}
{"page": 11, "bbox": [{"x": 0.7070534825325012, "y": 0.1705494523048401}, {"x": 0.7406143546104431, "y": 0.1705494523048401}, {"x": 0.7406143546104431, "y": 0.17758241295814514}, {"x": 0.7070534825325012, "y": 0.17758241295814514}], "text": "Judge\n"}
{"page": 11, "bbox": [{"x": 0.14675767719745636, "y": 0.17846153676509857}, {"x": 0.1899886280298233, "y": 0.17802198231220245}, {"x": 0.1899886280298233, "y": 0.1846153885126114}, {"x": 0.14675767719745636, "y": 0.1850549429655075}], "text": "Retrieve\n"}
{"page": 11, "bbox": [{"x": 0.38168373703956604, "y": 0.17978021502494812}, {"x": 0.424914687871933, "y": 0.17978021502494812}, {"x": 0.424914687871933, "y": 0.18637362122535706}, {"x": 0.38168373703956604, "y": 0.18637362122535706}], "text": "Retrieve\n"}
{"page": 11, "bbox": [{"x": 0.700796365737915, "y": 0.21362636983394623}, {"x": 0.7445961236953735, "y": 0.21406593918800354}, {"x": 0.7445961236953735, "y": 0.21978022158145905}, {"x": 0.700796365737915, "y": 0.21934065222740173}], "text": "Retrieve\n"}
{"page": 11, "bbox": [{"x": 0.24971558153629303, "y": 0.21494504809379578}, {"x": 0.2775881588459015, "y": 0.21494504809379578}, {"x": 0.2775881588459015, "y": 0.21978022158145905}, {"x": 0.24971558153629303, "y": 0.21978022158145905}], "text": "Iterate\n"}
{"page": 11, "bbox": [{"x": 0.14505119621753693, "y": 0.22021977603435516}, {"x": 0.1939704269170761, "y": 0.22021977603435516}, {"x": 0.1939704269170761, "y": 0.2268131822347641}, {"x": 0.14505119621753693, "y": 0.2268131822347641}], "text": "Generate\n"}
{"page": 11, "bbox": [{"x": 0.3799772560596466, "y": 0.2210988998413086}, {"x": 0.42946529388427734, "y": 0.22197802364826202}, {"x": 0.42946529388427734, "y": 0.22857142984867096}, {"x": 0.3799772560596466, "y": 0.22769230604171753}], "text": "Generate\n"}
{"page": 11, "bbox": [{"x": 0.4624573290348053, "y": 0.21406593918800354}, {"x": 0.5403867959976196, "y": 0.21406593918800354}, {"x": 0.5403867959976196, "y": 0.23692308366298676}, {"x": 0.4624573290348053, "y": 0.23692308366298676}], "text": "Query\nTransformation /\nDecomposition\n"}
{"page": 11, "bbox": [{"x": 0.2599544823169708, "y": 0.22461538016796112}, {"x": 0.26678043603897095, "y": 0.22461538016796112}, {"x": 0.26678043603897095, "y": 0.2294505536556244}, {"x": 0.2599544823169708, "y": 0.2294505536556244}], "text": "N\n"}
{"page": 11, "bbox": [{"x": 0.25142207741737366, "y": 0.23472528159618378}, {"x": 0.276450514793396, "y": 0.23472528159618378}, {"x": 0.276450514793396, "y": 0.23956044018268585}, {"x": 0.25142207741737366, "y": 0.23956044018268585}], "text": "Times\n"}
{"page": 11, "bbox": [{"x": 0.6990898847579956, "y": 0.24791209399700165}, {"x": 0.7491467595100403, "y": 0.24835164844989777}, {"x": 0.7491467595100403, "y": 0.2549450695514679}, {"x": 0.6990898847579956, "y": 0.2545054852962494}], "text": "Generate\n"}
{"page": 11, "bbox": [{"x": 0.61774742603302, "y": 0.24835164844989777}, {"x": 0.6666666865348816, "y": 0.24879120290279388}, {"x": 0.6666666865348816, "y": 0.2549450695514679}, {"x": 0.61774742603302, "y": 0.2545054852962494}], "text": "Generate\n"}
{"page": 11, "bbox": [{"x": 0.7810011506080627, "y": 0.23956044018268585}, {"x": 0.8589305877685547, "y": 0.24043956398963928}, {"x": 0.8583617806434631, "y": 0.26505494117736816}, {"x": 0.7804322838783264, "y": 0.2641758322715759}], "text": "Query\nTransformation /\nDecomposition\n"}
{"page": 11, "bbox": [{"x": 0.15301479399204254, "y": 0.26989009976387024}, {"x": 0.18657565116882324, "y": 0.27032968401908875}, {"x": 0.18657565116882324, "y": 0.2778021991252899}, {"x": 0.15301479399204254, "y": 0.2773626446723938}], "text": "Judge\n"}
{"page": 11, "bbox": [{"x": 0.3879408538341522, "y": 0.2694505453109741}, {"x": 0.42150169610977173, "y": 0.271208792924881}, {"x": 0.4209328889846802, "y": 0.27824175357818604}, {"x": 0.3873720169067383, "y": 0.2769230902194977}], "text": "Judge\n"}
{"page": 11, "bbox": [{"x": 0.7070534825325012, "y": 0.2909890115261078}, {"x": 0.7411831617355347, "y": 0.2914285659790039}, {"x": 0.7411831617355347, "y": 0.2993406653404236}, {"x": 0.7070534825325012, "y": 0.29890111088752747}], "text": "Judge\n"}
{"page": 11, "bbox": [{"x": 0.41524460911750793, "y": 0.30065932869911194}, {"x": 0.5386803150177002, "y": 0.3002197742462158}, {"x": 0.5386803150177002, "y": 0.3072527348995209}, {"x": 0.41524460911750793, "y": 0.3076923191547394}], "text": "Max Depth (Tree) / Threshold\n"}
{"page": 11, "bbox": [{"x": 0.17974971234798431, "y": 0.30153846740722656}, {"x": 0.27588167786598206, "y": 0.30153846740722656}, {"x": 0.27588167786598206, "y": 0.30681318044662476}, {"x": 0.17974971234798431, "y": 0.30681318044662476}], "text": "Max Times/Threshold\n"}
{"page": 11, "bbox": [{"x": 0.7366325259208679, "y": 0.31780219078063965}, {"x": 0.8873720169067383, "y": 0.31780219078063965}, {"x": 0.8873720169067383, "y": 0.32351648807525635}, {"x": 0.7366325259208679, "y": 0.32351648807525635}], "text": "Generate Special Token / Threshold\n"}
{"page": 11, "bbox": [{"x": 0.1433447152376175, "y": 0.3217582404613495}, {"x": 0.19681456685066223, "y": 0.3217582404613495}, {"x": 0.19681456685066223, "y": 0.32923075556755066}, {"x": 0.1433447152376175, "y": 0.32923075556755066}], "text": "Response\n"}
{"page": 11, "bbox": [{"x": 0.3782707750797272, "y": 0.32131868600845337}, {"x": 0.4311717748641968, "y": 0.3217582404613495}, {"x": 0.4311717748641968, "y": 0.32967033982276917}, {"x": 0.3782707750797272, "y": 0.32923075556755066}], "text": "Response\n"}
{"page": 11, "bbox": [{"x": 0.6973834037780762, "y": 0.33450549840927124}, {"x": 0.7502844333648682, "y": 0.33494505286216736}, {"x": 0.7502844333648682, "y": 0.34241756796836853}, {"x": 0.6973834037780762, "y": 0.3419780135154724}], "text": "Response\n"}
{"page": 11, "bbox": [{"x": 0.07906711846590042, "y": 0.3727472424507141}, {"x": 0.9203640222549438, "y": 0.3727472424507141}, {"x": 0.9203640222549438, "y": 0.42725273966789246}, {"x": 0.07906711846590042, "y": 0.42725273966789246}], "text": "Fig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\n"}
{"page": 11, "bbox": [{"x": 0.5091012716293335, "y": 0.45538461208343506}, {"x": 0.9209328889846802, "y": 0.4562637507915497}, {"x": 0.9203640222549438, "y": 0.5758242011070251}, {"x": 0.5085324048995972, "y": 0.5749450325965881}], "text": "retrieval involves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\n"}
{"page": 11, "bbox": [{"x": 0.07906711846590042, "y": 0.4567033052444458}, {"x": 0.4908987581729889, "y": 0.4567033052444458}, {"x": 0.4908987581729889, "y": 0.6342856884002686}, {"x": 0.07906711846590042, "y": 0.6342856884002686}], "text": "base for LLMs. This approach has been shown to enhance\nthe robustness of subsequent answer generation by offering\nadditional contextual references through multiple retrieval\niterations. However, it may be affected by semantic discon-\ntinuity and the accumulation of irrelevant information. ITER-\nRETGEN [14] employs a synergistic approach that lever-\nages “retrieval-enhanced generation” alongside “generation-\nenhanced retrieval\" for tasks that necessitate the reproduction\nof specific information. The model harnesses the content\nrequired to address the input task as a contextual basis for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n"}
{"page": 11, "bbox": [{"x": 0.509670078754425, "y": 0.5969230532646179}, {"x": 0.6569966077804565, "y": 0.595604419708252}, {"x": 0.6569966077804565, "y": 0.6070329546928406}, {"x": 0.509670078754425, "y": 0.6083516478538513}], "text": "C. Adaptive Retrieval\n"}
{"page": 11, "bbox": [{"x": 0.07963594794273376, "y": 0.6580219864845276}, {"x": 0.23208190500736237, "y": 0.6580219864845276}, {"x": 0.23208190500736237, "y": 0.666373610496521}, {"x": 0.07963594794273376, "y": 0.666373610496521}], "text": "B. Recursive Retrieval\n"}
{"page": 11, "bbox": [{"x": 0.5091012716293335, "y": 0.616263747215271}, {"x": 0.9209328889846802, "y": 0.616263747215271}, {"x": 0.9209328889846802, "y": 0.944615364074707}, {"x": 0.5091012716293335, "y": 0.944615364074707}], "text": "Adaptive retrieval methods, exemplified by Flare [24] and\nSelf-RAG [25], refine the RAG framework by enabling LLMs\nto actively determine the optimal moments and content for\nretrieval, thus enhancing the efficiency and relevance of the\ninformation sourced.\nThese methods are part of a broader trend wherein\nLLMs employ active judgment in their operations, as seen\nin model agents like AutoGPT, Toolformer, and Graph-\nToolformer [107]–[109]. Graph-Toolformer, for instance, di-\nvides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and em-\nploy few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search for necessary\ninformation, akin to how an agent utilizes tools.\nWebGPT [110] integrates a reinforcement learning frame-\nwork to train the GPT-3 model in autonomously using a\nsearch engine during text generation. It navigates this process\nusing special tokens that facilitate actions such as search\nengine queries, browsing results, and citing references, thereby\nexpanding GPT-3's capabilities through the use of external\nsearch engines. Flare automates timing retrieval by monitoring\nthe confidence of the generation process, as indicated by the\n"}
{"page": 11, "bbox": [{"x": 0.07906711846590042, "y": 0.6764835119247437}, {"x": 0.49146756529808044, "y": 0.6764835119247437}, {"x": 0.49146756529808044, "y": 0.944615364074707}, {"x": 0.07906711846590042, "y": 0.944615364074707}], "text": "Recursive retrieval is often used in information retrieval and\nNLP to improve the depth and relevance of search results.\nThe process involves iteratively refining search queries based\non the results obtained from previous searches. Recursive\nRetrieval aims to enhance the search experience by gradu-\nally converging on the most pertinent information through a\nfeedback loop. IRCOT [61] uses chain-of-thought to guide\nthe retrieval process and refines the CoT with the obtained\nretrieval results. ToC [57] creates a clarification tree that\nsystematically optimizes the ambiguous parts in the Query. It\ncan be particularly useful in complex search scenarios where\nthe user's needs are not entirely clear from the outset or where\nthe information sought is highly specialized or nuanced. The\nrecursive nature of the process allows for continuous learning\nand adaptation to the user's requirements, often resulting in\nimproved satisfaction with the search outcomes.\nTo address specific data scenarios, recursive retrieval and\nmulti-hop retrieval techniques are utilized together. Recursive\n"}
{"page": 12, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9186575412750244, "y": 0.03384615480899811}, {"x": 0.9186575412750244, "y": 0.0391208790242672}, {"x": 0.9089874625205994, "y": 0.0391208790242672}], "text": "12\n"}
{"page": 12, "bbox": [{"x": 0.5091012716293335, "y": 0.07252747565507889}, {"x": 0.9215016961097717, "y": 0.07252747565507889}, {"x": 0.9215016961097717, "y": 0.2949450612068176}, {"x": 0.5091012716293335, "y": 0.2949450612068176}], "text": "of search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on the generator's capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\ncan be categorized based on the content's objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\nof the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation methods [29], [161], [163].\n"}
{"page": 12, "bbox": [{"x": 0.07906711846590042, "y": 0.07164835184812546}, {"x": 0.49146756529808044, "y": 0.07252747565507889}, {"x": 0.49032992124557495, "y": 0.3419780135154724}, {"x": 0.07792946696281433, "y": 0.3410989046096802}], "text": "probability of generated terms [24]. When the probability falls\nbelow a certain threshold would activates the retrieval system\nto collect relevant information, thus optimizing the retrieval\ncycle. Self-RAG [25] introduces “reflection tokens” that allow\nthe model to introspect its outputs. These tokens come in\ntwo varieties: “retrieve\" and \"critic\". The model autonomously\ndecides when to activate retrieval, or alternatively, a predefined\nthreshold may trigger the process. During retrieval, the gen-\nerator conducts a fragment-level beam search across multiple\nparagraphs to derive the most coherent sequence. Critic scores\nare used to update the subdivision scores, with the flexibility\nto adjust these weights during inference, tailoring the model's\nbehavior. Self-RAG's design obviates the need for additional\nclassifiers or reliance on Natural Language Inference (NLI)\nmodels, thus streamlining the decision-making process for\nwhen to engage retrieval mechanisms and improving the\nmodel's autonomous judgment capabilities in generating ac-\ncurate responses.\n"}
{"page": 12, "bbox": [{"x": 0.5102388858795166, "y": 0.31208792328834534}, {"x": 0.6604095697402954, "y": 0.31472527980804443}, {"x": 0.6598407030105591, "y": 0.3257142901420593}, {"x": 0.509670078754425, "y": 0.32307693362236023}], "text": "C. Evaluation Aspects\n"}
{"page": 12, "bbox": [{"x": 0.18543799221515656, "y": 0.36307692527770996}, {"x": 0.3839590549468994, "y": 0.36307692527770996}, {"x": 0.3839590549468994, "y": 0.3718681335449219}, {"x": 0.18543799221515656, "y": 0.3718681335449219}], "text": "VI. TASK AND EVALUATION\n"}
{"page": 12, "bbox": [{"x": 0.07849829643964767, "y": 0.3810988962650299}, {"x": 0.49146756529808044, "y": 0.38241758942604065}, {"x": 0.4908987581729889, "y": 0.5019780397415161}, {"x": 0.07792946696281433, "y": 0.5006593465805054}], "text": "The rapid advancement and growing adoption of RAG\nin the field of NLP have propelled the evaluation of RAG\nmodels to the forefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios. This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\n"}
{"page": 12, "bbox": [{"x": 0.07849829643964767, "y": 0.5252747535705566}, {"x": 0.22070534527301788, "y": 0.5252747535705566}, {"x": 0.22070534527301788, "y": 0.53362637758255}, {"x": 0.07849829643964767, "y": 0.53362637758255}], "text": "A. Downstream Task\n"}
{"page": 12, "bbox": [{"x": 0.07906711846590042, "y": 0.5446153879165649}, {"x": 0.4908987581729889, "y": 0.5446153879165649}, {"x": 0.4908987581729889, "y": 0.6589010953903198}, {"x": 0.07906711846590042, "y": 0.6589010953903198}], "text": "The core task of RAG remains Question Answering (QA),\nincluding traditional single-hop/multi-hop QA, multiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n"}
{"page": 12, "bbox": [{"x": 0.5091012716293335, "y": 0.33230769634246826}, {"x": 0.9215016961097717, "y": 0.33230769634246826}, {"x": 0.9209328889846802, "y": 0.944615364074707}, {"x": 0.5085324048995972, "y": 0.944615364074707}], "text": "Contemporary evaluation practices of RAG models empha-\nsize three primary quality scores and four essential abilities,\nwhich collectively inform the evaluation of the two principal\ntargets of the RAG model: retrieval and generation.\n1) Quality Scores: Quality scores include context rele-\nvance, answer faithfulness, and answer relevance. These qual-\nity scores evaluate the efficiency of the RAG model from\ndifferent perspectives in the process of information retrieval\nand generation [164]–[166].\nContext Relevance evaluates the precision and specificity\nof the retrieved context, ensuring relevance and minimizing\nprocessing costs associated with extraneous content.\nAnswer Faithfulness ensures that the generated answers\nremain true to the retrieved context, maintaining consistency\nand avoiding contradictions.\nAnswer Relevance requires that the generated answers are\ndirectly pertinent to the posed questions, effectively addressing\nthe core inquiry.\n2) Required Abilities: RAG evaluation also encompasses\nfour abilities indicative of its adaptability and efficiency:\nnoise robustness, negative rejection, information integration,\nand counterfactual robustness [167], [168]. These abilities are\ncritical for the model's performance under various challenges\nand complex scenarios, impacting the quality scores.\nNoise Robustness appraises the model's capability to man-\nage noise documents that are question-related but lack sub-\nstantive information.\nNegative Rejection assesses the model's discernment in\nrefraining from responding when the retrieved documents do\nnot contain the necessary knowledge to answer a question.\nInformation Integration evaluates the model's proficiency in\nsynthesizing information from multiple documents to address\ncomplex questions.\nCounterfactual Robustness tests the model's ability to rec-\nognize and disregard known inaccuracies within documents,\neven when instructed about potential misinformation.\nContext relevance and noise robustness are important for\nevaluating the quality of retrieval, while answer faithfulness,\nanswer relevance, negative rejection, information integration,\nand counterfactual robustness are important for evaluating the\nquality of generation.\n"}
{"page": 12, "bbox": [{"x": 0.07963594794273376, "y": 0.6857143044471741}, {"x": 0.22184300422668457, "y": 0.6874725222587585}, {"x": 0.22184300422668457, "y": 0.6975824236869812}, {"x": 0.07963594794273376, "y": 0.695824146270752}], "text": "B. Evaluation Target\n"}
{"page": 12, "bbox": [{"x": 0.07906711846590042, "y": 0.7063736319541931}, {"x": 0.4908987581729889, "y": 0.7063736319541931}, {"x": 0.4908987581729889, "y": 0.944615364074707}, {"x": 0.07906711846590042, "y": 0.944615364074707}], "text": "Historically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models. The main evaluation objectives include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n"}
{"page": 13, "bbox": [{"x": 0.9084186553955078, "y": 0.03340659290552139}, {"x": 0.9192264080047607, "y": 0.03340659290552139}, {"x": 0.9192264080047607, "y": 0.03868131712079048}, {"x": 0.9084186553955078, "y": 0.03868131712079048}], "text": "13\n"}
{"page": 13, "bbox": [{"x": 0.4732650816440582, "y": 0.07428571581840515}, {"x": 0.5267349481582642, "y": 0.07472527772188187}, {"x": 0.5267349481582642, "y": 0.08175823837518692}, {"x": 0.4732650816440582, "y": 0.0813186839222908}], "text": "TABLE II\n"}
{"page": 13, "bbox": [{"x": 0.37144482135772705, "y": 0.08615384250879288}, {"x": 0.6279863715171814, "y": 0.08615384250879288}, {"x": 0.6279863715171814, "y": 0.09274725615978241}, {"x": 0.37144482135772705, "y": 0.09274725615978241}], "text": "DOWNSTREAM TASKS AND DATASETS OF RAG\n"}
{"page": 13, "bbox": [{"x": 0.16837315261363983, "y": 0.11296703666448593}, {"x": 0.22980660200119019, "y": 0.11296703666448593}, {"x": 0.22980660200119019, "y": 0.12087912112474442}, {"x": 0.16837315261363983, "y": 0.12087912112474442}], "text": "Sub Task\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.11296703666448593}, {"x": 0.6751990914344788, "y": 0.11296703666448593}, {"x": 0.6751990914344788, "y": 0.12087912112474442}, {"x": 0.6251422166824341, "y": 0.12087912112474442}], "text": "Method\n"}
{"page": 13, "bbox": [{"x": 0.08020477741956711, "y": 0.11340659111738205}, {"x": 0.11092150211334229, "y": 0.11340659111738205}, {"x": 0.11092150211334229, "y": 0.12131868302822113}, {"x": 0.08020477741956711, "y": 0.12131868302822113}], "text": "Task\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.11296703666448593}, {"x": 0.4197952151298523, "y": 0.11340659111738205}, {"x": 0.4197952151298523, "y": 0.12175824493169785}, {"x": 0.37030717730522156, "y": 0.12131868302822113}], "text": "Dataset\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.148131862282753}, {"x": 0.5511945486068726, "y": 0.14901098608970642}, {"x": 0.5511945486068726, "y": 0.16043956577777863}, {"x": 0.3697383403778076, "y": 0.1595604419708252}], "text": "Natural Qustion(NQ) [111]\n"}
{"page": 13, "bbox": [{"x": 0.07963594794273376, "y": 0.1485714316368103}, {"x": 0.1046643927693367, "y": 0.14945055544376373}, {"x": 0.10409556329250336, "y": 0.16087912023067474}, {"x": 0.07906711846590042, "y": 0.1599999964237213}], "text": "QA\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.14989010989665985}, {"x": 0.23947668075561523, "y": 0.14989010989665985}, {"x": 0.23947668075561523, "y": 0.16043956577777863}, {"x": 0.1678043156862259, "y": 0.16043956577777863}], "text": "Single-hop\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.13406594097614288}, {"x": 0.9829351305961609, "y": 0.13406594097614288}, {"x": 0.9829351305961609, "y": 0.2514285743236542}, {"x": 0.6257110238075256, "y": 0.2514285743236542}], "text": "[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\n[20], [44], [72]\n[13], [30], [34], [45], [50], [64]\n[4], [27], [59], [62], [112]\n[22], [25], [43], [44], [71], [72]\n[20], [23], [30], [32], [45], [69], [112]\n[3], [4], [13], [30], [50], [68]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.19384615123271942}, {"x": 0.5187713503837585, "y": 0.19472527503967285}, {"x": 0.5187713503837585, "y": 0.20615383982658386}, {"x": 0.37030717730522156, "y": 0.20527473092079163}], "text": "TriviaQA(TQA) [113]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.224175825715065}, {"x": 0.46302616596221924, "y": 0.22505494952201843}, {"x": 0.46302616596221924, "y": 0.235604390501976}, {"x": 0.37030717730522156, "y": 0.23472528159618378}], "text": "SQUAD [114]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.23868131637573242}, {"x": 0.5637087821960449, "y": 0.23999999463558197}, {"x": 0.5631399154663086, "y": 0.28175824880599976}, {"x": 0.3691695034503937, "y": 0.280439555644989}], "text": "Web Questions(WebQ) [115]\nPopQA [116]\nMS MARCO [117]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.255384624004364}, {"x": 0.7172923684120178, "y": 0.255384624004364}, {"x": 0.7172923684120178, "y": 0.28087911009788513}, {"x": 0.6251422166824341, "y": 0.28087911009788513}], "text": "[7], [25], [67]\n[4], [40], [52]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.2993406653404236}, {"x": 0.23435722291469574, "y": 0.2993406653404236}, {"x": 0.23435722291469574, "y": 0.3103296756744385}, {"x": 0.1678043156862259, "y": 0.3103296756744385}], "text": "Multi-hop\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.2914285659790039}, {"x": 0.9391353726387024, "y": 0.2914285659790039}, {"x": 0.9391353726387024, "y": 0.3331868052482605}, {"x": 0.6257110238075256, "y": 0.3331868052482605}], "text": "[23], [26], [31], [34], [47], [51], [61], [82]\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\n[14], [24], [48], [59], [61], [91]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.29846152663230896}, {"x": 0.5386803150177002, "y": 0.29846152663230896}, {"x": 0.5386803150177002, "y": 0.3476923108100891}, {"x": 0.3697383403778076, "y": 0.3476923108100891}], "text": "HotpotQA [118]\n2WikiMultiHopQA [119]\nMuSiQue [120]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.3367033004760742}, {"x": 0.799203634262085, "y": 0.3367033004760742}, {"x": 0.799203634262085, "y": 0.383736252784729}, {"x": 0.6257110238075256, "y": 0.383736252784729}], "text": "[14], [51], [61], [91]\n[27], [34], [43], [49], [51]\n[45], [60], [63], [123]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.35692307353019714}, {"x": 0.44482365250587463, "y": 0.358681321144104}, {"x": 0.4442548453807831, "y": 0.3692307770252228}, {"x": 0.3697383403778076, "y": 0.3674725294113159}], "text": "ELI5 [121]\n"}
{"page": 13, "bbox": [{"x": 0.16723549365997314, "y": 0.358681321144104}, {"x": 0.2684869170188904, "y": 0.35780221223831177}, {"x": 0.2684869170188904, "y": 0.36879122257232666}, {"x": 0.16723549365997314, "y": 0.3696703314781189}], "text": "Long-form QA\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.3731868267059326}, {"x": 0.5415244698524475, "y": 0.3731868267059326}, {"x": 0.5415244698524475, "y": 0.3841758370399475}, {"x": 0.3697383403778076, "y": 0.3841758370399475}], "text": "NarrativeQA(NQA) [122]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.38857144117355347}, {"x": 0.4550625681877136, "y": 0.38857144117355347}, {"x": 0.4550625681877136, "y": 0.39912086725234985}, {"x": 0.37030717730522156, "y": 0.39912086725234985}], "text": "ASQA [124]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.38857144117355347}, {"x": 0.6882821321487427, "y": 0.38857144117355347}, {"x": 0.6882821321487427, "y": 0.39912086725234985}, {"x": 0.6257110238075256, "y": 0.39912086725234985}], "text": "[24], [57]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.4039560556411743}, {"x": 0.6968145370483398, "y": 0.4039560556411743}, {"x": 0.6968145370483398, "y": 0.4140659272670746}, {"x": 0.6257110238075256, "y": 0.4140659272670746}], "text": "[60], [123]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.4039560556411743}, {"x": 0.5034129619598389, "y": 0.4039560556411743}, {"x": 0.5034129619598389, "y": 0.4145054817199707}, {"x": 0.37030717730522156, "y": 0.4145054817199707}], "text": "QMSum(QM) [125]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.42417582869529724}, {"x": 0.24857792258262634, "y": 0.42461538314819336}, {"x": 0.24857792258262634, "y": 0.434725284576416}, {"x": 0.1678043156862259, "y": 0.4342857003211975}], "text": "Domain QA\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.42461538314819336}, {"x": 0.6882821321487427, "y": 0.42417582869529724}, {"x": 0.6882821321487427, "y": 0.434725284576416}, {"x": 0.6251422166824341, "y": 0.43516483902931213}], "text": "[60], [63]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.42461538314819336}, {"x": 0.4562002420425415, "y": 0.42417582869529724}, {"x": 0.4562002420425415, "y": 0.43516483902931213}, {"x": 0.37030717730522156, "y": 0.43560439348220825}], "text": "Qasper [126]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.4404395520687103}, {"x": 0.4908987581729889, "y": 0.4404395520687103}, {"x": 0.4908987581729889, "y": 0.45010989904403687}, {"x": 0.3697383403778076, "y": 0.45010989904403687}], "text": "COVID-QA [127]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.4404395520687103}, {"x": 0.6888509392738342, "y": 0.4404395520687103}, {"x": 0.6888509392738342, "y": 0.450549453496933}, {"x": 0.6251422166824341, "y": 0.450549453496933}], "text": "[35], [46]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.45494505763053894}, {"x": 0.6513082981109619, "y": 0.45494505763053894}, {"x": 0.6513082981109619, "y": 0.46593406796455383}, {"x": 0.3697383403778076, "y": 0.46593406796455383}], "text": "CMB [128], MMCU_Medical [129] [81]\n"}
{"page": 13, "bbox": [{"x": 0.16837315261363983, "y": 0.47472527623176575}, {"x": 0.28498294949531555, "y": 0.4764835238456726}, {"x": 0.28498294949531555, "y": 0.4874725341796875}, {"x": 0.16837315261363983, "y": 0.48571428656578064}], "text": "Multi-Choice QA\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.4769230782985687}, {"x": 0.4795221984386444, "y": 0.4769230782985687}, {"x": 0.4795221984386444, "y": 0.4870329797267914}, {"x": 0.3697383403778076, "y": 0.4870329797267914}], "text": "QUALITY [130]\n"}
{"page": 13, "bbox": [{"x": 0.6245733499526978, "y": 0.4769230782985687}, {"x": 0.6888509392738342, "y": 0.4769230782985687}, {"x": 0.6888509392738342, "y": 0.4874725341796875}, {"x": 0.6245733499526978, "y": 0.4874725341796875}], "text": "[60], [63]\n"}
{"page": 13, "bbox": [{"x": 0.3708759844303131, "y": 0.4901098906993866}, {"x": 0.44482365250587463, "y": 0.49186813831329346}, {"x": 0.4442548453807831, "y": 0.5024175643920898}, {"x": 0.37030717730522156, "y": 0.5006593465805054}], "text": "ARC [131]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.49186813831329346}, {"x": 0.6888509392738342, "y": 0.49186813831329346}, {"x": 0.6888509392738342, "y": 0.5024175643920898}, {"x": 0.6251422166824341, "y": 0.5024175643920898}], "text": "[25], [67]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.5063736438751221}, {"x": 0.530147910118103, "y": 0.5068131685256958}, {"x": 0.530147910118103, "y": 0.517362654209137}, {"x": 0.3697383403778076, "y": 0.5169230699539185}], "text": "CommonsenseQA [132]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.5068131685256958}, {"x": 0.6882821321487427, "y": 0.5063736438751221}, {"x": 0.6882821321487427, "y": 0.5169230699539185}, {"x": 0.6251422166824341, "y": 0.517362654209137}], "text": "[58], [66]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.5283516645431519}, {"x": 0.6518771052360535, "y": 0.5283516645431519}, {"x": 0.6518771052360535, "y": 0.538021981716156}, {"x": 0.6257110238075256, "y": 0.538021981716156}], "text": "[84]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.5283516645431519}, {"x": 0.23720136284828186, "y": 0.5279120802879333}, {"x": 0.23720136284828186, "y": 0.5389010906219482}, {"x": 0.1678043156862259, "y": 0.5393406748771667}], "text": "Graph QA\n"}
{"page": 13, "bbox": [{"x": 0.3708759844303131, "y": 0.5283516645431519}, {"x": 0.46700796484947205, "y": 0.5283516645431519}, {"x": 0.46700796484947205, "y": 0.5389010906219482}, {"x": 0.3708759844303131, "y": 0.5389010906219482}], "text": "GraphQA [84]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.5494505763053894}, {"x": 0.2895335555076599, "y": 0.5476922988891602}, {"x": 0.2895335555076599, "y": 0.558681309223175}, {"x": 0.1678043156862259, "y": 0.5604395866394043}], "text": "Dialog Generation\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.5476922988891602}, {"x": 0.6040955781936646, "y": 0.5494505763053894}, {"x": 0.6040955781936646, "y": 0.5617582201957703}, {"x": 0.3697383403778076, "y": 0.5600000023841858}], "text": "Wizard of Wikipedia (WoW) [133]\n"}
{"page": 13, "bbox": [{"x": 0.07963594794273376, "y": 0.5494505763053894}, {"x": 0.1251422017812729, "y": 0.5503296852111816}, {"x": 0.12457337975502014, "y": 0.5600000023841858}, {"x": 0.07906711846590042, "y": 0.5591208934783936}], "text": "Dialog\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.5494505763053894}, {"x": 0.7616609930992126, "y": 0.5494505763053894}, {"x": 0.7616609930992126, "y": 0.5604395866394043}, {"x": 0.6257110238075256, "y": 0.5604395866394043}], "text": "[13], [27], [34], [42]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.5635164976119995}, {"x": 0.27474403381347656, "y": 0.565274715423584}, {"x": 0.27474403381347656, "y": 0.5749450325965881}, {"x": 0.1678043156862259, "y": 0.5731868147850037}], "text": "Personal Dialog\n"}
{"page": 13, "bbox": [{"x": 0.3708759844303131, "y": 0.563076913356781}, {"x": 0.44254836440086365, "y": 0.5648351907730103}, {"x": 0.4419795274734497, "y": 0.5758242011070251}, {"x": 0.37030717730522156, "y": 0.5740659236907959}], "text": "KBP [134]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.5648351907730103}, {"x": 0.6962457299232483, "y": 0.5648351907730103}, {"x": 0.6962457299232483, "y": 0.5749450325965881}, {"x": 0.6251422166824341, "y": 0.5749450325965881}], "text": "[74], [135]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.5780220031738281}, {"x": 0.4726962447166443, "y": 0.5793406367301941}, {"x": 0.4726962447166443, "y": 0.5898901224136353}, {"x": 0.37030717730522156, "y": 0.5885714292526245}], "text": "DuleMon [136]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.5802198052406311}, {"x": 0.6513082981109619, "y": 0.5802198052406311}, {"x": 0.6513082981109619, "y": 0.5907692313194275}, {"x": 0.6257110238075256, "y": 0.5907692313194275}], "text": "[74]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.593406617641449}, {"x": 0.4704209268093109, "y": 0.5951648354530334}, {"x": 0.469852089881897, "y": 0.6057142615318298}, {"x": 0.3691695034503937, "y": 0.6039560437202454}], "text": "CamRest [137]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.5947252511978149}, {"x": 0.6888509392738342, "y": 0.5951648354530334}, {"x": 0.6888509392738342, "y": 0.6057142615318298}, {"x": 0.6257110238075256, "y": 0.6052747368812561}], "text": "[78], [79]\n"}
{"page": 13, "bbox": [{"x": 0.16837315261363983, "y": 0.593406617641449}, {"x": 0.30830490589141846, "y": 0.5942857265472412}, {"x": 0.3077360689640045, "y": 0.6189010739326477}, {"x": 0.1678043156862259, "y": 0.6180219650268555}], "text": "Task-oriented Dialog\nRecommendation\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.6092307567596436}, {"x": 0.6894198060035706, "y": 0.6096703410148621}, {"x": 0.6894198060035706, "y": 0.620659351348877}, {"x": 0.6257110238075256, "y": 0.6202197670936584}], "text": "[39], [40]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.6101098656654358}, {"x": 0.5972696542739868, "y": 0.6101098656654358}, {"x": 0.5972696542739868, "y": 0.6210988759994507}, {"x": 0.3697383403778076, "y": 0.6210988759994507}], "text": "Amazon(Toys,Sport, Beauty) [138]\n"}
{"page": 13, "bbox": [{"x": 0.07963594794273376, "y": 0.6316483616828918}, {"x": 0.09499431401491165, "y": 0.6316483616828918}, {"x": 0.09499431401491165, "y": 0.6395604610443115}, {"x": 0.07963594794273376, "y": 0.6395604610443115}], "text": "IE\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.6312087774276733}, {"x": 0.3503981828689575, "y": 0.6303296685218811}, {"x": 0.3503981828689575, "y": 0.6404395699501038}, {"x": 0.1678043156862259, "y": 0.641318678855896}], "text": "Event Argument Extraction\n"}
{"page": 13, "bbox": [{"x": 0.3708759844303131, "y": 0.6298900842666626}, {"x": 0.48009100556373596, "y": 0.6316483616828918}, {"x": 0.48009100556373596, "y": 0.6426373720169067}, {"x": 0.3708759844303131, "y": 0.6408790946006775}], "text": "WikiEvent [139]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.6316483616828918}, {"x": 0.7622298002243042, "y": 0.6316483616828918}, {"x": 0.7622298002243042, "y": 0.6421977877616882}, {"x": 0.6257110238075256, "y": 0.6421977877616882}], "text": "[13], [27], [37], [42]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.6448351740837097}, {"x": 0.457337886095047, "y": 0.6461538672447205}, {"x": 0.45676904916763306, "y": 0.6567032933235168}, {"x": 0.3697383403778076, "y": 0.6553846001625061}], "text": "RAMS [140]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.6465933918952942}, {"x": 0.6894198060035706, "y": 0.6465933918952942}, {"x": 0.6894198060035706, "y": 0.6567032933235168}, {"x": 0.6251422166824341, "y": 0.6567032933235168}], "text": "[36], [37]\n"}
{"page": 13, "bbox": [{"x": 0.3708759844303131, "y": 0.6606593132019043}, {"x": 0.5352673530578613, "y": 0.6615384817123413}, {"x": 0.5352673530578613, "y": 0.6725274920463562}, {"x": 0.3708759844303131, "y": 0.6716483235359192}], "text": "T-REX [141],ZSRE [142]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.661978006362915}, {"x": 0.6877133250236511, "y": 0.6615384817123413}, {"x": 0.6877133250236511, "y": 0.6720879077911377}, {"x": 0.6251422166824341, "y": 0.6725274920463562}], "text": "[27], [51]\n"}
{"page": 13, "bbox": [{"x": 0.07963594794273376, "y": 0.6826373338699341}, {"x": 0.1473265141248703, "y": 0.6821978092193604}, {"x": 0.1473265141248703, "y": 0.6927472352981567}, {"x": 0.07963594794273376, "y": 0.6931868195533752}], "text": "Reasoning\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.6826373338699341}, {"x": 0.6882821321487427, "y": 0.6821978092193604}, {"x": 0.6882821321487427, "y": 0.6927472352981567}, {"x": 0.6251422166824341, "y": 0.6931868195533752}], "text": "[20], [66]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.6826373338699341}, {"x": 0.48236632347106934, "y": 0.6821978092193604}, {"x": 0.48236632347106934, "y": 0.6931868195533752}, {"x": 0.37030717730522156, "y": 0.693626344203949}], "text": "HellaSwag [143]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.6989011168479919}, {"x": 0.6513082981109619, "y": 0.6989011168479919}, {"x": 0.6513082981109619, "y": 0.7081318497657776}, {"x": 0.6251422166824341, "y": 0.7081318497657776}], "text": "[27]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.6971428394317627}, {"x": 0.5130830407142639, "y": 0.6975824236869812}, {"x": 0.5130830407142639, "y": 0.7239560484886169}, {"x": 0.37030717730522156, "y": 0.7235164642333984}], "text": "CoT Reasoning [144]\nCSQA [145]\n"}
{"page": 13, "bbox": [{"x": 0.16723549365997314, "y": 0.6610988974571228}, {"x": 0.33902162313461304, "y": 0.6615384817123413}, {"x": 0.3384527862071991, "y": 0.7613186836242676}, {"x": 0.1666666716337204, "y": 0.7608790993690491}], "text": "Relation Extraction\nCommonsense Reasoning\nCOT Reasoning\nComplex Reasoning\nLanguage Understanding\nLanguage Modeling\n"}
{"page": 13, "bbox": [{"x": 0.6245733499526978, "y": 0.7134066224098206}, {"x": 0.6513082981109619, "y": 0.7134066224098206}, {"x": 0.6513082981109619, "y": 0.7235164642333984}, {"x": 0.6245733499526978, "y": 0.7235164642333984}], "text": "[55]\n"}
{"page": 13, "bbox": [{"x": 0.07963594794273376, "y": 0.7345054745674133}, {"x": 0.12343572080135345, "y": 0.7349450588226318}, {"x": 0.12343572080135345, "y": 0.7432966828346252}, {"x": 0.07963594794273376, "y": 0.7428571581840515}], "text": "Others\n"}
{"page": 13, "bbox": [{"x": 0.626279890537262, "y": 0.7345054745674133}, {"x": 0.8651877045631409, "y": 0.7345054745674133}, {"x": 0.8651877045631409, "y": 0.7459340691566467}, {"x": 0.626279890537262, "y": 0.7459340691566467}], "text": "[7], [27], [28], [42], [43], [47], [72]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.7331868410110474}, {"x": 0.5011376738548279, "y": 0.7349450588226318}, {"x": 0.5005688071250916, "y": 0.7604395747184753}, {"x": 0.3691695034503937, "y": 0.7591208815574646}], "text": "MMLU [146]\nWikiText-103 [147]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.7648351788520813}, {"x": 0.4880546033382416, "y": 0.7643955945968628}, {"x": 0.4880546033382416, "y": 0.7753846049308777}, {"x": 0.37030717730522156, "y": 0.7758241891860962}], "text": "StrategyQA [148]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.7494505643844604}, {"x": 0.8367462754249573, "y": 0.7494505643844604}, {"x": 0.8367462754249573, "y": 0.7907692193984985}, {"x": 0.6257110238075256, "y": 0.7907692193984985}], "text": "[5], [29], [64], [71]\n[14], [24], [48], [51], [55], [58]\n[4], [13], [27], [34], [42], [50]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.7784615159034729}, {"x": 0.4624573290348053, "y": 0.7797802090644836}, {"x": 0.4624573290348053, "y": 0.7903296947479248}, {"x": 0.37030717730522156, "y": 0.7890110015869141}], "text": "FEVER [149]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.7797802090644836}, {"x": 0.3447099030017853, "y": 0.7797802090644836}, {"x": 0.3447099030017853, "y": 0.7898901104927063}, {"x": 0.1678043156862259, "y": 0.7898901104927063}], "text": "Fact Checking/Verification\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.79340660572052}, {"x": 0.48009100556373596, "y": 0.7947252988815308}, {"x": 0.48009100556373596, "y": 0.8061538338661194}, {"x": 0.37030717730522156, "y": 0.8048351407051086}], "text": "PubHealth [150]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.7951648235321045}, {"x": 0.6894198060035706, "y": 0.7951648235321045}, {"x": 0.6894198060035706, "y": 0.8057143092155457}, {"x": 0.6257110238075256, "y": 0.8057143092155457}], "text": "[25], [67]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.8105494379997253}, {"x": 0.6518771052360535, "y": 0.8105494379997253}, {"x": 0.6518771052360535, "y": 0.820659339427948}, {"x": 0.6251422166824341, "y": 0.820659339427948}], "text": "[67]\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.8101099133491516}, {"x": 0.48009100556373596, "y": 0.8096703290939331}, {"x": 0.48009100556373596, "y": 0.8210989236831665}, {"x": 0.3697383403778076, "y": 0.8215384483337402}], "text": "Biography [151]\n"}
{"page": 13, "bbox": [{"x": 0.16837315261363983, "y": 0.8105494379997253}, {"x": 0.3020477890968323, "y": 0.8105494379997253}, {"x": 0.3020477890968323, "y": 0.8342857360839844}, {"x": 0.16837315261363983, "y": 0.8342857360839844}], "text": "Text Generation\nText Summarization\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.8237362504005432}, {"x": 0.47212740778923035, "y": 0.825054943561554}, {"x": 0.47212740778923035, "y": 0.8356043696403503}, {"x": 0.37030717730522156, "y": 0.8342857360839844}], "text": "WikiASP [152]\n"}
{"page": 13, "bbox": [{"x": 0.6245733499526978, "y": 0.8254945278167725}, {"x": 0.6513082981109619, "y": 0.8254945278167725}, {"x": 0.6513082981109619, "y": 0.8356043696403503}, {"x": 0.6245733499526978, "y": 0.8356043696403503}], "text": "[24]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.8386813402175903}, {"x": 0.45278725028038025, "y": 0.8404395580291748}, {"x": 0.4522184431552887, "y": 0.8514285683631897}, {"x": 0.3697383403778076, "y": 0.8496703505516052}], "text": "XSum [153]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.8404395580291748}, {"x": 0.6513082981109619, "y": 0.8404395580291748}, {"x": 0.6513082981109619, "y": 0.8505494594573975}, {"x": 0.6251422166824341, "y": 0.8505494594573975}], "text": "[17]\n"}
{"page": 13, "bbox": [{"x": 0.16837315261363983, "y": 0.8553845882415771}, {"x": 0.2906712293624878, "y": 0.8553845882415771}, {"x": 0.2906712293624878, "y": 0.8641757965087891}, {"x": 0.16837315261363983, "y": 0.8641757965087891}], "text": "Text Classification\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.8540659546852112}, {"x": 0.4653014838695526, "y": 0.8558241724967957}, {"x": 0.46473264694213867, "y": 0.866373598575592}, {"x": 0.3697383403778076, "y": 0.8646153807640076}], "text": "VioLens [154]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.8558241724967957}, {"x": 0.6518771052360535, "y": 0.8558241724967957}, {"x": 0.6518771052360535, "y": 0.866373598575592}, {"x": 0.6251422166824341, "y": 0.866373598575592}], "text": "[19]\n"}
{"page": 13, "bbox": [{"x": 0.3708759844303131, "y": 0.869450569152832}, {"x": 0.4522184431552887, "y": 0.870769202709198}, {"x": 0.45164960622787476, "y": 0.8813186883926392}, {"x": 0.37030717730522156, "y": 0.8799999952316284}], "text": "TREC [155]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.870769202709198}, {"x": 0.6518771052360535, "y": 0.870769202709198}, {"x": 0.6518771052360535, "y": 0.8804395794868469}, {"x": 0.6257110238075256, "y": 0.8804395794868469}], "text": "[33]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.8839560151100159}, {"x": 0.4510807693004608, "y": 0.8857142925262451}, {"x": 0.4505119323730469, "y": 0.89670330286026}, {"x": 0.3697383403778076, "y": 0.8949450254440308}], "text": "SST-2 [156]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.8848351836204529}, {"x": 0.725824773311615, "y": 0.8852747082710266}, {"x": 0.725824773311615, "y": 0.8962637186050415}, {"x": 0.6251422166824341, "y": 0.8958241939544678}], "text": "[20], [33], [38]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.8848351836204529}, {"x": 0.31740614771842957, "y": 0.8848351836204529}, {"x": 0.31740614771842957, "y": 0.9243956208229065}, {"x": 0.1678043156862259, "y": 0.9243956208229065}], "text": "Sentiment\nCode Search\nRobustness Evaluation\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.9002197980880737}, {"x": 0.5130830407142639, "y": 0.901098906993866}, {"x": 0.5130830407142639, "y": 0.9112088084220886}, {"x": 0.37030717730522156, "y": 0.9103296995162964}], "text": "CodeSearchNet [157]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.901098906993866}, {"x": 0.6513082981109619, "y": 0.901098906993866}, {"x": 0.6513082981109619, "y": 0.9112088084220886}, {"x": 0.6251422166824341, "y": 0.9112088084220886}], "text": "[76]\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.9147253036499023}, {"x": 0.48521047830581665, "y": 0.9160439372062683}, {"x": 0.48521047830581665, "y": 0.9270329475402832}, {"x": 0.37030717730522156, "y": 0.9257143139839172}], "text": "NOMIRACL [56]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.9160439372062683}, {"x": 0.6518771052360535, "y": 0.9164835214614868}, {"x": 0.6518771052360535, "y": 0.9265934228897095}, {"x": 0.6251422166824341, "y": 0.926153838634491}], "text": "[56]\n"}
{"page": 13, "bbox": [{"x": 0.16837315261363983, "y": 0.9301099181175232}, {"x": 0.20193400979042053, "y": 0.9305494427680969}, {"x": 0.20193400979042053, "y": 0.9389011263847351}, {"x": 0.16837315261363983, "y": 0.9384615421295166}], "text": "Math\n"}
{"page": 13, "bbox": [{"x": 0.37030717730522156, "y": 0.9292307496070862}, {"x": 0.4664391279220581, "y": 0.9305494427680969}, {"x": 0.4664391279220581, "y": 0.9410989284515381}, {"x": 0.37030717730522156, "y": 0.9397802352905273}], "text": "GSM8K [158]\n"}
{"page": 13, "bbox": [{"x": 0.6251422166824341, "y": 0.9301099181175232}, {"x": 0.6524459719657898, "y": 0.9305494427680969}, {"x": 0.6518771052360535, "y": 0.9410989284515381}, {"x": 0.6245733499526978, "y": 0.9406593441963196}], "text": "[73]\n"}
{"page": 13, "bbox": [{"x": 0.1678043156862259, "y": 0.9463736414909363}, {"x": 0.30432310700416565, "y": 0.9463736414909363}, {"x": 0.30432310700416565, "y": 0.9547252655029297}, {"x": 0.1678043156862259, "y": 0.9547252655029297}], "text": "Machine Translation\n"}
{"page": 13, "bbox": [{"x": 0.3697383403778076, "y": 0.9459340572357178}, {"x": 0.489761084318161, "y": 0.9459340572357178}, {"x": 0.489761084318161, "y": 0.9564835429191589}, {"x": 0.3697383403778076, "y": 0.9564835429191589}], "text": "JRC-Acquis [159]\n"}
{"page": 13, "bbox": [{"x": 0.6257110238075256, "y": 0.9459340572357178}, {"x": 0.6524459719657898, "y": 0.94681316614151}, {"x": 0.6518771052360535, "y": 0.9578021764755249}, {"x": 0.6251422166824341, "y": 0.9569230675697327}], "text": "[17]\n"}
{"page": 14, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9186575412750244, "y": 0.03384615480899811}, {"x": 0.9186575412750244, "y": 0.03868131712079048}, {"x": 0.9089874625205994, "y": 0.03868131712079048}], "text": "14\n"}
{"page": 14, "bbox": [{"x": 0.47098976373672485, "y": 0.07472527772188187}, {"x": 0.5290102362632751, "y": 0.07472527772188187}, {"x": 0.5290102362632751, "y": 0.0813186839222908}, {"x": 0.47098976373672485, "y": 0.0813186839222908}], "text": "TABLE III\n"}
{"page": 14, "bbox": [{"x": 0.2974971532821655, "y": 0.08571428805589676}, {"x": 0.7019340395927429, "y": 0.08571428805589676}, {"x": 0.7019340395927429, "y": 0.09274725615978241}, {"x": 0.2974971532821655, "y": 0.09274725615978241}], "text": "SUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\n"}
{"page": 14, "bbox": [{"x": 0.3037542700767517, "y": 0.11560439318418503}, {"x": 0.35608646273612976, "y": 0.11560439318418503}, {"x": 0.35608646273612976, "y": 0.13362637162208557}, {"x": 0.3037542700767517, "y": 0.13362637162208557}], "text": "Context\nRelevance\n"}
{"page": 14, "bbox": [{"x": 0.4601820111274719, "y": 0.11560439318418503}, {"x": 0.5136518478393555, "y": 0.11560439318418503}, {"x": 0.5136518478393555, "y": 0.13362637162208557}, {"x": 0.4601820111274719, "y": 0.13362637162208557}], "text": "Answer\nRelevance\n"}
{"page": 14, "bbox": [{"x": 0.3771331012248993, "y": 0.12131868302822113}, {"x": 0.4402730464935303, "y": 0.12131868302822113}, {"x": 0.4402730464935303, "y": 0.12791208922863007}, {"x": 0.3771331012248993, "y": 0.12791208922863007}], "text": "Faithfulness\n"}
{"page": 14, "bbox": [{"x": 0.5329920649528503, "y": 0.11428571492433548}, {"x": 0.841296911239624, "y": 0.11428571492433548}, {"x": 0.841296911239624, "y": 0.13538461923599243}, {"x": 0.5329920649528503, "y": 0.13538461923599243}], "text": "Noise Negative Information Counterfactual\nRobustness Rejection Integration Robustness\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.14989010989665985}, {"x": 0.21501706540584564, "y": 0.14901098608970642}, {"x": 0.21501706540584564, "y": 0.1595604419708252}, {"x": 0.1535836160182953, "y": 0.16043956577777863}], "text": "Accuracy\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.16483516991138458}, {"x": 0.17633675038814545, "y": 0.16439560055732727}, {"x": 0.17633675038814545, "y": 0.17274725437164307}, {"x": 0.1535836160182953, "y": 0.17318680882453918}], "text": "EM\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.17978021502494812}, {"x": 0.1951080709695816, "y": 0.17978021502494812}, {"x": 0.1951080709695816, "y": 0.1876923143863678}, {"x": 0.1535836160182953, "y": 0.1876923143863678}], "text": "Recall\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.19472527503967285}, {"x": 0.21274174749851227, "y": 0.19472527503967285}, {"x": 0.21274174749851227, "y": 0.20351648330688477}, {"x": 0.1535836160182953, "y": 0.20351648330688477}], "text": "Precision\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.21054944396018982}, {"x": 0.19965870678424835, "y": 0.21098901331424713}, {"x": 0.19965870678424835, "y": 0.2184615433216095}, {"x": 0.1535836160182953, "y": 0.21802197396755219}], "text": "R-Rate\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.22285714745521545}, {"x": 0.27076223492622375, "y": 0.22505494952201843}, {"x": 0.2701933979988098, "y": 0.235604390501976}, {"x": 0.15301479399204254, "y": 0.23340658843517303}], "text": "Cosine Similarity\n"}
{"page": 14, "bbox": [{"x": 0.15415245294570923, "y": 0.2408791184425354}, {"x": 0.20989760756492615, "y": 0.2408791184425354}, {"x": 0.20989760756492615, "y": 0.24835164844989777}, {"x": 0.15415245294570923, "y": 0.24835164844989777}], "text": "Hit Rate\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.255384624004364}, {"x": 0.18885096907615662, "y": 0.25582417845726013}, {"x": 0.18885096907615662, "y": 0.2637362778186798}, {"x": 0.1535836160182953, "y": 0.2632966935634613}], "text": "MRR\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.26989009976387024}, {"x": 0.19852104783058167, "y": 0.26989009976387024}, {"x": 0.19852104783058167, "y": 0.27868130803108215}, {"x": 0.1535836160182953, "y": 0.27868130803108215}], "text": "NDCG\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.2861538529396057}, {"x": 0.19567690789699554, "y": 0.2861538529396057}, {"x": 0.19567690789699554, "y": 0.294065922498703}, {"x": 0.1535836160182953, "y": 0.294065922498703}], "text": "BLEU\n"}
{"page": 14, "bbox": [{"x": 0.4755403995513916, "y": 0.28439560532569885}, {"x": 0.4908987581729889, "y": 0.28483515977859497}, {"x": 0.489761084318161, "y": 0.30945053696632385}, {"x": 0.4744027256965637, "y": 0.30901098251342773}], "text": "V\n"}
{"page": 14, "bbox": [{"x": 0.1535836160182953, "y": 0.30065932869911194}, {"x": 0.2844141125679016, "y": 0.30065932869911194}, {"x": 0.2844141125679016, "y": 0.30901098251342773}, {"x": 0.1535836160182953, "y": 0.30901098251342773}], "text": "ROUGE/ROUGE-L\n"}
{"page": 14, "bbox": [{"x": 0.07906711846590042, "y": 0.34637361764907837}, {"x": 0.49146756529808044, "y": 0.34637361764907837}, {"x": 0.49146756529808044, "y": 0.44835165143013}, {"x": 0.07906711846590042, "y": 0.44835165143013}], "text": "The specific metrics for each evaluation aspect are sum-\nmarized in Table III. It is essential to recognize that these\nmetrics, derived from related work, are traditional measures\nand do not yet represent a mature or standardized approach for\nquantifying RAG evaluation aspects. Custom metrics tailored\nto the nuances of RAG models, though not included here, have\nalso been developed in some evaluation studies.\n"}
{"page": 14, "bbox": [{"x": 0.5079635977745056, "y": 0.34549450874328613}, {"x": 0.9215016961097717, "y": 0.34637361764907837}, {"x": 0.9203640222549438, "y": 0.5705494284629822}, {"x": 0.5068259239196777, "y": 0.5696703195571899}], "text": "are not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quickly locate the original\nreferences for LLMs to help users verify the generated an-\nswers. The entire retrieval and reasoning process is observable,\nwhile generation solely relying on long context remains a\nblack box. Conversely, the expansion of context provides new\nopportunities for the development of RAG, enabling it to\naddress more complex problems and integrative or summary\nquestions that require reading a large amount of material to\nanswer [49]. Developing new RAG methods in the context of\nsuper-long contexts is one of the future research trends.\n"}
{"page": 14, "bbox": [{"x": 0.07906711846590042, "y": 0.4707692265510559}, {"x": 0.4920364022254944, "y": 0.47252747416496277}, {"x": 0.4908987581729889, "y": 0.6676923036575317}, {"x": 0.07792946696281433, "y": 0.6659340858459473}], "text": "D. Evaluation Benchmarks and Tools\nA series of benchmark tests and tools have been proposed\nto facilitate the evaluation of RAG.These instruments furnish\nquantitative metrics that not only gauge RAG model perfor-\nmance but also enhance comprehension of the model's capabil-\nities across various evaluation aspects. Prominent benchmarks\nsuch as RGB, RECALL and CRUD [167]-[169] focus on\nappraising the essential abilities of RAG models. Concur-\nrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens³ employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\n"}
{"page": 14, "bbox": [{"x": 0.5091012716293335, "y": 0.5916483402252197}, {"x": 0.6416382193565369, "y": 0.5916483402252197}, {"x": 0.6416382193565369, "y": 0.5995604395866394}, {"x": 0.5091012716293335, "y": 0.5995604395866394}], "text": "B. RAG Robustness\n"}
{"page": 14, "bbox": [{"x": 0.07963594794273376, "y": 0.687912106513977}, {"x": 0.4908987581729889, "y": 0.6874725222587585}, {"x": 0.4908987581729889, "y": 0.7621977925300598}, {"x": 0.07963594794273376, "y": 0.7626373767852783}], "text": "VII. DISCUSSION AND FUTURE PROSPECTS\nDespite the considerable progress in RAG technology, sev-\neral challenges persist that warrant in-depth research. This\nchapter will mainly introduce the current challenges and future\nresearch directions faced by RAG.\n"}
{"page": 14, "bbox": [{"x": 0.5091012716293335, "y": 0.6096703410148621}, {"x": 0.9203640222549438, "y": 0.6096703410148621}, {"x": 0.9203640222549438, "y": 0.8474725484848022}, {"x": 0.5091012716293335, "y": 0.8474725484848022}], "text": "The presence of noise or contradictory information during\nretrieval can detrimentally affect RAG's output quality. This\nsituation is figuratively referred to as \"Misinformation can\nbe worse than no information at all”. Improving RAG's\nresistance to such adversarial or counterfactual inputs is gain-\ning research momentum and has become a key performance\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\ntype of documents should be retrieved, evaluate the relevance\nof the documents to the prompt, their position, and the\nnumber included in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\nof reduced quality. These results underscore the importance\nof developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\n"}
{"page": 14, "bbox": [{"x": 0.07792946696281433, "y": 0.7863736152648926}, {"x": 0.24857792258262634, "y": 0.7863736152648926}, {"x": 0.24857792258262634, "y": 0.7964835166931152}, {"x": 0.07792946696281433, "y": 0.7964835166931152}], "text": "A. RAG vs Long Context\n"}
{"page": 14, "bbox": [{"x": 0.07906711846590042, "y": 0.8048351407051086}, {"x": 0.4908987581729889, "y": 0.8048351407051086}, {"x": 0.4908987581729889, "y": 0.9063736200332642}, {"x": 0.07906711846590042, "y": 0.9063736200332642}], "text": "With the deepening of related research, the context of LLMs\nis continuously expanding [170]–[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens 9. This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate the entire\ndocument directly into the prompt. This has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n"}
{"page": 14, "bbox": [{"x": 0.5102388858795166, "y": 0.8676922917366028}, {"x": 0.6615471839904785, "y": 0.868571400642395}, {"x": 0.6615471839904785, "y": 0.8808791041374207}, {"x": 0.5102388858795166, "y": 0.8799999952316284}], "text": "C. Hybrid Approaches\n"}
{"page": 14, "bbox": [{"x": 0.5091012716293335, "y": 0.8879120945930481}, {"x": 0.9203640222549438, "y": 0.8874725103378296}, {"x": 0.9203640222549438, "y": 0.9450549483299255}, {"x": 0.5091012716293335, "y": 0.945494532585144}], "text": "Combining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-to-\nend joint training-and how to harness both parameterized\n"}
{"page": 14, "bbox": [{"x": 0.09840728342533112, "y": 0.9221978187561035}, {"x": 0.424914687871933, "y": 0.9221978187561035}, {"x": 0.424914687871933, "y": 0.9314285516738892}, {"x": 0.09840728342533112, "y": 0.9314285516738892}], "text": "https://www.trulens.org/trulens_eval/core_concepts_rag_triad/\n"}
{"page": 14, "bbox": [{"x": 0.09840728342533112, "y": 0.9345055222511292}, {"x": 0.22980660200119019, "y": 0.9345055222511292}, {"x": 0.22980660200119019, "y": 0.9432967305183411}, {"x": 0.09840728342533112, "y": 0.9432967305183411}], "text": "https://kimi.moonshot.cn\n"}
{"page": 15, "bbox": [{"x": 0.9084186553955078, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.03956044092774391}, {"x": 0.9084186553955078, "y": 0.03956044092774391}], "text": "15\n"}
{"page": 15, "bbox": [{"x": 0.4704209268093109, "y": 0.07428571581840515}, {"x": 0.5284414291381836, "y": 0.07384615391492844}, {"x": 0.5284414291381836, "y": 0.08087912201881409}, {"x": 0.4704209268093109, "y": 0.0813186839222908}], "text": "TABLE IV\n"}
{"page": 15, "bbox": [{"x": 0.3833902180194855, "y": 0.08615384250879288}, {"x": 0.6166098117828369, "y": 0.08615384250879288}, {"x": 0.6166098117828369, "y": 0.09318681061267853}, {"x": 0.3833902180194855, "y": 0.09318681061267853}], "text": "SUMMARY OF EVALUATION FRAMEWORKS\n"}
{"page": 15, "bbox": [{"x": 0.31456199288368225, "y": 0.11384615302085876}, {"x": 0.42207053303718567, "y": 0.11516483873128891}, {"x": 0.42207053303718567, "y": 0.12351648509502411}, {"x": 0.31456199288368225, "y": 0.12219779938459396}], "text": "Evaluation Targets\n"}
{"page": 15, "bbox": [{"x": 0.1535836160182953, "y": 0.11560439318418503}, {"x": 0.2855517566204071, "y": 0.11560439318418503}, {"x": 0.2855517566204071, "y": 0.12219779938459396}, {"x": 0.1535836160182953, "y": 0.12219779938459396}], "text": "Evaluation Framework\n"}
{"page": 15, "bbox": [{"x": 0.6825938820838928, "y": 0.11516483873128891}, {"x": 0.799203634262085, "y": 0.11428571492433548}, {"x": 0.799203634262085, "y": 0.12263736128807068}, {"x": 0.6825938820838928, "y": 0.12351648509502411}], "text": "Quantitative Metrics\n"}
{"page": 15, "bbox": [{"x": 0.45164960622787476, "y": 0.11296703666448593}, {"x": 0.6268486976623535, "y": 0.11428571492433548}, {"x": 0.6257110238075256, "y": 0.1934065967798233}, {"x": 0.4505119323730469, "y": 0.19208791851997375}], "text": "Evaluation Aspects\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\n"}
{"page": 15, "bbox": [{"x": 0.7098976373672485, "y": 0.13934065401554108}, {"x": 0.7718998789787292, "y": 0.13934065401554108}, {"x": 0.7718998789787292, "y": 0.17890110611915588}, {"x": 0.7098976373672485, "y": 0.17890110611915588}], "text": "Accuracy\nEM\nAccuracy\n"}
{"page": 15, "bbox": [{"x": 0.1990898698568344, "y": 0.1595604419708252}, {"x": 0.2377701997756958, "y": 0.15912087261676788}, {"x": 0.2377701997756958, "y": 0.16879120469093323}, {"x": 0.1990898698568344, "y": 0.16923077404499054}], "text": "RGB+\n"}
{"page": 15, "bbox": [{"x": 0.3048919141292572, "y": 0.15076923370361328}, {"x": 0.4328782856464386, "y": 0.15296703577041626}, {"x": 0.43230944871902466, "y": 0.17978021502494812}, {"x": 0.30432310700416565, "y": 0.17758241295814514}], "text": "Retrieval Quality\nGeneration Quality\n"}
{"page": 15, "bbox": [{"x": 0.7093287706375122, "y": 0.18417581915855408}, {"x": 0.7724686861038208, "y": 0.18417581915855408}, {"x": 0.7724686861038208, "y": 0.19428572058677673}, {"x": 0.7093287706375122, "y": 0.19428572058677673}], "text": "Accuracy\n"}
{"page": 15, "bbox": [{"x": 0.18373151123523712, "y": 0.20395603775978088}, {"x": 0.2531285583972931, "y": 0.20307692885398865}, {"x": 0.2531285583972931, "y": 0.2131868153810501}, {"x": 0.18373151123523712, "y": 0.21406593918800354}], "text": "RECALL+\n"}
{"page": 15, "bbox": [{"x": 0.30432310700416565, "y": 0.20263735949993134}, {"x": 0.43230944871902466, "y": 0.20483516156673431}, {"x": 0.4317406117916107, "y": 0.2158241719007492}, {"x": 0.3037542700767517, "y": 0.21362636983394623}], "text": "Generation Quality\n"}
{"page": 15, "bbox": [{"x": 0.6461888551712036, "y": 0.20395603775978088}, {"x": 0.8361774682998657, "y": 0.20527473092079163}, {"x": 0.8361774682998657, "y": 0.21626374125480652}, {"x": 0.6461888551712036, "y": 0.21494504809379578}], "text": "R-Rate (Reappearance Rate)\n"}
{"page": 15, "bbox": [{"x": 0.4505119323730469, "y": 0.20527473092079163}, {"x": 0.6268486976623535, "y": 0.20483516156673431}, {"x": 0.6268486976623535, "y": 0.23604395985603333}, {"x": 0.4505119323730469, "y": 0.23648351430892944}], "text": "Counterfactual Robustness\nContext Relevance\n"}
{"page": 15, "bbox": [{"x": 0.7377701997756958, "y": 0.2272527515888214}, {"x": 0.7445961236953735, "y": 0.2272527515888214}, {"x": 0.7445961236953735, "y": 0.23164835572242737}, {"x": 0.7377701997756958, "y": 0.23164835572242737}], "text": "*\n"}
{"page": 15, "bbox": [{"x": 0.7377701997756958, "y": 0.24263736605644226}, {"x": 0.744027316570282, "y": 0.24263736605644226}, {"x": 0.744027316570282, "y": 0.24703297019004822}, {"x": 0.7377701997756958, "y": 0.24703297019004822}], "text": "*\n"}
{"page": 15, "bbox": [{"x": 0.18828213214874268, "y": 0.24175824224948883}, {"x": 0.24175198376178741, "y": 0.2413186877965927}, {"x": 0.24175198376178741, "y": 0.25010988116264343}, {"x": 0.18828213214874268, "y": 0.25054946541786194}], "text": "RAGAS\n"}
{"page": 15, "bbox": [{"x": 0.3048919141292572, "y": 0.2325274795293808}, {"x": 0.4328782856464386, "y": 0.23472528159618378}, {"x": 0.43230944871902466, "y": 0.2606593370437622}, {"x": 0.30432310700416565, "y": 0.25846153497695923}], "text": "Retrieval Quality\nGeneration Quality\n"}
{"page": 15, "bbox": [{"x": 0.47724688053131104, "y": 0.24219779670238495}, {"x": 0.6006826162338257, "y": 0.24219779670238495}, {"x": 0.6006826162338257, "y": 0.2654944956302643}, {"x": 0.47724688053131104, "y": 0.2654944956302643}], "text": "Faithfulness\nAnswer Relevance\n"}
{"page": 15, "bbox": [{"x": 0.6831626892089844, "y": 0.2545054852962494}, {"x": 0.8003413081169128, "y": 0.25626373291015625}, {"x": 0.7997724413871765, "y": 0.26725274324417114}, {"x": 0.6825938820838928, "y": 0.2654944956302643}], "text": "Cosine Similarity\n"}
{"page": 15, "bbox": [{"x": 0.1951080709695816, "y": 0.2936263680458069}, {"x": 0.23606370389461517, "y": 0.29318681359291077}, {"x": 0.23606370389461517, "y": 0.3019780218601227}, {"x": 0.1951080709695816, "y": 0.3024175763130188}], "text": "ARES\n"}
{"page": 15, "bbox": [{"x": 0.4766780436038971, "y": 0.27912089228630066}, {"x": 0.6006826162338257, "y": 0.27912089228630066}, {"x": 0.6006826162338257, "y": 0.31736263632774353}, {"x": 0.4766780436038971, "y": 0.31736263632774353}], "text": "Context Relevance\nFaithfulness\nAnswer Relevance\n"}
{"page": 15, "bbox": [{"x": 0.3048919141292572, "y": 0.28439560532569885}, {"x": 0.4328782856464386, "y": 0.28659340739250183}, {"x": 0.43230944871902466, "y": 0.3129670321941376}, {"x": 0.30432310700416565, "y": 0.3107692301273346}], "text": "Retrieval Quality\nGeneration Quality\n"}
{"page": 15, "bbox": [{"x": 0.7098976373672485, "y": 0.2795604467391968}, {"x": 0.7730375528335571, "y": 0.2800000011920929}, {"x": 0.7724686861038208, "y": 0.3199999928474426}, {"x": 0.7093287706375122, "y": 0.3195604383945465}], "text": "Accuracy\nAccuracy\nAccuracy\n"}
{"page": 15, "bbox": [{"x": 0.7377701997756958, "y": 0.33142855763435364}, {"x": 0.744027316570282, "y": 0.33142855763435364}, {"x": 0.744027316570282, "y": 0.3353846073150635}, {"x": 0.7377701997756958, "y": 0.3353846073150635}], "text": "*\n"}
{"page": 15, "bbox": [{"x": 0.4766780436038971, "y": 0.3309890031814575}, {"x": 0.6012514233589172, "y": 0.3305494487285614}, {"x": 0.6012514233589172, "y": 0.35384616255760193}, {"x": 0.4766780436038971, "y": 0.35428571701049805}], "text": "Context Relevance\nFaithfulness\n"}
{"page": 15, "bbox": [{"x": 0.7377701997756958, "y": 0.34637361764907837}, {"x": 0.7445961236953735, "y": 0.34637361764907837}, {"x": 0.7445961236953735, "y": 0.3507692217826843}, {"x": 0.7377701997756958, "y": 0.3507692217826843}], "text": "*\n"}
{"page": 15, "bbox": [{"x": 0.18941979110240936, "y": 0.34505495429039}, {"x": 0.2428896427154541, "y": 0.3446153700351715}, {"x": 0.2428896427154541, "y": 0.35384616255760193}, {"x": 0.18941979110240936, "y": 0.35428571701049805}], "text": "TruLens\n"}
{"page": 15, "bbox": [{"x": 0.30546075105667114, "y": 0.3358241617679596}, {"x": 0.4328782856464386, "y": 0.3380219638347626}, {"x": 0.43230944871902466, "y": 0.3643956184387207}, {"x": 0.3048919141292572, "y": 0.3621978163719177}], "text": "Retrieval Quality\nGeneration Quality\n"}
{"page": 15, "bbox": [{"x": 0.7377701997756958, "y": 0.3613186776638031}, {"x": 0.7445961236953735, "y": 0.3613186776638031}, {"x": 0.7445961236953735, "y": 0.36571428179740906}, {"x": 0.7377701997756958, "y": 0.36571428179740906}], "text": "*\n"}
{"page": 15, "bbox": [{"x": 0.4550625681877136, "y": 0.360879123210907}, {"x": 0.6222980618476868, "y": 0.360879123210907}, {"x": 0.6222980618476868, "y": 0.436923086643219}, {"x": 0.4550625681877136, "y": 0.436923086643219}], "text": "Answer Relevance\nCreative Generation\nKnowledge-intensive QA\nError Correction\nSummarization\n"}
{"page": 15, "bbox": [{"x": 0.19283276796340942, "y": 0.4039560556411743}, {"x": 0.2428896427154541, "y": 0.4030769169330597}, {"x": 0.2428896427154541, "y": 0.41318681836128235}, {"x": 0.19340158998966217, "y": 0.4140659272670746}], "text": "CRUD+\n"}
{"page": 15, "bbox": [{"x": 0.3048919141292572, "y": 0.3951648473739624}, {"x": 0.43344709277153015, "y": 0.3973626494407654}, {"x": 0.4328782856464386, "y": 0.4237362742424011}, {"x": 0.30432310700416565, "y": 0.42153847217559814}], "text": "Retrieval Quality\nGeneration Quality\n"}
{"page": 15, "bbox": [{"x": 0.69112628698349, "y": 0.38285714387893677}, {"x": 0.7923777103424072, "y": 0.38285714387893677}, {"x": 0.7923777103424072, "y": 0.43780219554901123}, {"x": 0.69112628698349, "y": 0.43780219554901123}], "text": "BLEU\nROUGE-L\nBertScore\nRAGQuestEval\n"}
{"page": 15, "bbox": [{"x": 0.0813424363732338, "y": 0.45802196860313416}, {"x": 0.9197952151298523, "y": 0.4589011073112488}, {"x": 0.9197952151298523, "y": 0.5006593465805054}, {"x": 0.0813424363732338, "y": 0.49978020787239075}], "text": "† represents a benchmark, and represents a tool. * denotes customized quantitative metrics, which deviate from traditional\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\nmetrics, as required.\n"}
{"page": 15, "bbox": [{"x": 0.07963594794273376, "y": 0.5305494666099548}, {"x": 0.489761084318161, "y": 0.5305494666099548}, {"x": 0.489761084318161, "y": 0.6303296685218811}, {"x": 0.07963594794273376, "y": 0.6303296685218811}], "text": "and non-parameterized advantages are areas ripe for explo-\nration [27]. Another trend is to introduce SLMs with specific\nfunctionalities into RAG and fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved docu-\nments for a query and triggers different knowledge retrieval\nactions based on confidence levels.\n"}
{"page": 15, "bbox": [{"x": 0.07963594794273376, "y": 0.6545054912567139}, {"x": 0.24232082068920135, "y": 0.6536263823509216}, {"x": 0.24232082068920135, "y": 0.6646153926849365}, {"x": 0.07963594794273376, "y": 0.6654945015907288}], "text": "D. Scaling laws of RAG\n"}
{"page": 15, "bbox": [{"x": 0.5079635977745056, "y": 0.5279120802879333}, {"x": 0.9220705628395081, "y": 0.5292307734489441}, {"x": 0.9203640222549438, "y": 0.9054945111274719}, {"x": 0.5062571167945862, "y": 0.9041758179664612}], "text": "inadvertent disclosure of document sources or metadata by\nLLMs are critical engineering challenges that remain to be\naddressed [175].\nThe development of the RAG ecosystem is greatly impacted\nby the progression of its technical stack. Key tools like\nLangChain and LLamaIndex have quickly gained popularity\nwith the emergence of ChatGPT, providing extensive RAG-\nrelated APIs and becoming essential in the realm of LLMs.The\nemerging technology stack, while not as rich in features as\nLangChain and LLamaIndex, stands out through its specialized\nproducts. For example, Flowise AI prioritizes a low-code\napproach, allowing users to deploy AI applications, including\nRAG, through a user-friendly drag-and-drop interface. Other\ntechnologies like HayStack, Meltano, and Cohere Coral are\nalso gaining attention for their unique contributions to the field.\nIn addition to AI-focused vendors, traditional software and\ncloud service providers are expanding their offerings to include\nRAG-centric services. Weaviate's Verba is designed for\npersonal assistant applications, while Amazon's Kendra\noffers intelligent enterprise search services, enabling users to\nbrowse various content repositories using built-in connectors.\nIn the development of RAG technology, there is a clear\ntrend towards different specialization directions, such as: 1)\nCustomization - tailoring RAG to meet specific requirements.\n2) Simplification making RAG easier to use to reduce the\n"}
{"page": 15, "bbox": [{"x": 0.07963594794273376, "y": 0.6738461256027222}, {"x": 0.4926052391529083, "y": 0.6742857098579407}, {"x": 0.4926052391529083, "y": 0.8215384483337402}, {"x": 0.07963594794273376, "y": 0.8210989236831665}], "text": "End-to-end RAG models and pre-trained models based\non RAG are still one of the focuses of current re-\nsearchers [173]. The parameters of these models are one of\nthe key factors. While scaling laws [174] are established for\nLLMs, their applicability to RAG remains uncertain. Initial\nstudies like RETRO++ [44] have begun to address this, yet the\nparameter count in RAG models still lags behind that of LLMs.\nThe possibility of an Inverse Scaling Law 10, where smaller\nmodels outperform larger ones, is particularly intriguing and\nmerits further investigation.\n"}
{"page": 15, "bbox": [{"x": 0.7940841913223267, "y": 0.7859340906143188}, {"x": 0.8031854629516602, "y": 0.7859340906143188}, {"x": 0.8031854629516602, "y": 0.7920879125595093}, {"x": 0.7940841913223267, "y": 0.7920879125595093}], "text": "11\n"}
{"page": 15, "bbox": [{"x": 0.9078498482704163, "y": 0.8013187050819397}, {"x": 0.9186575412750244, "y": 0.8013187050819397}, {"x": 0.9186575412750244, "y": 0.8065934181213379}, {"x": 0.9078498482704163, "y": 0.8065934181213379}], "text": "12\n"}
{"page": 15, "bbox": [{"x": 0.07963594794273376, "y": 0.8439560532569885}, {"x": 0.25767919421195984, "y": 0.8439560532569885}, {"x": 0.25767919421195984, "y": 0.8540659546852112}, {"x": 0.07963594794273376, "y": 0.8540659546852112}], "text": "E. Production-Ready RAG\n"}
{"page": 15, "bbox": [{"x": 0.07906711846590042, "y": 0.8628571629524231}, {"x": 0.49032992124557495, "y": 0.8628571629524231}, {"x": 0.49032992124557495, "y": 0.919560432434082}, {"x": 0.07906711846590042, "y": 0.919560432434082}], "text": "RAG's practicality and alignment with engineering require-\nments have facilitated its adoption. However, enhancing re-\ntrieval efficiency, improving document recall in large knowl-\nedge bases, and ensuring data security—such as preventing\n"}
{"page": 15, "bbox": [{"x": 0.522753119468689, "y": 0.9213187098503113}, {"x": 0.7189988493919373, "y": 0.9208791255950928}, {"x": 0.7189988493919373, "y": 0.9437362551689148}, {"x": 0.522753119468689, "y": 0.9441758394241333}], "text": "https://github.com/weaviate/Verba\n12https://aws.amazon.com/cn/kendra/\n"}
{"page": 15, "bbox": [{"x": 0.09271899610757828, "y": 0.9327472448348999}, {"x": 0.31058019399642944, "y": 0.9336263537406921}, {"x": 0.31058019399642944, "y": 0.944615364074707}, {"x": 0.09271899610757828, "y": 0.9437362551689148}], "text": "10https://github.com/inverse-scaling/prize\n"}
{"page": 16, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9203640222549438, "y": 0.03384615480899811}, {"x": 0.9203640222549438, "y": 0.0391208790242672}, {"x": 0.9089874625205994, "y": 0.0391208790242672}], "text": "16\n"}
{"page": 16, "bbox": [{"x": 0.5216154456138611, "y": 0.08747252821922302}, {"x": 0.6251422166824341, "y": 0.08879120647907257}, {"x": 0.6251422166824341, "y": 0.09758241474628448}, {"x": 0.5216154456138611, "y": 0.09626373648643494}], "text": "➤ RAG Prospect\n"}
{"page": 16, "bbox": [{"x": 0.17349261045455933, "y": 0.08791209012269974}, {"x": 0.2895335555076599, "y": 0.08923076838254929}, {"x": 0.2895335555076599, "y": 0.0980219766497612}, {"x": 0.17349261045455933, "y": 0.09670329838991165}], "text": ">RAG Ecosystem\n"}
{"page": 16, "bbox": [{"x": 0.658703088760376, "y": 0.10505494475364685}, {"x": 0.7315130829811096, "y": 0.10417582094669342}, {"x": 0.7315130829811096, "y": 0.11032967269420624}, {"x": 0.658703088760376, "y": 0.11120878905057907}], "text": "Modality Extension\n"}
{"page": 16, "bbox": [{"x": 0.7508532404899597, "y": 0.10505494475364685}, {"x": 0.7974971532821655, "y": 0.10549450665712357}, {"x": 0.7974971532821655, "y": 0.11120878905057907}, {"x": 0.7508532404899597, "y": 0.11076922714710236}], "text": "Ecosystem\n"}
{"page": 16, "bbox": [{"x": 0.5585892796516418, "y": 0.10549450665712357}, {"x": 0.6069397330284119, "y": 0.10549450665712357}, {"x": 0.6069397330284119, "y": 0.11120878905057907}, {"x": 0.5585892796516418, "y": 0.11120878905057907}], "text": "Challenges\n"}
{"page": 16, "bbox": [{"x": 0.18430034816265106, "y": 0.10725274682044983}, {"x": 0.2974971532821655, "y": 0.10681318491697311}, {"x": 0.2974971532821655, "y": 0.11384615302085876}, {"x": 0.18430034816265106, "y": 0.11428571492433548}], "text": "Downstream Tasks\n"}
{"page": 16, "bbox": [{"x": 0.3566552996635437, "y": 0.10725274682044983}, {"x": 0.4692832827568054, "y": 0.10681318491697311}, {"x": 0.4692832827568054, "y": 0.11516483873128891}, {"x": 0.3566552996635437, "y": 0.11560439318418503}], "text": "Technology Stacks\n"}
{"page": 16, "bbox": [{"x": 0.7457337975502014, "y": 0.12351648509502411}, {"x": 0.8031854629516602, "y": 0.12351648509502411}, {"x": 0.8031854629516602, "y": 0.1287912130355835}, {"x": 0.7457337975502014, "y": 0.1287912130355835}], "text": "Customization\n"}
{"page": 16, "bbox": [{"x": 0.5238907933235168, "y": 0.1230769231915474}, {"x": 0.6387940645217896, "y": 0.12351648509502411}, {"x": 0.6387940645217896, "y": 0.12967033684253693}, {"x": 0.5238907933235168, "y": 0.12923076748847961}], "text": "RAG in Long Context Length\n"}
{"page": 16, "bbox": [{"x": 0.6831626892089844, "y": 0.12395604699850082}, {"x": 0.7070534825325012, "y": 0.12483516335487366}, {"x": 0.7064846158027649, "y": 0.13010989129543304}, {"x": 0.6831626892089844, "y": 0.12923076748847961}], "text": "Image\n"}
{"page": 16, "bbox": [{"x": 0.42832764983177185, "y": 0.1261538416147232}, {"x": 0.4738338887691498, "y": 0.1261538416147232}, {"x": 0.4738338887691498, "y": 0.13098901510238647}, {"x": 0.42832764983177185, "y": 0.13098901510238647}], "text": "Llamalndex\n"}
{"page": 16, "bbox": [{"x": 0.1990898698568344, "y": 0.1257142871618271}, {"x": 0.3373151421546936, "y": 0.1257142871618271}, {"x": 0.3373151421546936, "y": 0.1318681389093399}, {"x": 0.1990898698568344, "y": 0.1318681389093399}], "text": "Dialogue Question answering\n"}
{"page": 16, "bbox": [{"x": 0.36632537841796875, "y": 0.12659341096878052}, {"x": 0.4072810113430023, "y": 0.12659341096878052}, {"x": 0.4072810113430023, "y": 0.13230769336223602}, {"x": 0.36632537841796875, "y": 0.13230769336223602}], "text": "Langchain\n"}
{"page": 16, "bbox": [{"x": 0.6843003630638123, "y": 0.14153845608234406}, {"x": 0.7070534825325012, "y": 0.14197802543640137}, {"x": 0.7070534825325012, "y": 0.14725275337696075}, {"x": 0.6843003630638123, "y": 0.14681318402290344}], "text": "Audio\n"}
{"page": 16, "bbox": [{"x": 0.5910125374794006, "y": 0.14197802543640137}, {"x": 0.6365187764167786, "y": 0.14241757988929749}, {"x": 0.6365187764167786, "y": 0.14725275337696075}, {"x": 0.5910125374794006, "y": 0.14681318402290344}], "text": "Robustness\n"}
{"page": 16, "bbox": [{"x": 0.532423198223114, "y": 0.14241757988929749}, {"x": 0.5574516654014587, "y": 0.14197802543640137}, {"x": 0.5574516654014587, "y": 0.14769230782985687}, {"x": 0.532423198223114, "y": 0.148131862282753}], "text": "Hybrid\n"}
{"page": 16, "bbox": [{"x": 0.18657565116882324, "y": 0.14593406021595}, {"x": 0.32992035150527954, "y": 0.14593406021595}, {"x": 0.32992035150527954, "y": 0.1512087881565094}, {"x": 0.18657565116882324, "y": 0.1512087881565094}], "text": "Summarization Fact verification\n"}
{"page": 16, "bbox": [{"x": 0.43344709277153015, "y": 0.14681318402290344}, {"x": 0.46814560890197754, "y": 0.14681318402290344}, {"x": 0.46814560890197754, "y": 0.1512087881565094}, {"x": 0.43344709277153015, "y": 0.1512087881565094}], "text": "AutoGen\n"}
{"page": 16, "bbox": [{"x": 0.3680318593978882, "y": 0.14637362957000732}, {"x": 0.4055745303630829, "y": 0.14637362957000732}, {"x": 0.4055745303630829, "y": 0.1516483575105667}, {"x": 0.3680318593978882, "y": 0.1516483575105667}], "text": "FlowiseAl\n"}
{"page": 16, "bbox": [{"x": 0.7474402785301208, "y": 0.15076923370361328}, {"x": 0.8020477890968323, "y": 0.1512087881565094}, {"x": 0.8020477890968323, "y": 0.15736263990402222}, {"x": 0.7474402785301208, "y": 0.1569230705499649}], "text": "Simplification\n"}
{"page": 16, "bbox": [{"x": 0.6854379773139954, "y": 0.1599999964237213}, {"x": 0.7081910967826843, "y": 0.1599999964237213}, {"x": 0.7081910967826843, "y": 0.16483516991138458}, {"x": 0.6854379773139954, "y": 0.16483516991138458}], "text": "Video\n"}
{"page": 16, "bbox": [{"x": 0.5398179888725281, "y": 0.1599999964237213}, {"x": 0.6268486976623535, "y": 0.15912087261676788}, {"x": 0.6268486976623535, "y": 0.16571427881717682}, {"x": 0.5398179888725281, "y": 0.16659340262413025}], "text": "Scaling-laws for RAG\n"}
{"page": 16, "bbox": [{"x": 0.20477814972400665, "y": 0.1736263781785965}, {"x": 0.329351544380188, "y": 0.17538461089134216}, {"x": 0.329351544380188, "y": 0.18417581915855408}, {"x": 0.20477814972400665, "y": 0.1824175864458084}], "text": "The RAG Paradigm\n"}
{"page": 16, "bbox": [{"x": 0.746302604675293, "y": 0.17846153676509857}, {"x": 0.8026165962219238, "y": 0.17846153676509857}, {"x": 0.8026165962219238, "y": 0.18373626470565796}, {"x": 0.746302604675293, "y": 0.18373626470565796}], "text": "Specialization\n"}
{"page": 16, "bbox": [{"x": 0.6848691701889038, "y": 0.17890110611915588}, {"x": 0.7059158086776733, "y": 0.17890110611915588}, {"x": 0.7059158086776733, "y": 0.18373626470565796}, {"x": 0.6848691701889038, "y": 0.18373626470565796}], "text": "Code\n"}
{"page": 16, "bbox": [{"x": 0.5369738340377808, "y": 0.17890110611915588}, {"x": 0.628555178642273, "y": 0.17890110611915588}, {"x": 0.628555178642273, "y": 0.1846153885126114}, {"x": 0.5369738340377808, "y": 0.1846153885126114}], "text": "Production-ready RAG\n"}
{"page": 16, "bbox": [{"x": 0.3759954571723938, "y": 0.2074725329875946}, {"x": 0.447098970413208, "y": 0.2074725329875946}, {"x": 0.447098970413208, "y": 0.21450549364089966}, {"x": 0.3759954571723938, "y": 0.21450549364089966}], "text": "Modular RAG\n"}
{"page": 16, "bbox": [{"x": 0.522753119468689, "y": 0.21406593918800354}, {"x": 0.6524459719657898, "y": 0.21406593918800354}, {"x": 0.6524459719657898, "y": 0.2210988998413086}, {"x": 0.522753119468689, "y": 0.2210988998413086}], "text": "➤ Evaluation of RAG\n"}
{"page": 16, "bbox": [{"x": 0.27474403381347656, "y": 0.21714285016059875}, {"x": 0.3464163839817047, "y": 0.21714285016059875}, {"x": 0.3464163839817047, "y": 0.22241757810115814}, {"x": 0.27474403381347656, "y": 0.22241757810115814}], "text": "Advanced RAG\n"}
{"page": 16, "bbox": [{"x": 0.2104664444923401, "y": 0.22285714745521545}, {"x": 0.2542662024497986, "y": 0.22285714745521545}, {"x": 0.2542662024497986, "y": 0.22769230604171753}, {"x": 0.2104664444923401, "y": 0.22769230604171753}], "text": "Naive RAG\n"}
{"page": 16, "bbox": [{"x": 0.5233219861984253, "y": 0.22901098430156708}, {"x": 0.6205915808677673, "y": 0.23032967746257782}, {"x": 0.6200227737426758, "y": 0.25054946541786194}, {"x": 0.522753119468689, "y": 0.2492307722568512}], "text": "Evaluation Target\nRetrieval Quality\n"}
{"page": 16, "bbox": [{"x": 0.6973834037780762, "y": 0.2435164898633957}, {"x": 0.7724686861038208, "y": 0.24439559876918793}, {"x": 0.7724686861038208, "y": 0.25010988116264343}, {"x": 0.6973834037780762, "y": 0.2492307722568512}], "text": "Generation Quality\n"}
{"page": 16, "bbox": [{"x": 0.21103526651859283, "y": 0.24967032670974731}, {"x": 0.40386801958084106, "y": 0.24835164844989777}, {"x": 0.40386801958084106, "y": 0.2580219805240631}, {"x": 0.21103526651859283, "y": 0.25934067368507385}], "text": "➤ Techniques for Better RAG\n"}
{"page": 16, "bbox": [{"x": 0.5233219861984253, "y": 0.26197803020477295}, {"x": 0.6149032711982727, "y": 0.2628571391105652}, {"x": 0.6149032711982727, "y": 0.27032968401908875}, {"x": 0.5233219861984253, "y": 0.2694505453109741}], "text": "Evaluation Aspects\n"}
{"page": 16, "bbox": [{"x": 0.30944254994392395, "y": 0.266813188791275}, {"x": 0.36746302247047424, "y": 0.2663736343383789}, {"x": 0.36746302247047424, "y": 0.271208792924881}, {"x": 0.30944254994392395, "y": 0.2716483473777771}], "text": "Iterative Retrieval\n"}
{"page": 16, "bbox": [{"x": 0.3890784978866577, "y": 0.26725274324417114}, {"x": 0.4601820111274719, "y": 0.26769229769706726}, {"x": 0.4601820111274719, "y": 0.2725274860858917}, {"x": 0.3890784978866577, "y": 0.2720879018306732}], "text": "Retriever Fine-tuning\n"}
{"page": 16, "bbox": [{"x": 0.21615472435951233, "y": 0.26769229769706726}, {"x": 0.2895335555076599, "y": 0.26769229769706726}, {"x": 0.2895335555076599, "y": 0.2892307639122009}, {"x": 0.21615472435951233, "y": 0.2892307639122009}], "text": "Chunk Optimization\nQuery Transformation\n"}
{"page": 16, "bbox": [{"x": 0.5500568747520447, "y": 0.2773626446723938}, {"x": 0.6234357357025146, "y": 0.2773626446723938}, {"x": 0.6234357357025146, "y": 0.2821978032588959}, {"x": 0.5500568747520447, "y": 0.2821978032588959}], "text": "Answer Relevance\n"}
{"page": 16, "bbox": [{"x": 0.3060295879840851, "y": 0.2821978032588959}, {"x": 0.3708759844303131, "y": 0.2821978032588959}, {"x": 0.3708759844303131, "y": 0.2861538529396057}, {"x": 0.3060295879840851, "y": 0.2861538529396057}], "text": "Recursive Retrieval\n"}
{"page": 16, "bbox": [{"x": 0.38680317997932434, "y": 0.28131869435310364}, {"x": 0.46188852190971375, "y": 0.2821978032588959}, {"x": 0.4613196849822998, "y": 0.3024175763130188}, {"x": 0.3862343430519104, "y": 0.30153846740722656}], "text": "Generator Fine-tuning\nDual Fine-tuning\n"}
{"page": 16, "bbox": [{"x": 0.3071672320365906, "y": 0.296263724565506}, {"x": 0.3691695034503937, "y": 0.29582417011260986}, {"x": 0.3691695034503937, "y": 0.30109891295433044}, {"x": 0.3071672320365906, "y": 0.30153846740722656}], "text": "Adaptive Retrieval\n"}
{"page": 16, "bbox": [{"x": 0.6825938820838928, "y": 0.2773626446723938}, {"x": 0.7912400364875793, "y": 0.2778021991252899}, {"x": 0.7906712293624878, "y": 0.3226373493671417}, {"x": 0.6820250153541565, "y": 0.3221977949142456}], "text": "Noise Robustness\nNegation Rejection\nInformation Integration\nCounterfactual Robustness\n"}
{"page": 16, "bbox": [{"x": 0.22298066318035126, "y": 0.29802197217941284}, {"x": 0.2827076315879822, "y": 0.29802197217941284}, {"x": 0.2827076315879822, "y": 0.3024175763130188}, {"x": 0.22298066318035126, "y": 0.3024175763130188}], "text": "Context Selection\n"}
{"page": 16, "bbox": [{"x": 0.5494880676269531, "y": 0.2975824177265167}, {"x": 0.6251422166824341, "y": 0.2975824177265167}, {"x": 0.6251422166824341, "y": 0.3028571307659149}, {"x": 0.5494880676269531, "y": 0.3028571307659149}], "text": "Context Relevance\n"}
{"page": 16, "bbox": [{"x": 0.5455062389373779, "y": 0.31736263632774353}, {"x": 0.6274175047874451, "y": 0.31736263632774353}, {"x": 0.6274175047874451, "y": 0.3226373493671417}, {"x": 0.5455062389373779, "y": 0.3226373493671417}], "text": "Answer Faithfulness\n"}
{"page": 16, "bbox": [{"x": 0.23094426095485687, "y": 0.3217582404613495}, {"x": 0.3629123866558075, "y": 0.32087913155555725}, {"x": 0.3629123866558075, "y": 0.32923075556755066}, {"x": 0.23094426095485687, "y": 0.3301098942756653}], "text": "➤ Key Issues of RAG\n"}
{"page": 16, "bbox": [{"x": 0.522753119468689, "y": 0.3367033004760742}, {"x": 0.6291239857673645, "y": 0.3367033004760742}, {"x": 0.6291239857673645, "y": 0.34241756796836853}, {"x": 0.522753119468689, "y": 0.34241756796836853}], "text": "Evaluation Framework\n"}
{"page": 16, "bbox": [{"x": 0.6308304667472839, "y": 0.34813186526298523}, {"x": 0.6541524529457092, "y": 0.34813186526298523}, {"x": 0.6541524529457092, "y": 0.3529670238494873}, {"x": 0.6308304667472839, "y": 0.3529670238494873}], "text": "CRUD\n"}
{"page": 16, "bbox": [{"x": 0.6979522109031677, "y": 0.34813186526298523}, {"x": 0.7150170803070068, "y": 0.34813186526298523}, {"x": 0.7150170803070068, "y": 0.3529670238494873}, {"x": 0.6979522109031677, "y": 0.3529670238494873}], "text": "RGB\n"}
{"page": 16, "bbox": [{"x": 0.7531285285949707, "y": 0.34813186526298523}, {"x": 0.786120593547821, "y": 0.34813186526298523}, {"x": 0.786120593547821, "y": 0.3529670238494873}, {"x": 0.7531285285949707, "y": 0.3529670238494873}], "text": "RECALL\n"}
{"page": 16, "bbox": [{"x": 0.3862343430519104, "y": 0.34593406319618225}, {"x": 0.42832764983177185, "y": 0.34637361764907837}, {"x": 0.42832764983177185, "y": 0.35956043004989624}, {"x": 0.3862343430519104, "y": 0.3591208755970001}], "text": "How to use\nRetrieval\n"}
{"page": 16, "bbox": [{"x": 0.5517633557319641, "y": 0.3507692217826843}, {"x": 0.6018202304840088, "y": 0.3507692217826843}, {"x": 0.6018202304840088, "y": 0.3551648259162903}, {"x": 0.5517633557319641, "y": 0.3551648259162903}], "text": "Benchmarks\n"}
{"page": 16, "bbox": [{"x": 0.24345847964286804, "y": 0.3476923108100891}, {"x": 0.27588167786598206, "y": 0.3476923108100891}, {"x": 0.27588167786598206, "y": 0.360879123210907}, {"x": 0.24345847964286804, "y": 0.360879123210907}], "text": "What to\nretrieve\n"}
{"page": 16, "bbox": [{"x": 0.3162684738636017, "y": 0.3476923108100891}, {"x": 0.3498293459415436, "y": 0.3476923108100891}, {"x": 0.3498293459415436, "y": 0.360879123210907}, {"x": 0.3162684738636017, "y": 0.360879123210907}], "text": "When to\nretrieve\n"}
{"page": 16, "bbox": [{"x": 0.7588168382644653, "y": 0.36263737082481384}, {"x": 0.7804322838783264, "y": 0.36263737082481384}, {"x": 0.7804322838783264, "y": 0.3674725294113159}, {"x": 0.7588168382644653, "y": 0.3674725294113159}], "text": "ARES\n"}
{"page": 16, "bbox": [{"x": 0.6922639608383179, "y": 0.36307692527770996}, {"x": 0.7201365232467651, "y": 0.36307692527770996}, {"x": 0.7201365232467651, "y": 0.3674725294113159}, {"x": 0.6922639608383179, "y": 0.3674725294113159}], "text": "RAGAS\n"}
{"page": 16, "bbox": [{"x": 0.6268486976623535, "y": 0.3639560341835022}, {"x": 0.658703088760376, "y": 0.3639560341835022}, {"x": 0.658703088760376, "y": 0.36879122257232666}, {"x": 0.6268486976623535, "y": 0.36879122257232666}], "text": "TruLens\n"}
{"page": 16, "bbox": [{"x": 0.5517633557319641, "y": 0.3643956184387207}, {"x": 0.57337886095047, "y": 0.3648351728916168}, {"x": 0.57337886095047, "y": 0.3696703314781189}, {"x": 0.5517633557319641, "y": 0.3692307770252228}], "text": "Tools\n"}
{"page": 16, "bbox": [{"x": 0.07963594794273376, "y": 0.4057142734527588}, {"x": 0.2804323136806488, "y": 0.40703296661376953}, {"x": 0.2804323136806488, "y": 0.4167032837867737}, {"x": 0.07963594794273376, "y": 0.4153846204280853}], "text": "Fig. 6. Summary of RAG ecosystem\n"}
{"page": 16, "bbox": [{"x": 0.07906711846590042, "y": 0.4457142949104309}, {"x": 0.49146756529808044, "y": 0.4457142949104309}, {"x": 0.49146756529808044, "y": 0.6074725389480591}, {"x": 0.07906711846590042, "y": 0.6074725389480591}], "text": "initial learning curve. 3) Specialization - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept is still in the future, requiring\nfurther innovation and development.\n"}
{"page": 16, "bbox": [{"x": 0.5091012716293335, "y": 0.4457142949104309}, {"x": 0.9209328889846802, "y": 0.4457142949104309}, {"x": 0.9209328889846802, "y": 0.6232966780662537}, {"x": 0.5091012716293335, "y": 0.6232966780662537}], "text": "Vid2Seq augments language models with specialized temporal\nmarkers, facilitating the prediction of event boundaries and\ntextual descriptions within a unified output sequence [181].\nCode. RBPS [182] excels in small-scale learning tasks by\nretrieving code examples that align with developers' objectives\nthrough encoding and frequency analysis. This approach has\ndemonstrated efficacy in tasks such as test assertion genera-\ntion and program repair. For structured knowledge, the CoK\nmethod [106] first extracts facts pertinent to the input query\nfrom a knowledge graph, then integrates these facts as hints\nwithin the input, enhancing performance in knowledge graph\nquestion-answering tasks.\n"}
{"page": 16, "bbox": [{"x": 0.07963594794273376, "y": 0.6281318664550781}, {"x": 0.21672354638576508, "y": 0.6281318664550781}, {"x": 0.21672354638576508, "y": 0.6364834904670715}, {"x": 0.07963594794273376, "y": 0.6364834904670715}], "text": "F. Multi-modal RAG\n"}
{"page": 16, "bbox": [{"x": 0.6456200480461121, "y": 0.6421977877616882}, {"x": 0.7827076315879822, "y": 0.6426373720169067}, {"x": 0.7827076315879822, "y": 0.6514285802841187}, {"x": 0.6456200480461121, "y": 0.6509889960289001}], "text": "VIII. CONCLUSION\n"}
{"page": 16, "bbox": [{"x": 0.07849829643964767, "y": 0.6465933918952942}, {"x": 0.4908987581729889, "y": 0.6461538672447205}, {"x": 0.49146756529808044, "y": 0.944615364074707}, {"x": 0.07906711846590042, "y": 0.9450549483299255}], "text": "RAG has transcended its initial text-based question-\nanswering confines, embracing a diverse array of modal data.\nThis expansion has spawned innovative multimodal models\nthat integrate RAG concepts across various domains:\nImage. RA-CM3 [176] stands as a pioneering multimodal\nmodel of both retrieving and generating text and images.\nBLIP-2 [177] leverages frozen image encoders alongside\nLLMs for efficient visual language pre-training, enabling zero-\nshot image-to-text conversions. The \"Visualize Before You\nWrite\" method [178] employs image generation to steer the\nLM's text generation, showing promise in open-ended text\ngeneration tasks.\nAudio and Video. The GSS method retrieves and stitches\ntogether audio clips to convert machine-translated data into\nspeech-translated data [179]. UEOP marks a significant ad-\nvancement in end-to-end automatic speech recognition by\nincorporating external, offline strategies for voice-to-text con-\nversion [180]. Additionally, KNN-based attention fusion lever-\nages audio embeddings and semantically related text embed-\ndings to refine ASR, thereby accelerating domain adaptation.\n"}
{"page": 16, "bbox": [{"x": 0.5091012716293335, "y": 0.6610988974571228}, {"x": 0.9209328889846802, "y": 0.6610988974571228}, {"x": 0.9209328889846802, "y": 0.944615364074707}, {"x": 0.5091012716293335, "y": 0.944615364074707}], "text": "The summary of this paper, as depicted in Figure 6, empha-\nsizes RAG's significant advancement in enhancing the capa-\nbilities of LLMs by integrating parameterized knowledge from\nlanguage models with extensive non-parameterized data from\nexternal knowledge bases. The survey showcases the evolution\nof RAG technologies and their application on many different\ntasks. The analysis outlines three developmental paradigms\nwithin the RAG framework: Naive, Advanced, and Modu-\nlar RAG, each representing a progressive enhancement over\nits predecessors. RAG's technical integration with other AI\nmethodologies, such as fine-tuning and reinforcement learning,\nhas further expanded its capabilities. Despite the progress in\nRAG technology, there are research opportunities to improve\nits robustness and its ability to handle extended contexts.\nRAG's application scope is expanding into multimodal do-\nmains, adapting its principles to interpret and process diverse\ndata forms like images, videos, and code. This expansion high-\nlights RAG's significant practical implications for AI deploy-\nment, attracting interest from academic and industrial sectors.\n"}
{"page": 17, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9186575412750244, "y": 0.03384615480899811}, {"x": 0.9186575412750244, "y": 0.0391208790242672}, {"x": 0.9089874625205994, "y": 0.0391208790242672}], "text": "17\n"}
{"page": 17, "bbox": [{"x": 0.07906711846590042, "y": 0.07032967358827591}, {"x": 0.49146756529808044, "y": 0.07076922804117203}, {"x": 0.49146756529808044, "y": 0.17582418024539948}, {"x": 0.07906711846590042, "y": 0.17538461089134216}], "text": "The growing ecosystem of RAG is evidenced by the rise in\nRAG-centric AI applications and the continuous development\nof supportive tools. As RAG's application landscape broadens,\nthere is a need to refine evaluation methodologies to keep\npace with its evolution. Ensuring accurate and representative\nperformance assessments is crucial for fully capturing RAG's\ncontributions to the AI research and development community.\n"}
{"page": 17, "bbox": [{"x": 0.23947668075561523, "y": 0.2013186812400818}, {"x": 0.3304891884326935, "y": 0.2017582356929779}, {"x": 0.3304891884326935, "y": 0.2101098895072937}, {"x": 0.23947668075561523, "y": 0.20967033505439758}], "text": "REFERENCES\n"}
{"page": 17, "bbox": [{"x": 0.09271899610757828, "y": 0.22505494952201843}, {"x": 0.4926052391529083, "y": 0.22549450397491455}, {"x": 0.4920364022254944, "y": 0.5534065961837769}, {"x": 0.09215017408132553, "y": 0.5529670119285583}], "text": "[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, \"Large\nlanguage models struggle to learn long-tail knowledge,\" in Interna-\ntional Conference on Machine Learning. PMLR, 2023, pp. 15696-\n15 707.\n[2] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\nY. Zhang, Y. Chen et al., “Siren's song in the ai ocean: A survey on hal-\nlucination in large language models,\" arXiv preprint arXiv:2309.01219,\n2023.\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\nA. Sharma, \"Gar-meets-rag paradigm for zero-shot information re-\ntrieval,\" arXiv preprint arXiv:2310.20158, 2023.\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel et al., “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,\" Advances in\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\n“Improving language models by retrieving from trillions of tokens,”\nin International conference on machine learning. PMLR, 2022, pp.\n2206-2240.\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\nmodels to follow instructions with human feedback,\" Advances in\nneural information processing systems, vol. 35, pp. 27730-27744,\n2022.\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\ning for retrieval-augmented large language models,\" arXiv preprint\narXiv:2305.14283, 2023.\nILIN,\n"}
{"page": 17, "bbox": [{"x": 0.5153583884239197, "y": 0.07428571581840515}, {"x": 0.9209328889846802, "y": 0.07428571581840515}, {"x": 0.9215016961097717, "y": 0.9441758394241333}, {"x": 0.5159271955490112, "y": 0.9441758394241333}], "text": "[19] X. Li, E. Nie, and S. Liang, \"From classification to generation:\nInsights into crosslingual retrieval augmented icl,\" arXiv preprint\narXiv:2311.06595, 2023.\n[20] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun,\nF. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval\nfor improving zero-shot evaluation,\" arXiv preprint arXiv:2303.08518,\n2023.\n[21] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\nK. B. Hall, and M.-W. Chang, \"Promptagator: Few-shot dense retrieval\nfrom 8 examples,\" arXiv preprint arXiv:2209.11755, 2022.\n[22] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, “Recitation-augmented\nlanguage models,\" arXiv preprint arXiv:2210.01296, 2022.\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\nand M. Zaharia, “Demonstrate-search-predict: Composing retrieval\nand language models for knowledge-intensive nlp,\" arXiv preprint\narXiv:2212.14024, 2022.\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\nJ. Callan, and G. Neubig, \"Active retrieval augmented generation,\"\narXiv preprint arXiv:2305.06983, 2023.\n[25] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag:\nLearning to retrieve, generate, and critique through self-reflection,\"\narXiv preprint arXiv:2310.11511, 2023.\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\n\"Bridging the preference gap between retrievers and llms,\" arXiv\npreprint arXiv:2401.06954, 2024.\n[27] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-\naugmented dual instruction tuning,\" arXiv preprint arXiv:2310.01352,\n2023.\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or\nretrieval? comparing knowledge injection in Ilms,\" arXiv preprint\narXiv:2312.05934, 2023.\n[29] T. Lan, D. Cai, Y. Wang, H. Huang, and X.-L. Mao, \"Copy is all\nyou need,\" in The Eleventh International Conference on Learning\nRepresentations, 2022.\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\nH. Zhang, “Dense x retrieval: What retrieval granularity should we\nuse?\" arXiv preprint arXiv:2312.06648, 2023.\n[31] F. Luo and M. Surdeanu, \"Divide & conquer for entailment-aware\nmulti-hop evidence retrieval,\" arXiv preprint arXiv:2311.02616, 2023.\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y. Li, and N. Cam-Tu,\n\"Diversify question generation with retrieval-augmented style transfer,\"\narXiv preprint arXiv:2310.14503, 2023.\n[33] Z. Guo, S. Cheng, Y. Wang, P. Li, and Y. Liu, “Prompt-guided re-\ntrieval augmentation for non-knowledge-intensive tasks,\" arXiv preprint\narXiv:2305.17653, 2023.\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning\nto filter context for retrieval-augmented generation,\" arXiv preprint\narXiv:2311.08377, 2023.\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\ndata augmentation for low-resource domain tasks,\" arXiv preprint\narXiv:2402.13482, 2024.\n[36] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is not\na good few-shot information extractor, but a good reranker for hard\nsamples!\" arXiv preprint arXiv:2303.08559, 2023.\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\nfor event argument extraction,\" arXiv preprint arXiv:2211.07067, 2022.\n[38] L. Wang, N. Yang, and F. Wei, \"Learning to retrieve in-context\nexamples for large language models,\" arXiv preprint arXiv:2307.07164,\n2023.\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\nL. Hong, Y. Tay, V. Q. Tran, J. Samost et al., “Recommender systems\nwith generative retrieval,\" arXiv preprint arXiv:2305.05065, 2023.\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\nY. Li, H. Lu et al., “Language models as semantic indexers,\" arXiv\npreprint arXiv:2310.07815, 2023.\n[41] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, \"Context tuning\nfor retrieval augmented generation,\" arXiv preprint arXiv:2312.05708,\n2023.\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, \"Few-shot\nlearning with retrieval augmented language models,\" arXiv preprint\narXiv:2208.03299, 2022.\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\nzaro, \"Raven: In-context learning with retrieval augmented encoder-\ndecoder language models,\" arXiv preprint arXiv:2308.07922, 2023.\n"}
{"page": 17, "bbox": [{"x": 0.21274174749851227, "y": 0.5450549721717834}, {"x": 0.2701933979988098, "y": 0.5454944968223572}, {"x": 0.2701933979988098, "y": 0.5529670119285583}, {"x": 0.21274174749851227, "y": 0.5525274872779846}], "text": "\"Advanced\n"}
{"page": 17, "bbox": [{"x": 0.47838452458381653, "y": 0.5459340810775757}, {"x": 0.48919227719306946, "y": 0.5459340810775757}, {"x": 0.48919227719306946, "y": 0.5525274872779846}, {"x": 0.47838452458381653, "y": 0.5525274872779846}], "text": "il-\n"}
{"page": 17, "bbox": [{"x": 0.09271899610757828, "y": 0.5463736057281494}, {"x": 0.12286689132452011, "y": 0.5463736057281494}, {"x": 0.12286689132452011, "y": 0.5542857050895691}, {"x": 0.09271899610757828, "y": 0.5542857050895691}], "text": "[8] I.\n"}
{"page": 17, "bbox": [{"x": 0.2997724711894989, "y": 0.5450549721717834}, {"x": 0.4908987581729889, "y": 0.5450549721717834}, {"x": 0.4908987581729889, "y": 0.5665934085845947}, {"x": 0.2997724711894989, "y": 0.5665934085845947}], "text": "rag techniques: an\nhttps://pub.towardsai.net/\n"}
{"page": 17, "bbox": [{"x": 0.11604095250368118, "y": 0.5573626160621643}, {"x": 0.16040955483913422, "y": 0.5573626160621643}, {"x": 0.16040955483913422, "y": 0.5643956065177917}, {"x": 0.11604095250368118, "y": 0.5643956065177917}], "text": "lustrated\n"}
{"page": 17, "bbox": [{"x": 0.23208190500736237, "y": 0.5578022003173828}, {"x": 0.28725823760032654, "y": 0.5578022003173828}, {"x": 0.28725823760032654, "y": 0.5648351907730103}, {"x": 0.23208190500736237, "y": 0.5648351907730103}], "text": "overview,\"\n"}
{"page": 17, "bbox": [{"x": 0.11604095250368118, "y": 0.5683516263961792}, {"x": 0.4584755301475525, "y": 0.5683516263961792}, {"x": 0.4584755301475525, "y": 0.5771428346633911}, {"x": 0.11604095250368118, "y": 0.5771428346633911}], "text": "advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\n"}
{"page": 17, "bbox": [{"x": 0.11604095250368118, "y": 0.5797802209854126}, {"x": 0.14448235929012299, "y": 0.5797802209854126}, {"x": 0.14448235929012299, "y": 0.58681321144104}, {"x": 0.11604095250368118, "y": 0.58681321144104}], "text": "2023.\n"}
{"page": 17, "bbox": [{"x": 0.0853242352604866, "y": 0.5916483402252197}, {"x": 0.49146756529808044, "y": 0.591208815574646}, {"x": 0.4920364022254944, "y": 0.8637362718582153}, {"x": 0.08589305728673935, "y": 0.8641757965087891}], "text": "[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\n“Large language model based long-tail query rewriting in taobao\nsearch,\" arXiv preprint arXiv:2311.03758, 2023.\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\nand D. Zhou, \"Take a step back: Evoking reasoning via abstraction in\nlarge language models,\" arXiv preprint arXiv:2310.06117, 2023.\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, \"Precise zero-shot dense retrieval\nwithout relevance labels,\" arXiv preprint arXiv:2212.10496, 2022.\n[12] V. Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\nsityranker and lostinthemiddleranker,\" https://towardsdatascience.com/\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\n[13] W. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\nand M. Jiang, “Generate rather than retrieve: Large language models\nare strong context generators,\" arXiv preprint arXiv:2209.10063, 2022.\n[14] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen,\n\"Enhancing retrieval-augmented large language models with iterative\nretrieval-generation synergy,\" arXiv preprint arXiv:2305.15294, 2023.\n[15] X. Wang, Q. Yang, Y. Qiu, J. Liang, Q. He, Z. Gu, Y. Xiao,\nand W. Wang, “Knowledgpt: Enhancing large language models with\nretrieval and storage access on knowledge bases,\" arXiv preprint\narXiv:2308.11761, 2023.\nRaudaschl,\nrag-fusion,\"\nhttps://towardsdatascience.com/\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\n"}
{"page": 17, "bbox": [{"x": 0.4118316173553467, "y": 0.8325274586677551}, {"x": 0.42832764983177185, "y": 0.8325274586677551}, {"x": 0.42832764983177185, "y": 0.8386813402175903}, {"x": 0.4118316173553467, "y": 0.8386813402175903}], "text": "the\n"}
{"page": 17, "bbox": [{"x": 0.15927189588546753, "y": 0.8329670429229736}, {"x": 0.1717861145734787, "y": 0.8329670429229736}, {"x": 0.1717861145734787, "y": 0.8391208648681641}, {"x": 0.15927189588546753, "y": 0.8391208648681641}], "text": "H.\n"}
{"page": 17, "bbox": [{"x": 0.45904436707496643, "y": 0.8325274586677551}, {"x": 0.48919227719306946, "y": 0.8325274586677551}, {"x": 0.48919227719306946, "y": 0.8395604491233826}, {"x": 0.45904436707496643, "y": 0.8395604491233826}], "text": "future\n"}
{"page": 17, "bbox": [{"x": 0.29124003648757935, "y": 0.8316483497619629}, {"x": 0.38168373703956604, "y": 0.8325274586677551}, {"x": 0.38168373703956604, "y": 0.841318666934967}, {"x": 0.29124003648757935, "y": 0.8404395580291748}], "text": "\"Forget rag,\n"}
{"page": 17, "bbox": [{"x": 0.08646188676357269, "y": 0.8316483497619629}, {"x": 0.12741751968860626, "y": 0.8316483497619629}, {"x": 0.12741751968860626, "y": 0.850109875202179}, {"x": 0.08646188676357269, "y": 0.850109875202179}], "text": "[16] A.\nis\n"}
{"page": 17, "bbox": [{"x": 0.08646188676357269, "y": 0.866373598575592}, {"x": 0.49146756529808044, "y": 0.866373598575592}, {"x": 0.49146756529808044, "y": 0.9428571462631226}, {"x": 0.08646188676357269, "y": 0.9428571462631226}], "text": "[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, \"Lift\nyourself up: Retrieval-augmented text generation with self memory,\"\narXiv preprint arXiv:2305.02437, 2023.\n[18] S. Wang, Y. Xu, Y. Fang, Y. Liu, S. Sun, R. Xu, C. Zhu, and\nM. Zeng, “Training data is more valuable than you think: A simple\nand effective method by retrieving from training data,\" arXiv preprint\narXiv:2203.08773, 2022.\n"}
{"page": 18, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9180887341499329, "y": 0.03384615480899811}, {"x": 0.9180887341499329, "y": 0.03868131712079048}, {"x": 0.9089874625205994, "y": 0.03868131712079048}], "text": "18\n"}
{"page": 18, "bbox": [{"x": 0.08589305728673935, "y": 0.07384615391492844}, {"x": 0.49146756529808044, "y": 0.07340659201145172}, {"x": 0.4920364022254944, "y": 0.5819780230522156}, {"x": 0.08646188676357269, "y": 0.5824176073074341}], "text": "[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\nO. Kuchaiev, B. Li, C. Xiao et al., \"Shall we pretrain autoregressive\nlanguage models with retrieval? a comprehensive study,\" arXiv preprint\narXiv:2304.06762, 2023.\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\nzaro, \"Instructretro: Instruction tuning post retrieval-augmented pre-\ntraining,\" arXiv preprint arXiv:2310.07713, 2023.\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\nand S. Nanayakkara, “Improving the domain adaptation of retrieval\naugmented generation (rag) models for open domain question answer-\ning,\" Transactions of the Association for Computational Linguistics,\nvol. 11, pp. 1-17, 2023.\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\nimproves generalization of language models as generic plug-in,\" arXiv\npreprint arXiv:2305.17331, 2023.\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\naugmented language models robust to irrelevant context,\" arXiv\npreprint arXiv:2310.01558, 2023.\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-\ntrieval augmentation for long-form question answering,\" arXiv preprint\narXiv:2310.12150, 2023.\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:\nEnhancing robustness in retrieval-augmented language models,\" arXiv\npreprint arXiv:2311.09210, 2023.\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, \"Search-in-the-\nchain: Towards accurate, credible and traceable large language models\nfor knowledgeintensive tasks,\" CoRR, vol. abs/2304.14732, 2023.\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\n“Optimizing retrieval-augmented reader models via token elimination,\"\narXiv preprint arXiv:2310.13682, 2023.\n[53] J. Lála, O. O'Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\nscientific research,\" arXiv preprint arXiv:2312.07559, 2023.\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\nY. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\nRedefining retrieval for rag systems,\" arXiv preprint arXiv:2401.14887,\n2024.\n[55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and\nZ. Cao, “Iag: Induction-augmented generation framework for answer-\ning reasoning questions,” in Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023, pp. 1-14.\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\n\"Nomiracl: Knowing when you don't know for robust multilingual\nretrieval-augmented generation,\" arXiv preprint arXiv:2312.11361,\n"}
{"page": 18, "bbox": [{"x": 0.5153583884239197, "y": 0.072967030107975}, {"x": 0.9209328889846802, "y": 0.07252747565507889}, {"x": 0.9215016961097717, "y": 0.5964835286140442}, {"x": 0.5159271955490112, "y": 0.5969230532646179}], "text": "[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning\nzero-shot semi-parametric language models from multiple tasks,\" arXiv\npreprint arXiv:2210.00185, 2022.\n[67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, “Corrective retrieval\naugmented generation,\" arXiv preprint arXiv:2401.15884, 2024.\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer\ngeneration and evidence retrieval,\" arXiv preprint arXiv:2310.16568,\n2023.\n[69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, \"Prca:\nFitting black-box large language models for retrieval question answer-\ning via pluggable reward-driven contextual adapter,\" arXiv preprint\narXiv:2310.18347, 2023.\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large\nlanguage models are strong zero-shot query likelihood models for\ndocument ranking,\" arXiv preprint arXiv:2310.13243, 2023.\n[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented\nlms with compression and selective augmentation,\" arXiv preprint\narXiv:2310.04408, 2023.\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\nmoyer, and W.-t. Yih, \"Replug: Retrieval-augmented black-box lan-\nguage models,\" arXiv preprint arXiv:2301.12652, 2023.\n[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-\ntionale memory for retrieval augmented generation,\" arXiv preprint\narXiv:2311.04177, 2023.\n[74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi,\nJ. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source\nretrieval-augmented generation for personalized dialogue systems,”\narXiv preprint arXiv:2401.13256, 2024.\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\n\"Augmented large language models with parametric knowledge guid-\ning,\" arXiv preprint arXiv:2305.04757, 2023.\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, “\"Structure-\naware language model pretraining improves dense retrieval on struc-\ntured data,\" arXiv preprint arXiv:2305.19912, 2023.\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge\ngraph-augmented language models for knowledge-grounded dialogue\ngeneration,\" arXiv preprint arXiv:2305.18846, 2023.\n[78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, \"Retrieval-\ngeneration alignment for end-to-end task-oriented dialogue system,”\"\narXiv preprint arXiv:2310.08877, 2023.\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback\nknowledge retrieval for task-oriented dialogue systems,\" arXiv preprint\narXiv:2310.14528, 2023.\n[80] P. Ranade and A. Joshi, \"Fabula: Intelligence report generation\nusing retrieval-augmented narrative construction,\" arXiv preprint\n"}
{"page": 18, "bbox": [{"x": 0.11604095250368118, "y": 0.5837362408638}, {"x": 0.14562001824378967, "y": 0.5841758251190186}, {"x": 0.14562001824378967, "y": 0.5907692313194275}, {"x": 0.11604095250368118, "y": 0.590329647064209}], "text": "2023.\n"}
{"page": 18, "bbox": [{"x": 0.5460751056671143, "y": 0.5986813306808472}, {"x": 0.6780432462692261, "y": 0.5986813306808472}, {"x": 0.6780432462692261, "y": 0.6057142615318298}, {"x": 0.5460751056671143, "y": 0.6057142615318298}], "text": "arXiv:2310.13848, 2023.\n"}
{"page": 18, "bbox": [{"x": 0.08589305728673935, "y": 0.5951648354530334}, {"x": 0.4908987581729889, "y": 0.5951648354530334}, {"x": 0.4908987581729889, "y": 0.944615364074707}, {"x": 0.08589305728673935, "y": 0.944615364074707}], "text": "[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \"Tree of clarifica-\ntions: Answering ambiguous questions with retrieval-augmented large\nlanguage models,\" arXiv preprint arXiv:2310.14696, 2023.\n[58] Y. Wang, P. Li, M. Sun, and Y. Liu, “Self-knowledge guided\nretrieval augmentation for large language models,\" arXiv preprint\narXiv:2310.05002, 2023.\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, \"Retrieval-\ngeneration synergy augmented large language models,\" arXiv preprint\narXiv:2310.05149, 2023.\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\ncontext large language models,\" arXiv preprint arXiv:2310.03025,\n2023.\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \"Interleav-\ning retrieval with chain-of-thought reasoning for knowledge-intensive\nmulti-step questions,\" arXiv preprint arXiv:2212.10509, 2022.\n[62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\nR. Wen, and H. Wang, “Investigating the factual knowledge boundary\nof large language models with retrieval augmentation,\" arXiv preprint\narXiv:2307.11019, 2023.\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\nManning, \"Raptor: Recursive abstractive processing for tree-organized\nretrieval,\" arXiv preprint arXiv:2401.18059, 2024.\n[64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\nBrown, and Y. Shoham, \"In-context retrieval-augmented language\nmodels,\" arXiv preprint arXiv:2302.00083, 2023.\n[65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \"Retrieve-and-\nsample: Document-level event argument extraction via hybrid retrieval\naugmentation,\" in Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\n2023, pp. 293-306.\n"}
{"page": 18, "bbox": [{"x": 0.5159271955490112, "y": 0.6101098656654358}, {"x": 0.9857792854309082, "y": 0.6101098656654358}, {"x": 0.9857792854309082, "y": 0.9323077201843262}, {"x": 0.5159271955490112, "y": 0.9323077201843262}], "text": "[81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang,\nH. Ding, X. Chu, J. Zhao et al., \"Think and retrieval: A hypothesis\nknowledge graph enhanced medical large language models,\" arXiv\npreprint arXiv:2312.15883, 2023.\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\n\"Knowledge-augmented language model verification,\" arXiv preprint\narXiv:2310.12836, 2023.\n[83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful\nand interpretable large language model reasoning,\" arXiv preprint\narXiv:2310.01061, 2023.\n[84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun,\nX. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\nfor textual graph understanding and question answering,\" arXiv preprint\narXiv:2402.07630, 2024.\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\nX. Li, A. Su et al., \"Tablegpt: Towards unifying tables, nature language\nand commands into one gpt,\" arXiv preprint arXiv:2307.08674, 2023.\n[86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, “Iseeq: Information\nseeking question generation using dynamic meta-information retrieval\nand knowledge graphs,\" in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10672-10 680.\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Schärli,\nand D. Zhou, “Large language models can be easily distracted by\nirrelevant context,\" in International Conference on Machine Learning.\nPMLR, 2023, pp. 31210-31 227.\n[88] R. Teja, \"Evaluating the ideal chunk size for a rag\nsystem using llamaindex,\" https://www.llamaindex.ai/blog/\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n"}
{"page": 18, "bbox": [{"x": 0.5460751056671143, "y": 0.9349450469017029}, {"x": 0.5739476680755615, "y": 0.9345055222511292}, {"x": 0.5739476680755615, "y": 0.9415384531021118}, {"x": 0.5460751056671143, "y": 0.9419780373573303}], "text": "2023.\n"}
{"page": 19, "bbox": [{"x": 0.9089874625205994, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.03384615480899811}, {"x": 0.9192264080047607, "y": 0.0391208790242672}, {"x": 0.9089874625205994, "y": 0.0391208790242672}], "text": "19\n"}
{"page": 19, "bbox": [{"x": 0.08646188676357269, "y": 0.07428571581840515}, {"x": 0.489761084318161, "y": 0.07428571581840515}, {"x": 0.489761084318161, "y": 0.09406593441963196}, {"x": 0.08646188676357269, "y": 0.09406593441963196}], "text": "[89] Langchain, “Recursively split by character,” https://python.langchain.\ncom/docs/modules/data_connection/document_transformers/recursive_\n"}
{"page": 19, "bbox": [{"x": 0.11604095250368118, "y": 0.09758241474628448}, {"x": 0.2144482433795929, "y": 0.09626373648643494}, {"x": 0.2144482433795929, "y": 0.10505494475364685}, {"x": 0.11604095250368118, "y": 0.1063736230134964}], "text": "text_splitter, 2023.\n"}
{"page": 19, "bbox": [{"x": 0.4408418536186218, "y": 0.10813187062740326}, {"x": 0.489761084318161, "y": 0.10857142508029938}, {"x": 0.489761084318161, "y": 0.11516483873128891}, {"x": 0.4408418536186218, "y": 0.1147252768278122}], "text": "Small-to-\n"}
{"page": 19, "bbox": [{"x": 0.23435722291469574, "y": 0.10813187062740326}, {"x": 0.2918088734149933, "y": 0.10813187062740326}, {"x": 0.2918088734149933, "y": 0.11516483873128891}, {"x": 0.23435722291469574, "y": 0.11516483873128891}], "text": "\"Advanced\n"}
{"page": 19, "bbox": [{"x": 0.08589305728673935, "y": 0.10813187062740326}, {"x": 0.12457337975502014, "y": 0.10725274682044983}, {"x": 0.1251422017812729, "y": 0.11604395508766174}, {"x": 0.08646188676357269, "y": 0.11692307889461517}], "text": "[90] S.\n"}
{"page": 19, "bbox": [{"x": 0.16609783470630646, "y": 0.10901098698377609}, {"x": 0.19567690789699554, "y": 0.10901098698377609}, {"x": 0.19567690789699554, "y": 0.11648351699113846}, {"x": 0.16609783470630646, "y": 0.11648351699113846}], "text": "Yang,\n"}
{"page": 19, "bbox": [{"x": 0.32992035150527954, "y": 0.11032967269420624}, {"x": 0.3464163839817047, "y": 0.11032967269420624}, {"x": 0.3464163839817047, "y": 0.11648351699113846}, {"x": 0.32992035150527954, "y": 0.11648351699113846}], "text": "rag\n"}
{"page": 19, "bbox": [{"x": 0.20250284671783447, "y": 0.11956044286489487}, {"x": 0.25369739532470703, "y": 0.11956044286489487}, {"x": 0.25369739532470703, "y": 0.12703296542167664}, {"x": 0.20250284671783447, "y": 0.12703296542167664}], "text": "retrieval,\"\n"}
{"page": 19, "bbox": [{"x": 0.11547213047742844, "y": 0.11999999731779099}, {"x": 0.1331057995557785, "y": 0.11999999731779099}, {"x": 0.1331057995557785, "y": 0.12791208922863007}, {"x": 0.11547213047742844, "y": 0.12791208922863007}], "text": "big\n"}
{"page": 19, "bbox": [{"x": 0.08589305728673935, "y": 0.10901098698377609}, {"x": 0.4908987581729889, "y": 0.10857142508029938}, {"x": 0.49146756529808044, "y": 0.29802197217941284}, {"x": 0.08646188676357269, "y": 0.29846152663230896}], "text": "01:\nhttps://towardsdatascience.com/\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\n[91] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\n\"Knowledge graph prompting for multi-document question answering,\"\narXiv preprint arXiv:2308.11730, 2023.\n[92] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\nenables complex reasoning in large language models,\" arXiv preprint\narXiv:2205.10625, 2022.\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\nand J. Weston, “Chain-of-verification reduces hallucination in large\nlanguage models,\" arXiv preprint arXiv:2309.11495, 2023.\n[94] X. Li and J. Li, “Angle-optimized text embeddings,\" arXiv preprint\narXiv:2309.12871, 2023.\n[95] VoyageAI, “Voyage's embedding models,\" https://docs.voyageai.com/\nembeddings/, 2023.\n"}
{"page": 19, "bbox": [{"x": 0.33674630522727966, "y": 0.30065932869911194}, {"x": 0.4908987581729889, "y": 0.30065932869911194}, {"x": 0.4908987581729889, "y": 0.30945053696632385}, {"x": 0.33674630522727966, "y": 0.30945053696632385}], "text": "https://github.com/FlagOpen/\n"}
{"page": 19, "bbox": [{"x": 0.08646188676357269, "y": 0.30109891295433044}, {"x": 0.15073947608470917, "y": 0.30109891295433044}, {"x": 0.15073947608470917, "y": 0.30901098251342773}, {"x": 0.08646188676357269, "y": 0.30901098251342773}], "text": "[96] BAAI,\n"}
{"page": 19, "bbox": [{"x": 0.11604095250368118, "y": 0.30109891295433044}, {"x": 0.2906712293624878, "y": 0.3002197742462158}, {"x": 0.2906712293624878, "y": 0.32043954730033875}, {"x": 0.11604095250368118, "y": 0.32131868600845337}], "text": "\"Flagembedding,”\nFlagEmbedding, 2023.\n"}
{"page": 19, "bbox": [{"x": 0.08646188676357269, "y": 0.32351648807525635}, {"x": 0.4908987581729889, "y": 0.3226373493671417}, {"x": 0.4908987581729889, "y": 0.3437362611293793}, {"x": 0.08646188676357269, "y": 0.3446153700351715}], "text": "[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y. Nie, “Retrieve anything\nto augment large language models,\" arXiv preprint arXiv:2310.07554,\n"}
{"page": 19, "bbox": [{"x": 0.11547213047742844, "y": 0.34593406319618225}, {"x": 0.14562001824378967, "y": 0.34637361764907837}, {"x": 0.14562001824378967, "y": 0.3529670238494873}, {"x": 0.11547213047742844, "y": 0.3525274693965912}], "text": "2023.\n"}
{"page": 19, "bbox": [{"x": 0.08020477741956711, "y": 0.35736262798309326}, {"x": 0.49146756529808044, "y": 0.35692307353019714}, {"x": 0.49146756529808044, "y": 0.5481318831443787}, {"x": 0.08020477741956711, "y": 0.5485714077949524}], "text": "[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\nand P. Liang, “Lost in the middle: How language models use long\ncontexts,\" arXiv preprint arXiv:2307.03172, 2023.\n[99] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\nrec: Towards interactive and explainable llms-augmented recommender\nsystem,\" arXiv preprint arXiv:2303.14524, 2023.\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\nscenarios for live interpretation and automatic dubbing,” in Proceedings\nof the 15th Biennial Conference of the Association for Machine\nTranslation in the Americas (Volume 2: Users and Providers Track\nand Government Track), J. Campbell, S. Larocca, J. Marciano,\nK. Savenkov, and A. Yanishevsky, Eds. Orlando, USA: Association\nfor Machine Translation in the Americas, Sep. 2022, pp. 202–209.\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu,\n\"Longllmlingua: Accelerating and enhancing llms in long context\nscenarios via prompt compression,” arXiv preprint arXiv:2310.06839,\n"}
{"page": 19, "bbox": [{"x": 0.509670078754425, "y": 0.07120878994464874}, {"x": 0.9209328889846802, "y": 0.07120878994464874}, {"x": 0.9209328889846802, "y": 0.9441758394241333}, {"x": 0.509670078754425, "y": 0.9441758394241333}], "text": "of the Association for Computational Linguistics, vol. 7, pp. 453–466,\n2019.\n[112] Y. Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y. Zhou,\n\"Exploring the integration strategies of retriever and large language\nmodels,\" arXiv preprint arXiv:2308.12574, 2023.\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, \"Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehen-\nsion,\" arXiv preprint arXiv:1705.03551, 2017.\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \"Squad: 100,000+\nquestions for machine comprehension of text,\" arXiv preprint\narXiv:1606.05250, 2016.\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\nfreebase from question-answer pairs,\" in Proceedings of the 2013\nconference on empirical methods in natural language processing, 2013,\npp. 1533-1544.\n[116] A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\n\"When not to trust language models: Investigating effectiveness and\nlimitations of parametric and non-parametric memories,” arXiv preprint\narXiv:2212.10511, 2022.\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\nand L. Deng, \"Ms marco: A human-generated machine reading com-\nprehension dataset,\" 2016.\n[118] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdi-\nnov, and C. D. Manning, \"Hotpotqa: A dataset for diverse, explain-\nable multi-hop question answering,\" arXiv preprint arXiv:1809.09600,\n2018.\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, \"Constructing a\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,\"\narXiv preprint arXiv:2011.01060, 2020.\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \"Musique:\nMultihop questions via single-hop question composition,\" Transactions\nof the Association for Computational Linguistics, vol. 10, pp. 539–554,\n2022.\n[121] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\nLong form question answering,\" arXiv preprint arXiv:1907.09190,\n2019.\n[122] T. Kočiskỳ, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\nlenge,\" Transactions of the Association for Computational Linguistics,\nvol. 6, pp. 317-328, 2018.\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, \"A human-\ninspired reading agent with gist memory of very long contexts,\" arXiv\npreprint arXiv:2402.09727, 2024.\n[124] I. Stelmakh, Y. Luan, B. Dhingra, and M.-W. Chang, \"Asqa: Factoid\nquestions meet long-form answers,\" arXiv preprint arXiv:2204.06092,\n2022.\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\nAwadallah, A. Celikyilmaz, Y. Liu, X. Qiu et al., \"Qmsum: A new\nbenchmark for query-based multi-domain meeting summarization,\"\narXiv preprint arXiv:2104.05938, 2021.\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\n\"A dataset of information-seeking questions and answers anchored in\nresearch papers,\" arXiv preprint arXiv:2105.03011, 2021.\n[127] T. Möller, A. Reina, R. Jayakumar, and M. Pietsch, \"Covid-qa: A\nquestion answering dataset for covid-19,\" in ACL 2020 Workshop on\nNatural Language Processing for COVID-19 (NLP-COVID), 2020.\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\nJ. Li, X. Wan, B. Wang et al., “Cmb: A comprehensive medical\nbenchmark in chinese,” arXiv preprint arXiv:2308.08833, 2023.\n[129] H. Zeng, “Measuring massive multitask chinese understanding,\" arXiv\npreprint arXiv:2304.12986, 2023.\n[130] R. Y. Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V. Pad-\nmakumar, J. Ma, J. Thompson, H. He et al., “Quality: Question an-\nswering with long input texts, yes!\" arXiv preprint arXiv:2112.08608,\n2021.\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\nand O. Tafjord, \"Think you have solved question answering? try arc,\nthe ai2 reasoning challenge,\" arXiv preprint arXiv:1803.05457, 2018.\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\nA question answering challenge targeting commonsense knowledge,”\narXiv preprint arXiv:1811.00937, 2018.\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\n\"Wizard of wikipedia: Knowledge-powered conversational agents,”\narXiv preprint arXiv:1811.01241, 2018.\n[134] H. Wang, M. Hu, Y. Deng, R. Wang, F. Mi, W. Wang, Y. Wang, W.-\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source\n"}
{"page": 19, "bbox": [{"x": 0.11604095250368118, "y": 0.5503296852111816}, {"x": 0.14505119621753693, "y": 0.5503296852111816}, {"x": 0.14505119621753693, "y": 0.5564835071563721}, {"x": 0.11604095250368118, "y": 0.5564835071563721}], "text": "2023.\n"}
{"page": 19, "bbox": [{"x": 0.07906711846590042, "y": 0.5613186955451965}, {"x": 0.49146756529808044, "y": 0.560879111289978}, {"x": 0.4920364022254944, "y": 0.9441758394241333}, {"x": 0.07963594794273376, "y": 0.944615364074707}], "text": "[102] V. Karpukhin, B. Oğuz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\nand W.-t. Yih, \"Dense passage retrieval for open-domain question\nanswering,\" arXiv preprint arXiv:2004.04906, 2020.\n[103] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is\nnot a good few-shot information extractor, but a good reranker for\nhard samples!\" ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:257532405\n[104] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, \"Chatlaw: Open-source\nlegal large language model with integrated external knowledge bases,\"\narXiv preprint arXiv:2306.16092, 2023.\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\naugmented language models robust to irrelevant context,\" arXiv\npreprint arXiv:2310.01558, 2023.\n[106] X. Li, R. Zhao, Y. K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\n\"Chain of knowledge: A framework for grounding large language mod-\nels with structured knowledge bases,” arXiv preprint arXiv:2305.13269,\n2023.\n[107] H. Yang, S. Yue, and Y. He, \"Auto-gpt for online decision\nmaking: Benchmarks and additional opinions,\" arXiv preprint\narXiv:2306.02224, 2023.\n[108] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettle-\nmoyer, N. Cancedda, and T. Scialom, \"Toolformer: Language models\ncan teach themselves to use tools,\" arXiv preprint arXiv:2302.04761,\n2023.\n[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-\nsoning ability via prompt augmented by chatgpt,\" arXiv preprint\narXiv:2304.11116, 2023.\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., \"Webgpt: Browser-\nassisted question-answering with human feedback,\" arXiv preprint\narXiv:2112.09332, 2021.\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\nquestions: a benchmark for question answering research,\" Transactions\n"}
{"page": 20, "bbox": [{"x": 0.9078498482704163, "y": 0.03340659290552139}, {"x": 0.9192264080047607, "y": 0.03340659290552139}, {"x": 0.9192264080047607, "y": 0.0391208790242672}, {"x": 0.9078498482704163, "y": 0.0391208790242672}], "text": "20\n"}
{"page": 20, "bbox": [{"x": 0.07963594794273376, "y": 0.09714286029338837}, {"x": 0.10693970322608948, "y": 0.09670329838991165}, {"x": 0.10693970322608948, "y": 0.10505494475364685}, {"x": 0.07963594794273376, "y": 0.10549450665712357}], "text": "[135]\n"}
{"page": 20, "bbox": [{"x": 0.509670078754425, "y": 0.07428571581840515}, {"x": 0.9215016961097717, "y": 0.07428571581840515}, {"x": 0.9215016961097717, "y": 0.2439560443162918}, {"x": 0.509670078754425, "y": 0.2439560443162918}], "text": "[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n\"Codesearchnet challenge: Evaluating the state of semantic code\nsearch,\" arXiv preprint arXiv:1909.09436, 2019.\n[158] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\nto solve math word problems,\" arXiv preprint arXiv:2110.14168, 2021.\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\nand D. Varga, \"The jrc-acquis: A multilingual aligned parallel corpus\nwith 20+ languages,” arXiv preprint cs/0609058, 2006.\n[160] Y. Hoshi, D. Miyashita, Y. Ng, K. Tatsuno, Y. Morioka, O. Torii,\nand J. Deguchi, \"Ralle: A framework for developing and eval-\nuating retrieval-augmented large language models,\" arXiv preprint\narXiv:2308.10633, 2023.\n[161] J. Liu, “Building production-ready rag applications,\" https://www.ai.\nengineer/summit/schedule/building-production-ready-rag-applications,\n"}
{"page": 20, "bbox": [{"x": 0.5455062389373779, "y": 0.2461538463830948}, {"x": 0.5739476680755615, "y": 0.2461538463830948}, {"x": 0.5739476680755615, "y": 0.25318682193756104}, {"x": 0.5455062389373779, "y": 0.25318682193756104}], "text": "2023.\n"}
{"page": 20, "bbox": [{"x": 0.07963594794273376, "y": 0.07428571581840515}, {"x": 0.4920364022254944, "y": 0.07428571581840515}, {"x": 0.4920364022254944, "y": 0.9331868290901184}, {"x": 0.07963594794273376, "y": 0.9331868290901184}], "text": "planner for personalized knowledge-grounded dialogue,” arXiv preprint\narXiv:2310.08840, 2023.\n\"Large language models as source planner for personal-\nized knowledge-grounded dialogue,\" arXiv preprint arXiv:2310.08840,\n2023.\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, and S. Wang,\n\"Long time no see! open-domain conversation with long-term persona\nmemory,\" arXiv preprint arXiv:2203.05797, 2022.\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\nSu, S. Ultes, D. Vandyke, and S. Young, \"Conditional generation\nand snapshot learning in neural dialogue systems,\" arXiv preprint\narXiv:1606.03352, 2016.\n[138] R. He and J. McAuley, \"Ups and downs: Modeling the visual evolution\nof fashion trends with one-class collaborative filtering,\" in proceedings\nof the 25th international conference on world wide web, 2016, pp.\n507-517.\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction\nby conditional generation,\" arXiv preprint arXiv:2104.05919, 2021.\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, \"Multi-\nsentence argument linking,\" arXiv preprint arXiv:1911.03766, 2019.\n[141] H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\nand E. Simperl, “T-rex: A large scale alignment of natural language\nwith knowledge base triples,\" in Proceedings of the Eleventh Inter-\nnational Conference on Language Resources and Evaluation (LREC\n2018), 2018.\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-\ntraction via reading comprehension,\" arXiv preprint arXiv:1706.04115,\n2017.\n[143] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, \"Hel-\nlaswag: Can a machine really finish your sentence?\" arXiv preprint\narXiv:1905.07830, 2019.\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\n\"The cot collection: Improving zero-shot and few-shot learning of\nlanguage models via chain-of-thought fine-tuning,\" arXiv preprint\narXiv:2305.14045, 2023.\n[145] A. Saha, V. Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\n\"Complex sequential question answering: Towards learning to converse\nover linked question answer pairs with a knowledge graph,” in Proceed-\nings of the AAAI conference on artificial intelligence, vol. 32, no. 1,\n2018.\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, \"Measuring massive multitask language understanding,\"\narXiv preprint arXiv:2009.03300, 2020.\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, \"Pointer sentinel\nmixture models,\" arXiv preprint arXiv:1609.07843, 2016.\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\n\"Did aristotle use a laptop? a question answering benchmark with\nimplicit reasoning strategies,\" Transactions of the Association for\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, \"Fever: a\nlarge-scale dataset for fact extraction and verification,\" arXiv preprint\narXiv:1803.05355, 2018.\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\npublic health claims,\" arXiv preprint arXiv:2010.09926, 2020.\n[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\nstructured data with application to the biography domain,\" arXiv\npreprint arXiv:1603.07771, 2016.\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\nand G. Neubig, \"Wikiasp: A dataset for multi-domain aspect-based\nsummarization,\" Transactions of the Association for Computational\nLinguistics, vol. 9, pp. 211–225, 2021.\n[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don't give me the details,\njust the summary! topic-aware convolutional neural networks for ex-\ntreme summarization,\" arXiv preprint arXiv:1808.08745, 2018.\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\nS. I. Ahmed, N. Mohammed, and M. R. Amin, \"Vio-lens: A novel\ndataset of annotated social network posts leading to different forms\nof communal violence and its evaluation,” in Proceedings of the First\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72-\n84.\n[155] X. Li and D. Roth, “Learning question classifiers,\" in COLING 2002:\nThe 19th International Conference on Computational Linguistics, 2002.\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng,\nand C. Potts, \"Recursive deep models for semantic compositionality\nover a sentiment treebank,” in Proceedings of the 2013 conference on\nempirical methods in natural language processing, 2013, pp. 1631–\n"}
{"page": 20, "bbox": [{"x": 0.5091012716293335, "y": 0.25670328736305237}, {"x": 0.9209328889846802, "y": 0.25670328736305237}, {"x": 0.9215016961097717, "y": 0.944615364074707}, {"x": 0.509670078754425, "y": 0.944615364074707}], "text": "[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,\"\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, \"Best practices for\nIlm evaluation of rag applications,\" https://www.databricks.com/blog/\nLLM-auto-eval-best-practices-RAG, 2023.\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, \"Ragas: Au-\ntomated evaluation of retrieval augmented generation,\" arXiv preprint\narXiv:2309.15217, 2023.\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, \"Ares: An\nautomated evaluation framework for retrieval-augmented generation\nsystems,\" arXiv preprint arXiv:2311.09476, 2023.\n[166] C. Jarvis and J. Allard, \"A survey of techniques for\nmaximizing Ilm performance,\" https://community.openai.\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\na-survey-of-techniques-for-maximizing-Ilm-performance-2, 2023.\n[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\nguage models in retrieval-augmented generation,\" arXiv preprint\narXiv:2309.01431, 2023.\n[168] Y. Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\nX. Sun, \"Recall: A benchmark for llms robustness against external\ncounterfactual knowledge,” arXiv preprint arXiv:2311.08147, 2023.\n[169] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\nfor retrieval-augmented generation of large language models,\" arXiv\npreprint arXiv:2401.17043, 2024.\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\ncontext large language models,\" arXiv preprint arXiv:2310.03025,\n2023.\n[171] C. Packer, V. Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\nzalez, \"Memgpt: Towards llms as operating systems,\" arXiv preprint\narXiv:2310.08560, 2023.\n[172] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, “Efficient\nstreaming language models with attention sinks,\" arXiv preprint\narXiv:2309.17453, 2023.\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\nGonzalez, \"Raft: Adapting language model to domain specific rag,\"\narXiv preprint arXiv:2403.10131, 2024.\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\nfor neural language models,\" arXiv preprint arXiv:2001.08361, 2020.\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, \"Neuro-\nsymbolic language modeling with automaton-augmented retrieval,\" in\nInternational Conference on Machine Learning.\nPMLR, 2022, pp.\n468-485.\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-\nmodal language modeling,\" arXiv preprint arXiv:2211.12561, 2022.\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, \"Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language\nmodels,\" arXiv preprint arXiv:2301.12597, 2023.\n[178] W. Zhu, A. Yan, Y. Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y.\nWang, \"Visualize before you write: Imagination-guided open-ended\ntext generation,\" arXiv preprint arXiv:2210.03765, 2022.\n[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from\nspokenvocab for speech translation,\" arXiv preprint arXiv:2210.08174,\n2022.\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, \"Using external\noff-policy speech-to-text mappings in contextual end-to-end automated\nspeech recognition,” arXiv preprint arXiv:2301.02736, 2023.\n"}
{"page": 20, "bbox": [{"x": 0.11717861145734787, "y": 0.9353846311569214}, {"x": 0.14562001824378967, "y": 0.9353846311569214}, {"x": 0.14562001824378967, "y": 0.9415384531021118}, {"x": 0.11717861145734787, "y": 0.9415384531021118}], "text": "1642.\n"}
{"page": 21, "bbox": [{"x": 0.9078498482704163, "y": 0.03384615480899811}, {"x": 0.9175199270248413, "y": 0.03384615480899811}, {"x": 0.9175199270248413, "y": 0.0391208790242672}, {"x": 0.9078498482704163, "y": 0.0391208790242672}], "text": "21\n"}
{"page": 21, "bbox": [{"x": 0.08020477741956711, "y": 0.07428571581840515}, {"x": 0.4920364022254944, "y": 0.07428571581840515}, {"x": 0.4920364022254944, "y": 0.17186813056468964}, {"x": 0.08020477741956711, "y": 0.17186813056468964}], "text": "[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\nJ. Sivic, and C. Schmid, \"Vid2seq: Large-scale pretraining of a visual\nlanguage model for dense video captioning,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2023, pp. 10714-10726.\n[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt\nselection for code-related few-shot learning,” in 2023 IEEE/ACM 45th\nInternational Conference on Software Engineering (ICSE), 2023, pp.\n2450-2462.\n"}
{"page": 1, "bbox": [{"x": 0.09499431401491165, "y": 0.10417582094669342}, {"x": 0.9038680195808411, "y": 0.10681318491697311}, {"x": 0.9038680195808411, "y": 0.12659341096878052}, {"x": 0.09499431401491165, "y": 0.12395604699850082}], "text": "Seven Failure Points When Engineering a Retrieval Augmented\n"}
{"page": 1, "bbox": [{"x": 0.3788395822048187, "y": 0.12923076748847961}, {"x": 0.6183162927627563, "y": 0.13318681716918945}, {"x": 0.61774742603302, "y": 0.14945055544376373}, {"x": 0.3782707750797272, "y": 0.1454945057630539}], "text": "Generation System\n"}
{"page": 1, "bbox": [{"x": 0.12457337975502014, "y": 0.16483516991138458}, {"x": 0.8771331310272217, "y": 0.16131867468357086}, {"x": 0.8777019381523132, "y": 0.22197802364826202}, {"x": 0.1251422017812729, "y": 0.22549450397491455}], "text": "Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\n{scott.barnett, stefanus.kurniawan, srikanth.thudumu, zach.brannelly, mohamed.abdelrazek}@deakin.edu.au\nApplied Artificial Intelligence Institute\nGeelong, Australia\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.23736263811588287}, {"x": 0.18259385228157043, "y": 0.23692308366298676}, {"x": 0.18259385228157043, "y": 0.2465934008359909}, {"x": 0.08816837519407272, "y": 0.24703297019004822}], "text": "ABSTRACT\n"}
{"page": 1, "bbox": [{"x": 0.08646188676357269, "y": 0.25582417845726013}, {"x": 0.4829351603984833, "y": 0.25582417845726013}, {"x": 0.4829351603984833, "y": 0.515164852142334}, {"x": 0.08646188676357269, "y": 0.515164852142334}], "text": "Software engineers are increasingly adding semantic search capabil-\nities to applications using a strategy known as Retrieval Augmented\nGeneration (RAG). A RAG system involves finding documents that\nsemantically match a query and then passing the documents to a\nlarge language model (LLM) such as ChatGPT to extract the right\nanswer using an LLM. RAG systems aim to: a) reduce the problem\nof hallucinated responses from LLMs, b) link sources/references\nto generated responses, and c) remove the need for annotating\ndocuments with meta-data. However, RAG systems suffer from lim-\nitations inherent to information retrieval systems and from reliance\non LLMs. In this paper, we present an experience report on the\nfailure points of RAG systems from three case studies from separate\ndomains: research, education, and biomedical. We share the lessons\nlearned and present 7 failure points to consider when designing a\nRAG system. The two key takeaways arising from our work are: 1)\nvalidation of a RAG system is only feasible during operation, and\n2) the robustness of a RAG system evolves rather than designed in\nat the start. We conclude with a list of potential research directions\non RAG systems for the software engineering community.\n"}
{"page": 1, "bbox": [{"x": 0.027872582897543907, "y": 0.7054945230484009}, {"x": 0.029010238125920296, "y": 0.2769230902194977}, {"x": 0.058589305728673935, "y": 0.2769230902194977}, {"x": 0.057451650500297546, "y": 0.7054945230484009}], "text": "arXiv:2401.05856v1 [cs.SE] 11 Jan 2024\n"}
{"page": 1, "bbox": [{"x": 0.5182024836540222, "y": 0.2382417619228363}, {"x": 0.9152445793151855, "y": 0.2382417619228363}, {"x": 0.9152445793151855, "y": 0.8298901319503784}, {"x": 0.5182024836540222, "y": 0.8298901319503784}], "text": "build new HCI solutions, complete complex tasks, summarise docu-\nments, answer questions in a given artefact(s), and generate new\ncontent. However, LLMs suffer from limitations when it comes\nto up-to-date knowledge or domain-specific knowledge currently\ncaptured in company's repositories. Two options to address this\nproblem are: a) Finetuning LLMs (continue training an LLM using\ndomain specific artifacts) which requires managing or serving a\nfine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG)\nSystems that rely on LLMs for generation of answers using existing\n(extensible) knowledge artifacts. Both options have\nand cons\npros\nrelated to privacy/security of data, scalability, cost, skills required,\netc. In this paper, we focus on the RAG option.\nRetrieval-Augmented Generation (RAG) systems offer a com-\npelling solution to this challenge. By integrating retrieval mecha-\nnisms with the generative capabilities of LLMS, RAG systems can\nsynthesise contextually relevant, accurate, and up-to-date informa-\ntion. A Retrieval-Augmented Generation (RAG) system combines\ninformation retrieval capabilities, and generative prowess of LLMs.\nThe retrieval component focuses on retrieving relevant information\nfor a user query from a data store. The generation component fo-\ncuses on using the retrieved information as a context to generate an\nanswer for the user query. RAG systems are an important use case\nas all unstructured information can now be indexed and available\nto query reducing development time no knowledge graph creation\nand limited data curation and cleaning.\nSoftware engineers building RAG systems are expected to pre-\nprocess domain knowledge captured as artifacts in different formats,\nstore processed information in appropriate data store (vector data-\nbase), implement or integrate the right query-artifact matching\nstrategy, rank matched artifacts, and call the LLMS API passing in\nuser queries and context documents. New advances for building\nRAG systems are constantly emerging [8, 12] but how they relate\nand perform for a specific application context has to be discovered.\nIn this work we present the lessons learned and 7 failure points\narising from 3 case studies. The purpose of this paper is to provide\n1) a reference to practitioners and 2) to present a research road\nmap for RAG systems. To the best of our knowledge, we present\nthe first empirical insight into the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.5318681597709656}, {"x": 0.22013652324676514, "y": 0.5318681597709656}, {"x": 0.22013652324676514, "y": 0.5410988926887512}, {"x": 0.08816837519407272, "y": 0.5410988926887512}], "text": "CCS CONCEPTS\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.5542857050895691}, {"x": 0.09158134460449219, "y": 0.5542857050895691}, {"x": 0.09158134460449219, "y": 0.5564835071563721}, {"x": 0.08816837519407272, "y": 0.5564835071563721}], "text": "•\n"}
{"page": 1, "bbox": [{"x": 0.09613196551799774, "y": 0.5503296852111816}, {"x": 0.48350396752357483, "y": 0.5503296852111816}, {"x": 0.48350396752357483, "y": 0.5604395866394043}, {"x": 0.09613196551799774, "y": 0.5604395866394043}], "text": "Software and its engineering → Empirical software valida-\n"}
{"page": 1, "bbox": [{"x": 0.08759954571723938, "y": 0.5648351907730103}, {"x": 0.11774744093418121, "y": 0.5648351907730103}, {"x": 0.11774744093418121, "y": 0.5718681216239929}, {"x": 0.08759954571723938, "y": 0.5718681216239929}], "text": "tion.\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.5916483402252197}, {"x": 0.18885096907615662, "y": 0.5916483402252197}, {"x": 0.18885096907615662, "y": 0.6000000238418579}, {"x": 0.08816837519407272, "y": 0.6000000238418579}], "text": "KEYWORDS\n"}
{"page": 1, "bbox": [{"x": 0.08646188676357269, "y": 0.6101098656654358}, {"x": 0.4829351603984833, "y": 0.6101098656654358}, {"x": 0.4829351603984833, "y": 0.7028571367263794}, {"x": 0.08646188676357269, "y": 0.7028571367263794}], "text": "Retrieval Augmented Generation, RAG, SE4AI, Case Study\nACM Reference Format:\nScott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mo-\nhamed Abdelrazek. 2024. Seven Failure Points When Engineering a Retrieval\nAugmented Generation System. In Proceedings of 3rd International Confer-\nence on AI Engineering - Software Engineering for AI (CAIN 2024). ACM,\nNew York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n"}
{"page": 1, "bbox": [{"x": 0.11490330100059509, "y": 0.7230769395828247}, {"x": 0.2548350393772125, "y": 0.7230769395828247}, {"x": 0.2548350393772125, "y": 0.7323076725006104}, {"x": 0.11490330100059509, "y": 0.7323076725006104}], "text": "INTRODUCTION\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.7243956327438354}, {"x": 0.09670079499483109, "y": 0.7243956327438354}, {"x": 0.09670079499483109, "y": 0.7318681478500366}, {"x": 0.08816837519407272, "y": 0.7318681478500366}], "text": "1\n"}
{"page": 1, "bbox": [{"x": 0.08703071624040604, "y": 0.7415384650230408}, {"x": 0.48350396752357483, "y": 0.7415384650230408}, {"x": 0.48350396752357483, "y": 0.766153872013092}, {"x": 0.08703071624040604, "y": 0.766153872013092}], "text": "The new advancements of Large Language Models (LLMs), includ-\ning ChatGPT, have given software engineers new capabilities to\n"}
{"page": 1, "bbox": [{"x": 0.08703071624040604, "y": 0.7841758131980896}, {"x": 0.4817974865436554, "y": 0.7841758131980896}, {"x": 0.4817974865436554, "y": 0.8531867861747742}, {"x": 0.08703071624040604, "y": 0.8531867861747742}], "text": "Permission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\n"}
{"page": 1, "bbox": [{"x": 0.5455062389373779, "y": 0.84351646900177}, {"x": 0.9135380983352661, "y": 0.84351646900177}, {"x": 0.9135380983352661, "y": 0.8953846096992493}, {"x": 0.5455062389373779, "y": 0.8953846096992493}], "text": "• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperiment involved 15,000 documents and 1000 question\n"}
{"page": 1, "bbox": [{"x": 0.08703071624040604, "y": 0.8558241724967957}, {"x": 0.3100113868713379, "y": 0.8558241724967957}, {"x": 0.3100113868713379, "y": 0.8949450254440308}, {"x": 0.08703071624040604, "y": 0.8949450254440308}], "text": "CAIN 2024, April 2024, Lisbon, Portugal\n© 2024 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n"}
{"page": 2, "bbox": [{"x": 0.08816837519407272, "y": 0.07780219614505768}, {"x": 0.27303755283355713, "y": 0.07868131995201111}, {"x": 0.27303755283355713, "y": 0.08703296631574631}, {"x": 0.08816837519407272, "y": 0.08615384250879288}], "text": "CAIN 2024, April 2024, Lisbon, Portugal\n"}
{"page": 2, "bbox": [{"x": 0.46814560890197754, "y": 0.07868131995201111}, {"x": 0.9095563292503357, "y": 0.07868131995201111}, {"x": 0.9095563292503357, "y": 0.08615384250879288}, {"x": 0.46814560890197754, "y": 0.08615384250879288}], "text": "Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\n"}
{"page": 2, "bbox": [{"x": 0.5187713503837585, "y": 0.10901098698377609}, {"x": 0.914675772190094, "y": 0.10945054888725281}, {"x": 0.914675772190094, "y": 0.24527472257614136}, {"x": 0.5187713503837585, "y": 0.24483516812324524}], "text": "but is incorrect, and 2) unbounded no way to direct or update\nthe content of the output (other than through prompt engineering).\nA RAG system is an information retrieval approach designed to\novercome the limitations of using a LLM directly.\nRAG works by taking a natural language query is converted into\nan embedding which is used to semantically search a set of docu-\nments. Retrieved documents are then passed to a large language\nmodel to generate an answer. An overview of a RAG system is\nshown in Figure 1 as two separate processes, Index and Query. See\nthis\nsurvey for more details [19]\n"}
{"page": 2, "bbox": [{"x": 0.10409556329250336, "y": 0.10945054888725281}, {"x": 0.4829351603984833, "y": 0.10989011079072952}, {"x": 0.4829351603984833, "y": 0.3327472507953644}, {"x": 0.10409556329250336, "y": 0.33230769634246826}], "text": "and answer pairs. We indexed all documents then ran the\nqueries and stored the generated responses using GPT-4. All\nquestion and answer pairs were then validated with OpenAI\nevals 1. Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience report from 3 case studies of implementing a\nRAG system. Two currently running at Deakin University.\n• A research direction for RAG systems based on the lessons\nlearned from the 3 case studies.\n"}
{"page": 2, "bbox": [{"x": 0.5597269535064697, "y": 0.26461538672447205}, {"x": 0.670648455619812, "y": 0.26505494117736816}, {"x": 0.670648455619812, "y": 0.2738461494445801}, {"x": 0.5597269535064697, "y": 0.27340659499168396}], "text": "Index Process\n"}
{"page": 2, "bbox": [{"x": 0.5199089646339417, "y": 0.2659340798854828}, {"x": 0.5409556031227112, "y": 0.2659340798854828}, {"x": 0.5409556031227112, "y": 0.27340659499168396}, {"x": 0.5199089646339417, "y": 0.27340659499168396}], "text": "3.1\n"}
{"page": 2, "bbox": [{"x": 0.11490330100059509, "y": 0.35120880603790283}, {"x": 0.25654152035713196, "y": 0.35120880603790283}, {"x": 0.25654152035713196, "y": 0.35956043004989624}, {"x": 0.11490330100059509, "y": 0.35956043004989624}], "text": "RELATED WORK\n"}
{"page": 2, "bbox": [{"x": 0.08759954571723938, "y": 0.35208791494369507}, {"x": 0.095563143491745, "y": 0.35208791494369507}, {"x": 0.095563143491745, "y": 0.35956043004989624}, {"x": 0.08759954571723938, "y": 0.35956043004989624}], "text": "2\n"}
{"page": 2, "bbox": [{"x": 0.5176336765289307, "y": 0.2830769121646881}, {"x": 0.9141069650650024, "y": 0.2830769121646881}, {"x": 0.9141069650650024, "y": 0.542417585849762}, {"x": 0.5176336765289307, "y": 0.542417585849762}], "text": "In a RAG system, the retrieval system works using embeddings\nthat provide a compressed semantic representation of the docu-\nment. An embedding is expressed as a vector of numbers. During\nthe Index process each document is split into smaller chunks that\nare converted into an embedding using an embedding model. The\noriginal chunk and the embedding are then indexed in a database.\nSoftware engineers face design decisions around how best to chunk\nthe document and how large a chunk should be. If chunks are too\nsmall certain questions cannot be answered, if the chunks are too\nlong then the answers include generated noise.\nDifferent types of documents require different chunking and pro-\ncessing stages. For example, video content requires a transcription\npipeline to extract the audio and convert to text prior to encoding\n(see subsection 4.2. The choice of which embedding to use also\nmatters as changing the embedding strategy requires re-indexing\nall chunks. An embedding should be chosen based on the ability to\nsemantically retrieve correct responses. This process depends on\nthe size of the chunks, the types of questions expected, the structure\nof the content and the application domain.\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.3692307770252228}, {"x": 0.4829351603984833, "y": 0.3692307770252228}, {"x": 0.4829351603984833, "y": 0.7406593561172485}, {"x": 0.08703071624040604, "y": 0.7406593561172485}], "text": "Retrieval augmented generation encompasses using documents\nto augment large language models through pre-training and at\ninference time [7, 9, 12]. Due to the compute cost, data preparation\ntime and required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existing work by presenting challenges faced during the\nimplementation of a RAG system with a focus on practitioners.\nErrors and failures that arise from RAG systems overlap with\nother information retrieval systems including 1) no metrics for\nquery rewriting, 2) document re-ranking, and 3) effective content\nsummarisation [19]. Our results confirm this The unique aspects\nare related to the semantic and generative nature of the use of large\nlanguage models including evaluating factual accuracy [16].\n"}
{"page": 2, "bbox": [{"x": 0.5193401575088501, "y": 0.5626373887062073}, {"x": 0.6740614175796509, "y": 0.5626373887062073}, {"x": 0.6740614175796509, "y": 0.5740659236907959}, {"x": 0.5193401575088501, "y": 0.5740659236907959}], "text": "3.2 Query Process\n"}
{"page": 2, "bbox": [{"x": 0.5187713503837585, "y": 0.5797802209854126}, {"x": 0.9152445793151855, "y": 0.5802198052406311}, {"x": 0.914675772190094, "y": 0.89670330286026}, {"x": 0.5182024836540222, "y": 0.8962637186050415}], "text": "The Query process takes place at run time. A question expressed\nas natural language is first converted into a general query. Το gen-\neralise the query a large language model is used which enables\nadditional context such as previous chat history to be included\nin the new query. An embedding is then calculated from the new\nquery to use for locating relevant documents from the database.\nTop-k similar documents are retrieved using a similarity method\nsuch as cosine similarity (vector databases have techniques such as\ninverted indexes to speed up retrieval time). The intuition is that\nchunks that are semantically close to the query are likely to contain\nthe answer.\nRetrieved documents are then re-ranked to maximise the likeli-\nhood that the chunk with the answer is located near the top. The\nnext stage is the Consolidator which is responsible for processing\nthe chunks. This stage is needed to overcome the limitations of\nlarge language models 1) token limit and 2) rate limit. Services such\nas OpenAI have hard limits on the amount of text to include in a\nprompt. This restricts the number of chunks to include in a prompt\nto extract out an answer and a reduction strategy is needed to chain\nprompts to obtain an answer. These online services also restrict the\nnumber of tokens to use within a time frame restricting the latency\nof a system. Software engineers need to consider these tradeoffs\nwhen designing a RAG system.\n"}
{"page": 2, "bbox": [{"x": 0.08589305728673935, "y": 0.7560439705848694}, {"x": 0.4829351603984833, "y": 0.7564834952354431}, {"x": 0.4829351603984833, "y": 0.8408791422843933}, {"x": 0.08589305728673935, "y": 0.8404395580291748}], "text": "3 RETRIEVAL AUGMENTED GENERATION\nWith the explosion in popularity of large language model services\nsuch as ChatGPT², Claude³, and Bard 4, people have explored their\nuse as a question and answering systems. While the performance\nis impressive [16] there are two fundamental challenges: 1) hallu-\ncinations where the LLM produces a response that looks right\n"}
{"page": 2, "bbox": [{"x": 0.08646188676357269, "y": 0.8531867861747742}, {"x": 0.23947668075561523, "y": 0.8531867861747742}, {"x": 0.23947668075561523, "y": 0.8953846096992493}, {"x": 0.08646188676357269, "y": 0.8953846096992493}], "text": "https://github.com/openai/evals\n2https://chat.openai.com/\n3https://claude.ai/\nhttps://bard.google.com/\n"}
{"page": 3, "bbox": [{"x": 0.08759954571723938, "y": 0.07868131995201111}, {"x": 0.47497156262397766, "y": 0.07868131995201111}, {"x": 0.47497156262397766, "y": 0.08659340441226959}, {"x": 0.08759954571723938, "y": 0.08659340441226959}], "text": "Seven Failure Points When Engineering a Retrieval Augmented Generation System\n"}
{"page": 3, "bbox": [{"x": 0.7275313138961792, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.7275313138961792, "y": 0.08659340441226959}], "text": "CAIN 2024, April 2024, Lisbon, Portugal\n"}
{"page": 3, "bbox": [{"x": 0.5927190184593201, "y": 0.11032967269420624}, {"x": 0.61774742603302, "y": 0.11076922714710236}, {"x": 0.61774742603302, "y": 0.11824175715446472}, {"x": 0.5927190184593201, "y": 0.117802195250988}], "text": "Kev\n"}
{"page": 3, "bbox": [{"x": 0.10409556329250336, "y": 0.11999999731779099}, {"x": 0.18885096907615662, "y": 0.11999999731779099}, {"x": 0.18885096907615662, "y": 0.12703296542167664}, {"x": 0.10409556329250336, "y": 0.12703296542167664}], "text": "Index Process\n"}
{"page": 3, "bbox": [{"x": 0.48236632347106934, "y": 0.1261538416147232}, {"x": 0.5284414291381836, "y": 0.1261538416147232}, {"x": 0.5284414291381836, "y": 0.1454945057630539}, {"x": 0.48236632347106934, "y": 0.1454945057630539}], "text": "Missing\nContent\n"}
{"page": 3, "bbox": [{"x": 0.7815699577331543, "y": 0.13406594097614288}, {"x": 0.8344709873199463, "y": 0.13362637162208557}, {"x": 0.8344709873199463, "y": 0.14109890162944794}, {"x": 0.7815699577331543, "y": 0.14153845608234406}], "text": "Data flow\n"}
{"page": 3, "bbox": [{"x": 0.6456200480461121, "y": 0.13670329749584198}, {"x": 0.7195677161216736, "y": 0.13846154510974884}, {"x": 0.7189988493919373, "y": 0.14769230782985687}, {"x": 0.6450511813163757, "y": 0.14593406021595}], "text": "Failure point\n"}
{"page": 3, "bbox": [{"x": 0.7935153841972351, "y": 0.1569230705499649}, {"x": 0.8174061179161072, "y": 0.1569230705499649}, {"x": 0.8174061179161072, "y": 0.16351647675037384}, {"x": 0.7935153841972351, "y": 0.16351647675037384}], "text": "Text\n"}
{"page": 3, "bbox": [{"x": 0.20762230455875397, "y": 0.1564835160970688}, {"x": 0.25881683826446533, "y": 0.1569230705499649}, {"x": 0.25881683826446533, "y": 0.16439560055732727}, {"x": 0.20762230455875397, "y": 0.16395604610443115}], "text": "Chunker\n"}
{"page": 3, "bbox": [{"x": 0.6450511813163757, "y": 0.15780219435691833}, {"x": 0.7098976373672485, "y": 0.15780219435691833}, {"x": 0.7098976373672485, "y": 0.17890110611915588}, {"x": 0.6450511813163757, "y": 0.17890110611915588}], "text": "Processing\nstage\n"}
{"page": 3, "bbox": [{"x": 0.41808873414993286, "y": 0.16615384817123413}, {"x": 0.4738338887691498, "y": 0.16615384817123413}, {"x": 0.4738338887691498, "y": 0.17318680882453918}, {"x": 0.41808873414993286, "y": 0.17318680882453918}], "text": "Database\n"}
{"page": 3, "bbox": [{"x": 0.7935153841972351, "y": 0.16879120469093323}, {"x": 0.8668941855430603, "y": 0.16923077404499054}, {"x": 0.8668941855430603, "y": 0.17802198231220245}, {"x": 0.7935153841972351, "y": 0.17758241295814514}], "text": "intput/output\n"}
{"page": 3, "bbox": [{"x": 0.30830490589141846, "y": 0.17802198231220245}, {"x": 0.35324230790138245, "y": 0.17802198231220245}, {"x": 0.35324230790138245, "y": 0.1846153885126114}, {"x": 0.30830490589141846, "y": 0.1846153885126114}], "text": "Chunks\n"}
{"page": 3, "bbox": [{"x": 0.10693970322608948, "y": 0.17802198231220245}, {"x": 0.17463025450706482, "y": 0.17846153676509857}, {"x": 0.17463025450706482, "y": 0.18549451231956482}, {"x": 0.10693970322608948, "y": 0.1850549429655075}], "text": "Documents\n"}
{"page": 3, "bbox": [{"x": 0.829351544380188, "y": 0.2017582356929779}, {"x": 0.8907849788665771, "y": 0.2013186812400818}, {"x": 0.8907849788665771, "y": 0.22241757810115814}, {"x": 0.829351544380188, "y": 0.22285714745521545}], "text": "Incorrect\nSpecificity\n"}
{"page": 3, "bbox": [{"x": 0.10352673381567001, "y": 0.21934065222740173}, {"x": 0.19169510900974274, "y": 0.21802197396755219}, {"x": 0.19169510900974274, "y": 0.22637362778186798}, {"x": 0.10352673381567001, "y": 0.22769230604171753}], "text": "Query Process\n"}
{"page": 3, "bbox": [{"x": 0.806598424911499, "y": 0.2413186877965927}, {"x": 0.8663253784179688, "y": 0.24175824224948883}, {"x": 0.8663253784179688, "y": 0.25098901987075806}, {"x": 0.806598424911499, "y": 0.25054946541786194}], "text": "Response\n"}
{"page": 3, "bbox": [{"x": 0.14505119621753693, "y": 0.24703297019004822}, {"x": 0.18145619332790375, "y": 0.24703297019004822}, {"x": 0.18145619332790375, "y": 0.25582417845726013}, {"x": 0.14505119621753693, "y": 0.25582417845726013}], "text": "Query\n"}
{"page": 3, "bbox": [{"x": 0.27701935172080994, "y": 0.24175824224948883}, {"x": 0.3139931857585907, "y": 0.24219779670238495}, {"x": 0.3139931857585907, "y": 0.2628571391105652}, {"x": 0.27701935172080994, "y": 0.26241758465766907}], "text": "New\nQuery\n"}
{"page": 3, "bbox": [{"x": 0.5489192008972168, "y": 0.25098901987075806}, {"x": 0.5944254994392395, "y": 0.25054946541786194}, {"x": 0.5944254994392395, "y": 0.27076923847198486}, {"x": 0.5489192008972168, "y": 0.271208792924881}], "text": "Ranked\nChunks\n"}
{"page": 3, "bbox": [{"x": 0.6604095697402954, "y": 0.2514285743236542}, {"x": 0.7229806780815125, "y": 0.25098901987075806}, {"x": 0.7229806780815125, "y": 0.2716483473777771}, {"x": 0.6604095697402954, "y": 0.2720879018306732}], "text": "Processed\nChunks\n"}
{"page": 3, "bbox": [{"x": 0.41808873414993286, "y": 0.2641758322715759}, {"x": 0.46302616596221924, "y": 0.2641758322715759}, {"x": 0.46302616596221924, "y": 0.2716483473777771}, {"x": 0.41808873414993286, "y": 0.2716483473777771}], "text": "Chunks\n"}
{"page": 3, "bbox": [{"x": 0.796928346157074, "y": 0.2738461494445801}, {"x": 0.852104663848877, "y": 0.2738461494445801}, {"x": 0.852104663848877, "y": 0.29274725914001465}, {"x": 0.796928346157074, "y": 0.29274725914001465}], "text": "Not\nExtracted\n"}
{"page": 3, "bbox": [{"x": 0.1990898698568344, "y": 0.28747251629829407}, {"x": 0.24744027853012085, "y": 0.28747251629829407}, {"x": 0.24744027853012085, "y": 0.2945055067539215}, {"x": 0.1990898698568344, "y": 0.2945055067539215}], "text": "Rewriter\n"}
{"page": 3, "bbox": [{"x": 0.5944254994392395, "y": 0.28747251629829407}, {"x": 0.6700796484947205, "y": 0.28747251629829407}, {"x": 0.6700796484947205, "y": 0.2945055067539215}, {"x": 0.5944254994392395, "y": 0.2945055067539215}], "text": "Consolidator\n"}
{"page": 3, "bbox": [{"x": 0.7309442758560181, "y": 0.2879121005535126}, {"x": 0.7747440338134766, "y": 0.2879121005535126}, {"x": 0.7747440338134766, "y": 0.2949450612068176}, {"x": 0.7309442758560181, "y": 0.2949450612068176}], "text": "Reader\n"}
{"page": 3, "bbox": [{"x": 0.34698522090911865, "y": 0.2879121005535126}, {"x": 0.40102389454841614, "y": 0.2883516550064087}, {"x": 0.40102389454841614, "y": 0.296263724565506}, {"x": 0.34698522090911865, "y": 0.29582417011260986}], "text": "Retriever\n"}
{"page": 3, "bbox": [{"x": 0.47838452458381653, "y": 0.2883516550064087}, {"x": 0.5335608720779419, "y": 0.2883516550064087}, {"x": 0.5335608720779419, "y": 0.29582417011260986}, {"x": 0.47838452458381653, "y": 0.29582417011260986}], "text": "Reranker\n"}
{"page": 3, "bbox": [{"x": 0.4067121744155884, "y": 0.30417582392692566}, {"x": 0.4744027256965637, "y": 0.3046153783798218}, {"x": 0.4738338887691498, "y": 0.3252747356891632}, {"x": 0.40614333748817444, "y": 0.3248351514339447}], "text": "Missed Top\nRanked\n"}
{"page": 3, "bbox": [{"x": 0.5967007875442505, "y": 0.3054945170879364}, {"x": 0.6433447003364563, "y": 0.3059340715408325}, {"x": 0.6433447003364563, "y": 0.3252747356891632}, {"x": 0.5967007875442505, "y": 0.3248351514339447}], "text": "Not in\nContext\n"}
{"page": 3, "bbox": [{"x": 0.7997724413871765, "y": 0.31252747774124146}, {"x": 0.8646188974380493, "y": 0.31252747774124146}, {"x": 0.8646188974380493, "y": 0.32131868600845337}, {"x": 0.7997724413871765, "y": 0.32131868600845337}], "text": "Incomplete\n"}
{"page": 3, "bbox": [{"x": 0.7195677161216736, "y": 0.3081318736076355}, {"x": 0.7616609930992126, "y": 0.3085714280605316}, {"x": 0.7610921263694763, "y": 0.3279120922088623}, {"x": 0.7189988493919373, "y": 0.3274725377559662}], "text": "Wrong\nFormat\n"}
{"page": 3, "bbox": [{"x": 0.08759954571723938, "y": 0.356483519077301}, {"x": 0.9129692912101746, "y": 0.356483519077301}, {"x": 0.9129692912101746, "y": 0.3947252631187439}, {"x": 0.08759954571723938, "y": 0.3947252631187439}], "text": "Figure 1: Indexing and Query processes required for creating a Retrieval Augmented Generation (RAG) system. The indexing\nprocess is typically done at development time and queries at runtime. Failure points identified in this study are shown in red\nboxes. All required stages are underlined. Figure expanded from [19].\n"}
{"page": 3, "bbox": [{"x": 0.5193401575088501, "y": 0.4197802245616913}, {"x": 0.9118316173553467, "y": 0.4197802245616913}, {"x": 0.9118316173553467, "y": 0.45846155285835266}, {"x": 0.5193401575088501, "y": 0.45846155285835266}], "text": "on a robust data processing pipeline to handle uploaded documents\ni.e. no quality control possible at development time. This system\nalso uses a ranking algorithm to sort the uploaded documents.\n"}
{"page": 3, "bbox": [{"x": 0.5597269535064697, "y": 0.47472527623176575}, {"x": 0.6296928524971008, "y": 0.47516483068466187}, {"x": 0.6296928524971008, "y": 0.4839560389518738}, {"x": 0.5597269535064697, "y": 0.48351648449897766}], "text": "AI Tutor\n"}
{"page": 3, "bbox": [{"x": 0.5187713503837585, "y": 0.4760439693927765}, {"x": 0.5415244698524475, "y": 0.4760439693927765}, {"x": 0.5415244698524475, "y": 0.48351648449897766}, {"x": 0.5187713503837585, "y": 0.48351648449897766}], "text": "4.2\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.4197802245616913}, {"x": 0.48350396752357483, "y": 0.41934067010879517}, {"x": 0.48350396752357483, "y": 0.5951648354530334}, {"x": 0.08703071624040604, "y": 0.595604419708252}], "text": "The final stage of a RAG pipeline is when the answer is extracted\nfrom the generated text. Readers are responsible for filtering the\nnoise from the prompt, adhering to formatting instructions (i.e. an-\nswer the question as a list of options), and producing the output to\nreturn for the query. Implementation of a RAG system requires cus-\ntomising multiple prompts to process questions and answers. This\nprocess ensures that questions relevant for the domain are returned.\nThe use of large language models to answer real time questions\nfrom documents opens up new application domains where question\nand answering is new capability. Thus, RAG systems are difficult\nto test as no data exists and needs to be experimentally discov-\nered through either a) synthetic data generation, or b) piloting the\nsystem with minimal testing.\n"}
{"page": 3, "bbox": [{"x": 0.5176336765289307, "y": 0.4931868016719818}, {"x": 0.914675772190094, "y": 0.4931868016719818}, {"x": 0.914675772190094, "y": 0.7252747416496277}, {"x": 0.5176336765289307, "y": 0.7252747416496277}], "text": "The AI Tutor is a RAG system where students ask questions about\nthe unit and answers are sourced from the learning content. Stu-\ndents are able to verify the answers by accessing a sources list from\nwhere the answer came from. The AI Tutor works by integrating\ninto Deakin's learning management system, indexing all of the\ncontent including PDF documents, videos, and text documents. As\npart of the Index process, videos are transcribed using the deep\nlearning model Whisper [17] before being chunked. The AI Tutor\nwas developed between August 2023 to November 2023 for a pilot\nin a unit with 200 students that commenced the 30th of October\n2023. Our intention is to present the lessons learned during imple-\nmentation and present a followup findings at the conclusion of the\npilot. This RAG pipeline includes a rewriter to generalise queries.\nWe implemented a chat interface where previous dialogue between\nthe user and the AI Tutor was used as part of the context for each\nquestion. The rewriter considers this context and rewrites the query\nto resolve ambiguous requests such as ‘Explain this concept further.\n"}
{"page": 3, "bbox": [{"x": 0.08589305728673935, "y": 0.6127472519874573}, {"x": 0.23833902180194855, "y": 0.6127472519874573}, {"x": 0.23833902180194855, "y": 0.6219780445098877}, {"x": 0.08589305728673935, "y": 0.6219780445098877}], "text": "4 CASE STUDIES\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.6316483616828918}, {"x": 0.4817974865436554, "y": 0.6316483616828918}, {"x": 0.4817974865436554, "y": 0.7107692360877991}, {"x": 0.08703071624040604, "y": 0.7107692360877991}], "text": "This study conducted three case studies to discover the challenges\nthat arise when implementing RAG systems. A summary of each of\nthe case studies is shown in Table 1. All scripts, data, and examples\nof each of the failure points for the BioASQ case study are available\nonline 5. The other two case studies have been excluded due to\nconfidentiality concerns.\n"}
{"page": 3, "bbox": [{"x": 0.08589305728673935, "y": 0.7274725437164307}, {"x": 0.48236632347106934, "y": 0.7265934348106384}, {"x": 0.4829351603984833, "y": 0.867252767086029}, {"x": 0.08646188676357269, "y": 0.8681318759918213}], "text": "4.1 Cognitive Reviewer\nCognitive Reviewer is a RAG system designed to support researchers\nin analysing scientific documents. Researchers specify a research\nquestion or objective and then upload a collection of related re-\nsearch papers. All of the documents are then ranked in accordance\nwith the stated objective for the researcher to manually review.\nThe researcher can also ask questions directly against all of the\ndocuments. Cognitive Reviewer is currently used by PhD students\nfrom Deakin University to support their literature reviews. The\nCognitive Reviewer does the Index process at run time and relies\n"}
{"page": 3, "bbox": [{"x": 0.5182024836540222, "y": 0.7415384650230408}, {"x": 0.9152445793151855, "y": 0.7415384650230408}, {"x": 0.9152445793151855, "y": 0.8962637186050415}, {"x": 0.5182024836540222, "y": 0.8962637186050415}], "text": "4.3 Biomedical Question and Answer\nThe previous case studies focused on documents with smaller con-\ntent sizes. To explore the issues at a larger scale we created a RAG\nsystem using the BioASQ [10] dataset comprised of questions, links\nto document, and answers. The answers to questions were one of\nyes/no, text summarisation, factoid, or list. This dataset was pre-\npared by biomedical experts and contains domain specific question\nand answer pairs. We downloaded 4017 open access documents\nfrom the BioASQ dataset and had a total of 1000 questions. All\ndocuments were indexed and the questions asked against the RAG\nsystem. The generated questions were then evaluated using the\n"}
{"page": 3, "bbox": [{"x": 0.09271899610757828, "y": 0.8861538171768188}, {"x": 0.2969283163547516, "y": 0.8848351836204529}, {"x": 0.2969283163547516, "y": 0.8940659165382385}, {"x": 0.09271899610757828, "y": 0.8953846096992493}], "text": "https://figshare.com/s/fbf7805b5f20d7f7e356\n"}
{"page": 4, "bbox": [{"x": 0.08816837519407272, "y": 0.07780219614505768}, {"x": 0.27303755283355713, "y": 0.07868131995201111}, {"x": 0.27303755283355713, "y": 0.08703296631574631}, {"x": 0.08816837519407272, "y": 0.08615384250879288}], "text": "CAIN 2024, April 2024, Lisbon, Portugal\n"}
{"page": 4, "bbox": [{"x": 0.467576801776886, "y": 0.07868131995201111}, {"x": 0.9101251363754272, "y": 0.07868131995201111}, {"x": 0.9101251363754272, "y": 0.08615384250879288}, {"x": 0.467576801776886, "y": 0.08615384250879288}], "text": "Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\n"}
{"page": 4, "bbox": [{"x": 0.12343572080135345, "y": 0.10769230872392654}, {"x": 0.1962457299232483, "y": 0.10901098698377609}, {"x": 0.1962457299232483, "y": 0.117802195250988}, {"x": 0.12343572080135345, "y": 0.11648351699113846}], "text": "Case Study\n"}
{"page": 4, "bbox": [{"x": 0.5159271955490112, "y": 0.10769230872392654}, {"x": 0.5921501517295837, "y": 0.10945054888725281}, {"x": 0.5915813446044922, "y": 0.117802195250988}, {"x": 0.5153583884239197, "y": 0.11604395508766174}], "text": "RAG Stages\n"}
{"page": 4, "bbox": [{"x": 0.670648455619812, "y": 0.10857142508029938}, {"x": 0.7889647483825684, "y": 0.10857142508029938}, {"x": 0.7889647483825684, "y": 0.11824175715446472}, {"x": 0.670648455619812, "y": 0.11824175715446472}], "text": "Sample Questions\n"}
{"page": 4, "bbox": [{"x": 0.2286689430475235, "y": 0.10857142508029938}, {"x": 0.2827076315879822, "y": 0.10857142508029938}, {"x": 0.2827076315879822, "y": 0.13010989129543304}, {"x": 0.2286689430475235, "y": 0.13010989129543304}], "text": "Domain\nResearch\n"}
{"page": 4, "bbox": [{"x": 0.3117178678512573, "y": 0.10813187062740326}, {"x": 0.38054606318473816, "y": 0.10857142508029938}, {"x": 0.38054606318473816, "y": 0.13054944574832916}, {"x": 0.3117178678512573, "y": 0.13010989129543304}], "text": "Doc Types\nPDFs\n"}
{"page": 4, "bbox": [{"x": 0.4209328889846802, "y": 0.10857142508029938}, {"x": 0.49829351902008057, "y": 0.10813187062740326}, {"x": 0.4988623559474945, "y": 0.1318681389093399}, {"x": 0.42150169610977173, "y": 0.13230769336223602}], "text": "Dataset Size\n(Any size)\n"}
{"page": 4, "bbox": [{"x": 0.5159271955490112, "y": 0.12175824493169785}, {"x": 0.6569966077804565, "y": 0.12219779938459396}, {"x": 0.6569966077804565, "y": 0.13098901510238647}, {"x": 0.5159271955490112, "y": 0.13054944574832916}], "text": "Chunker, Rewriter, Re-\n"}
{"page": 4, "bbox": [{"x": 0.12002275139093399, "y": 0.12219779938459396}, {"x": 0.17747440934181213, "y": 0.12175824493169785}, {"x": 0.17747440934181213, "y": 0.1318681389093399}, {"x": 0.12002275139093399, "y": 0.13230769336223602}], "text": "Cognitive\n"}
{"page": 4, "bbox": [{"x": 0.6695107817649841, "y": 0.11824175715446472}, {"x": 0.8799772262573242, "y": 0.12263736128807068}, {"x": 0.8788396120071411, "y": 0.1512087881565094}, {"x": 0.668373167514801, "y": 0.14681318402290344}], "text": "What are the key points covered in\nthis paper?\n"}
{"page": 4, "bbox": [{"x": 0.11945392191410065, "y": 0.13626374304294586}, {"x": 0.18031854927539825, "y": 0.13538461923599243}, {"x": 0.18031854927539825, "y": 0.14373625814914703}, {"x": 0.11945392191410065, "y": 0.14461538195610046}], "text": "Reviewer*\n"}
{"page": 4, "bbox": [{"x": 0.5159271955490112, "y": 0.13670329749584198}, {"x": 0.6023890972137451, "y": 0.13538461923599243}, {"x": 0.6023890972137451, "y": 0.14417582750320435}, {"x": 0.5159271955490112, "y": 0.1454945057630539}], "text": "triever, Reader\n"}
{"page": 4, "bbox": [{"x": 0.11945392191410065, "y": 0.15032966434955597}, {"x": 0.17519909143447876, "y": 0.14989010989665985}, {"x": 0.17519909143447876, "y": 0.15780219435691833}, {"x": 0.11945392191410065, "y": 0.15824176371097565}], "text": "AI Tutor*\n"}
{"page": 4, "bbox": [{"x": 0.22923776507377625, "y": 0.15032966434955597}, {"x": 0.2883959114551544, "y": 0.15076923370361328}, {"x": 0.2883959114551544, "y": 0.15868131816387177}, {"x": 0.22923776507377625, "y": 0.15824176371097565}], "text": "Education\n"}
{"page": 4, "bbox": [{"x": 0.4533560872077942, "y": 0.1516483575105667}, {"x": 0.46587032079696655, "y": 0.1516483575105667}, {"x": 0.46587032079696655, "y": 0.15736263990402222}, {"x": 0.4533560872077942, "y": 0.15736263990402222}], "text": "38\n"}
{"page": 4, "bbox": [{"x": 0.3111490309238434, "y": 0.14989010989665985}, {"x": 0.40614333748817444, "y": 0.15076923370361328}, {"x": 0.4055745303630829, "y": 0.17318680882453918}, {"x": 0.31058019399642944, "y": 0.17230768501758575}], "text": "Videos, HTML,\nPDF\n"}
{"page": 4, "bbox": [{"x": 0.6700796484947205, "y": 0.1485714316368103}, {"x": 0.8805460929870605, "y": 0.14901098608970642}, {"x": 0.8805460929870605, "y": 0.20087912678718567}, {"x": 0.6700796484947205, "y": 0.20043955743312836}], "text": "What were the topics covered in\nweek 6?\nDefine pseudotumor cerebri. How\nis it treated?\n"}
{"page": 4, "bbox": [{"x": 0.5159271955490112, "y": 0.14989010989665985}, {"x": 0.6564277410507202, "y": 0.14989010989665985}, {"x": 0.6564277410507202, "y": 0.20043955743312836}, {"x": 0.5159271955490112, "y": 0.20043955743312836}], "text": "Chunker, Rewriter,\nRetriever, Reader\nChunker, Retriever,\nReader\n"}
{"page": 4, "bbox": [{"x": 0.22923776507377625, "y": 0.17802198231220245}, {"x": 0.2957906723022461, "y": 0.17846153676509857}, {"x": 0.2957906723022461, "y": 0.18637362122535706}, {"x": 0.22923776507377625, "y": 0.18593406677246094}], "text": "Biomedical\n"}
{"page": 4, "bbox": [{"x": 0.3117178678512573, "y": 0.17846153676509857}, {"x": 0.40216153860092163, "y": 0.17890110611915588}, {"x": 0.40216153860092163, "y": 0.18681319057941437}, {"x": 0.3117178678512573, "y": 0.18637362122535706}], "text": "Scientific PDFS\n"}
{"page": 4, "bbox": [{"x": 0.44653013348579407, "y": 0.17978021502494812}, {"x": 0.4726962447166443, "y": 0.17978021502494812}, {"x": 0.4726962447166443, "y": 0.18593406677246094}, {"x": 0.44653013348579407, "y": 0.18593406677246094}], "text": "4017\n"}
{"page": 4, "bbox": [{"x": 0.12002275139093399, "y": 0.17846153676509857}, {"x": 0.16723549365997314, "y": 0.17890110611915588}, {"x": 0.16723549365997314, "y": 0.18813186883926392}, {"x": 0.12002275139093399, "y": 0.1876923143863678}], "text": "BioASQ\n"}
{"page": 4, "bbox": [{"x": 0.08816837519407272, "y": 0.190769225358963}, {"x": 0.9129692912101746, "y": 0.20791208744049072}, {"x": 0.9118316173553467, "y": 0.24747252464294434}, {"x": 0.08703071624040604, "y": 0.23076923191547394}], "text": "Table 1: A summary of the RAG case studies presented in this paper. Case studies marked with a are running systems currently\nin use.\n"}
{"page": 4, "bbox": [{"x": 0.5284414291381836, "y": 0.24703297019004822}, {"x": 0.9129692912101746, "y": 0.24747252464294434}, {"x": 0.9129692912101746, "y": 0.3274725377559662}, {"x": 0.5284414291381836, "y": 0.3270329535007477}], "text": "FP7 Incomplete Incomplete answers are not incorrect but miss\nsome of the information even though that information was in\nthe context and available for extraction. An example question\nsuch as \"What are the key points covered in documents\nA, B and C?\" A better approach is to ask these questions\nseparately.\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.2461538463830948}, {"x": 0.48350396752357483, "y": 0.2461538463830948}, {"x": 0.48350396752357483, "y": 0.34241756796836853}, {"x": 0.08759954571723938, "y": 0.34241756796836853}], "text": "OpenEvals technique implemented by OpenAIб. From the gener-\nated questions we manually inspected 40 issues and all issues that\nthe OpenEvals flagged as inaccurate. We found that the automated\nevaluation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n"}
{"page": 4, "bbox": [{"x": 0.5199089646339417, "y": 0.3446153700351715}, {"x": 0.5290102362632751, "y": 0.3446153700351715}, {"x": 0.5290102362632751, "y": 0.3525274693965912}, {"x": 0.5199089646339417, "y": 0.3525274693965912}], "text": "6\n"}
{"page": 4, "bbox": [{"x": 0.08816837519407272, "y": 0.35736262798309326}, {"x": 0.09613196551799774, "y": 0.35736262798309326}, {"x": 0.09613196551799774, "y": 0.36571428179740906}, {"x": 0.08816837519407272, "y": 0.36571428179740906}], "text": "5\n"}
{"page": 4, "bbox": [{"x": 0.5176336765289307, "y": 0.3437362611293793}, {"x": 0.9135380983352661, "y": 0.34329670667648315}, {"x": 0.9135380983352661, "y": 0.4426373541355133}, {"x": 0.5176336765289307, "y": 0.4430769085884094}], "text": "LESSONS AND FUTURE RESEARCH\nDIRECTIONS\nThe lessons learned from the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n"}
{"page": 4, "bbox": [{"x": 0.5193401575088501, "y": 0.46065935492515564}, {"x": 0.9152445793151855, "y": 0.46065935492515564}, {"x": 0.9152445793151855, "y": 0.7529670596122742}, {"x": 0.5193401575088501, "y": 0.7529670596122742}], "text": "6.1 Chunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\non the embeddings of the chunk then affects the similarity and\nmatching of chunks to user queries. There are two ways of chunk-\ning: heuristics based (using punctuation, end of paragraph, etc.),\nand semantic chunking (using the semantics in the text to inform\nstart-end of a chunk). Further research should explore the tradeoffs\nbetween these methods and their effects on critical downstream\nprocesses like embedding and similarity matching. A systematic\nevaluation framework comparing chunking techniques on metrics\nlike\nquery relevance and retrieval accuracy would benefit the field.\nEmbeddings represent another active research area, including\ngenerating embeddings for multimedia and multimodal chunks\nsuch as tables, figures, formulas, etc. Chunk embeddings are typ-\nically created once during system development or when a new\ndocument is indexed. Query preprocessing significantly impacts\na RAG system's performance, particularly handling negative or\nambiguous queries. Further research is needed on architectural pat-\nterns and approaches [5] to address the inherent limitations with\nembeddings (quality of a match is domain specific).\n"}
{"page": 4, "bbox": [{"x": 0.08646188676357269, "y": 0.3560439646244049}, {"x": 0.48407280445098877, "y": 0.356483519077301}, {"x": 0.48350396752357483, "y": 0.872967004776001}, {"x": 0.08589305728673935, "y": 0.8725274801254272}], "text": "FAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing Content The first fail case is when asking a ques-\ntion that cannot be answered from the available documents.\nIn the happy case the RAG system will respond with some-\nthing like \"Sorry, I don't know\". However, for questions that\nare related to the content but don't have answers the system\ncould be fooled into giving a response.\nFP2 Missed the Top Ranked Documents The answer to the\nquestion is in the document but did not rank highly enough\nto be returned to the user. In theory, all documents are ranked\nand used in the next steps. However, in practice the top K\ndocuments are returned where K is a value selected based\non performance.\nFP3 Not in Context - Consolidation strategy Limitations\nDocuments with the answer were retrieved from the data-\nbase but did not make it into the context for generating an\nanswer. This occurs when many documents are returned\nfrom the database and a consolidation process takes place to\nretrieve the answer.\nFP4 Not Extracted Here the answer is present in the context,\nbut the large language model failed to extract out the correct\nanswer. Typically, this occurs when there is too much noise\nor contradicting information in the context.\nFP5 Wrong Format The question involved extracting informa-\ntion in a certain format such as a table or list and the large\nlanguage model ignored the instruction.\nFP6 Incorrect Specificity The answer is returned in the re-\nsponse but is not specific enough or is too specific to address\nthe user's need. This occurs when the RAG system designers\nhave a desired outcome for a given question such as teach-\ners for students. In this case, specific educational content\nshould be provided with answers not just the answer. Incor-\nrect specificity also occurs when users are not sure how to\nask a question and are too general.\n"}
{"page": 4, "bbox": [{"x": 0.5193401575088501, "y": 0.768351674079895}, {"x": 0.7144482135772705, "y": 0.769670307636261}, {"x": 0.7144482135772705, "y": 0.7810989022254944}, {"x": 0.5193401575088501, "y": 0.7797802090644836}], "text": "6.2 RAG vs Finetuning\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.7881318926811218}, {"x": 0.9135380983352661, "y": 0.7881318926811218}, {"x": 0.9135380983352661, "y": 0.8953846096992493}, {"x": 0.5187713503837585, "y": 0.8953846096992493}], "text": "LLMs are great world models due to the amount of training data, and\nfinetuning tasks applied on the model before it's released. However,\nthese models are general-purpose models (may not know the very\nspecifics of your domain) and also not up to date (there is a cutoff\ndate on their knowledge). Fine-tuning and RAG offer two potential\ncustomisation pathways, each with distinct tradeoffs. Finetuning\nrequires curating internal datasets to adapt and train the LLM on.\nHowever, all your data are baked into the model and you need to\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.8865934014320374}, {"x": 0.24061433970928192, "y": 0.8865934014320374}, {"x": 0.24061433970928192, "y": 0.8949450254440308}, {"x": 0.08759954571723938, "y": 0.8949450254440308}], "text": "\"https://github.com/openai/evals\n"}
{"page": 5, "bbox": [{"x": 0.08759954571723938, "y": 0.07912088185548782}, {"x": 0.4755403995513916, "y": 0.07912088185548782}, {"x": 0.4755403995513916, "y": 0.08659340441226959}, {"x": 0.08759954571723938, "y": 0.08659340441226959}], "text": "Seven Failure Points When Engineering a Retrieval Augmented Generation System\n"}
{"page": 5, "bbox": [{"x": 0.7275313138961792, "y": 0.07912088185548782}, {"x": 0.912400484085083, "y": 0.07912088185548782}, {"x": 0.912400484085083, "y": 0.08659340441226959}, {"x": 0.7275313138961792, "y": 0.08659340441226959}], "text": "CAIN 2024, April 2024, Lisbon, Portugal\n"}
{"page": 5, "bbox": [{"x": 0.17463025450706482, "y": 0.10901098698377609}, {"x": 0.2195676863193512, "y": 0.10901098698377609}, {"x": 0.2195676863193512, "y": 0.11604395508766174}, {"x": 0.17463025450706482, "y": 0.11604395508766174}], "text": "Lesson\n"}
{"page": 5, "bbox": [{"x": 0.4920364022254944, "y": 0.10857142508029938}, {"x": 0.5688282251358032, "y": 0.10857142508029938}, {"x": 0.5688282251358032, "y": 0.11824175715446472}, {"x": 0.4920364022254944, "y": 0.11824175715446472}], "text": "Description\n"}
{"page": 5, "bbox": [{"x": 0.8083049058914185, "y": 0.10857142508029938}, {"x": 0.8902161717414856, "y": 0.10857142508029938}, {"x": 0.8902161717414856, "y": 0.13010989129543304}, {"x": 0.8083049058914185, "y": 0.13010989129543304}], "text": "Case Studies\nAI Tutor\n"}
{"page": 5, "bbox": [{"x": 0.11604095250368118, "y": 0.10901098698377609}, {"x": 0.13765642046928406, "y": 0.10901098698377609}, {"x": 0.13765642046928406, "y": 0.12967033684253693}, {"x": 0.11604095250368118, "y": 0.12967033684253693}], "text": "FP\nFP4\n"}
{"page": 5, "bbox": [{"x": 0.4908987581729889, "y": 0.12175824493169785}, {"x": 0.7935153841972351, "y": 0.12263736128807068}, {"x": 0.7935153841972351, "y": 0.14769230782985687}, {"x": 0.4908987581729889, "y": 0.14681318402290344}], "text": "A larger context enabled more accurate responses\n(8K vs 4K). Contrary to prior work with GPT-3.5 [13]\n"}
{"page": 5, "bbox": [{"x": 0.17406143248081207, "y": 0.12219779938459396}, {"x": 0.4766780436038971, "y": 0.12219779938459396}, {"x": 0.4766780436038971, "y": 0.17406593263149261}, {"x": 0.17406143248081207, "y": 0.17406593263149261}], "text": "Larger context get better results (Context refers to a\nparticular setting or situation in which the content\noccurs)\nSemantic caching drives cost and latency down\n"}
{"page": 5, "bbox": [{"x": 0.8083049058914185, "y": 0.16439560055732727}, {"x": 0.8600682616233826, "y": 0.16395604610443115}, {"x": 0.8600682616233826, "y": 0.17142857611179352}, {"x": 0.8083049058914185, "y": 0.17186813056468964}], "text": "AI Tutor\n"}
{"page": 5, "bbox": [{"x": 0.11604095250368118, "y": 0.16483516991138458}, {"x": 0.1370875984430313, "y": 0.16483516991138458}, {"x": 0.1370875984430313, "y": 0.17142857611179352}, {"x": 0.11604095250368118, "y": 0.17142857611179352}], "text": "FP1\n"}
{"page": 5, "bbox": [{"x": 0.49146756529808044, "y": 0.16307692229747772}, {"x": 0.7940841913223267, "y": 0.16395604610443115}, {"x": 0.7940841913223267, "y": 0.20263735949993134}, {"x": 0.49146756529808044, "y": 0.2017582356929779}], "text": "RAG systems struggle with concurrent users due to\nrate limits and the cost of LLMs. Prepopulate the\nsemantic cache with frequently asked questions [1].\n"}
{"page": 5, "bbox": [{"x": 0.11092150211334229, "y": 0.20483516156673431}, {"x": 0.1433447152376175, "y": 0.20571428537368774}, {"x": 0.14277587831020355, "y": 0.21362636983394623}, {"x": 0.11035267263650894, "y": 0.2127472460269928}], "text": "FP5-7\n"}
{"page": 5, "bbox": [{"x": 0.17235495150089264, "y": 0.20351648330688477}, {"x": 0.8612059354782104, "y": 0.20527473092079163}, {"x": 0.8612059354782104, "y": 0.23120878636837006}, {"x": 0.17235495150089264, "y": 0.2294505536556244}], "text": "Jailbreaks bypass the RAG system and hit the safety Research suggests fine-tuning LLMs reverses safety AI Tutor\ntraining.\n"}
{"page": 5, "bbox": [{"x": 0.49146756529808044, "y": 0.21934065222740173}, {"x": 0.7952218651771545, "y": 0.21934065222740173}, {"x": 0.7952218651771545, "y": 0.2294505536556244}, {"x": 0.49146756529808044, "y": 0.2294505536556244}], "text": "training [11], test all fine-tuned LLMs for RAG sys-\n"}
{"page": 5, "bbox": [{"x": 0.49146756529808044, "y": 0.23428571224212646}, {"x": 0.5164960026741028, "y": 0.23384615778923035}, {"x": 0.5164960026741028, "y": 0.24043956398963928}, {"x": 0.49146756529808044, "y": 0.2408791184425354}], "text": "tem.\n"}
{"page": 5, "bbox": [{"x": 0.10125142335891724, "y": 0.24703297019004822}, {"x": 0.1524459570646286, "y": 0.2465934008359909}, {"x": 0.1524459570646286, "y": 0.255384624004364}, {"x": 0.10125142335891724, "y": 0.25582417845726013}], "text": "FP2, FP4\n"}
{"page": 5, "bbox": [{"x": 0.8083049058914185, "y": 0.24791209399700165}, {"x": 0.8606370687484741, "y": 0.24791209399700165}, {"x": 0.8606370687484741, "y": 0.2549450695514679}, {"x": 0.8083049058914185, "y": 0.2549450695514679}], "text": "AI Tutor\n"}
{"page": 5, "bbox": [{"x": 0.17349261045455933, "y": 0.24703297019004822}, {"x": 0.3964732587337494, "y": 0.2465934008359909}, {"x": 0.3964732587337494, "y": 0.25670328736305237}, {"x": 0.17349261045455933, "y": 0.2571428716182709}], "text": "Adding meta-data improves retrieval.\n"}
{"page": 5, "bbox": [{"x": 0.4908987581729889, "y": 0.2461538463830948}, {"x": 0.7952218651771545, "y": 0.24571429193019867}, {"x": 0.7952218651771545, "y": 0.28439560532569885}, {"x": 0.4908987581729889, "y": 0.28483515977859497}], "text": "Adding the file name and chunk number into the\nretrieved context helped the reader extract the re-\nquired information. Useful for chat dialogue.\n"}
{"page": 5, "bbox": [{"x": 0.09613196551799774, "y": 0.2879121005535126}, {"x": 0.914675772190094, "y": 0.2879121005535126}, {"x": 0.914675772190094, "y": 0.3103296756744385}, {"x": 0.09613196551799774, "y": 0.3103296756744385}], "text": "FP2, FP4-7 Open source embedding models perform better for Opensource sentence embedding models performed BioASQ, AI Tutor\nsmall text.\n"}
{"page": 5, "bbox": [{"x": 0.8083049058914185, "y": 0.3142857253551483}, {"x": 0.9141069650650024, "y": 0.31560438871383667}, {"x": 0.9141069650650024, "y": 0.3257142901420593}, {"x": 0.8083049058914185, "y": 0.3243955969810486}], "text": "AI Tutor, BioASQ\n"}
{"page": 5, "bbox": [{"x": 0.11035267263650894, "y": 0.3164835274219513}, {"x": 0.1433447152376175, "y": 0.3169230818748474}, {"x": 0.1433447152376175, "y": 0.3243955969810486}, {"x": 0.11035267263650894, "y": 0.32395604252815247}], "text": "FP2-7\n"}
{"page": 5, "bbox": [{"x": 0.17406143248081207, "y": 0.3160439431667328}, {"x": 0.4419795274734497, "y": 0.31516483426094055}, {"x": 0.4419795274734497, "y": 0.3252747356891632}, {"x": 0.17406143248081207, "y": 0.32615384459495544}], "text": "RAG systems require continuous calibration.\n"}
{"page": 5, "bbox": [{"x": 0.10125142335891724, "y": 0.3441758155822754}, {"x": 0.1524459570646286, "y": 0.3441758155822754}, {"x": 0.1524459570646286, "y": 0.3525274693965912}, {"x": 0.10125142335891724, "y": 0.3525274693965912}], "text": "FP1, FP2\n"}
{"page": 5, "bbox": [{"x": 0.17349261045455933, "y": 0.3437362611293793}, {"x": 0.44254836440086365, "y": 0.3437362611293793}, {"x": 0.44254836440086365, "y": 0.35428571701049805}, {"x": 0.17349261045455933, "y": 0.35428571701049805}], "text": "Implement a RAG pipeline for configuration.\n"}
{"page": 5, "bbox": [{"x": 0.4908987581729889, "y": 0.3019780218601227}, {"x": 0.7940841913223267, "y": 0.3024175763130188}, {"x": 0.7940841913223267, "y": 0.3969230651855469}, {"x": 0.4908987581729889, "y": 0.39648351073265076}], "text": "as well as closed source alternatives on small text.\nRAG systems receive unknown input at runtime\nrequiring constant monitoring.\nA RAG system requires calibrating chunk size,\nembedding strategy, chunking strategy, retrieval\nstrategy, consolidation strategy, context size, and\nprompts.\n"}
{"page": 5, "bbox": [{"x": 0.8083049058914185, "y": 0.34285715222358704}, {"x": 0.9271900057792664, "y": 0.3437362611293793}, {"x": 0.9271900057792664, "y": 0.3674725294113159}, {"x": 0.8083049058914185, "y": 0.3665934205055237}], "text": "Cognitive Reviewer,\nAI Tutor, BioASQ\n"}
{"page": 5, "bbox": [{"x": 0.10182025283575058, "y": 0.39956045150756836}, {"x": 0.15187713503837585, "y": 0.39868131279945374}, {"x": 0.15187713503837585, "y": 0.40747252106666565}, {"x": 0.10182025283575058, "y": 0.4083516597747803}], "text": "FP2, FP4\n"}
{"page": 5, "bbox": [{"x": 0.17463025450706482, "y": 0.39340659976005554}, {"x": 0.9152445793151855, "y": 0.39912086725234985}, {"x": 0.914675772190094, "y": 0.4276922941207886}, {"x": 0.17406143248081207, "y": 0.42197802662849426}], "text": "RAG pipelines created by assembling bespoke solu- End-to-end training enhances domain adaptation BioASQ, AI Tutor\ntions are suboptima.\nin RAG systems [18].\n"}
{"page": 5, "bbox": [{"x": 0.11035267263650894, "y": 0.42725273966789246}, {"x": 0.1433447152376175, "y": 0.4276922941207886}, {"x": 0.1433447152376175, "y": 0.434725284576416}, {"x": 0.11035267263650894, "y": 0.4342857003211975}], "text": "FP2-7\n"}
{"page": 5, "bbox": [{"x": 0.8083049058914185, "y": 0.4254944920539856}, {"x": 0.9271900057792664, "y": 0.4263736307621002}, {"x": 0.9271900057792664, "y": 0.44967031478881836}, {"x": 0.8083049058914185, "y": 0.4487912058830261}], "text": "Cognitive Reviewer,\nAI Tutor\n"}
{"page": 5, "bbox": [{"x": 0.17463025450706482, "y": 0.4404395520687103}, {"x": 0.2610921561717987, "y": 0.44087910652160645}, {"x": 0.2610921561717987, "y": 0.44835165143013}, {"x": 0.17463025450706482, "y": 0.4479120969772339}], "text": "ble at runtime.\n"}
{"page": 5, "bbox": [{"x": 0.17406143248081207, "y": 0.4254944920539856}, {"x": 0.7929465174674988, "y": 0.4263736307621002}, {"x": 0.7929465174674988, "y": 0.4650549590587616}, {"x": 0.17406143248081207, "y": 0.464175820350647}], "text": "Testing performance characteristics are only possi- Offline evaluation techniques such as G-Evals [14]\nlook promising but are premised on having access\nto labelled question and answer pairs.\n"}
{"page": 5, "bbox": [{"x": 0.1370875984430313, "y": 0.46945056319236755}, {"x": 0.861774742603302, "y": 0.46945056319236755}, {"x": 0.861774742603302, "y": 0.47999998927116394}, {"x": 0.1370875984430313, "y": 0.47999998927116394}], "text": "Table 2: The lessons learned from the three case studies with key takeaways for future RAG implementations\n"}
{"page": 5, "bbox": [{"x": 0.5187713503837585, "y": 0.4958241879940033}, {"x": 0.914675772190094, "y": 0.4958241879940033}, {"x": 0.914675772190094, "y": 0.5753846168518066}, {"x": 0.5187713503837585, "y": 0.5753846168518066}], "text": "This characteristic has previously been studied for machine learn-\ning systems [5, 6] but the required adaptations (if any) have yet to\nbe applied to LLM based systems such as RAGs. Another idea is to\nincorporate ideas from self-adaptive systems to support monitoring\nand adapting RAG systems, preliminary work has started for other\nmachine learning applications [2].\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.4958241879940033}, {"x": 0.48236632347106934, "y": 0.4958241879940033}, {"x": 0.48236632347106934, "y": 0.6694505214691162}, {"x": 0.08703071624040604, "y": 0.6694505214691162}], "text": "sort out the security/privacy (who can access what). Furthermore,\nas the foundation model itself evolves or you get new data to add to\nthe model, you will need to run finetuning again. On the other side,\nRAG systems seem to offer a pragmatic solution allowing you to\nchunk your data as needed and only use relevant chunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n"}
{"page": 5, "bbox": [{"x": 0.5199089646339417, "y": 0.6035164594650269}, {"x": 0.5284414291381836, "y": 0.6035164594650269}, {"x": 0.5284414291381836, "y": 0.6109890341758728}, {"x": 0.5199089646339417, "y": 0.6109890341758728}], "text": "7\n"}
{"page": 5, "bbox": [{"x": 0.5466439127922058, "y": 0.6026373505592346}, {"x": 0.660978376865387, "y": 0.6021978259086609}, {"x": 0.660978376865387, "y": 0.611868143081665}, {"x": 0.5466439127922058, "y": 0.6123076677322388}], "text": "CONCLUSION\n"}
{"page": 5, "bbox": [{"x": 0.5187713503837585, "y": 0.6210988759994507}, {"x": 0.9152445793151855, "y": 0.6210988759994507}, {"x": 0.9152445793151855, "y": 0.797802209854126}, {"x": 0.5187713503837585, "y": 0.797802209854126}], "text": "RAG systems are a new information retrieval that leverages LLMs.\nSoftware engineers increasingly interact with RAG systems a)\nthrough implementing semantic search, or b) through new code-\ndependent tasks. This paper presented the lessons learned from 3\ncase studies including an empirical investigation involving 15,000\ndocuments and 1000 questions. Our findings provide a guide to\npractitioners by presenting the challenges faced when implement-\ning RAG systems. We also included future research directions for\nRAG systems related to 1) chunking and embeddings, 2) RAG vs\nFinetuning, and 3) Testing and Monitoring. Large language models\nare going to continue to obtain new capabilities of interest to engi-\nneers and researchers. This paper presents the first investigation\ninto RAG systems from a software engineering perspective.\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.7134066224098206}, {"x": 0.48350396752357483, "y": 0.7138461470603943}, {"x": 0.48350396752357483, "y": 0.8958241939544678}, {"x": 0.08703071624040604, "y": 0.8953846096992493}], "text": "6.3 Testing and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questions and answers remains an open\nproblem.\nOnce suitable test data is available quality metrics are also re-\nquired to assist engineers in making quality tradeoffs. Using large\nlanguage models is expensive, introduces latency concerns, and has\nperformance characteristics that all change with each new release.\n"}
{"page": 5, "bbox": [{"x": 0.5193401575088501, "y": 0.8254945278167725}, {"x": 0.711604118347168, "y": 0.8254945278167725}, {"x": 0.711604118347168, "y": 0.8342857360839844}, {"x": 0.5193401575088501, "y": 0.8342857360839844}], "text": "ACKNOWLEDGMENTS\n"}
{"page": 5, "bbox": [{"x": 0.5182024836540222, "y": 0.8426373600959778}, {"x": 0.9129692912101746, "y": 0.84351646900177}, {"x": 0.9129692912101746, "y": 0.89670330286026}, {"x": 0.5182024836540222, "y": 0.8958241939544678}], "text": "To Amanda Edgar, Rajesh Vasa, Kon Mouzakis, Matteo Vergani,\nTrish McCluskey, Kathryn Perus, Tara Draper, Joan Sutherland and\nRuary Ross for their support and involvement in making the AI\nTutor project possible.\n"}
{"page": 6, "bbox": [{"x": 0.08816837519407272, "y": 0.07780219614505768}, {"x": 0.27303755283355713, "y": 0.07868131995201111}, {"x": 0.27303755283355713, "y": 0.08703296631574631}, {"x": 0.08816837519407272, "y": 0.08615384250879288}], "text": "CAIN 2024, April 2024, Lisbon, Portugal\n"}
{"page": 6, "bbox": [{"x": 0.467576801776886, "y": 0.07868131995201111}, {"x": 0.9101251363754272, "y": 0.07868131995201111}, {"x": 0.9101251363754272, "y": 0.08615384250879288}, {"x": 0.467576801776886, "y": 0.08615384250879288}], "text": "Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\n"}
{"page": 6, "bbox": [{"x": 0.08816837519407272, "y": 0.10857142508029938}, {"x": 0.20136518776416779, "y": 0.10901098698377609}, {"x": 0.20136518776416779, "y": 0.11824175715446472}, {"x": 0.08816837519407272, "y": 0.117802195250988}], "text": "REFERENCES\n"}
{"page": 6, "bbox": [{"x": 0.5455062389373779, "y": 0.11032967269420624}, {"x": 0.6541524529457092, "y": 0.1120879128575325}, {"x": 0.6541524529457092, "y": 0.11999999731779099}, {"x": 0.5455062389373779, "y": 0.11824175715446472}], "text": "arXiv:2007.01282 (2020).\n"}
{"page": 6, "bbox": [{"x": 0.5193401575088501, "y": 0.12175824493169785}, {"x": 0.9141069650650024, "y": 0.12175824493169785}, {"x": 0.9141069650650024, "y": 0.31164833903312683}, {"x": 0.5193401575088501, "y": 0.31164833903312683}], "text": "[10] Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios\nPaliouras. 2023. BioASQ-QA: A manually curated corpus for biomedical question\nanswering. Scientific Data 10 (2023), 170. Citation Key: 422.\n[11] Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 2023. LORA Fine-tuning\nEfficiently Undoes Safety Training in Llama 2-Chat 70B. arXiv:2310.20624 [cs.LG]\n[12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459-9474.\n[13] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models\nuse long contexts. arXiv preprint arXiv:2307.03172 (2023).\n[14] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang\nZhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment, may\n2023. arXiv preprint arXiv:2303.16634 (2023).\n[15] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt selec-\ntion for code-related few-shot learning. In Proceedings of the 45th International\nConference on Software Engineering (ICSE'23).\n[16] OpenAI. 2023. GPT-4 Technical Report. https://doi.org/10.48550/ARXIV.2303.\n"}
{"page": 6, "bbox": [{"x": 0.09271899610757828, "y": 0.12439560145139694}, {"x": 0.48407280445098877, "y": 0.12527473270893097}, {"x": 0.4829351603984833, "y": 0.42989009618759155}, {"x": 0.09158134460449219, "y": 0.4290109872817993}], "text": "[1] Fu Bang. 2023. GPTCache: An Open-Source Semantic Cache for LLM Applications\nEnabling Faster Answers and Cost Savings. In 3rd Workshop for Natural Language\nProcessing Open Source Software.\n[2] Maria Casimiro, Paolo Romano, David Garlan, Gabriel Moreno, Eunsuk Kang, and\nMark Klein. 2022. Self-adaptive Machine Learning Systems: Research Challenges\nand Opportunities. 133-155. https://doi.org/10.1007/978-3-031-15116-3_7\n[3] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking\nLarge Language Models in Retrieval-Augmented Generation. arXiv preprint\narXiv:2309.01431 (2023).\n[4] Mingda Chen, Xilun Chen, and Wen-tau Yih. 2023. Efficient Open Domain\nMulti-Hop Question Answering with Few-Shot Data Synthesis. arXiv preprint\narXiv:2305.13691 (2023).\n[5] Alex Cummaudo, Scott Barnett, Rajesh Vasa, and John Grundy. 2020. Threshy:\nSupporting safe usage of intelligent web services. In Proceedings of the 28th ACM\nJoint Meeting on European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering. 1645-1649.\n[6] Alex Cummaudo, Scott Barnett, Rajesh Vasa, John Grundy, and Mohamed Ab-\ndelrazek. 2020. Beware the evolving 'intelligent'web service! An integration\narchitecture tactic to guard Al-first components. In Proceedings of the 28th ACM\nJoint Meeting on European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering. 269-280.\n[7] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.\nRetrieval augmented language model pre-training. In International conference on\nmachine learning. PMLR, 3929-3938.\n[8] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-\nlight: Efficient and effective retrieval-augmented text generation. In Proceedings\nof the 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval. 1437-1447.\n[9] Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with\ngenerative models for open domain question answering.\narXiv preprint\n"}
{"page": 6, "bbox": [{"x": 0.5449374318122864, "y": 0.3138461410999298}, {"x": 0.5711035132408142, "y": 0.3142857253551483}, {"x": 0.5711035132408142, "y": 0.3195604383945465}, {"x": 0.5449374318122864, "y": 0.3191208839416504}], "text": "08774\n"}
{"page": 6, "bbox": [{"x": 0.5193401575088501, "y": 0.32351648807525635}, {"x": 0.9141069650650024, "y": 0.32351648807525635}, {"x": 0.9141069650650024, "y": 0.43252748250961304}, {"x": 0.5193401575088501, "y": 0.43252748250961304}], "text": "[17] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and\nIlya Sutskever. 2023. Robust speech recognition via large-scale weak supervision.\nIn International Conference on Machine Learning. PMLR, 28492-28518.\n[18] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-\narachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (RAG) models for open domain\nquestion answering. Transactions of the Association for Computational Linguistics\n11 (2023), 1-17.\n[19] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-\nlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for\ninformation retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).\n"}
{"page": 1, "bbox": [{"x": 0.1240045502781868, "y": 0.10593406856060028}, {"x": 0.8742889761924744, "y": 0.10593406856060028}, {"x": 0.8742889761924744, "y": 0.12483516335487366}, {"x": 0.1240045502781868, "y": 0.12483516335487366}], "text": "The Power of Noise: Redefining Retrieval for RAG Systems\n"}
{"page": 1, "bbox": [{"x": 0.404436856508255, "y": 0.13802197575569153}, {"x": 0.5955631136894226, "y": 0.13802197575569153}, {"x": 0.5955631136894226, "y": 0.19780220091342926}, {"x": 0.404436856508255, "y": 0.19780220091342926}], "text": "Giovanni Trappolini*\ntrappolini@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n"}
{"page": 1, "bbox": [{"x": 0.13594993948936462, "y": 0.1397802233695984}, {"x": 0.3270762264728546, "y": 0.13934065401554108}, {"x": 0.32764506340026855, "y": 0.19780220091342926}, {"x": 0.13651877641677856, "y": 0.19824175536632538}], "text": "Florin Cuconasu*\ncuconasu@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n"}
{"page": 1, "bbox": [{"x": 0.6723549365997314, "y": 0.1402197778224945}, {"x": 0.864050030708313, "y": 0.1397802233695984}, {"x": 0.864050030708313, "y": 0.19824175536632538}, {"x": 0.6723549365997314, "y": 0.1986813247203827}], "text": "Federico Siciliano\nsiciliano@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n"}
{"page": 1, "bbox": [{"x": 0.1240045502781868, "y": 0.21494504809379578}, {"x": 0.33788394927978516, "y": 0.21494504809379578}, {"x": 0.33788394927978516, "y": 0.257582426071167}, {"x": 0.1240045502781868, "y": 0.257582426071167}], "text": "Simone Filice\nfilice.simone@gmail.com\nTechnology Innovation Institute\n"}
{"page": 1, "bbox": [{"x": 0.4032992124557495, "y": 0.2131868153810501}, {"x": 0.5955631136894226, "y": 0.2131868153810501}, {"x": 0.5955631136894226, "y": 0.2716483473777771}, {"x": 0.4032992124557495, "y": 0.2716483473777771}], "text": "Cesare Campagnano\ncampagnano@di.uniroma1.it\nSapienza University of Rome\nRome, Italy\n"}
{"page": 1, "bbox": [{"x": 0.6604095697402954, "y": 0.21450549364089966}, {"x": 0.8748577833175659, "y": 0.21362636983394623}, {"x": 0.8754266500473022, "y": 0.27076923847198486}, {"x": 0.660978376865387, "y": 0.2716483473777771}], "text": "Yoelle Maarek\nyoelle@yahoo.com\nTechnology Innovation Institute\nHaifa, Israel\n"}
{"page": 1, "bbox": [{"x": 0.19169510900974274, "y": 0.26153847575187683}, {"x": 0.27189987897872925, "y": 0.2606593370437622}, {"x": 0.27189987897872925, "y": 0.26989009976387024}, {"x": 0.19169510900974274, "y": 0.27076923847198486}], "text": "Haifa, Israel\n"}
{"page": 1, "bbox": [{"x": 0.2957906723022461, "y": 0.2883516550064087}, {"x": 0.4351535737514496, "y": 0.2883516550064087}, {"x": 0.4351535737514496, "y": 0.29846152663230896}, {"x": 0.2957906723022461, "y": 0.29846152663230896}], "text": "Nicola Tonellotto\n"}
{"page": 1, "bbox": [{"x": 0.2804323136806488, "y": 0.30373626947402954}, {"x": 0.4522184431552887, "y": 0.3050549328327179}, {"x": 0.4522184431552887, "y": 0.3160439431667328}, {"x": 0.2804323136806488, "y": 0.31472527980804443}], "text": "nicola.tonellotto@unipi.it\n"}
{"page": 1, "bbox": [{"x": 0.5381115078926086, "y": 0.2883516550064087}, {"x": 0.7292377948760986, "y": 0.2879121005535126}, {"x": 0.7298066020011902, "y": 0.34593406319618225}, {"x": 0.5386803150177002, "y": 0.34637361764907837}], "text": "Fabrizio Silvestri\nfsilvestri@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n"}
{"page": 1, "bbox": [{"x": 0.3060295879840851, "y": 0.3191208839416504}, {"x": 0.4237770140171051, "y": 0.3186813294887543}, {"x": 0.4237770140171051, "y": 0.34593406319618225}, {"x": 0.3060295879840851, "y": 0.34637361764907837}], "text": "University of Pisa\nPisa, Italy\n"}
{"page": 1, "bbox": [{"x": 0.08759954571723938, "y": 0.3591208755970001}, {"x": 0.18259385228157043, "y": 0.35956043004989624}, {"x": 0.18259385228157043, "y": 0.36879122257232666}, {"x": 0.08759954571723938, "y": 0.36835163831710815}], "text": "ABSTRACT\n"}
{"page": 1, "bbox": [{"x": 0.5199089646339417, "y": 0.36000001430511475}, {"x": 0.6200227737426758, "y": 0.36000001430511475}, {"x": 0.6200227737426758, "y": 0.36835163831710815}, {"x": 0.5199089646339417, "y": 0.36835163831710815}], "text": "KEYWORDS\n"}
{"page": 1, "bbox": [{"x": 0.5193401575088501, "y": 0.3780219852924347}, {"x": 0.7178612351417542, "y": 0.37714284658432007}, {"x": 0.7178612351417542, "y": 0.385934054851532}, {"x": 0.5193401575088501, "y": 0.3868131935596466}], "text": "RAG, LLM, Information Retrieval\n"}
{"page": 1, "bbox": [{"x": 0.5187713503837585, "y": 0.4004395604133606}, {"x": 0.660978376865387, "y": 0.4008791148662567}, {"x": 0.660978376865387, "y": 0.40791207551956177}, {"x": 0.5187713503837585, "y": 0.40747252106666565}], "text": "ACM Reference Format:\n"}
{"page": 1, "bbox": [{"x": 0.5182024836540222, "y": 0.41318681836128235}, {"x": 0.914675772190094, "y": 0.41274726390838623}, {"x": 0.914675772190094, "y": 0.49670329689979553}, {"x": 0.5182024836540222, "y": 0.49714285135269165}], "text": "Florin Cuconasu*, Giovanni Trappolini*, Federico Siciliano, Simone Fil-\nice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio\nSilvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Sys-\ntems. In Proceedings of the 47th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR '24), July 14–18,\n2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https:\n//doi.org/10.1145/3626772.3657834\n"}
{"page": 1, "bbox": [{"x": 0.025028441101312637, "y": 0.6993406414985657}, {"x": 0.0284414105117321, "y": 0.2738461494445801}, {"x": 0.05915813520550728, "y": 0.2738461494445801}, {"x": 0.055745165795087814, "y": 0.6993406414985657}], "text": "arXiv:2401.14887v4 [cs.IR] 1 May 2024\n"}
{"page": 1, "bbox": [{"x": 0.5466439127922058, "y": 0.5208791494369507}, {"x": 0.6860068440437317, "y": 0.5208791494369507}, {"x": 0.6860068440437317, "y": 0.5301098823547363}, {"x": 0.5466439127922058, "y": 0.5301098823547363}], "text": "INTRODUCTION\n"}
{"page": 1, "bbox": [{"x": 0.520477831363678, "y": 0.5221977829933167}, {"x": 0.5284414291381836, "y": 0.5221977829933167}, {"x": 0.5284414291381836, "y": 0.5301098823547363}, {"x": 0.520477831363678, "y": 0.5301098823547363}], "text": "1\n"}
{"page": 1, "bbox": [{"x": 0.08646188676357269, "y": 0.3780219852924347}, {"x": 0.48350396752357483, "y": 0.3780219852924347}, {"x": 0.48350396752357483, "y": 0.692307710647583}, {"x": 0.08646188676357269, "y": 0.692307710647583}], "text": "Retrieval-Augmented Generation (RAG) has recently emerged as\na method to extend beyond the pre-trained knowledge of Large\nLanguage Models by augmenting the original prompt with relevant\npassages or documents retrieved by an Information Retrieval (IR)\nsystem. RAG has become increasingly important for Generative\nAI solutions, especially in enterprise settings or in any domain in\nwhich knowledge is constantly refreshed and cannot be memorized\nin the LLM. We argue here that the retrieval component of RAG\nsystems, be it dense or sparse, deserves increased attention from\nthe research community, and accordingly, we conduct the first com-\nprehensive and systematic examination of the retrieval strategy\nof RAG systems. We focus, in particular, on the type of passages\nIR systems within a RAG solution should retrieve. Our analysis\nconsiders multiple factors, such as the relevance of the passages in-\ncluded in the prompt context, their position, and their number. One\ncounter-intuitive finding of this work is that the retriever's highest-\nscoring documents that are not directly relevant to the query (e.g.,\ndo not contain the answer) negatively impact the effectiveness of\nthe LLM. Even more surprising, we discovered that adding random\ndocuments in the prompt improves the LLM accuracy by up to\n35%. These results highlight the need to investigate the appropriate\nstrategies when integrating retrieval with LLMs, thereby laying the\ngroundwork for future research in this area.¹\n"}
{"page": 1, "bbox": [{"x": 0.08759954571723938, "y": 0.7103296518325806}, {"x": 0.22013652324676514, "y": 0.7103296518325806}, {"x": 0.22013652324676514, "y": 0.719560444355011}, {"x": 0.08759954571723938, "y": 0.719560444355011}], "text": "CCS CONCEPTS\n"}
{"page": 1, "bbox": [{"x": 0.5187713503837585, "y": 0.5389010906219482}, {"x": 0.914675772190094, "y": 0.5389010906219482}, {"x": 0.914675772190094, "y": 0.8958241939544678}, {"x": 0.5187713503837585, "y": 0.8958241939544678}], "text": "Large Language Models (LLMs) [9] have demonstrated unprece-\ndented proficiency in various tasks, ranging from text generation\nand complex question answering [6], to information retrieval (IR)\ntasks [22, 57]. However, LLMs have limitations in the handling of\nlong contexts [52], a constraint that leads to an increased reliance\non their pre-trained knowledge. This limitation not only confines\ntheir ability to effectively manage extended discourse, such as in\nbooks or long conversations, but also increases the probability of\ngenerating hallucinations, instances for which the model produces\nfactually incorrect or nonsensical information [41]. To improve the\naccuracy of responses generated by LLMs, Retrieval-Augmented\nGeneration (RAG) has emerged as a promising solution [28]. RAG\nis primarily designed to improve factual accuracy by providing\nthe model access to auxiliary information, thereby augmenting the\noriginal prompt with information not necessarily memorized in\nthe LLM. A key benefit of this approach is that it helps ground the\nprompt with relevant information that might help the LLM gener-\nate more accurate answers at inference time. At their core, RAG\nsystems consist of two fundamental components: a retriever and a\ngenerator. The retriever is responsible for invoking an external IR\nsystem (dense and/or sparse) and feeding the selected results to a\ngenerator component.\nThis study focuses on the IR aspect of RAG, posing the following\nresearch question: \"What characteristics are desirable in a retriever\nto optimize prompt construction for RAG systems? Are current re-\ntrievers ideal?\". We focus on the three main types of documents\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.7283516526222229}, {"x": 0.4738338887691498, "y": 0.7283516526222229}, {"x": 0.4738338887691498, "y": 0.7389010787010193}, {"x": 0.08816837519407272, "y": 0.7389010787010193}], "text": "⚫ Information systems → Novelty in information retrieval.\n"}
{"page": 1, "bbox": [{"x": 0.08703071624040604, "y": 0.7604395747184753}, {"x": 0.4493742883205414, "y": 0.7604395747184753}, {"x": 0.4493742883205414, "y": 0.7810989022254944}, {"x": 0.08703071624040604, "y": 0.7810989022254944}], "text": "1The code and data are available at github.com/florin-git/The-Power-of-Noise\nThese authors contributed equally to this work.\n"}
{"page": 1, "bbox": [{"x": 0.08646188676357269, "y": 0.8043956160545349}, {"x": 0.48122867941856384, "y": 0.8043956160545349}, {"x": 0.48122867941856384, "y": 0.8949450254440308}, {"x": 0.08646188676357269, "y": 0.8949450254440308}], "text": "Permission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSIGIR '24, July 14-18, 2024, Washington, DC, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0431-4/24/07.\nhttps://doi.org/10.1145/3626772.3657834\n"}
{"page": 2, "bbox": [{"x": 0.7656427621841431, "y": 0.0782417580485344}, {"x": 0.9118316173553467, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.7656427621841431, "y": 0.08571428805589676}], "text": "Cuconasu and Trappolini, et al.\n"}
{"page": 2, "bbox": [{"x": 0.08759954571723938, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.08659340441226959}, {"x": 0.08759954571723938, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 2, "bbox": [{"x": 0.5187713503837585, "y": 0.11032967269420624}, {"x": 0.9152445793151855, "y": 0.11032967269420624}, {"x": 0.9152445793151855, "y": 0.28659340739250183}, {"x": 0.5187713503837585, "y": 0.28659340739250183}], "text": "1980s are the basis for quantifying textual similarity. These re-\ntrieval methods are characterized by their use of high-dimensional\nand sparse feature vectors and have been essential in developing\na full generation of IR systems. BM25 represents the most famous\ncurrent iteration [40]. A significant evolution in IR is the intro-\nduction of dense retrievers, which emerged from advancements\nin deep learning; they utilize low-dimensional dense vectors for\ntextual representation, and allow to capture semantic relationships.\nThis is in contrast to traditional IR methods (referred to as sparse\nin opposition to dense), which typically rely on lexical match and\nstruggle with semantic match [32]. In the last few years, dense\nmethods such as DPR [19] and others [15, 24] have demonstrated\nthat they can compete with sparse methods.\n"}
{"page": 2, "bbox": [{"x": 0.08589305728673935, "y": 0.10901098698377609}, {"x": 0.48350396752357483, "y": 0.10901098698377609}, {"x": 0.48350396752357483, "y": 0.3727472424507141}, {"x": 0.08589305728673935, "y": 0.3727472424507141}], "text": "(or passages²) that a retriever can return: relevant, distracting, and\nrandom. Relevant documents contain pertinent information that\neither directly answers or might inform the query. Distracting doc-\numents, while not directly answering the query, are semantically\nor contextually linked to the topic. For instance, if one asks for\nthe color of Napoléon's horse, a passage describing the color of\nJoséphine de Beauharnais' (Napoléon's first wife) horse, while not\ncontaining the right information, would be highly related. Random\ndocuments have no relation whatsoever to the query and can be\nseen as a kind of informational noise within the retrieval process.\nOne of the key goals of our study is to determine the role of each\ntype of document and the relative value they bring to the LLM\neffectiveness. In particular, we verify whether there is a need to\nrevisit some of the commonly accepted assumptions in IR systems\nwhen used in the context of LLMs. The main contributions of our\nwork are the following:\n(1) We conduct the first comprehensive study examining the\nimpact of the type of retrieved documents in RAG on the\nLLM effectiveness.\n"}
{"page": 2, "bbox": [{"x": 0.10523322224617004, "y": 0.3784615397453308}, {"x": 0.14505119621753693, "y": 0.37714284658432007}, {"x": 0.14562001824378967, "y": 0.3868131935596466}, {"x": 0.10580204427242279, "y": 0.38813185691833496}], "text": "(2) We\n"}
{"page": 2, "bbox": [{"x": 0.10523322224617004, "y": 0.37714284658432007}, {"x": 0.48350396752357483, "y": 0.3758241832256317}, {"x": 0.48350396752357483, "y": 0.42989009618759155}, {"x": 0.10523322224617004, "y": 0.4312087893486023}], "text": "propose retrieval RAG heuristics that leverage the unex-\npected results of this study.\n(3) We release all associated code and data to the community to\nencourage further research.\n"}
{"page": 2, "bbox": [{"x": 0.5187713503837585, "y": 0.3081318736076355}, {"x": 0.9152445793151855, "y": 0.3085714280605316}, {"x": 0.914675772190094, "y": 0.6153846383094788}, {"x": 0.5182024836540222, "y": 0.6149450540542603}], "text": "2.3 Retrieve and Generate\nRAG introduces a new approach in AI, combining the strengths of\nboth retrieval-based and generative models. The concept of RAG\nwas coined and popularized in [28], which introduced a model that\ncombines a dense passage retriever with a sequence-to-sequence\nmodel, demonstrating substantial improvements in knowledge-\nintensive tasks. Similar methods/variations have also been pro-\nposed concurrently or soon after, such as [2, 4, 8, 13, 21]; see [33]\nfor a survey on augmented language models. Researchers and prac-\ntitioners have recently started to explore these RAG systems' in-\nner workings. Notably, [44, 51] analyzed the impact of different\ntypes of documents on cascading IR/NLP systems. Other works\nhave tried to study how attentive transformers are to their input\n[23, 30, 31, 39, 46]. [7] studied the effect of the retriever's similarity\nmetric, which was found to be insufficient for reasoning. In [25, 55],\nauthors analyzed LLM's receptiveness to external evidence against\ninternal memory. In [60], they test the model's (in)ability to ground\nreferences.\nIn this paper, we want to provide the first comprehensive analysis\nof the implications of using a retriever module in a RAG system,\nstudying the impact of several key factors, like the type, number,\nand position of documents that should augment the prompt to the\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.44923076033592224}, {"x": 0.4829351603984833, "y": 0.44923076033592224}, {"x": 0.4829351603984833, "y": 0.7881318926811218}, {"x": 0.08703071624040604, "y": 0.7881318926811218}], "text": "2 RELATED WORKS\n2.1 Generative Language Models\nThe inception of the modern LLM era can be traced back to the\nseminal paper titled \"Attention Is All You Need\" [52]. This work in-\ntroduced the transformer architecture, a framework that adopts an\nattention mechanism instead of recurrent layers, enabling the model\nto capture global dependencies within the data. The following year,\nBERT (Bidirectional Encoder Representations from Transformers)\n[22] offered a significant improvement over the state-of-the-art via\na novel bidirectional, unsupervised language representation. The\nevolution of transformer-based models continued with the devel-\nopment of the Generative Pre-trained Transformer (GPT) [37]. Its\nsuccessor, GPT-2 [38], expanded upon this foundation with a larger\nscale model and demonstrated improved performance across a vari-\nety of language tasks without task-specific training. The subsequent\niteration, GPT-3 [9], represented a further enhancement in model\nscale and capabilities, particularly in the realm of few-shot learning.\nFinally, recent times have seen a surge in the production of large,\npublicly available language models. Several actors have released\ntheir models, most notably, Llama [49, 50], Falcon [1], Mosaic MPT\n[47], and Phi [16, 29]. There are also versions of these models that\nhave been fine-tuned on specific languages [5, 10, 12, 17, 43]. The\nproliferation and quality of these models are expanding the range\nof tasks and the vision they address [48, 54, 56].\n"}
{"page": 2, "bbox": [{"x": 0.5193401575088501, "y": 0.618461549282074}, {"x": 0.5494880676269531, "y": 0.618461549282074}, {"x": 0.5494880676269531, "y": 0.6254944801330566}, {"x": 0.5193401575088501, "y": 0.6254944801330566}], "text": "LLM.\n"}
{"page": 2, "bbox": [{"x": 0.5199089646339417, "y": 0.6501098871231079}, {"x": 0.5836177468299866, "y": 0.6501098871231079}, {"x": 0.5836177468299866, "y": 0.6584615111351013}, {"x": 0.5199089646339417, "y": 0.6584615111351013}], "text": "3 RAG\n"}
{"page": 2, "bbox": [{"x": 0.5187713503837585, "y": 0.6676923036575317}, {"x": 0.9141069650650024, "y": 0.6668131947517395}, {"x": 0.9141069650650024, "y": 0.6927472352981567}, {"x": 0.5187713503837585, "y": 0.693626344203949}], "text": "In this paper, we explore the application of RAG in the context of\nQuestion Answering, arguably its most popular application.\n"}
{"page": 2, "bbox": [{"x": 0.5182024836540222, "y": 0.7138461470603943}, {"x": 0.914675772190094, "y": 0.7138461470603943}, {"x": 0.914675772190094, "y": 0.8953846096992493}, {"x": 0.5182024836540222, "y": 0.8953846096992493}], "text": "3.1 Open-Domain Question Answering\nOpen-domain Question Answering (OpenQA) refers to the task\nof developing systems capable of providing accurate and contex-\ntually relevant answers to a broad range of questions posed in\nnatural language without limitations to specific domains or prede-\nfined datasets. In general, we want to find an answer A to a query\nq. To do so, we draw information from a corpus of documents\nD= {d1, d2,..., dn}, which is usually assumed to be large in size.\nA prevalent approach for this task involves a two-step architecture,\ntypically comprising a retriever and a reasoner (typically a gen-\nerator). This methodology addresses the inherent complexities of\nOpenQA by dividing the process into distinct phases: first finding\nthe appropriate set of documents that can potentially address the\n"}
{"page": 2, "bbox": [{"x": 0.127986341714859, "y": 0.8061538338661194}, {"x": 0.3077360689640045, "y": 0.8061538338661194}, {"x": 0.3077360689640045, "y": 0.8149450421333313}, {"x": 0.127986341714859, "y": 0.8149450421333313}], "text": "Information Retrieval\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.8074725270271301}, {"x": 0.11035267263650894, "y": 0.8074725270271301}, {"x": 0.11035267263650894, "y": 0.8149450421333313}, {"x": 0.08703071624040604, "y": 0.8149450421333313}], "text": "2.2\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.8246153593063354}, {"x": 0.48350396752357483, "y": 0.8246153593063354}, {"x": 0.48350396752357483, "y": 0.8487911820411682}, {"x": 0.08703071624040604, "y": 0.8487911820411682}], "text": "Foundational information retrieval methodologies, such as the Vec-\ntor Space Model and the TF-IDF scoring [42] introduced in the\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.875164806842804}, {"x": 0.4806598424911499, "y": 0.875164806842804}, {"x": 0.4806598424911499, "y": 0.8949450254440308}, {"x": 0.08703071624040604, "y": 0.8949450254440308}], "text": "2We interchangeably use here the terms \"passage\" or \"document\" to represent the\nindexing/retrieval unit of the IR system.\n"}
{"page": 3, "bbox": [{"x": 0.6780432462692261, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.6780432462692261, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 3, "bbox": [{"x": 0.08759954571723938, "y": 0.07912088185548782}, {"x": 0.361774742603302, "y": 0.07912088185548782}, {"x": 0.361774742603302, "y": 0.08659340441226959}, {"x": 0.08759954571723938, "y": 0.08659340441226959}], "text": "The Power of Noise: Redefining Retrieval for RAG Systems\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.10813187062740326}, {"x": 0.48009100556373596, "y": 0.10901098698377609}, {"x": 0.48009100556373596, "y": 0.13582417368888855}, {"x": 0.08703071624040604, "y": 0.13494504988193512}], "text": "query and then synthesizing an answer, which can be consumed\nby the user of the QA system.\n"}
{"page": 3, "bbox": [{"x": 0.127986341714859, "y": 0.14989010989665985}, {"x": 0.20364050567150116, "y": 0.14945055544376373}, {"x": 0.20364050567150116, "y": 0.15824176371097565}, {"x": 0.127986341714859, "y": 0.15868131816387177}], "text": "Retriever\n"}
{"page": 3, "bbox": [{"x": 0.08759954571723938, "y": 0.14989010989665985}, {"x": 0.1097838431596756, "y": 0.14989010989665985}, {"x": 0.1097838431596756, "y": 0.15912087261676788}, {"x": 0.08759954571723938, "y": 0.15912087261676788}], "text": "3.2\n"}
{"page": 3, "bbox": [{"x": 0.5187713503837585, "y": 0.10725274682044983}, {"x": 0.9135380983352661, "y": 0.10725274682044983}, {"x": 0.9135380983352661, "y": 0.21758241951465607}, {"x": 0.5187713503837585, "y": 0.21758241951465607}], "text": "the kind pn (d|q) x exp(q. d). Given our formalization of the RAG\ntask, we notice how the generative component pe depends on a\ngiven text, that is the query, and a dynamic text, that is the set of\nretrieved documents. We study in the next two sections the impact\nof changing the set of retrieved documents on the generator and,\nconsequently, the whole end-to-end system. In particular, we aim\nto find the best set of documents Dr that a retriever should feed\nthe generator to maximize the system's effectiveness.\n"}
{"page": 3, "bbox": [{"x": 0.5187713503837585, "y": 0.2325274795293808}, {"x": 0.5290102362632751, "y": 0.2325274795293808}, {"x": 0.5290102362632751, "y": 0.24219779670238495}, {"x": 0.5187713503837585, "y": 0.24219779670238495}], "text": "4\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.1683516502380371}, {"x": 0.48350396752357483, "y": 0.1683516502380371}, {"x": 0.48350396752357483, "y": 0.3476923108100891}, {"x": 0.08703071624040604, "y": 0.3476923108100891}], "text": "The retriever plays a critical role in the OpenQA task. Its goal\nis to find a sufficiently small subset of documents Dr to allow\nthe reasoner to answer the query correctly. Among the various\nretrieval methodologies, the use of a dense retriever has gained\nprominence due to its effectiveness in handling semantic matches.\nDense retrieval requires transforming textual data into vector rep-\nresentations, which is typically achieved with a neural network,\noften a transformer-based encoder, like BERT [22]. The dense re-\ntriever processes both the query q and potential source documents\nto generate corresponding embeddings & for the query and d¡ for\neach document d₁ = D. The embedding process can be represented\nq = Encoderq(q); d₁ = Encoderd (di)\n"}
{"page": 3, "bbox": [{"x": 0.5193401575088501, "y": 0.2325274795293808}, {"x": 0.914675772190094, "y": 0.2325274795293808}, {"x": 0.914675772190094, "y": 0.2892307639122009}, {"x": 0.5193401575088501, "y": 0.2892307639122009}], "text": "EXPERIMENTAL METHODOLOGY\nIn this section, we detail the experimental framework. We start by\ndescribing the data used in the experiments and then discuss the\ntype of documents that a retriever can return and pass to the LLM.\n"}
{"page": 3, "bbox": [{"x": 0.5187713503837585, "y": 0.3054945170879364}, {"x": 0.5420932769775391, "y": 0.3054945170879364}, {"x": 0.5420932769775391, "y": 0.3129670321941376}, {"x": 0.5187713503837585, "y": 0.3129670321941376}], "text": "4.1\n"}
{"page": 3, "bbox": [{"x": 0.08759954571723938, "y": 0.3252747356891632}, {"x": 0.10352673381567001, "y": 0.3252747356891632}, {"x": 0.10352673381567001, "y": 0.3301098942756653}, {"x": 0.08759954571723938, "y": 0.3301098942756653}], "text": "as:\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.35384616255760193}, {"x": 0.48122867941856384, "y": 0.35384616255760193}, {"x": 0.48122867941856384, "y": 0.5054945349693298}, {"x": 0.08703071624040604, "y": 0.5054945349693298}], "text": "where Encoder, and Encoderà are neural network-based encoders,\npotentially sharing weights or architecture, designed to map the\ntextual data into a vector space. Once the embeddings are generated,\nthe retrieval process involves computing the similarity between\nthe query embedding and each document embedding. The most\ncommon approach is to use dot product [20], defined as: s(q, d;) =\nqdi. This score quantifies the relevance of each document to\nthe query by measuring their similarity in the embedded vector\nspace, with higher scores indicating greater relevance. According\nto these scores, the top-ranked documents are selected for further\nprocessing in the generator component.\n"}
{"page": 3, "bbox": [{"x": 0.5182024836540222, "y": 0.30417582392692566}, {"x": 0.9141069650650024, "y": 0.30417582392692566}, {"x": 0.9141069650650024, "y": 0.6501098871231079}, {"x": 0.5182024836540222, "y": 0.6501098871231079}], "text": "Natural Question Dataset\nThe Natural Questions (NQ) dataset [26] is a large-scale collection\nof real-world queries derived from Google search data. Each en-\ntry in the dataset consists of a user query and the corresponding\nWikipedia page containing the answer. The NQ-open dataset [27], a\nsubset of the NQ dataset, differs by removing the restriction of link-\ning answers to specific Wikipedia passages, thereby mimicking a\nmore general information retrieval scenario similar to web searches.\nThis open-domain nature significantly impacts our experimental de-\nsign, particularly in the selection and categorization of documents.\nFollowing the methodology of Lee et al. [27], our primary source\nfor answering queries is the English Wikipedia dump as of 20 De-\ncember 2018. Consistently with the Dense Passage Retrieval (DPR)\napproach [20], each Wikipedia article in this dump was segmented\ninto non-overlapping passages of 100 words. A significant challenge\nin open-domain question answering is the potential temporal mis-\nmatch between the Wikipedia dump and the question-answer pairs\nin the dataset, which can lead to missing answers in the dataset,\nas highlighted in the AmbigQA study [34]. To mitigate this, we\nintegrated the gold documents from the original NQ dataset into\nour Wikipedia document set. Given the open-domain nature of our\ntask, there may be additional documents relevant to the query, i.e.,\ncontaining the answer, but we will not consider them as gold. The\nfinal dataset comprises 21, 035, 236 documents, with 72, 209 queries\nin the train set and 2, 889 in the test set.\n"}
{"page": 3, "bbox": [{"x": 0.08816837519407272, "y": 0.5199999809265137}, {"x": 0.11035267263650894, "y": 0.5199999809265137}, {"x": 0.11035267263650894, "y": 0.5287911891937256}, {"x": 0.08816837519407272, "y": 0.5287911891937256}], "text": "3.3\n"}
{"page": 3, "bbox": [{"x": 0.127986341714859, "y": 0.5199999809265137}, {"x": 0.2042093276977539, "y": 0.5199999809265137}, {"x": 0.2042093276977539, "y": 0.5287911891937256}, {"x": 0.127986341714859, "y": 0.5287911891937256}], "text": "Reasoner\n"}
{"page": 3, "bbox": [{"x": 0.4562002420425415, "y": 0.5419780015945435}, {"x": 0.4817974865436554, "y": 0.5406593680381775}, {"x": 0.48236632347106934, "y": 0.5472527742385864}, {"x": 0.45676904916763306, "y": 0.5485714077949524}], "text": "syn-\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.5367032885551453}, {"x": 0.4829351603984833, "y": 0.538021981716156}, {"x": 0.48236632347106934, "y": 0.6329670548439026}, {"x": 0.08646188676357269, "y": 0.6316483616828918}], "text": "The second step involves a generator component in charge of:\nthesizing an answer, typically implemented via an LLM. Generative\nlanguage models operate by predicting the probability distribution\nof the next token, given the previous tokens. For a given sequence\nof words w1, W2, ..., Wn, a generative language model aims to max-\nimize the likelihood of this sequence, expressed using the chain\nrule of probability:\n"}
{"page": 3, "bbox": [{"x": 0.27076223492622375, "y": 0.6465933918952942}, {"x": 0.2804323136806488, "y": 0.6465933918952942}, {"x": 0.2804323136806488, "y": 0.6523076891899109}, {"x": 0.27076223492622375, "y": 0.6523076891899109}], "text": "N\n"}
{"page": 3, "bbox": [{"x": 0.14277587831020355, "y": 0.6549450755119324}, {"x": 0.42320817708969116, "y": 0.6549450755119324}, {"x": 0.42320817708969116, "y": 0.6716483235359192}, {"x": 0.14277587831020355, "y": 0.6716483235359192}], "text": "P(W1, W2, … … …, Wn) = [[ P(wi|W1, W2, … … …, Wi−1)\n"}
{"page": 3, "bbox": [{"x": 0.26791808009147644, "y": 0.6738461256027222}, {"x": 0.28384527564048767, "y": 0.6738461256027222}, {"x": 0.28384527564048767, "y": 0.6791208982467651}, {"x": 0.26791808009147644, "y": 0.6791208982467651}], "text": "i=1\n"}
{"page": 3, "bbox": [{"x": 0.08646188676357269, "y": 0.6839560270309448}, {"x": 0.4829351603984833, "y": 0.6848351359367371}, {"x": 0.4829351603984833, "y": 0.7516483664512634}, {"x": 0.08646188676357269, "y": 0.7507692575454712}], "text": "where P(wi|w1, W2, ..., Wi-1) is the conditional probability of the\nword w; given the preceding sequence of words w1, W2, . . ., Wi−1·\nIn RAG, the generative language model takes a query q and the\nretrieved documents Dr as input and generates a response by se-\nquentially predicting the next token in the sequence. More formally,\n"}
{"page": 3, "bbox": [{"x": 0.22241182625293732, "y": 0.7586812973022461}, {"x": 0.2326507419347763, "y": 0.7586812973022461}, {"x": 0.2326507419347763, "y": 0.7643955945968628}, {"x": 0.22241182625293732, "y": 0.7643955945968628}], "text": "N\n"}
{"page": 3, "bbox": [{"x": 0.20364050567150116, "y": 0.7731868028640747}, {"x": 0.21160408854484558, "y": 0.7731868028640747}, {"x": 0.21160408854484558, "y": 0.7775824069976807}, {"x": 0.20364050567150116, "y": 0.7775824069976807}], "text": "≈\n"}
{"page": 3, "bbox": [{"x": 0.14277587831020355, "y": 0.7679120898246765}, {"x": 0.42320817708969116, "y": 0.7679120898246765}, {"x": 0.42320817708969116, "y": 0.7841758131980896}, {"x": 0.14277587831020355, "y": 0.7841758131980896}], "text": "Prag(y|q) = [] Σ Pn(d|q)Po(Yi|q, d, Y1:i−1),\n"}
{"page": 3, "bbox": [{"x": 0.5176336765289307, "y": 0.6659340858459473}, {"x": 0.9141069650650024, "y": 0.6654945015907288}, {"x": 0.9141069650650024, "y": 0.8958241939544678}, {"x": 0.5176336765289307, "y": 0.8962637186050415}], "text": "4.2 Types of Documents\nIn our study, we categorize documents into four distinct types,\neach represented by a unique symbol, based on their relevance and\nrelationship to the queries:\n★ Gold Document. The gold document, identified by ★, refers\nto the original context in the NQ dataset, specifically the passage of\na Wikipedia page containing the answer and contextually relevant\nto a given query.\nRelevant Documents. Denoted by %, relevant documents are\npassages that, akin to the gold document, contain the correct answer\nand are contextually useful for answering the query. They provide\nadditional sources of information that are correct and pertinent to\nthe query. Notably, the gold document is a relevant document.\nDistracting Documents. Symbolized by 5, distracting docu-\nments are semantically similar to the query but do not contain the\n"}
{"page": 3, "bbox": [{"x": 0.22525596618652344, "y": 0.7859340906143188}, {"x": 0.27076223492622375, "y": 0.7872527241706848}, {"x": 0.2701933979988098, "y": 0.79340660572052}, {"x": 0.22525596618652344, "y": 0.7925274968147278}], "text": "i de Dr\n"}
{"page": 3, "bbox": [{"x": 0.08646188676357269, "y": 0.8017582297325134}, {"x": 0.4817974865436554, "y": 0.8017582297325134}, {"x": 0.4817974865436554, "y": 0.8958241939544678}, {"x": 0.08646188676357269, "y": 0.8958241939544678}], "text": "where pn (d|q) is the retrieval component that provides a (trun-\ncated) probability distribution for the top-scoring documents, and\nPo (Yi|q, d, Y1:i−1) is a probability distribution parameterized by\nthat generates a current token based on the previously generated\ntokens, the query, and the retrieved document; this role is filled by\nthe LLM. In the case of dense retrieval, the probability distribution\nfor the top-scoring documents may assume a functional form of\n"}
{"page": 4, "bbox": [{"x": 0.7656427621841431, "y": 0.07780219614505768}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.7656427621841431, "y": 0.08571428805589676}], "text": "Cuconasu and Trappolini, et al.\n"}
{"page": 4, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 4, "bbox": [{"x": 0.5443686246871948, "y": 0.11912088096141815}, {"x": 0.6990898847579956, "y": 0.117802195250988}, {"x": 0.6990898847579956, "y": 0.1287912130355835}, {"x": 0.5443686246871948, "y": 0.13010989129543304}], "text": "LLM Input - Only Gold ★\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.11032967269420624}, {"x": 0.48350396752357483, "y": 0.10945054888725281}, {"x": 0.48350396752357483, "y": 0.1595604419708252}, {"x": 0.08759954571723938, "y": 0.16043956577777863}], "text": "correct answer. They serve a crucial role in evaluating the genera-\ntor's proficiency in discerning between relevant and non-relevant\ninformation. In practice, these are the top-scoring retrieved docu-\nments that are not relevant.\n"}
{"page": 4, "bbox": [{"x": 0.5449374318122864, "y": 0.14637362957000732}, {"x": 0.8890784978866577, "y": 0.1454945057630539}, {"x": 0.8890784978866577, "y": 0.19692307710647583}, {"x": 0.5449374318122864, "y": 0.19780220091342926}], "text": "You are given a question and you MUST respond by EX-\nTRACTING the answer (max 5 tokens) from one of the pro-\nvided documents. If none of the documents contain the answer,\nrespond with NO-RES.\n"}
{"page": 4, "bbox": [{"x": 0.5449374318122864, "y": 0.20219780504703522}, {"x": 0.6234357357025146, "y": 0.20263735949993134}, {"x": 0.6234357357025146, "y": 0.2101098895072937}, {"x": 0.5449374318122864, "y": 0.20967033505439758}], "text": "Documents:\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.17494505643844604}, {"x": 0.4829351603984833, "y": 0.17582418024539948}, {"x": 0.4829351603984833, "y": 0.24263736605644226}, {"x": 0.08759954571723938, "y": 0.24175824224948883}], "text": "Random Documents. Indicated by , random documents are\nneither related to the query nor contain the answer. They are in-\nstrumental in assessing the model's ability to handle completely\nunrelated information. In practice, in our tests, we will randomly\nsample these documents from the corpus.\n"}
{"page": 4, "bbox": [{"x": 0.5432309508323669, "y": 0.21494504809379578}, {"x": 0.8868032097816467, "y": 0.2101098895072937}, {"x": 0.8879408240318298, "y": 0.24879120290279388}, {"x": 0.5443686246871948, "y": 0.25362637639045715}], "text": "Document [3](Title: Millennium Falcon) Han Solo won\nthe Millennium Falcon from Lando Calrissian in the card\ngame sabacc...\n"}
{"page": 4, "bbox": [{"x": 0.5443686246871948, "y": 0.26989009976387024}, {"x": 0.8879408240318298, "y": 0.269010990858078}, {"x": 0.8879408240318298, "y": 0.2914285659790039}, {"x": 0.5443686246871948, "y": 0.29230770468711853}], "text": "Question: who owned the millennium falcon be-\nfore han solo\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.257582426071167}, {"x": 0.48350396752357483, "y": 0.257582426071167}, {"x": 0.48350396752357483, "y": 0.3248351514339447}, {"x": 0.08703071624040604, "y": 0.3248351514339447}], "text": "In our analysis, the entire set of documents fetched by the retriever\nis represented by the symbol . This possibly encompasses all doc-\nument types - gold, relevant, distracting, or random — and serves\nto discuss the retrieval output in a generalized manner without\nspecifying individual document categories.\n"}
{"page": 4, "bbox": [{"x": 0.5443686246871948, "y": 0.29802197217941284}, {"x": 0.6564277410507202, "y": 0.2975824177265167}, {"x": 0.6564277410507202, "y": 0.30637362599372864}, {"x": 0.5443686246871948, "y": 0.30681318044662476}], "text": "Answer: Han Solo\n"}
{"page": 4, "bbox": [{"x": 0.12855517864227295, "y": 0.34285715222358704}, {"x": 0.29010239243507385, "y": 0.34241756796836853}, {"x": 0.29010239243507385, "y": 0.35164836049079895}, {"x": 0.12855517864227295, "y": 0.35208791494369507}], "text": "Document Retrieval\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.3437362611293793}, {"x": 0.10921501368284225, "y": 0.3437362611293793}, {"x": 0.10921501368284225, "y": 0.35164836049079895}, {"x": 0.08759954571723938, "y": 0.35164836049079895}], "text": "4.3\n"}
{"page": 4, "bbox": [{"x": 0.5170648694038391, "y": 0.34241756796836853}, {"x": 0.914675772190094, "y": 0.34285715222358704}, {"x": 0.914675772190094, "y": 0.4373626410961151}, {"x": 0.5170648694038391, "y": 0.436923086643219}], "text": "Figure 1: Example LLM input with an erroneous output, high-\nlighted in red. The input consists of an italicized task instruc-\ntion, followed by the context (documents), and the\nThe\nquery.\nLLM's response is marked under 'Answer'. The gold color\nhighlights both the gold document and the correct answer,\n\"Lando Calrissian”, indicating the expected source and con-\ntent of the accurate response.\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.3617582321166992}, {"x": 0.48236632347106934, "y": 0.3617582321166992}, {"x": 0.48236632347106934, "y": 0.4821977913379669}, {"x": 0.08703071624040604, "y": 0.4821977913379669}], "text": "Our methodology utilizes a two-step approach in line with a typical\nRAG setting, as explained in Section 3.2. As the first component,\nour experiments use Contriever [15], a BERT-based dense retriever,\nas the default retriever. It is trained without supervision using a\ncontrastive loss. To enhance the efficiency of similarity searches\nwithin our corpus, comprising about 21 million documents, we also\nemploy the FAISS IndexFlatIP indexing system [11]. The embedding\nof each document and query is obtained by averaging the hidden\nstate of the last layer of the model.\n"}
{"page": 4, "bbox": [{"x": 0.5193401575088501, "y": 0.4628571569919586}, {"x": 0.9118316173553467, "y": 0.46373626589775085}, {"x": 0.9118316173553467, "y": 0.48879119753837585}, {"x": 0.5193401575088501, "y": 0.4879120886325836}], "text": "only report on the latter, as while the behavior is consistent across\nboth, the instruct versions demonstrate superior performance.\n"}
{"page": 4, "bbox": [{"x": 0.08646188676357269, "y": 0.5002197623252869}, {"x": 0.2133105844259262, "y": 0.5019780397415161}, {"x": 0.2133105844259262, "y": 0.5125274658203125}, {"x": 0.08646188676357269, "y": 0.510769248008728}], "text": "4.4 LLM Input\n"}
{"page": 4, "bbox": [{"x": 0.5443686246871948, "y": 0.4936263859272003}, {"x": 0.9152445793151855, "y": 0.4936263859272003}, {"x": 0.9152445793151855, "y": 0.74021977186203}, {"x": 0.5443686246871948, "y": 0.74021977186203}], "text": "• Llama2. The 7B parameters version of the Llama2 family [50]\nshows state-of-the-art performance on most downstream\ntasks compared to models of the same size. It was trained\nwith a 4096 tokens context window and uses multi-query\nattention [45].\n• Falcon. Falcon 7B, the smallest model of the Falcon series, [1]\nwas trained on the RefinedWeb dataset [35], a large, filtered,\nand deduplicated corpus. Similarly to Llama2, it uses multi-\nquery attention, with a context length of 2048 tokens.\n● Phi-2. This is the smallest model used in this work (2.7B\nparameters). Despite its modest size, it achieves performance\ncomparable to the other models [16, 29], thanks to its pre-\ntraining on \"textbook-quality\" data. It has a context window\nof 2048 tokens.\n• MPT. This 7B parameters model uses ALiBi attention [36, 47]\nfor a virtually unlimited context length. In our experiments,\nto leverage the model's full potential, we set the limit to 2048\ntokens, i.e., the same used for the model's pre-training.\n"}
{"page": 4, "bbox": [{"x": 0.08646188676357269, "y": 0.5178021788597107}, {"x": 0.48350396752357483, "y": 0.5178021788597107}, {"x": 0.48350396752357483, "y": 0.7375824451446533}, {"x": 0.08646188676357269, "y": 0.7375824451446533}], "text": "Upon receiving a query, the retriever selects the top-k documents\nfrom the corpus according to a given similarity measure. These\ndocuments, in conjunction with the task instruction and the query,\nconstitute the input for the LLM to generate a response. The NQ-\nopen dataset was structured to include only those queries whose\nanswers consist of no more than five tokens [27]. Consequently,\nthe LLM is tasked with extracting a query response, confined to a\nmaximum of five tokens, from the provided documents. The input is\nencoded into a prompt, whose template is shown in Figure 1, begin-\nning with the task instruction, presented in italics for clarity. This\nis followed by the context, which comprises the selected documents\nfollowed by the query string. This prompt design aligns with the\nmethodological approach outlined in [30]. While the composition\nof the context will vary according to the single experiment, the\ninstruction will always be placed at the beginning of the prompt\nand the query always at the end.\n"}
{"page": 4, "bbox": [{"x": 0.12855517864227295, "y": 0.7560439705848694}, {"x": 0.22810012102127075, "y": 0.7556043863296509}, {"x": 0.22810012102127075, "y": 0.7648351788520813}, {"x": 0.12855517864227295, "y": 0.765274703502655}], "text": "LLMs Tested\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.7569230794906616}, {"x": 0.10864619165658951, "y": 0.7569230794906616}, {"x": 0.10864619165658951, "y": 0.7648351788520813}, {"x": 0.08703071624040604, "y": 0.7648351788520813}], "text": "4.5\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.7560439705848694}, {"x": 0.5420932769775391, "y": 0.7564834952354431}, {"x": 0.5420932769775391, "y": 0.7665933966636658}, {"x": 0.5187713503837585, "y": 0.766153872013092}], "text": "4.6\n"}
{"page": 4, "bbox": [{"x": 0.5585892796516418, "y": 0.7560439705848694}, {"x": 0.6353811025619507, "y": 0.7569230794906616}, {"x": 0.6353811025619507, "y": 0.767472505569458}, {"x": 0.5585892796516418, "y": 0.7665933966636658}], "text": "Accuracy\n"}
{"page": 4, "bbox": [{"x": 0.08589305728673935, "y": 0.772747278213501}, {"x": 0.4817974865436554, "y": 0.7723076939582825}, {"x": 0.4817974865436554, "y": 0.8940659165382385}, {"x": 0.08589305728673935, "y": 0.894505500793457}], "text": "We consider several LLMs in our experiments. Consistently across\nall models, we adopt a greedy generation approach with a maxi-\nmum response length of 15 tokens. Acknowledging the constraints\nimposed by memory and computational resources, we have im-\nplemented a model quantization strategy, reducing all models to a\n4-bit representation. Besides the above prompt, the models are not\nprovided with additional exemplars for few-shot learning, which,\nwhile of interest, is outside the scope of this paper. We conduct tests\non both the base and the instruct versions of the LLMs. However, we\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.7745054960250854}, {"x": 0.9135380983352661, "y": 0.7740659117698669}, {"x": 0.9135380983352661, "y": 0.8953846096992493}, {"x": 0.5187713503837585, "y": 0.8958241939544678}], "text": "The NQ-open dataset allows a range of potential answers for each\nquery. Frequently, these answers are different variants of the same\nconcept (e.g., \"President D. Roosevelt\" or \"President Roosevelt\"),\nwhile in some cases, a single query may accept multiple distinct\ncorrect answers. To evaluate the accuracy of responses generated\nby LLMs, we use an assessment technique in line with [18, 30].\nThis methodology examines whether at least one of the predefined\ncorrect answers is contained within the response produced by the\nLLM. We measure the correctness of the LLM's responses as either\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.07780219614505768}, {"x": 0.361774742603302, "y": 0.07868131995201111}, {"x": 0.361774742603302, "y": 0.08703296631574631}, {"x": 0.08703071624040604, "y": 0.08615384250879288}], "text": "The Power of Noise: Redefining Retrieval for RAG Systems\n"}
{"page": 5, "bbox": [{"x": 0.6780432462692261, "y": 0.07868131995201111}, {"x": 0.912400484085083, "y": 0.07868131995201111}, {"x": 0.912400484085083, "y": 0.08615384250879288}, {"x": 0.6780432462692261, "y": 0.08615384250879288}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 5, "bbox": [{"x": 0.08646188676357269, "y": 0.10989011079072952}, {"x": 0.4829351603984833, "y": 0.11032967269420624}, {"x": 0.4829351603984833, "y": 0.2571428716182709}, {"x": 0.08646188676357269, "y": 0.25670328736305237}], "text": "accurate or inaccurate based on the presence of the answer in a\nbinary fashion. Nevertheless, this evaluation strategy is not without\nchallenges. A principal issue arises in determining response cor-\nrectness, particularly in instances involving date representations\nor varying phrasings conveying identical meanings. For example,\nif the LLM generates “Roosevelt” in response to a query where the\nestablished correct answer is \"President Roosevelt\", the response\nwould be deemed incorrect under our current evaluation schema.\nRecognizing this limitation, we acknowledge the necessity for a\nmore advanced analysis of answer variations, which we leave to\nfuture research.\n"}
{"page": 5, "bbox": [{"x": 0.08816837519407272, "y": 0.2751648426055908}, {"x": 0.19112628698349, "y": 0.2751648426055908}, {"x": 0.19112628698349, "y": 0.28395605087280273}, {"x": 0.08816837519407272, "y": 0.28395605087280273}], "text": "5 RESULTS\n"}
{"page": 5, "bbox": [{"x": 0.08418657630681992, "y": 0.2936263680458069}, {"x": 0.4829351603984833, "y": 0.29318681359291077}, {"x": 0.48350396752357483, "y": 0.5103296637535095}, {"x": 0.08475540578365326, "y": 0.510769248008728}], "text": "Studying the characteristics of optimal prompts for RAG systems\ncorresponds to answering our research question (RQ): \"What char-\nacteristics are desirable in a retriever to optimize prompt construction\nfor RAG systems in order to increase the LLM effectiveness?\". More\nspecifically, we focus on three essential elements of the configura-\ntion: type, number, and positioning of the documents, and for each,\nwe test various prompt combinations. To facilitate the understand-\ning of our experimental setup, we employ a streamlined schema for\nrepresenting the composition of prompts via the following symbols:\n[I,,,,, Q]. The task instruction (I) and the query (Q) are\nconsistently positioned at the beginning and end, respectively. The\nmiddle section varies and represents different contextual elements\n- in this instance, these are gold, relevant, distracting, and random,\nappearing in that specific sequence. Additionally, the number of\ncontextual documents is a variable in its own right and will be\nreported in the results tables below.\n"}
{"page": 5, "bbox": [{"x": 0.5187713503837585, "y": 0.11032967269420624}, {"x": 0.9152445793151855, "y": 0.11032967269420624}, {"x": 0.914675772190094, "y": 0.8123077154159546}, {"x": 0.5182024836540222, "y": 0.8123077154159546}], "text": "In our first set of experiments, we use a selection of 10K queries\nfrom the training set of the NQ-open dataset and assume an oracle\nsetup in which the gold document for the query is known. To this\neffect, we add to the gold document a set of distracting documents,\ni.e., documents with high retrieval scores but not containing the\nanswer, in order to measure their impact on the system; schemat-\nically [I,,,Q]. Figure 2 shows an example of this setup's\nvisualization. Results of this experiment are shown in Table 1 (far,\nmid, and near relate to the distance between the gold document\nand the query; more details in the following sub-section). A crit-\nical observation emerging from this analysis is a clear pattern of\nprogressive accuracy degradation as the number of distracting doc-\numents included in the context increases. This was observed across\nall LLMs, with accuracy deteriorating by more than 0.38 (-67%)\nin some cases. Even more importantly, adding just one distracting\ndocument causes a sharp reduction in accuracy, with peaks of 0.24\n(-25%), as can be seen by comparing the row with 0 distracting\ndocuments (only gold scenario, as seen in Figure 1) with that of 1\ndistracting document. This experiment highlights a critical issue\nfor RAG systems, particularly in real-world IR settings where re-\nlated but non-answer-containing documents are commonplace. Our\nempirical analysis suggests that introducing semantically aligned\nyet non-relevant documents adds a layer of complexity, potentially\nmisguiding LLMs away from the correct response. A visual ex-\nplanation can be seen in Figure 3, which illustrates the attention\nscores within the prompt's context for a specific example in which\nthe LLM incorrectly answers. This figure highlights the model's\ndisproportionate focus on a distracting document (leftmost) at the\nexpense of the gold document (rightmost), likely contributing to\nthe erroneous response. Note that for consistency of results across\nLLMs, we need to account for their various input token capabilities:\nLlama2 can process up to 4096 tokens, but other models are lim-\nited to 2048 tokens. This led to the exclusion of evaluations with a\nhigher number of distracting documents (namely greater than 10)\nas reflected by the empty values in the tables.\nIn addition, we wanted to verify that our results were not overly\ndependent on the type of dense retrieval system we used. We\nwanted, in particular, to check whether another dense retriever\nspecifically trained on \"hard negatives\" would better distinguish\nbetween directly relevant and distracting documents, potentially\nleading to different results. To explore this hypothesis, we used\nADORE [59], a state-of-the-art retriever trained with “dynamic\nhard negatives\", to select the distracting documents. In scenarios\nwith 1, 2, and 4 distracting documents in the [I,, ★, Q] setting\nwith Llama2, we obtain an accuracy of 0.4068, 0.3815, and 0.3626,\nrespectively. This is significantly lower than the baseline accuracy\nof 0.5642, where no distracting documents were included, and than\nthe results obtained with Contriever in the same settings. We con-\nclude from this that distinguishing between relevant and distracting\ninformation is a hard problem that cannot be mitigated simply by\nchanging the dense retrieval method at this stage.\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.5279120802879333}, {"x": 0.3964732587337494, "y": 0.5270329713821411}, {"x": 0.3964732587337494, "y": 0.5393406748771667}, {"x": 0.08703071624040604, "y": 0.540219783782959}], "text": "5.1 Impact of Distracting Documents\n"}
{"page": 5, "bbox": [{"x": 0.11262798309326172, "y": 0.5749450325965881}, {"x": 0.3310580253601074, "y": 0.5736263990402222}, {"x": 0.3310580253601074, "y": 0.5846154093742371}, {"x": 0.11262798309326172, "y": 0.585934042930603}], "text": "LLM Input - Distracting and Gold\n"}
{"page": 5, "bbox": [{"x": 0.11319681257009506, "y": 0.6013186573982239}, {"x": 0.2155858874320984, "y": 0.6013186573982239}, {"x": 0.2155858874320984, "y": 0.6232966780662537}, {"x": 0.11319681257009506, "y": 0.6232966780662537}], "text": "Task Instruction...\nDocuments:\n"}
{"page": 5, "bbox": [{"x": 0.11035267263650894, "y": 0.6285714507102966}, {"x": 0.45790672302246094, "y": 0.6259340643882751}, {"x": 0.45961320400238037, "y": 0.7476922869682312}, {"x": 0.11205916106700897, "y": 0.7503296732902527}], "text": "Document [1](Title: Han Solo) Before the events of the\nfilm, he and Chewbacca had lost the \"Millennium Falcon\"\nto thieves, but they reclaim the ship after it...\nDocument [2] (Title: Millennium Falcon) The \"Falcon❞ has\nbeen depicted many times in the franchise, and ownership\nhas changed several times...\nDocument [3] (Title: Millennium Falcon) Han Solo won\nthe Millennium Falcon from Lando Calrissian in the card\ngame sabacc...\n"}
{"page": 5, "bbox": [{"x": 0.11262798309326172, "y": 0.766153872013092}, {"x": 0.45676904916763306, "y": 0.766153872013092}, {"x": 0.45676904916763306, "y": 0.7894505262374878}, {"x": 0.11262798309326172, "y": 0.7894505262374878}], "text": "Question: who owned the millennium falcon be-\nfore han solo\n"}
{"page": 5, "bbox": [{"x": 0.11262798309326172, "y": 0.795604407787323}, {"x": 0.2246871441602707, "y": 0.7951648235321045}, {"x": 0.2246871441602707, "y": 0.8030769228935242}, {"x": 0.11262798309326172, "y": 0.8035165071487427}], "text": "Answer: Han Solo\n"}
{"page": 5, "bbox": [{"x": 0.08759954571723938, "y": 0.8386813402175903}, {"x": 0.48236632347106934, "y": 0.8395604491233826}, {"x": 0.48236632347106934, "y": 0.8918681144714355}, {"x": 0.08759954571723938, "y": 0.8909890055656433}], "text": "Figure 2: Example LLM input with an erroneous output, high-\nlighted in red. The context of the prompt is composed of\ndistracting documents and the gold near the query. The task\ninstruction is as in Figure 1.\n"}
{"page": 5, "bbox": [{"x": 0.5182024836540222, "y": 0.8518681526184082}, {"x": 0.9129692912101746, "y": 0.8523076772689819}, {"x": 0.9129692912101746, "y": 0.89670330286026}, {"x": 0.5182024836540222, "y": 0.8962637186050415}], "text": "5.2 Impact of Gold Positioning\nWe conduct here another experiment where we systematically shift\nthe position of the gold document within the context to study its\n"}
{"page": 6, "bbox": [{"x": 0.7656427621841431, "y": 0.07780219614505768}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.7656427621841431, "y": 0.08571428805589676}], "text": "Cuconasu and Trappolini, et al.\n"}
{"page": 6, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 6, "bbox": [{"x": 0.08759954571723938, "y": 0.1063736230134964}, {"x": 0.9135380983352661, "y": 0.1063736230134964}, {"x": 0.9135380983352661, "y": 0.18681319057941437}, {"x": 0.08759954571723938, "y": 0.18681319057941437}], "text": "Table 1: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ★ and a varying number\nof distracting documents. The table illustrates how the inclusion of an increasing number of distracting documents affects\nLLM's performance. Scenarios where the prompt exceeded the model's input limit, leading to potential data truncation, are not\nincluded ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document\nscenario [I, ★, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores\nfor the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).\n"}
{"page": 6, "bbox": [{"x": 0.6985210180282593, "y": 0.20835164189338684}, {"x": 0.8196814656257629, "y": 0.20923076570034027}, {"x": 0.8196814656257629, "y": 0.22021977603435516}, {"x": 0.6985210180282593, "y": 0.21934065222740173}], "text": "Near [I,,,Q]\n"}
{"page": 6, "bbox": [{"x": 0.22923776507377625, "y": 0.20835164189338684}, {"x": 0.3395904302597046, "y": 0.20879121124744415}, {"x": 0.3395904302597046, "y": 0.22021977603435516}, {"x": 0.22923776507377625, "y": 0.21978022158145905}], "text": "Far - [I,,,Q]\n"}
{"page": 6, "bbox": [{"x": 0.45563140511512756, "y": 0.20791208744049072}, {"x": 0.5915813446044922, "y": 0.20879121124744415}, {"x": 0.5915813446044922, "y": 0.22065934538841248}, {"x": 0.45563140511512756, "y": 0.21978022158145905}], "text": "Mid - [I, 3,★, 3, Q]\n"}
{"page": 6, "bbox": [{"x": 0.7343572378158569, "y": 0.2131868153810501}, {"x": 0.7394766807556152, "y": 0.2131868153810501}, {"x": 0.7394766807556152, "y": 0.2153846174478531}, {"x": 0.7343572378158569, "y": 0.2153846174478531}], "text": "-\n"}
{"page": 6, "bbox": [{"x": 0.13253697752952576, "y": 0.22857142984867096}, {"x": 0.21729238331317902, "y": 0.2294505536556244}, {"x": 0.21729238331317902, "y": 0.2382417619228363}, {"x": 0.13253697752952576, "y": 0.23736263811588287}], "text": "# Llama2\n"}
{"page": 6, "bbox": [{"x": 0.5381115078926086, "y": 0.2294505536556244}, {"x": 0.5699658989906311, "y": 0.2298901081085205}, {"x": 0.5699658989906311, "y": 0.23736263811588287}, {"x": 0.5381115078926086, "y": 0.23692308366298676}], "text": "Phi-2\n"}
{"page": 6, "bbox": [{"x": 0.2428896427154541, "y": 0.2298901081085205}, {"x": 0.27189987897872925, "y": 0.2298901081085205}, {"x": 0.27189987897872925, "y": 0.23736263811588287}, {"x": 0.2428896427154541, "y": 0.23736263811588287}], "text": "MPT\n"}
{"page": 6, "bbox": [{"x": 0.7184300422668457, "y": 0.22857142984867096}, {"x": 0.8668941855430603, "y": 0.2298901081085205}, {"x": 0.8668941855430603, "y": 0.23868131637573242}, {"x": 0.7184300422668457, "y": 0.23736263811588287}], "text": "MPT Phi-2 Falcon\n"}
{"page": 6, "bbox": [{"x": 0.15017065405845642, "y": 0.25010988116264343}, {"x": 0.21501706540584564, "y": 0.25054946541786194}, {"x": 0.2144482433795929, "y": 0.28439560532569885}, {"x": 0.14960181713104248, "y": 0.28395605087280273}], "text": "0 0.5642\n1 0.4586\n0.3455\n"}
{"page": 6, "bbox": [{"x": 0.233788400888443, "y": 0.22857142984867096}, {"x": 0.5159271955490112, "y": 0.22901098430156708}, {"x": 0.5159271955490112, "y": 0.31252747774124146}, {"x": 0.233788400888443, "y": 0.31208792328834534}], "text": "Phi-2 Falcon Llama2 MPT\n0.2148 0.4438 0.4330 0.5642 0.2148\n0.1976 0.3585 0.3469 no-mid no-mid\n0.1913 0.3430 0.3246 0.3322 0.1802\n0.2209* 0.3019 0.2670\n0.1775\n0.2171* 0.2943 0.2392\n"}
{"page": 6, "bbox": [{"x": 0.47610920667648315, "y": 0.2294505536556244}, {"x": 0.8725824952125549, "y": 0.23032967746257782}, {"x": 0.8720136284828186, "y": 0.3270329535007477}, {"x": 0.4755403995513916, "y": 0.32615384459495544}], "text": "Falcon Llama2\n0.4438 0.4330 0.5642 0.2148 0.4438 0.4330\nno-mid no-mid 0.4283 0.1791 0.4227 0.3602\n0.3375 0.2823 0.3974 0.2002 0.3975 0.3111\n0.2885 0.2378 0.3795 0.2059* 0.3701 0.2736\n0.1424 0.2625 0.2103 0.3880 0.1892 0.3623 0.2656\n0.1002 0.2360 0.1745 0.3748 0.1944 0.3423 0.2424\n"}
{"page": 6, "bbox": [{"x": 0.15017065405845642, "y": 0.2778021991252899}, {"x": 0.15699659287929535, "y": 0.2778021991252899}, {"x": 0.15699659287929535, "y": 0.28395605087280273}, {"x": 0.15017065405845642, "y": 0.28395605087280273}], "text": "2\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.2909890115261078}, {"x": 0.21387940645217896, "y": 0.2909890115261078}, {"x": 0.21387940645217896, "y": 0.2975824177265167}, {"x": 0.1769055724143982, "y": 0.2975824177265167}], "text": "0.2745\n"}
{"page": 6, "bbox": [{"x": 0.4163822531700134, "y": 0.29054945707321167}, {"x": 0.45278725028038025, "y": 0.2909890115261078}, {"x": 0.45278725028038025, "y": 0.29802197217941284}, {"x": 0.4163822531700134, "y": 0.2975824177265167}], "text": "0.2857\n"}
{"page": 6, "bbox": [{"x": 0.15017065405845642, "y": 0.2914285659790039}, {"x": 0.15813423693180084, "y": 0.2914285659790039}, {"x": 0.15813423693180084, "y": 0.2975824177265167}, {"x": 0.15017065405845642, "y": 0.2975824177265167}], "text": "4\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.3050549328327179}, {"x": 0.21274174749851227, "y": 0.30417582392692566}, {"x": 0.21274174749851227, "y": 0.3112087845802307}, {"x": 0.1769055724143982, "y": 0.31208792328834534}], "text": "0.2898\n"}
{"page": 6, "bbox": [{"x": 0.15017065405845642, "y": 0.3054945170879364}, {"x": 0.15813423693180084, "y": 0.3054945170879364}, {"x": 0.15813423693180084, "y": 0.31164833903312683}, {"x": 0.15017065405845642, "y": 0.31164833903312683}], "text": "6\n"}
{"page": 6, "bbox": [{"x": 0.4158134162425995, "y": 0.3054945170879364}, {"x": 0.45278725028038025, "y": 0.3054945170879364}, {"x": 0.45278725028038025, "y": 0.31164833903312683}, {"x": 0.4158134162425995, "y": 0.31164833903312683}], "text": "0.2698\n"}
{"page": 6, "bbox": [{"x": 0.14960181713104248, "y": 0.3191208839416504}, {"x": 0.1564277559518814, "y": 0.3191208839416504}, {"x": 0.1564277559518814, "y": 0.3248351514339447}, {"x": 0.14960181713104248, "y": 0.3248351514339447}], "text": "8\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.3186813294887543}, {"x": 0.21387940645217896, "y": 0.3186813294887543}, {"x": 0.21387940645217896, "y": 0.3252747356891632}, {"x": 0.1769055724143982, "y": 0.3252747356891632}], "text": "0.2643\n"}
{"page": 6, "bbox": [{"x": 0.4163822531700134, "y": 0.3186813294887543}, {"x": 0.4522184431552887, "y": 0.3186813294887543}, {"x": 0.4522184431552887, "y": 0.3252747356891632}, {"x": 0.4163822531700134, "y": 0.3252747356891632}], "text": "0.2268\n"}
{"page": 6, "bbox": [{"x": 0.23606370389461517, "y": 0.3186813294887543}, {"x": 0.39362913370132446, "y": 0.31780219078063965}, {"x": 0.39362913370132446, "y": 0.32615384459495544}, {"x": 0.23606370389461517, "y": 0.3270329535007477}], "text": "0.2077* 0.2513 0.1878\n"}
{"page": 6, "bbox": [{"x": 0.4158134162425995, "y": 0.3327472507953644}, {"x": 0.4522184431552887, "y": 0.33230769634246826}, {"x": 0.4522184431552887, "y": 0.3389011025428772}, {"x": 0.4158134162425995, "y": 0.3393406569957733}], "text": "0.2180\n"}
{"page": 6, "bbox": [{"x": 0.6552901268005371, "y": 0.33186814188957214}, {"x": 0.69112628698349, "y": 0.3327472507953644}, {"x": 0.69112628698349, "y": 0.34021976590156555}, {"x": 0.6552901268005371, "y": 0.3393406569957733}], "text": "0.3716\n"}
{"page": 6, "bbox": [{"x": 0.14391353726387024, "y": 0.3331868052482605}, {"x": 0.15585893392562866, "y": 0.3331868052482605}, {"x": 0.15585893392562866, "y": 0.3393406569957733}, {"x": 0.14391353726387024, "y": 0.3393406569957733}], "text": "10\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.3327472507953644}, {"x": 0.21387940645217896, "y": 0.3327472507953644}, {"x": 0.21387940645217896, "y": 0.33978021144866943}, {"x": 0.1769055724143982, "y": 0.33978021144866943}], "text": "0.2537\n"}
{"page": 6, "bbox": [{"x": 0.6552901268005371, "y": 0.34637361764907837}, {"x": 0.6905574798583984, "y": 0.34637361764907837}, {"x": 0.6905574798583984, "y": 0.3529670238494873}, {"x": 0.6552901268005371, "y": 0.3529670238494873}], "text": "0.3991\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.34593406319618225}, {"x": 0.21387940645217896, "y": 0.34637361764907837}, {"x": 0.21387940645217896, "y": 0.3534066081047058}, {"x": 0.1769055724143982, "y": 0.3529670238494873}], "text": "0.2688\n"}
{"page": 6, "bbox": [{"x": 0.14391353726387024, "y": 0.3468131721019745}, {"x": 0.15699659287929535, "y": 0.3468131721019745}, {"x": 0.15699659287929535, "y": 0.3529670238494873}, {"x": 0.14391353726387024, "y": 0.3529670238494873}], "text": "12\n"}
{"page": 6, "bbox": [{"x": 0.4158134162425995, "y": 0.3468131721019745}, {"x": 0.45278725028038025, "y": 0.3468131721019745}, {"x": 0.45278725028038025, "y": 0.3529670238494873}, {"x": 0.4158134162425995, "y": 0.3529670238494873}], "text": "0.2382\n"}
{"page": 6, "bbox": [{"x": 0.14391353726387024, "y": 0.360879123210907}, {"x": 0.15699659287929535, "y": 0.360879123210907}, {"x": 0.15699659287929535, "y": 0.3665934205055237}, {"x": 0.14391353726387024, "y": 0.3665934205055237}], "text": "14\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.36000001430511475}, {"x": 0.21387940645217896, "y": 0.36043956875801086}, {"x": 0.21387940645217896, "y": 0.3674725294113159}, {"x": 0.1769055724143982, "y": 0.3670329749584198}], "text": "0.2583\n"}
{"page": 6, "bbox": [{"x": 0.4158134162425995, "y": 0.360879123210907}, {"x": 0.45278725028038025, "y": 0.360879123210907}, {"x": 0.45278725028038025, "y": 0.3670329749584198}, {"x": 0.4158134162425995, "y": 0.3670329749584198}], "text": "0.2280\n"}
{"page": 6, "bbox": [{"x": 0.6552901268005371, "y": 0.360879123210907}, {"x": 0.69112628698349, "y": 0.360879123210907}, {"x": 0.69112628698349, "y": 0.3670329749584198}, {"x": 0.6552901268005371, "y": 0.3670329749584198}], "text": "0.4118\n"}
{"page": 6, "bbox": [{"x": 0.4158134162425995, "y": 0.37406593561172485}, {"x": 0.45278725028038025, "y": 0.37362638115882874}, {"x": 0.45278725028038025, "y": 0.38021978735923767}, {"x": 0.4158134162425995, "y": 0.3806593418121338}], "text": "0.2024\n"}
{"page": 6, "bbox": [{"x": 0.14448235929012299, "y": 0.37450549006462097}, {"x": 0.1564277559518814, "y": 0.37450549006462097}, {"x": 0.1564277559518814, "y": 0.3806593418121338}, {"x": 0.14448235929012299, "y": 0.3806593418121338}], "text": "16\n"}
{"page": 6, "bbox": [{"x": 0.6547212600708008, "y": 0.37450549006462097}, {"x": 0.6922639608383179, "y": 0.37450549006462097}, {"x": 0.6922639608383179, "y": 0.3806593418121338}, {"x": 0.6547212600708008, "y": 0.3806593418121338}], "text": "0.3889\n"}
{"page": 6, "bbox": [{"x": 0.1769055724143982, "y": 0.37450549006462097}, {"x": 0.2133105844259262, "y": 0.3749450445175171}, {"x": 0.2133105844259262, "y": 0.3810988962650299}, {"x": 0.1769055724143982, "y": 0.3806593418121338}], "text": "0.2413\n"}
{"page": 6, "bbox": [{"x": 0.4158134162425995, "y": 0.38813185691833496}, {"x": 0.45278725028038025, "y": 0.38813185691833496}, {"x": 0.45278725028038025, "y": 0.3951648473739624}, {"x": 0.4158134162425995, "y": 0.3951648473739624}], "text": "0.1795\n"}
{"page": 6, "bbox": [{"x": 0.6552901268005371, "y": 0.38857144117355347}, {"x": 0.69112628698349, "y": 0.38857144117355347}, {"x": 0.69112628698349, "y": 0.3947252631187439}, {"x": 0.6552901268005371, "y": 0.3947252631187439}], "text": "0.3781\n"}
{"page": 6, "bbox": [{"x": 0.14448235929012299, "y": 0.38857144117355347}, {"x": 0.2133105844259262, "y": 0.3890109956264496}, {"x": 0.2133105844259262, "y": 0.3951648473739624}, {"x": 0.14448235929012299, "y": 0.3947252631187439}], "text": "18 0.2348\n"}
{"page": 6, "bbox": [{"x": 0.08703071624040604, "y": 0.4180219769477844}, {"x": 0.912400484085083, "y": 0.4180219769477844}, {"x": 0.912400484085083, "y": 0.4984615445137024}, {"x": 0.08703071624040604, "y": 0.4984615445137024}], "text": "Table 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ★ and a varying number\nof random documents. Surprisingly, increasing the number of random documents in the Near setting improves LLM's\nperformance. Scenarios where the prompt exceeded the model's input limit, leading to potential data truncation, are not\nincluded ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document\nscenario [I, ✶, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores\nfor the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).\n"}
{"page": 6, "bbox": [{"x": 0.7315130829811096, "y": 0.5248351693153381}, {"x": 0.7360637187957764, "y": 0.5248351693153381}, {"x": 0.7360637187957764, "y": 0.5265933871269226}, {"x": 0.7315130829811096, "y": 0.5265933871269226}], "text": "-\n"}
{"page": 6, "bbox": [{"x": 0.4562002420425415, "y": 0.5199999809265137}, {"x": 0.5870307087898254, "y": 0.5208791494369507}, {"x": 0.5870307087898254, "y": 0.5318681597709656}, {"x": 0.4562002420425415, "y": 0.5309889912605286}], "text": "Mid [I,,, B, Q]\n"}
{"page": 6, "bbox": [{"x": 0.5369738340377808, "y": 0.5406593680381775}, {"x": 0.5688282251358032, "y": 0.5410988926887512}, {"x": 0.5688282251358032, "y": 0.5485714077949524}, {"x": 0.5369738340377808, "y": 0.5481318831443787}], "text": "Phi-2\n"}
{"page": 6, "bbox": [{"x": 0.30034130811691284, "y": 0.5410988926887512}, {"x": 0.33276450634002686, "y": 0.5410988926887512}, {"x": 0.33276450634002686, "y": 0.5485714077949524}, {"x": 0.30034130811691284, "y": 0.5485714077949524}], "text": "Phi-2\n"}
{"page": 6, "bbox": [{"x": 0.17918089032173157, "y": 0.540219783782959}, {"x": 0.22184300422668457, "y": 0.5410988926887512}, {"x": 0.22184300422668457, "y": 0.5494505763053894}, {"x": 0.17918089032173157, "y": 0.5485714077949524}], "text": "Llama2\n"}
{"page": 6, "bbox": [{"x": 0.13936291635036469, "y": 0.5415384769439697}, {"x": 0.1473265141248703, "y": 0.5410988926887512}, {"x": 0.14789533615112305, "y": 0.5485714077949524}, {"x": 0.13993173837661743, "y": 0.5490109920501709}], "text": "#\n"}
{"page": 6, "bbox": [{"x": 0.15529009699821472, "y": 0.5613186955451965}, {"x": 0.16211603581905365, "y": 0.5613186955451965}, {"x": 0.16211603581905365, "y": 0.5679121017456055}, {"x": 0.15529009699821472, "y": 0.5679121017456055}], "text": "0\n"}
{"page": 6, "bbox": [{"x": 0.15585893392562866, "y": 0.5753846168518066}, {"x": 0.16097839176654816, "y": 0.5753846168518066}, {"x": 0.16097839176654816, "y": 0.5815384387969971}, {"x": 0.15585893392562866, "y": 0.5815384387969971}], "text": "1\n"}
{"page": 6, "bbox": [{"x": 0.18031854927539825, "y": 0.5195604562759399}, {"x": 0.8674630522727966, "y": 0.5195604562759399}, {"x": 0.8674630522727966, "y": 0.6509889960289001}, {"x": 0.18031854927539825, "y": 0.6509889960289001}], "text": "Far - [I, ★, Œ, Q]\nNear [I,,,Q]\nMPT\nFalcon Llama2 MPT\nFalcon Llama2 MPT Phi-2 Falcon\n0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330\n0.4733 0.2447 0.4329 0.4035 no-mid no-mid no-mid no-mid 0.4862 0.2125* 0.4587 0.4091\n0.3776 0.2639 0.4249 0.3805 0.3928 0.2584 0.4293 0.3612 0.5032 0.2660 0.4614 0.3912\n0.3109 0.2933 0.4091 0.3468 0.3998 0.2577 0.3985 0.3462 0.5221 0.2930 0.4311 0.3949\n0.3547 0.3036 0.4130 0.3250 0.4138 0.2265 0.3891 0.3196 0.5681* 0.2890 0.4388 0.3908\n0.3106 0.3039 0.3812 0.2543 0.3734 0.1566 0.3596 0.2767 0.5609* 0.2911 0.4258 0.3704\n0.3390\n0.3675\n0.5579*\n"}
{"page": 6, "bbox": [{"x": 0.15529009699821472, "y": 0.5894505381584167}, {"x": 0.1615472137928009, "y": 0.5894505381584167}, {"x": 0.1615472137928009, "y": 0.595604419708252}, {"x": 0.15529009699821472, "y": 0.595604419708252}], "text": "2\n"}
{"page": 6, "bbox": [{"x": 0.15529009699821472, "y": 0.6030769348144531}, {"x": 0.16211603581905365, "y": 0.6030769348144531}, {"x": 0.16211603581905365, "y": 0.6092307567596436}, {"x": 0.15529009699821472, "y": 0.6092307567596436}], "text": "4\n"}
{"page": 6, "bbox": [{"x": 0.15529009699821472, "y": 0.616263747215271}, {"x": 0.16211603581905365, "y": 0.616263747215271}, {"x": 0.16211603581905365, "y": 0.6228571534156799}, {"x": 0.15529009699821472, "y": 0.6228571534156799}], "text": "6\n"}
{"page": 6, "bbox": [{"x": 0.15529009699821472, "y": 0.6307692527770996}, {"x": 0.1615472137928009, "y": 0.6307692527770996}, {"x": 0.1615472137928009, "y": 0.6364834904670715}, {"x": 0.15529009699821472, "y": 0.6364834904670715}], "text": "8\n"}
{"page": 6, "bbox": [{"x": 0.14903299510478973, "y": 0.6443955898284912}, {"x": 0.16097839176654816, "y": 0.6443955898284912}, {"x": 0.16097839176654816, "y": 0.6505494713783264}, {"x": 0.14903299510478973, "y": 0.6505494713783264}], "text": "10\n"}
{"page": 6, "bbox": [{"x": 0.414675772190094, "y": 0.6571428775787354}, {"x": 0.4505119323730469, "y": 0.6575824022293091}, {"x": 0.4505119323730469, "y": 0.6650549173355103}, {"x": 0.414675772190094, "y": 0.6646153926849365}], "text": "0.3641\n"}
{"page": 6, "bbox": [{"x": 0.6535836458206177, "y": 0.6575824022293091}, {"x": 0.6905574798583984, "y": 0.6571428775787354}, {"x": 0.6905574798583984, "y": 0.6646153926849365}, {"x": 0.6535836458206177, "y": 0.6650549173355103}], "text": "0.5836\n"}
{"page": 6, "bbox": [{"x": 0.14903299510478973, "y": 0.6584615111351013}, {"x": 0.16040955483913422, "y": 0.6584615111351013}, {"x": 0.16040955483913422, "y": 0.664175808429718}, {"x": 0.14903299510478973, "y": 0.664175808429718}], "text": "12\n"}
{"page": 6, "bbox": [{"x": 0.18145619332790375, "y": 0.6580219864845276}, {"x": 0.21899886429309845, "y": 0.6580219864845276}, {"x": 0.21899886429309845, "y": 0.6646153926849365}, {"x": 0.18145619332790375, "y": 0.6646153926849365}], "text": "0.3736\n"}
{"page": 6, "bbox": [{"x": 0.414675772190094, "y": 0.6716483235359192}, {"x": 0.45164960622787476, "y": 0.6716483235359192}, {"x": 0.45164960622787476, "y": 0.6782417297363281}, {"x": 0.414675772190094, "y": 0.6782417297363281}], "text": "0.3372\n"}
{"page": 6, "bbox": [{"x": 0.1820250302553177, "y": 0.6712087988853455}, {"x": 0.21786120533943176, "y": 0.6712087988853455}, {"x": 0.21786120533943176, "y": 0.6786813139915466}, {"x": 0.1820250302553177, "y": 0.6786813139915466}], "text": "0.3527\n"}
{"page": 6, "bbox": [{"x": 0.14960181713104248, "y": 0.6720879077911377}, {"x": 0.1615472137928009, "y": 0.6720879077911377}, {"x": 0.1615472137928009, "y": 0.6782417297363281}, {"x": 0.14960181713104248, "y": 0.6782417297363281}], "text": "14\n"}
{"page": 6, "bbox": [{"x": 0.6518771052360535, "y": 0.6720879077911377}, {"x": 0.6922639608383179, "y": 0.6720879077911377}, {"x": 0.6922639608383179, "y": 0.6786813139915466}, {"x": 0.6518771052360535, "y": 0.6786813139915466}], "text": "0.5859\n"}
{"page": 6, "bbox": [{"x": 0.14903299510478973, "y": 0.6861538290977478}, {"x": 0.16097839176654816, "y": 0.6861538290977478}, {"x": 0.16097839176654816, "y": 0.6918681263923645}, {"x": 0.14903299510478973, "y": 0.6918681263923645}], "text": "16\n"}
{"page": 6, "bbox": [{"x": 0.1820250302553177, "y": 0.6852747201919556}, {"x": 0.21786120533943176, "y": 0.6857143044471741}, {"x": 0.21786120533943176, "y": 0.6927472352981567}, {"x": 0.1820250302553177, "y": 0.692307710647583}], "text": "0.3401\n"}
{"page": 6, "bbox": [{"x": 0.414675772190094, "y": 0.6861538290977478}, {"x": 0.4510807693004608, "y": 0.6861538290977478}, {"x": 0.4510807693004608, "y": 0.6927472352981567}, {"x": 0.414675772190094, "y": 0.6927472352981567}], "text": "0.3159\n"}
{"page": 6, "bbox": [{"x": 0.6507394909858704, "y": 0.6852747201919556}, {"x": 0.693401575088501, "y": 0.6852747201919556}, {"x": 0.693401575088501, "y": 0.7059340476989746}, {"x": 0.6507394909858704, "y": 0.7059340476989746}], "text": "0.5722\n0.5588*\n"}
{"page": 6, "bbox": [{"x": 0.14903299510478973, "y": 0.6997802257537842}, {"x": 0.16040955483913422, "y": 0.6997802257537842}, {"x": 0.16040955483913422, "y": 0.7054945230484009}, {"x": 0.14903299510478973, "y": 0.7054945230484009}], "text": "18\n"}
{"page": 6, "bbox": [{"x": 0.1820250302553177, "y": 0.6997802257537842}, {"x": 0.2184300273656845, "y": 0.6997802257537842}, {"x": 0.2184300273656845, "y": 0.7059340476989746}, {"x": 0.1820250302553177, "y": 0.7059340476989746}], "text": "0.3466\n"}
{"page": 6, "bbox": [{"x": 0.414675772190094, "y": 0.6993406414985657}, {"x": 0.4510807693004608, "y": 0.6993406414985657}, {"x": 0.4510807693004608, "y": 0.7063736319541931}, {"x": 0.414675772190094, "y": 0.7063736319541931}], "text": "0.2982\n"}
{"page": 6, "bbox": [{"x": 0.08759954571723938, "y": 0.7371428608894348}, {"x": 0.4806598424911499, "y": 0.7349450588226318}, {"x": 0.4806598424911499, "y": 0.7586812973022461}, {"x": 0.08759954571723938, "y": 0.7608790993690491}], "text": "impact on the model's effectiveness. We define the positions of the\ngold document as follows:\n"}
{"page": 6, "bbox": [{"x": 0.5187713503837585, "y": 0.7367032766342163}, {"x": 0.9141069650650024, "y": 0.7367032766342163}, {"x": 0.9141069650650024, "y": 0.7890110015869141}, {"x": 0.5187713503837585, "y": 0.7890110015869141}], "text": "the gold document is placed in the middle of the context. For in-\nstance, Llama2, with 18 distracting documents, reaches an accuracy\nof 0.37, 0.23, and 0.17, respectively. These results are consistent\nacross all models tested in the setting with distracting documents.\n"}
{"page": 6, "bbox": [{"x": 0.1137656420469284, "y": 0.7740659117698669}, {"x": 0.48350396752357483, "y": 0.7745054960250854}, {"x": 0.48350396752357483, "y": 0.8426373600959778}, {"x": 0.1137656420469284, "y": 0.8421977758407593}], "text": "• Near: placed adjacent to the query in the prompt [I, 3, ★,\nQ] (as in Figure 2)\n• Mid: inserted in the middle of the context [I, 3, ★, 3, Q]\n• Far: positioned as far as possible from the query in the con-\ntext [I,,,Q]\n"}
{"page": 6, "bbox": [{"x": 0.5193401575088501, "y": 0.8118681311607361}, {"x": 0.6865756511688232, "y": 0.8101099133491516}, {"x": 0.6865756511688232, "y": 0.8224175572395325}, {"x": 0.5193401575088501, "y": 0.8241758346557617}], "text": "5.3 Impact of Noise\n"}
{"page": 6, "bbox": [{"x": 0.5176336765289307, "y": 0.8290109634399414}, {"x": 0.9141069650650024, "y": 0.8290109634399414}, {"x": 0.9141069650650024, "y": 0.8962637186050415}, {"x": 0.5176336765289307, "y": 0.8962637186050415}], "text": "We devise an additional experimental setting aimed at evaluating\nthe robustness of the RAG system against noise. To this effect, we\ntake the gold document and add to it a certain number of docu-\nments picked at random from the corpus; see an example in Figure\n4. Against our expectations, the performance does not deteriorate\n"}
{"page": 6, "bbox": [{"x": 0.08703071624040604, "y": 0.8575823903083801}, {"x": 0.48236632347106934, "y": 0.8575823903083801}, {"x": 0.48236632347106934, "y": 0.8953846096992493}, {"x": 0.08703071624040604, "y": 0.8953846096992493}], "text": "Results in these settings partially corroborate evidence from [30].\nThe accuracy is higher when the gold document is near the query,\nlower when the gold document is furthest from it, and lowest when\n"}
{"page": 7, "bbox": [{"x": 0.08759954571723938, "y": 0.07780219614505768}, {"x": 0.361774742603302, "y": 0.07868131995201111}, {"x": 0.361774742603302, "y": 0.08703296631574631}, {"x": 0.08759954571723938, "y": 0.08615384250879288}], "text": "The Power of Noise: Redefining Retrieval for RAG Systems\n"}
{"page": 7, "bbox": [{"x": 0.6780432462692261, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.6780432462692261, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 7, "bbox": [{"x": 0.08703071624040604, "y": 0.10681318491697311}, {"x": 0.9135380983352661, "y": 0.10681318491697311}, {"x": 0.9135380983352661, "y": 0.18681319057941437}, {"x": 0.08703071624040604, "y": 0.18681319057941437}], "text": "Table 3: Accuracy of Llama2-7b in configurations involving random Wikipedia documents and retrieved documents [I,,, Q].\nRows denote the number of random documents added, and columns show the quantity of retrieved documents . The left\nsection reports results using Contriever, and the right section using BM25. Scenarios where the prompt exceeded the model's\ninput limit, leading to potential data truncation, are not included ( - ). Each value not marked with an asterisk * represents a\nstatistically significant change from the base case of retrieved documents only [I,, Q] (first row), as determined by a Wilcoxon\ntest (p-value < 0.01).\n"}
{"page": 7, "bbox": [{"x": 0.31740614771842957, "y": 0.20967033505439758}, {"x": 0.38225257396698, "y": 0.20967033505439758}, {"x": 0.38225257396698, "y": 0.21626374125480652}, {"x": 0.31740614771842957, "y": 0.21626374125480652}], "text": "Contriever\n"}
{"page": 7, "bbox": [{"x": 0.7053470015525818, "y": 0.20967033505439758}, {"x": 0.7394766807556152, "y": 0.20967033505439758}, {"x": 0.7394766807556152, "y": 0.21626374125480652}, {"x": 0.7053470015525818, "y": 0.21626374125480652}], "text": "BM25\n"}
{"page": 7, "bbox": [{"x": 0.09499431401491165, "y": 0.22505494952201843}, {"x": 0.8913537859916687, "y": 0.22505494952201843}, {"x": 0.8913537859916687, "y": 0.2461538463830948}, {"x": 0.09499431401491165, "y": 0.2461538463830948}], "text": "#HB # 1 2 3 4 5 8 10 1 2 3 4 5 8 10\n"}
{"page": 7, "bbox": [{"x": 0.14960181713104248, "y": 0.27076923847198486}, {"x": 0.15472127497196198, "y": 0.27076923847198486}, {"x": 0.15472127497196198, "y": 0.2764835059642792}, {"x": 0.14960181713104248, "y": 0.2764835059642792}], "text": "1\n"}
{"page": 7, "bbox": [{"x": 0.14903299510478973, "y": 0.2835164964199066}, {"x": 0.15529009699821472, "y": 0.2835164964199066}, {"x": 0.15529009699821472, "y": 0.2887912094593048}, {"x": 0.14903299510478973, "y": 0.2887912094593048}], "text": "2\n"}
{"page": 7, "bbox": [{"x": 0.14903299510478973, "y": 0.296263724565506}, {"x": 0.15472127497196198, "y": 0.296263724565506}, {"x": 0.15472127497196198, "y": 0.3019780218601227}, {"x": 0.14903299510478973, "y": 0.3019780218601227}], "text": "3\n"}
{"page": 7, "bbox": [{"x": 0.14903299510478973, "y": 0.25582417845726013}, {"x": 0.8998862504959106, "y": 0.25670328736305237}, {"x": 0.8998862504959106, "y": 0.3534066081047058}, {"x": 0.14903299510478973, "y": 0.3525274693965912}], "text": "0 0.1620 0.1866 0.1876 0.1866 0.1921 0.2198 0.2108 0.2008 0.2208 0.2084 0.2028 0.2243 0.2492 0.2447\n0.1308 0.1616 0.1717 0.1893* 0.1987* 0.2153* 0.2146* 0.1568 0.1963 0.1921 0.2115 0.2295* 0.2475* 0.2506*\n0.1315 0.1644 0.1859* 0.2008 0.2174 0.2156* 0.2368 0.1644 0.1973 0.2080* 0.2281 0.2558 0.2495* 0.2596\n0.1301 0.1727 0.2008 0.2316 0.2201 0.2198 0.2409 0.1568 0.2063 0.2160 0.2520 0.2579 0.2644 0.2707\n0.1464 0.2056 0.2233 0.2240 0.2150 0.2451 0.2482 0.1772 0.2402 0.2437 0.2520 0.2554 0.2804 0.2866\n0.2416 0.2364 0.1994 0.2451 0.2579 0.2769 0.2817 0.2859 0.2777\n0.2420\n0.2108 0.2589 0.2734 0.2835 0.2935 0.2853\n0.2243 0.2686 0.2790 0.2928\n"}
{"page": 7, "bbox": [{"x": 0.1484641581773758, "y": 0.3085714280605316}, {"x": 0.15472127497196198, "y": 0.3085714280605316}, {"x": 0.15472127497196198, "y": 0.3142857253551483}, {"x": 0.1484641581773758, "y": 0.3142857253551483}], "text": "5\n"}
{"page": 7, "bbox": [{"x": 0.2246871441602707, "y": 0.3191208839416504}, {"x": 0.4186575710773468, "y": 0.32087913155555725}, {"x": 0.4186575710773468, "y": 0.32923075556755066}, {"x": 0.2246871441602707, "y": 0.3274725377559662}], "text": "0.2066 0.2336 0.2375 0.2454\n"}
{"page": 7, "bbox": [{"x": 0.14903299510478973, "y": 0.32131868600845337}, {"x": 0.20591580867767334, "y": 0.3217582404613495}, {"x": 0.20591580867767334, "y": 0.3279120922088623}, {"x": 0.14903299510478973, "y": 0.3274725377559662}], "text": "8 0.1734\n"}
{"page": 7, "bbox": [{"x": 0.1433447152376175, "y": 0.33450549840927124}, {"x": 0.15415245294570923, "y": 0.33450549840927124}, {"x": 0.15415245294570923, "y": 0.33978021144866943}, {"x": 0.1433447152376175, "y": 0.33978021144866943}], "text": "10\n"}
{"page": 7, "bbox": [{"x": 0.1433447152376175, "y": 0.34637361764907837}, {"x": 0.15529009699821472, "y": 0.34637361764907837}, {"x": 0.15529009699821472, "y": 0.35208791494369507}, {"x": 0.1433447152376175, "y": 0.35208791494369507}], "text": "15\n"}
{"page": 7, "bbox": [{"x": 0.1433447152376175, "y": 0.3336263597011566}, {"x": 0.4197952151298523, "y": 0.33450549840927124}, {"x": 0.4197952151298523, "y": 0.3661538362503052}, {"x": 0.1433447152376175, "y": 0.36527472734451294}], "text": "0.1796 0.2174 0.2450 0.2502 0.2499\n0.2018 0.2354 0.2551 0.2530\n16 0.2032 0.2471 0.2558\n"}
{"page": 7, "bbox": [{"x": 0.5455062389373779, "y": 0.3591208755970001}, {"x": 0.6865756511688232, "y": 0.3591208755970001}, {"x": 0.6865756511688232, "y": 0.36527472734451294}, {"x": 0.5455062389373779, "y": 0.36527472734451294}], "text": "0.2323 0.2662 0.2838\n"}
{"page": 7, "bbox": [{"x": 0.5443686246871948, "y": 0.37142857909202576}, {"x": 0.5807735919952393, "y": 0.37054944038391113}, {"x": 0.5807735919952393, "y": 0.37714284658432007}, {"x": 0.5443686246871948, "y": 0.3780219852924347}], "text": "0.2326\n"}
{"page": 7, "bbox": [{"x": 0.17235495150089264, "y": 0.37142857909202576}, {"x": 0.2582480013370514, "y": 0.37142857909202576}, {"x": 0.2582480013370514, "y": 0.3775824308395386}, {"x": 0.17235495150089264, "y": 0.3775824308395386}], "text": "0.2039 0.2426\n"}
{"page": 7, "bbox": [{"x": 0.5967007875442505, "y": 0.37098902463912964}, {"x": 0.6336746215820312, "y": 0.37142857909202576}, {"x": 0.6336746215820312, "y": 0.3780219852924347}, {"x": 0.5967007875442505, "y": 0.3775824308395386}], "text": "0.2693\n"}
{"page": 7, "bbox": [{"x": 0.1433447152376175, "y": 0.3718681335449219}, {"x": 0.15415245294570923, "y": 0.3718681335449219}, {"x": 0.15415245294570923, "y": 0.3775824308395386}, {"x": 0.1433447152376175, "y": 0.3775824308395386}], "text": "17\n"}
{"page": 7, "bbox": [{"x": 0.14391353726387024, "y": 0.3841758370399475}, {"x": 0.15472127497196198, "y": 0.3841758370399475}, {"x": 0.15472127497196198, "y": 0.3894505500793457}, {"x": 0.14391353726387024, "y": 0.3894505500793457}], "text": "18\n"}
{"page": 7, "bbox": [{"x": 0.1706484705209732, "y": 0.3841758370399475}, {"x": 0.20762230455875397, "y": 0.3841758370399475}, {"x": 0.20762230455875397, "y": 0.3898901045322418}, {"x": 0.1706484705209732, "y": 0.3898901045322418}], "text": "0.2073\n"}
{"page": 7, "bbox": [{"x": 0.5455062389373779, "y": 0.3841758370399475}, {"x": 0.5796359777450562, "y": 0.3841758370399475}, {"x": 0.5796359777450562, "y": 0.39032965898513794}, {"x": 0.5455062389373779, "y": 0.39032965898513794}], "text": "0.2309\n"}
{"page": 7, "bbox": [{"x": 0.19169510900974274, "y": 0.41934067010879517}, {"x": 0.3452787399291992, "y": 0.41934067010879517}, {"x": 0.3452787399291992, "y": 0.42461538314819336}, {"x": 0.19169510900974274, "y": 0.42461538314819336}], "text": "Attention from Answer to Documents\n"}
{"page": 7, "bbox": [{"x": 0.10637087374925613, "y": 0.4285714328289032}, {"x": 0.11490330100059509, "y": 0.4285714328289032}, {"x": 0.11490330100059509, "y": 0.43252748250961304}, {"x": 0.10637087374925613, "y": 0.43252748250961304}], "text": "1-\n"}
{"page": 7, "bbox": [{"x": 0.4562002420425415, "y": 0.42989009618759155}, {"x": 0.47497156262397766, "y": 0.42989009618759155}, {"x": 0.47497156262397766, "y": 0.4334065914154053}, {"x": 0.4562002420425415, "y": 0.4334065914154053}], "text": "-0.35\n"}
{"page": 7, "bbox": [{"x": 0.5449374318122864, "y": 0.42945054173469543}, {"x": 0.7582480311393738, "y": 0.4281318783760071}, {"x": 0.7582480311393738, "y": 0.43824175000190735}, {"x": 0.5449374318122864, "y": 0.4395604431629181}], "text": "LLM Input - Random and Gold ✶\n"}
{"page": 7, "bbox": [{"x": 0.10580204427242279, "y": 0.45846155285835266}, {"x": 0.11604095250368118, "y": 0.45846155285835266}, {"x": 0.11604095250368118, "y": 0.4624175727367401}, {"x": 0.10580204427242279, "y": 0.4624175727367401}], "text": "5-\n"}
{"page": 7, "bbox": [{"x": 0.45563140511512756, "y": 0.464175820350647}, {"x": 0.4755403995513916, "y": 0.464175820350647}, {"x": 0.4755403995513916, "y": 0.4676923155784607}, {"x": 0.45563140511512756, "y": 0.4676923155784607}], "text": "-0.30\n"}
{"page": 7, "bbox": [{"x": 0.5449374318122864, "y": 0.4558241665363312}, {"x": 0.6473265290260315, "y": 0.4562637507915497}, {"x": 0.6473265290260315, "y": 0.4786813259124756}, {"x": 0.5449374318122864, "y": 0.47824177145957947}], "text": "Task instruction...\nDocuments:\n"}
{"page": 7, "bbox": [{"x": 0.10637087374925613, "y": 0.4874725341796875}, {"x": 0.11547213047742844, "y": 0.4874725341796875}, {"x": 0.11547213047742844, "y": 0.49142858386039734}, {"x": 0.10637087374925613, "y": 0.49142858386039734}], "text": "9-\n"}
{"page": 7, "bbox": [{"x": 0.4613196849822998, "y": 0.4989010989665985}, {"x": 0.47497156262397766, "y": 0.4989010989665985}, {"x": 0.47497156262397766, "y": 0.5028571486473083}, {"x": 0.4613196849822998, "y": 0.5028571486473083}], "text": "0.25\n"}
{"page": 7, "bbox": [{"x": 0.10238907486200333, "y": 0.517362654209137}, {"x": 0.11035267263650894, "y": 0.517362654209137}, {"x": 0.11035267263650894, "y": 0.5208791494369507}, {"x": 0.10238907486200333, "y": 0.5208791494369507}], "text": "13\n"}
{"page": 7, "bbox": [{"x": 0.4613196849822998, "y": 0.5331867933273315}, {"x": 0.4755403995513916, "y": 0.5331867933273315}, {"x": 0.4755403995513916, "y": 0.5371428728103638}, {"x": 0.4613196849822998, "y": 0.5371428728103638}], "text": "0.20\n"}
{"page": 7, "bbox": [{"x": 0.5426621437072754, "y": 0.4839560389518738}, {"x": 0.8890784978866577, "y": 0.4821977913379669}, {"x": 0.8902161717414856, "y": 0.6035164594650269}, {"x": 0.5437997579574585, "y": 0.6052747368812561}], "text": "Document [140] (Title: Richard Yates (novelist)) For much\nof his life, Yates's work met almost universal critical ac-\nclaim, yet not one of his books sold over 12,000 copies in...\nDocument [242] (Title: Android version history) Code\nname Version number Initial release date API level Security\npatches (No codename ) 1.0 September 23...\nDocument [3](Title: Millennium Falcon) Han Solo won\nthe Millennium Falcon from Lando Calrissian in the card\ngame sabacc...\n"}
{"page": 7, "bbox": [{"x": 0.09158134460449219, "y": 0.5679121017456055}, {"x": 0.09101251512765884, "y": 0.5239560604095459}, {"x": 0.09726962447166443, "y": 0.5239560604095459}, {"x": 0.09783845394849777, "y": 0.5679121017456055}], "text": "Attention Layers\n"}
{"page": 7, "bbox": [{"x": 0.10295790433883667, "y": 0.5468131899833679}, {"x": 0.1097838431596756, "y": 0.5468131899833679}, {"x": 0.1097838431596756, "y": 0.5503296852111816}, {"x": 0.10295790433883667, "y": 0.5503296852111816}], "text": "17\n"}
{"page": 7, "bbox": [{"x": 0.45563140511512756, "y": 0.5679121017456055}, {"x": 0.47497156262397766, "y": 0.5679121017456055}, {"x": 0.47497156262397766, "y": 0.5718681216239929}, {"x": 0.45563140511512756, "y": 0.5718681216239929}], "text": "-0.15\n"}
{"page": 7, "bbox": [{"x": 0.10238907486200333, "y": 0.5758242011070251}, {"x": 0.1097838431596756, "y": 0.5758242011070251}, {"x": 0.1097838431596756, "y": 0.5802198052406311}, {"x": 0.10238907486200333, "y": 0.5802198052406311}], "text": "21\n"}
{"page": 7, "bbox": [{"x": 0.4613196849822998, "y": 0.6026373505592346}, {"x": 0.47497156262397766, "y": 0.6026373505592346}, {"x": 0.47497156262397766, "y": 0.6065934300422668}, {"x": 0.4613196849822998, "y": 0.6065934300422668}], "text": "0.10\n"}
{"page": 7, "bbox": [{"x": 0.10238907486200333, "y": 0.6052747368812561}, {"x": 0.11035267263650894, "y": 0.6052747368812561}, {"x": 0.11035267263650894, "y": 0.6087912321090698}, {"x": 0.10238907486200333, "y": 0.6087912321090698}], "text": "25\n"}
{"page": 7, "bbox": [{"x": 0.5443686246871948, "y": 0.6215384602546692}, {"x": 0.8879408240318298, "y": 0.620659351348877}, {"x": 0.8879408240318298, "y": 0.6430768966674805}, {"x": 0.5443686246871948, "y": 0.6439560651779175}], "text": "Question: who owned the millennium falcon be-\nfore han solo\n"}
{"page": 7, "bbox": [{"x": 0.10182025283575058, "y": 0.6347252726554871}, {"x": 0.1097838431596756, "y": 0.6347252726554871}, {"x": 0.1097838431596756, "y": 0.6386812925338745}, {"x": 0.10182025283575058, "y": 0.6386812925338745}], "text": "29\n"}
{"page": 7, "bbox": [{"x": 0.45563140511512756, "y": 0.6373626589775085}, {"x": 0.4755403995513916, "y": 0.6373626589775085}, {"x": 0.4755403995513916, "y": 0.6408790946006775}, {"x": 0.45563140511512756, "y": 0.6408790946006775}], "text": "-0.05\n"}
{"page": 7, "bbox": [{"x": 0.5443686246871948, "y": 0.6505494713783264}, {"x": 0.7025028467178345, "y": 0.6505494713783264}, {"x": 0.7025028467178345, "y": 0.6580219864845276}, {"x": 0.5443686246871948, "y": 0.6580219864845276}], "text": "Answer: Lando Calrissian\n"}
{"page": 7, "bbox": [{"x": 0.10238907486200333, "y": 0.6571428775787354}, {"x": 0.10921501368284225, "y": 0.6571428775787354}, {"x": 0.10921501368284225, "y": 0.6610988974571228}, {"x": 0.10238907486200333, "y": 0.6610988974571228}], "text": "32\n"}
{"page": 7, "bbox": [{"x": 0.27986347675323486, "y": 0.668571412563324}, {"x": 0.3230944275856018, "y": 0.6624175906181335}, {"x": 0.3253697454929352, "y": 0.6729670166969299}, {"x": 0.2827076315879822, "y": 0.6795604228973389}], "text": "Doc_7\nDoc 8\n"}
{"page": 7, "bbox": [{"x": 0.3993174135684967, "y": 0.670769214630127}, {"x": 0.41922640800476074, "y": 0.6668131947517395}, {"x": 0.4209328889846802, "y": 0.6720879077911377}, {"x": 0.40102389454841614, "y": 0.6764835119247437}], "text": "GOLD\n"}
{"page": 7, "bbox": [{"x": 0.23321956396102905, "y": 0.6694505214691162}, {"x": 0.2542662024497986, "y": 0.6668131947517395}, {"x": 0.25540387630462646, "y": 0.6738461256027222}, {"x": 0.2349260449409485, "y": 0.6764835119247437}], "text": "Doc_5\n"}
{"page": 7, "bbox": [{"x": 0.25767919421195984, "y": 0.670769214630127}, {"x": 0.2775881588459015, "y": 0.6672527194023132}, {"x": 0.2792946398258209, "y": 0.6725274920463562}, {"x": 0.2593856751918793, "y": 0.6760439276695251}], "text": "Doc_6\n"}
{"page": 7, "bbox": [{"x": 0.13936291635036469, "y": 0.670769214630127}, {"x": 0.15984073281288147, "y": 0.6668131947517395}, {"x": 0.1615472137928009, "y": 0.6725274920463562}, {"x": 0.14106939733028412, "y": 0.6764835119247437}], "text": "Doc_1\n"}
{"page": 7, "bbox": [{"x": 0.1626848727464676, "y": 0.6698901057243347}, {"x": 0.18373151123523712, "y": 0.6668131947517395}, {"x": 0.18543799221515656, "y": 0.6734066009521484}, {"x": 0.16439135372638702, "y": 0.6764835119247437}], "text": "Doc_2\n"}
{"page": 7, "bbox": [{"x": 0.2093287855386734, "y": 0.6712087988853455}, {"x": 0.23094426095485687, "y": 0.666373610496521}, {"x": 0.2326507419347763, "y": 0.6720879077911377}, {"x": 0.21160408854484558, "y": 0.6769230961799622}], "text": "Doc_4\n"}
{"page": 7, "bbox": [{"x": 0.18657565116882324, "y": 0.6716483235359192}, {"x": 0.2081911265850067, "y": 0.6672527194023132}, {"x": 0.20989760756492615, "y": 0.6720879077911377}, {"x": 0.18828213214874268, "y": 0.6769230961799622}], "text": "Doc_3\n"}
{"page": 7, "bbox": [{"x": 0.3282138705253601, "y": 0.6712087988853455}, {"x": 0.3486917018890381, "y": 0.6676923036575317}, {"x": 0.3503981828689575, "y": 0.6725274920463562}, {"x": 0.32992035150527954, "y": 0.6764835119247437}], "text": "Doc 9\n"}
{"page": 7, "bbox": [{"x": 0.11547213047742844, "y": 0.6712087988853455}, {"x": 0.13538111746311188, "y": 0.6672527194023132}, {"x": 0.1370875984430313, "y": 0.6729670166969299}, {"x": 0.11717861145734787, "y": 0.6769230961799622}], "text": "Doc_0\n"}
{"page": 7, "bbox": [{"x": 0.3731513023376465, "y": 0.6734066009521484}, {"x": 0.3953356146812439, "y": 0.666373610496521}, {"x": 0.39761093258857727, "y": 0.6712087988853455}, {"x": 0.3759954571723938, "y": 0.6782417297363281}], "text": "Doc_11\n"}
{"page": 7, "bbox": [{"x": 0.34926050901412964, "y": 0.6734066009521484}, {"x": 0.3731513023376465, "y": 0.6672527194023132}, {"x": 0.3748577833175659, "y": 0.6716483235359192}, {"x": 0.351535826921463, "y": 0.6778022050857544}], "text": "Doc_10\n"}
{"page": 7, "bbox": [{"x": 0.23037542402744293, "y": 0.6800000071525574}, {"x": 0.30659839510917664, "y": 0.6795604228973389}, {"x": 0.30659839510917664, "y": 0.6843956112861633}, {"x": 0.23037542402744293, "y": 0.6848351359367371}], "text": "Documents in Context\n"}
{"page": 7, "bbox": [{"x": 0.5193401575088501, "y": 0.693626344203949}, {"x": 0.914675772190094, "y": 0.694505512714386}, {"x": 0.914675772190094, "y": 0.746813178062439}, {"x": 0.5193401575088501, "y": 0.7459340691566467}], "text": "Figure 4: Example LLM input with a correct output, high-\nlighted in green. The context of the prompt is composed of\nrandom documents and the gold near the query. The task\ninstruction is as in Figure 1.\n"}
{"page": 7, "bbox": [{"x": 0.08703071624040604, "y": 0.7063736319541931}, {"x": 0.4806598424911499, "y": 0.7059340476989746}, {"x": 0.4806598424911499, "y": 0.8237362504005432}, {"x": 0.08703071624040604, "y": 0.8241758346557617}], "text": "Figure 3: This heatmap depicts the attention distribution\nacross the context documents from the example shown in\nFigure 2, relative to the answer generated by Llama2-7b in\na prompt structured as [I, 3, ★, Q]. Cell (i, j) denotes the\nmean attention that tokens in the generated answer allocate\nto the tokens of the i-th document within the j-th attention\nlayer. This mean attention for each document is calculated\nby averaging the attention scores across all its constituent\ntokens.\n"}
{"page": 7, "bbox": [{"x": 0.5193401575088501, "y": 0.771868109703064}, {"x": 0.9141069650650024, "y": 0.7745054960250854}, {"x": 0.9129692912101746, "y": 0.8984615206718445}, {"x": 0.5182024836540222, "y": 0.8958241939544678}], "text": "the case of MPT. Furthermore, we observe that different models\nexhibit distinct behaviors. Both Llama2 and Phi-2 showed improve-\nments in this setting when the noise is introduced furthest from\nthe\nquery. . However, when the noise is positioned in the far [I, ★,\n,Q] and mid [I, §, ★, Œ, Q] settings, these models exhibit a\ndecline in performance. Notably, this performance degradation is\nmuch less accentuated when compared to the earlier setting with\ndistracting documents. This suggests that while Llama2 and Phi-2\ncan effectively handle noise far from the query, their ability to sift\n"}
{"page": 7, "bbox": [{"x": 0.08759954571723938, "y": 0.8571428656578064}, {"x": 0.4829351603984833, "y": 0.8567032814025879}, {"x": 0.4829351603984833, "y": 0.8958241939544678}, {"x": 0.08759954571723938, "y": 0.8962637186050415}], "text": "in the presence of noise, as can be seen in Table 2. Instead, we ob-\nserve an improvement in performance under the best-performing\nsetting (near [I,,★, Q]), with an improvement of 0.08 (+36%) in\n"}
{"page": 8, "bbox": [{"x": 0.7656427621841431, "y": 0.07780219614505768}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.7656427621841431, "y": 0.08571428805589676}], "text": "Cuconasu and Trappolini, et al.\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.07868131995201111}, {"x": 0.3219567835330963, "y": 0.07868131995201111}, {"x": 0.3219567835330963, "y": 0.08659340441226959}, {"x": 0.08759954571723938, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 8, "bbox": [{"x": 0.5182024836540222, "y": 0.11032967269420624}, {"x": 0.914675772190094, "y": 0.11032967269420624}, {"x": 0.914675772190094, "y": 0.20351648330688477}, {"x": 0.5182024836540222, "y": 0.20351648330688477}], "text": "but even enhances it, with an improvement of 0.023 (+9% accuracy)\nwhen compared to the previous best score. Pushing the randomness\neven further, we carry out another test where we consider nonsen-\nsical sentences made up of random words as random documents.\nRemarkably, even in this scenario, we observe a performance im-\nprovement when compared to the base case of Wikipedia random\ndocuments, as shown in the right side of Table 4.\n"}
{"page": 8, "bbox": [{"x": 0.08703071624040604, "y": 0.10989011079072952}, {"x": 0.4829351603984833, "y": 0.11032967269420624}, {"x": 0.4829351603984833, "y": 0.23120878636837006}, {"x": 0.08703071624040604, "y": 0.23076923191547394}], "text": "through irrelevant information diminishes as the noise is placed\ncloser to it. The MPT model presented a unique response; it showed\nan improvement in performance under all settings. Standing out\nfrom the rest, the Falcon model did not exhibit an improvement in\nperformance as observed in other models with the introduction of\nnoise. Peculiarly enough, Falcon and Llama2 do not consistently ex-\nhibit a \"lost in the middle\" phenomenon, having in some instances\nbetter accuracy in the mid than far setting, for instance, in the case\nwith 8 noisy documents added.\n"}
{"page": 8, "bbox": [{"x": 0.5199089646339417, "y": 0.21890109777450562}, {"x": 0.5455062389373779, "y": 0.2184615433216095}, {"x": 0.5455062389373779, "y": 0.22505494952201843}, {"x": 0.5199089646339417, "y": 0.22549450397491455}], "text": "5.4.3\n"}
{"page": 8, "bbox": [{"x": 0.08816837519407272, "y": 0.25010988116264343}, {"x": 0.25654152035713196, "y": 0.25010988116264343}, {"x": 0.25654152035713196, "y": 0.25934067368507385}, {"x": 0.08816837519407272, "y": 0.25934067368507385}], "text": "5.4 RAG in Practice\n"}
{"page": 8, "bbox": [{"x": 0.5187713503837585, "y": 0.21802197396755219}, {"x": 0.914675772190094, "y": 0.21802197396755219}, {"x": 0.914675772190094, "y": 0.3780219852924347}, {"x": 0.5187713503837585, "y": 0.3780219852924347}], "text": "Falcon. As shown in Table 2, Falcon does not reach the same\nperformance increase when random documents are added to the\ngold document [I, §,★, Q]. Accordingly, we want to verify whether\nit behaves differently when adding retrieved rather than gold docu-\nments. We find that the addition of random documents on top of\nretrieved documents [I,,,Q] does improve the effectiveness of\nFalcon; see detailed results in Table 5. These results are in contrast\nwith the ones obtained in the oracle setting, where Falcon was\nrobust to noise. This new finding further validates our experimen-\ntal evidence, namely that, outside the oracle setting, all the tested\nmodels show an improvement when a certain amount of noise is\nadded.\n"}
{"page": 8, "bbox": [{"x": 0.08703071624040604, "y": 0.269010990858078}, {"x": 0.4829351603984833, "y": 0.269010990858078}, {"x": 0.4829351603984833, "y": 0.4430769085884094}, {"x": 0.08703071624040604, "y": 0.4430769085884094}], "text": "To address our primary Research Question (RQ) about the char-\nacteristics of an effective RAG retriever, and following the results\nreported above, we now consider a more realistic scenario than an\noracle setup. Namely, given a query, we retrieve a set of documents\nthat can be either relevant or distracting. We then add random doc-\numents to this set of retrieved ones, schematically: [I,,, Q]. For\nthis second set of experiments, we use the test set of the NQ-open\ndataset. Results for this experiment, using Llama2, can be seen on\nthe left side of Table 3. These results show that, regardless of the\nnumber of retrieved documents, adding random documents up until\nthe context length is filled is almost always beneficial, with gains\nin terms of accuracy up to 0.07 (+35%) in the case of 4 retrieved\ndocuments.\n"}
{"page": 8, "bbox": [{"x": 0.5591581463813782, "y": 0.39868131279945374}, {"x": 0.7189988493919373, "y": 0.3982417583465576}, {"x": 0.7189988493919373, "y": 0.4087912142276764}, {"x": 0.5591581463813782, "y": 0.4092307686805725}], "text": "Retriever Trade-Off\n"}
{"page": 8, "bbox": [{"x": 0.5193401575088501, "y": 0.39868131279945374}, {"x": 0.5415244698524475, "y": 0.39868131279945374}, {"x": 0.5415244698524475, "y": 0.4092307686805725}, {"x": 0.5193401575088501, "y": 0.4092307686805725}], "text": "5.5\n"}
{"page": 8, "bbox": [{"x": 0.08703071624040604, "y": 0.4593406617641449}, {"x": 0.4829351603984833, "y": 0.4593406617641449}, {"x": 0.4829351603984833, "y": 0.7472527623176575}, {"x": 0.08703071624040604, "y": 0.7472527623176575}], "text": "5.4.1 Testing Sparse Retrievers. In an effort to validate our initial\nobservations, we replicate our experiment using a sparse retrieval\napproach, specifically BM25. The corresponding results are outlined\nin the right section of Table 3. Consistent with earlier findings, we\nobserve that including random documents leads to an improvement\nin the effectiveness of the LLM. Notably, the use of BM25 yields\nan average increase in accuracy of 3-4 percentage points. This im-\nprovement is attributed to the quality of documents retrieved by\nBM25. We quantitatively evaluate the effectiveness of the retrieval\nmethods by computing the top-k accuracy for varying numbers of\nretrieved documents. Note that this heuristic, while indicative, does\nnot capture the full spectrum of relevance. Our evaluation, based on\nthe\npresence of correct answers within documents, might overlook\nthe context-specific relevance due to potential lexical matches of\nthe answer string in documents. Despite this limitation, this method\naligns with established computational practices in literature [15, 19].\nIn our analysis, BM25 demonstrated higher relative top-k accuracy\n(0.2966, 0.4105, 0.5237, 0.6663 for k = 1, 2, 4, 10) compared to those\nof Contriever (0.2502, 0.3569, 0.4784, 0.6085 for the same k), under-\nscoring its effectiveness in retrieving more relevant documents in\nour experimental setup.\n"}
{"page": 8, "bbox": [{"x": 0.5187713503837585, "y": 0.4171428680419922}, {"x": 0.9152445793151855, "y": 0.4171428680419922}, {"x": 0.914675772190094, "y": 0.8958241939544678}, {"x": 0.5182024836540222, "y": 0.8958241939544678}], "text": "The experimental evidence detailed above not only contradicts the\ncommon perception that semantically close documents are help-\nful for LLMs but also highlights the need for a delicate balance\nbetween relevant and random documents. When arranged as de-\nscribed, random documents seem to exert a positive influence on\nLLM accuracy. However, for the LLM to generate accurate answers,\nsome degree of relevant information must exist in the context. On\nthe other hand, an overabundance of retrieved documents increases\nthe likelihood of including distracting and non-relevant informa-\ntion, leading to a sharp decline in performance. While establishing\na formal or comprehensive theory behind these findings remains an\nopen research challenge, we can still infer that there seems to be a\ntrade-off between the number of relevant and totally irrelevant doc-\numents. More specifically, we observed that the best effectiveness is\nachieved when a minimal set of documents is initially retrieved and\nthen supplemented with random documents until the context limit\nis reached. For the queries examined in this study, retrieving be-\ntween 3 and 5 documents is the most effective choice. Adding more\nincreases the risk of including too many distracting, thus coun-\nterproductive, documents. We argue here that there is a pressing\nneed for further research towards investigating how these initial\nfindings can be exploited. More importantly, it is evident that we\nhave yet to refine our understanding of the retriever's role within\na RAG system.\nOn The Unreasonable Effectiveness Of Random Documents. We can-\nnot close this paper without attempting to explain the results shown\nup to this point. We refer back to our RAG formulation, particularly\nthe conditioned function po(y\\., d). In hindsight, we can now state\nthat by adding random documents to the context, we are better\nconditioning this function, inducing enhanced accuracy. Previous\nresearch [3, 14], particularly [58], hints that there might be cases\nin which a pathologically low attention entropy causes the LLM to\ngenerate degenerate outputs with a sharp decrease in performance.\nThese episodes are named entropy collapse. Following this line of\n"}
{"page": 8, "bbox": [{"x": 0.08646188676357269, "y": 0.7613186836242676}, {"x": 0.4829351603984833, "y": 0.7613186836242676}, {"x": 0.4829351603984833, "y": 0.8962637186050415}, {"x": 0.08646188676357269, "y": 0.8962637186050415}], "text": "5.4.2 Increasing The Randomness. While our previous experiments\nshow the benefits of adding random documents, one might argue\nthat these documents are not totally random as they originate from\nthe same corpus (Wikipedia) and that they might help the LLM\nanswer in a fashion that is consistent with the corpus. For this rea-\nson, we carry out another experiment in which random documents\nare drawn from a drastically different corpus in terms of tone and\nstyle, namely Reddit Webis-TLDR-17 dataset [53]. The results are\noutlined on the left of Table 4. The inclusion of documents from the\nReddit corpus not only maintains the observed increase in accuracy\n"}
{"page": 9, "bbox": [{"x": 0.08703071624040604, "y": 0.07780219614505768}, {"x": 0.361774742603302, "y": 0.07868131995201111}, {"x": 0.361774742603302, "y": 0.08703296631574631}, {"x": 0.08703071624040604, "y": 0.08615384250879288}], "text": "The Power of Noise: Redefining Retrieval for RAG Systems\n"}
{"page": 9, "bbox": [{"x": 0.6780432462692261, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.6780432462692261, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 9, "bbox": [{"x": 0.08703071624040604, "y": 0.10681318491697311}, {"x": 0.9141069650650024, "y": 0.10681318491697311}, {"x": 0.9141069650650024, "y": 0.18725274503231049}, {"x": 0.08703071624040604, "y": 0.18725274503231049}], "text": "Table 4: Accuracy of Llama2-7b in configurations involving random documents and retrieved documents by Contriever [I, ⠀,\nQ]. Rows denote the number of random documents added, and columns show the quantity of retrieved documents.\nThe left section reports results with random documents from Reddit and the right section with nonsensical sentences made\nup of random words. Scenarios where the prompt exceeded the model's input limit, leading to potential data truncation, are\nnot included ( - ). Each value not marked with an asterisk * represents a statistically significant change from the base case of\nretrieved documents only [I,, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01).\n"}
{"page": 9, "bbox": [{"x": 0.6751990914344788, "y": 0.20923076570034027}, {"x": 0.7696245908737183, "y": 0.20923076570034027}, {"x": 0.7696245908737183, "y": 0.2158241719007492}, {"x": 0.6751990914344788, "y": 0.2158241719007492}], "text": "Random Words\n"}
{"page": 9, "bbox": [{"x": 0.28498294949531555, "y": 0.20923076570034027}, {"x": 0.414675772190094, "y": 0.20923076570034027}, {"x": 0.414675772190094, "y": 0.21626374125480652}, {"x": 0.28498294949531555, "y": 0.21626374125480652}], "text": "Random from Reddit\n"}
{"page": 9, "bbox": [{"x": 0.14448235929012299, "y": 0.22461538016796112}, {"x": 0.1535836160182953, "y": 0.22461538016796112}, {"x": 0.1535836160182953, "y": 0.2351648360490799}, {"x": 0.14448235929012299, "y": 0.2351648360490799}], "text": "Pm\n"}
{"page": 9, "bbox": [{"x": 0.24061433970928192, "y": 0.23472528159618378}, {"x": 0.24630261957645416, "y": 0.23472528159618378}, {"x": 0.24630261957645416, "y": 0.23999999463558197}, {"x": 0.24061433970928192, "y": 0.23999999463558197}], "text": "2\n"}
{"page": 9, "bbox": [{"x": 0.8242321014404297, "y": 0.2351648360490799}, {"x": 0.829351544380188, "y": 0.2351648360490799}, {"x": 0.829351544380188, "y": 0.23999999463558197}, {"x": 0.8242321014404297, "y": 0.23999999463558197}], "text": "8\n"}
{"page": 9, "bbox": [{"x": 0.29408419132232666, "y": 0.23472528159618378}, {"x": 0.2997724711894989, "y": 0.23472528159618378}, {"x": 0.2997724711894989, "y": 0.24043956398963928}, {"x": 0.29408419132232666, "y": 0.24043956398963928}], "text": "3\n"}
{"page": 9, "bbox": [{"x": 0.3464163839817047, "y": 0.23472528159618378}, {"x": 0.3543799817562103, "y": 0.23472528159618378}, {"x": 0.3543799817562103, "y": 0.24043956398963928}, {"x": 0.3464163839817047, "y": 0.24043956398963928}], "text": "4\n"}
{"page": 9, "bbox": [{"x": 0.39988622069358826, "y": 0.23472528159618378}, {"x": 0.40614333748817444, "y": 0.23472528159618378}, {"x": 0.40614333748817444, "y": 0.24043956398963928}, {"x": 0.39988622069358826, "y": 0.24043956398963928}], "text": "5\n"}
{"page": 9, "bbox": [{"x": 0.5039817690849304, "y": 0.23472528159618378}, {"x": 0.5147895216941833, "y": 0.23472528159618378}, {"x": 0.5147895216941833, "y": 0.24043956398963928}, {"x": 0.5039817690849304, "y": 0.24043956398963928}], "text": "10\n"}
{"page": 9, "bbox": [{"x": 0.6655290126800537, "y": 0.23472528159618378}, {"x": 0.6723549365997314, "y": 0.23472528159618378}, {"x": 0.6723549365997314, "y": 0.24043956398963928}, {"x": 0.6655290126800537, "y": 0.24043956398963928}], "text": "3\n"}
{"page": 9, "bbox": [{"x": 0.7713310718536377, "y": 0.23472528159618378}, {"x": 0.7770193219184875, "y": 0.23472528159618378}, {"x": 0.7770193219184875, "y": 0.24043956398963928}, {"x": 0.7713310718536377, "y": 0.24043956398963928}], "text": "5\n"}
{"page": 9, "bbox": [{"x": 0.18828213214874268, "y": 0.2351648360490799}, {"x": 0.19340158998966217, "y": 0.2351648360490799}, {"x": 0.19340158998966217, "y": 0.24043956398963928}, {"x": 0.18828213214874268, "y": 0.24043956398963928}], "text": "1\n"}
{"page": 9, "bbox": [{"x": 0.4533560872077942, "y": 0.2351648360490799}, {"x": 0.4584755301475525, "y": 0.2351648360490799}, {"x": 0.4584755301475525, "y": 0.24043956398963928}, {"x": 0.4533560872077942, "y": 0.24043956398963928}], "text": "8\n"}
{"page": 9, "bbox": [{"x": 0.5608646273612976, "y": 0.2351648360490799}, {"x": 0.5654152631759644, "y": 0.2351648360490799}, {"x": 0.5654152631759644, "y": 0.24043956398963928}, {"x": 0.5608646273612976, "y": 0.24043956398963928}], "text": "1\n"}
{"page": 9, "bbox": [{"x": 0.6131967902183533, "y": 0.2351648360490799}, {"x": 0.6188850998878479, "y": 0.2351648360490799}, {"x": 0.6188850998878479, "y": 0.24043956398963928}, {"x": 0.6131967902183533, "y": 0.24043956398963928}], "text": "2\n"}
{"page": 9, "bbox": [{"x": 0.7184300422668457, "y": 0.2351648360490799}, {"x": 0.7252559661865234, "y": 0.2351648360490799}, {"x": 0.7252559661865234, "y": 0.24043956398963928}, {"x": 0.7184300422668457, "y": 0.24043956398963928}], "text": "4\n"}
{"page": 9, "bbox": [{"x": 0.8759954571723938, "y": 0.23472528159618378}, {"x": 0.8873720169067383, "y": 0.23472528159618378}, {"x": 0.8873720169067383, "y": 0.2408791184425354}, {"x": 0.8759954571723938, "y": 0.2408791184425354}], "text": "10\n"}
{"page": 9, "bbox": [{"x": 0.09840728342533112, "y": 0.24043956398963928}, {"x": 0.1046643927693367, "y": 0.24043956398963928}, {"x": 0.1046643927693367, "y": 0.24703297019004822}, {"x": 0.09840728342533112, "y": 0.24703297019004822}], "text": "#\n"}
{"page": 9, "bbox": [{"x": 0.14903299510478973, "y": 0.25846153497695923}, {"x": 0.15585893392562866, "y": 0.25846153497695923}, {"x": 0.15585893392562866, "y": 0.2641758322715759}, {"x": 0.14903299510478973, "y": 0.2641758322715759}], "text": "0\n"}
{"page": 9, "bbox": [{"x": 0.861774742603302, "y": 0.2580219805240631}, {"x": 0.8998862504959106, "y": 0.257582426071167}, {"x": 0.8998862504959106, "y": 0.2764835059642792}, {"x": 0.861774742603302, "y": 0.2769230902194977}], "text": "0.2108\n0.2073*\n"}
{"page": 9, "bbox": [{"x": 0.14960181713104248, "y": 0.271208792924881}, {"x": 0.15472127497196198, "y": 0.271208792924881}, {"x": 0.15472127497196198, "y": 0.2769230902194977}, {"x": 0.14960181713104248, "y": 0.2769230902194977}], "text": "1\n"}
{"page": 9, "bbox": [{"x": 0.14903299510478973, "y": 0.28395605087280273}, {"x": 0.15529009699821472, "y": 0.28395605087280273}, {"x": 0.15529009699821472, "y": 0.2892307639122009}, {"x": 0.14903299510478973, "y": 0.2892307639122009}], "text": "2\n"}
{"page": 9, "bbox": [{"x": 0.8623435497283936, "y": 0.2835164964199066}, {"x": 0.9004550576210022, "y": 0.2835164964199066}, {"x": 0.9004550576210022, "y": 0.29010990262031555}, {"x": 0.8623435497283936, "y": 0.29010990262031555}], "text": "0.2084*\n"}
{"page": 9, "bbox": [{"x": 0.1706484705209732, "y": 0.2580219805240631}, {"x": 0.4197952151298523, "y": 0.25846153497695923}, {"x": 0.4197952151298523, "y": 0.3283516466617584}, {"x": 0.1706484705209732, "y": 0.3279120922088623}], "text": "0.1620 0.1866 0.1876 0.1866 0.1921\n0.1693* 0.1931 0.1845* 0.1907 0.2008\n0.1886 0.2018 0.2101 0.2143 0.2160\n0.2108 0.2212 0.2340 0.2371\n0.2215 0.2388 0.2468 0.2409\n0.2326 0.2354 0.2489 0.2440\n"}
{"page": 9, "bbox": [{"x": 0.14903299510478973, "y": 0.296263724565506}, {"x": 0.15585893392562866, "y": 0.296263724565506}, {"x": 0.15585893392562866, "y": 0.3019780218601227}, {"x": 0.14903299510478973, "y": 0.3019780218601227}], "text": "3\n"}
{"page": 9, "bbox": [{"x": 0.17349261045455933, "y": 0.2967033088207245}, {"x": 0.20648464560508728, "y": 0.2967033088207245}, {"x": 0.20648464560508728, "y": 0.3028571307659149}, {"x": 0.17349261045455933, "y": 0.3028571307659149}], "text": "0.1897\n"}
{"page": 9, "bbox": [{"x": 0.14903299510478973, "y": 0.30945053696632385}, {"x": 0.15529009699821472, "y": 0.30945053696632385}, {"x": 0.15529009699821472, "y": 0.31516483426094055}, {"x": 0.14903299510478973, "y": 0.31516483426094055}], "text": "5\n"}
{"page": 9, "bbox": [{"x": 0.17349261045455933, "y": 0.30901098251342773}, {"x": 0.20648464560508728, "y": 0.30901098251342773}, {"x": 0.20648464560508728, "y": 0.31560438871383667}, {"x": 0.17349261045455933, "y": 0.31560438871383667}], "text": "0.1897\n"}
{"page": 9, "bbox": [{"x": 0.17349261045455933, "y": 0.2571428716182709}, {"x": 0.9004550576210022, "y": 0.2580219805240631}, {"x": 0.9004550576210022, "y": 0.37934064865112305}, {"x": 0.17349261045455933, "y": 0.3784615397453308}], "text": "0.2198 0.2108 0.1620 0.1866 0.1876 0.1866 0.1921 0.2198\n0.2084 0.2084 0.1744 0.1924* 0.1969 0.2077 0.2091 0.2139*\n0.2222* 0.2219 0.1765 0.1855* 0.2094 0.2122 0.2181 0.2045\n0.2326 0.2319 0.1755 0.1990 0.2166 0.2201 0.2288 0.2032 0.2156*\n0.2769 0.2451 0.1862 0.2139 0.2319 0.2367 0.2232 0.2184* 0.2278\n0.2568 0.2364 0.1973 0.2274 0.2319 0.2316 0.2305 0.2357 0.2412\n0.2326 0.2451 0.2534 0.2551 0.2658\n0.2053 0.2271 0.2340 0.2385 0.2406 0.2499\n0.2240 0.2489 0.2689 0.2786\n0.2215 0.2416 0.2589 0.2634\n0.2219 0.2437 0.2568\n0.2201 0.2450\n"}
{"page": 9, "bbox": [{"x": 0.14960181713104248, "y": 0.3217582404613495}, {"x": 0.15529009699821472, "y": 0.3217582404613495}, {"x": 0.15529009699821472, "y": 0.3270329535007477}, {"x": 0.14960181713104248, "y": 0.3270329535007477}], "text": "8\n"}
{"page": 9, "bbox": [{"x": 0.17349261045455933, "y": 0.32131868600845337}, {"x": 0.20591580867767334, "y": 0.32131868600845337}, {"x": 0.20591580867767334, "y": 0.3279120922088623}, {"x": 0.17349261045455933, "y": 0.3279120922088623}], "text": "0.2011\n"}
{"page": 9, "bbox": [{"x": 0.14391353726387024, "y": 0.33450549840927124}, {"x": 0.15415245294570923, "y": 0.33450549840927124}, {"x": 0.15415245294570923, "y": 0.33978021144866943}, {"x": 0.14391353726387024, "y": 0.33978021144866943}], "text": "10\n"}
{"page": 9, "bbox": [{"x": 0.17349261045455933, "y": 0.3331868052482605}, {"x": 0.20762230455875397, "y": 0.3340659439563751}, {"x": 0.20705346763134003, "y": 0.3410989046096802}, {"x": 0.17292377352714539, "y": 0.34021976590156555}], "text": "0.2053\n"}
{"page": 9, "bbox": [{"x": 0.1433447152376175, "y": 0.347252756357193}, {"x": 0.15529009699821472, "y": 0.347252756357193}, {"x": 0.15529009699821472, "y": 0.3529670238494873}, {"x": 0.1433447152376175, "y": 0.3529670238494873}], "text": "15\n"}
{"page": 9, "bbox": [{"x": 0.14391353726387024, "y": 0.36000001430511475}, {"x": 0.15472127497196198, "y": 0.36000001430511475}, {"x": 0.15472127497196198, "y": 0.36571428179740906}, {"x": 0.14391353726387024, "y": 0.36571428179740906}], "text": "16\n"}
{"page": 9, "bbox": [{"x": 0.17349261045455933, "y": 0.36000001430511475}, {"x": 0.31456199288368225, "y": 0.36000001430511475}, {"x": 0.31456199288368225, "y": 0.3661538362503052}, {"x": 0.17349261045455933, "y": 0.3661538362503052}], "text": "0.2240 0.2561 0.2676\n"}
{"page": 9, "bbox": [{"x": 0.14391353726387024, "y": 0.3727472424507141}, {"x": 0.15472127497196198, "y": 0.3727472424507141}, {"x": 0.15472127497196198, "y": 0.3784615397453308}, {"x": 0.14391353726387024, "y": 0.3784615397453308}], "text": "17\n"}
{"page": 9, "bbox": [{"x": 0.1717861145734787, "y": 0.3727472424507141}, {"x": 0.2622298002243042, "y": 0.3731868267059326}, {"x": 0.2622298002243042, "y": 0.37890109419822693}, {"x": 0.1717861145734787, "y": 0.3784615397453308}], "text": "0.2243 0.2565\n"}
{"page": 9, "bbox": [{"x": 0.14391353726387024, "y": 0.3841758370399475}, {"x": 0.20648464560508728, "y": 0.3841758370399475}, {"x": 0.20648464560508728, "y": 0.39120879769325256}, {"x": 0.14391353726387024, "y": 0.39120879769325256}], "text": "18 0.2240\n"}
{"page": 9, "bbox": [{"x": 0.5455062389373779, "y": 0.3841758370399475}, {"x": 0.5790671110153198, "y": 0.38461539149284363}, {"x": 0.5790671110153198, "y": 0.39120879769325256}, {"x": 0.5455062389373779, "y": 0.39076924324035645}], "text": "0.2177\n"}
{"page": 9, "bbox": [{"x": 0.5466439127922058, "y": 0.4206593334674835}, {"x": 0.6717861294746399, "y": 0.4206593334674835}, {"x": 0.6717861294746399, "y": 0.42945054173469543}, {"x": 0.5466439127922058, "y": 0.42945054173469543}], "text": "CONCLUSIONS\n"}
{"page": 9, "bbox": [{"x": 0.5199089646339417, "y": 0.42153847217559814}, {"x": 0.5290102362632751, "y": 0.42153847217559814}, {"x": 0.5290102362632751, "y": 0.42945054173469543}, {"x": 0.5199089646339417, "y": 0.42945054173469543}], "text": "6\n"}
{"page": 9, "bbox": [{"x": 0.08589305728673935, "y": 0.41846153140068054}, {"x": 0.48236632347106934, "y": 0.4180219769477844}, {"x": 0.48236632347106934, "y": 0.5393406748771667}, {"x": 0.08589305728673935, "y": 0.5397801995277405}], "text": "Table 5: Accuracy of Falcon-7b on Reddit data in the random\n+ retrieved setting [I,,, Q]. Rows denote the number of\nrandom documents added, and columns show the quan-\ntity of retrieved documents . Scenarios where the prompt\nexceeded the model's input limit, leading to potential data\ntruncation, are not included ( - ). Each value not marked with\nan asterisk * represents a statistically significant change from\nthe base case (first row), as determined by a Wilcoxon test\n(p-value < 0.05).\n"}
{"page": 9, "bbox": [{"x": 0.5187713503837585, "y": 0.43868130445480347}, {"x": 0.9152445793151855, "y": 0.43868130445480347}, {"x": 0.9152445793151855, "y": 0.670769214630127}, {"x": 0.5187713503837585, "y": 0.670769214630127}], "text": "In this paper, we conducted the first comprehensive study focus-\ning on the impact of retrieved documents on the RAG framework,\naiming to understand the traits required in a retriever to optimize\nprompt construction for a RAG system. This study led to several im-\nportant findings, including two unexpected ones. First, the position\nof relevant information should be placed near the query; otherwise,\nthe model seriously struggles to attend to it. Second, in contrast to\ncommon perception, top-scoring retrieved documents that do not\ncontain the answer, when added to a prompt, negatively impact\nthe LLM effectiveness. Finally, and even more surprisingly, random,\nnoisy documents are actually helpful in increasing the accuracy of\nthese systems when correctly positioned within a prompt. While\nwe have proposed heuristics to exploit these findings, further re-\nsearch is needed both to uncover the inner mechanisms behind\nthis behavior and to develop a new generation of information re-\ntrieval techniques that are specifically designed to interact with the\ngenerative component.\n"}
{"page": 9, "bbox": [{"x": 0.13367463648319244, "y": 0.5639560222625732}, {"x": 0.13879407942295074, "y": 0.5639560222625732}, {"x": 0.13879407942295074, "y": 0.5687912106513977}, {"x": 0.13367463648319244, "y": 0.5687912106513977}], "text": "#\n"}
{"page": 9, "bbox": [{"x": 0.2377701997756958, "y": 0.5687912106513977}, {"x": 0.2440273016691208, "y": 0.5687912106513977}, {"x": 0.2440273016691208, "y": 0.5740659236907959}, {"x": 0.2377701997756958, "y": 0.5740659236907959}], "text": "2\n"}
{"page": 9, "bbox": [{"x": 0.2906712293624878, "y": 0.5687912106513977}, {"x": 0.29635950922966003, "y": 0.5687912106513977}, {"x": 0.29635950922966003, "y": 0.5740659236907959}, {"x": 0.2906712293624878, "y": 0.5740659236907959}], "text": "3\n"}
{"page": 9, "bbox": [{"x": 0.39590445160865784, "y": 0.5687912106513977}, {"x": 0.40216153860092163, "y": 0.5687912106513977}, {"x": 0.40216153860092163, "y": 0.5745055079460144}, {"x": 0.39590445160865784, "y": 0.5745055079460144}], "text": "5\n"}
{"page": 9, "bbox": [{"x": 0.4482366442680359, "y": 0.5687912106513977}, {"x": 0.4550625681877136, "y": 0.5687912106513977}, {"x": 0.4550625681877136, "y": 0.5745055079460144}, {"x": 0.4482366442680359, "y": 0.5745055079460144}], "text": "9\n"}
{"page": 9, "bbox": [{"x": 0.1860068291425705, "y": 0.5692307949066162}, {"x": 0.19112628698349, "y": 0.5692307949066162}, {"x": 0.19112628698349, "y": 0.5745055079460144}, {"x": 0.1860068291425705, "y": 0.5745055079460144}], "text": "1\n"}
{"page": 9, "bbox": [{"x": 0.34300342202186584, "y": 0.5692307949066162}, {"x": 0.3498293459415436, "y": 0.5692307949066162}, {"x": 0.3498293459415436, "y": 0.5745055079460144}, {"x": 0.34300342202186584, "y": 0.5745055079460144}], "text": "4\n"}
{"page": 9, "bbox": [{"x": 0.09840728342533112, "y": 0.5740659236907959}, {"x": 0.11831627041101456, "y": 0.5740659236907959}, {"x": 0.11831627041101456, "y": 0.5810989141464233}, {"x": 0.09840728342533112, "y": 0.5810989141464233}], "text": "# B\n"}
{"page": 9, "bbox": [{"x": 0.14789533615112305, "y": 0.5929670333862305}, {"x": 0.15415245294570923, "y": 0.5929670333862305}, {"x": 0.15415245294570923, "y": 0.5986813306808472}, {"x": 0.14789533615112305, "y": 0.5986813306808472}], "text": "0\n"}
{"page": 9, "bbox": [{"x": 0.3270762264728546, "y": 0.592527449131012}, {"x": 0.4692832827568054, "y": 0.591208815574646}, {"x": 0.469852089881897, "y": 0.6109890341758728}, {"x": 0.32764506340026855, "y": 0.6123076677322388}], "text": "0.1938 0.1942 0.1998\n0.1924* 0.1976*\n"}
{"page": 9, "bbox": [{"x": 0.16894197463989258, "y": 0.592527449131012}, {"x": 0.3122866749763489, "y": 0.5916483402252197}, {"x": 0.3122866749763489, "y": 0.611868143081665}, {"x": 0.16894197463989258, "y": 0.6127472519874573}], "text": "0.1568 0.1717 0.1855\n0.1551* 0.1793* 0.1897*\n"}
{"page": 9, "bbox": [{"x": 0.1484641581773758, "y": 0.6052747368812561}, {"x": 0.1535836160182953, "y": 0.6052747368812561}, {"x": 0.1535836160182953, "y": 0.6105494499206543}, {"x": 0.1484641581773758, "y": 0.6105494499206543}], "text": "1\n"}
{"page": 9, "bbox": [{"x": 0.14789533615112305, "y": 0.6171428561210632}, {"x": 0.1535836160182953, "y": 0.6171428561210632}, {"x": 0.1535836160182953, "y": 0.6224175691604614}, {"x": 0.14789533615112305, "y": 0.6224175691604614}], "text": "2\n"}
{"page": 9, "bbox": [{"x": 0.16894197463989258, "y": 0.6167032718658447}, {"x": 0.41695109009742737, "y": 0.614065945148468}, {"x": 0.41695109009742737, "y": 0.6232966780662537}, {"x": 0.16894197463989258, "y": 0.6259340643882751}], "text": "0.1529* 0.1762* 0.1938* 0.2011* 0.1976*\n"}
{"page": 9, "bbox": [{"x": 0.1473265141248703, "y": 0.6294505596160889}, {"x": 0.1535836160182953, "y": 0.6294505596160889}, {"x": 0.1535836160182953, "y": 0.6347252726554871}, {"x": 0.1473265141248703, "y": 0.6347252726554871}], "text": "3\n"}
{"page": 9, "bbox": [{"x": 0.16894197463989258, "y": 0.6290109753608704}, {"x": 0.2593856751918793, "y": 0.6272527575492859}, {"x": 0.2593856751918793, "y": 0.6351648569107056}, {"x": 0.16894197463989258, "y": 0.63692307472229}], "text": "0.1599* 0.1727*\n"}
{"page": 9, "bbox": [{"x": 0.16894197463989258, "y": 0.6404395699501038}, {"x": 0.20705346763134003, "y": 0.6399999856948853}, {"x": 0.20705346763134003, "y": 0.6474725008010864}, {"x": 0.16894197463989258, "y": 0.6479120850563049}], "text": "0.1606*\n"}
{"page": 9, "bbox": [{"x": 0.14789533615112305, "y": 0.6421977877616882}, {"x": 0.15415245294570923, "y": 0.6421977877616882}, {"x": 0.15415245294570923, "y": 0.6474725008010864}, {"x": 0.14789533615112305, "y": 0.6474725008010864}], "text": "4\n"}
{"page": 9, "bbox": [{"x": 0.16951081156730652, "y": 0.6290109753608704}, {"x": 0.4175198972225189, "y": 0.6290109753608704}, {"x": 0.4175198972225189, "y": 0.6606593132019043}, {"x": 0.16951081156730652, "y": 0.6606593132019043}], "text": "0.1911* 0.2021* 0.2118\n0.1758* 0.1959 0.2073 0.2108\n0.1627* 0.1762* 0.2000 0.2108\n"}
{"page": 9, "bbox": [{"x": 0.1473265141248703, "y": 0.6523076891899109}, {"x": 0.1535836160182953, "y": 0.6523076891899109}, {"x": 0.15301479399204254, "y": 0.694505512714386}, {"x": 0.14675767719745636, "y": 0.694505512714386}], "text": "567 ∞\n"}
{"page": 9, "bbox": [{"x": 0.16894197463989258, "y": 0.666373610496521}, {"x": 0.3117178678512573, "y": 0.666373610496521}, {"x": 0.3117178678512573, "y": 0.6852747201919556}, {"x": 0.16894197463989258, "y": 0.6852747201919556}], "text": "0.1651* 0.1848 0.2004\n0.1675 0.1848\n"}
{"page": 9, "bbox": [{"x": 0.1484641581773758, "y": 0.691428542137146}, {"x": 0.15415245294570923, "y": 0.691428542137146}, {"x": 0.15415245294570923, "y": 0.696703314781189}, {"x": 0.1484641581773758, "y": 0.696703314781189}], "text": "8\n"}
{"page": 9, "bbox": [{"x": 0.16951081156730652, "y": 0.691428542137146}, {"x": 0.20591580867767334, "y": 0.691428542137146}, {"x": 0.20591580867767334, "y": 0.6975824236869812}, {"x": 0.16951081156730652, "y": 0.6975824236869812}], "text": "0.1682\n"}
{"page": 9, "bbox": [{"x": 0.5193401575088501, "y": 0.7147252559661865}, {"x": 0.7110352516174316, "y": 0.7147252559661865}, {"x": 0.7110352516174316, "y": 0.7235164642333984}, {"x": 0.5193401575088501, "y": 0.7235164642333984}], "text": "ACKNOWLEDGMENTS\n"}
{"page": 9, "bbox": [{"x": 0.08646188676357269, "y": 0.7331868410110474}, {"x": 0.4829351603984833, "y": 0.7331868410110474}, {"x": 0.4829351603984833, "y": 0.8936263918876648}, {"x": 0.08646188676357269, "y": 0.8936263918876648}], "text": "research, we measure the entropy of the attention scores in the\ncase where only the gold document is supplied [I, ★, Q] against\nthe case in which random documents are added [I, Œ, ★, Q]. We\nfind that when we introduce random documents, the entropy of\nthe systems has a 3X increase. Although these experiments show a\npattern, we cannot yet answer this question in a definitive manner.\nWhile out of the scope of this work, which focuses on the retriever\ncomponent of RAG systems, we believe it is highly important to\ninvestigate the reasons for which the LLM shows this behavior.\nFuture studies should aim to elucidate why this noisy state is more\nadvantageous and identify the characteristics that contribute to its\neffectiveness.\n"}
{"page": 9, "bbox": [{"x": 0.5182024836540222, "y": 0.7331868410110474}, {"x": 0.9152445793151855, "y": 0.7327472567558289}, {"x": 0.9158134460449219, "y": 0.8949450254440308}, {"x": 0.5187713503837585, "y": 0.8953846096992493}], "text": "This work is supported by the Spoke “FutureHPC & BigData\" of\nthe ICSC - Centro Nazionale di Ricerca in High-Performance Com-\nputing, Big Data and Quantum Computing, the Spoke “Human-\ncentered AI\" of the M4C2 - Investimento 1.3, Partenariato Esteso\nPE00000013 - \"FAIR - Future Artificial Intelligence Research\", SER-\nICS (PE00000014), IR0000013 - SoBigData.it, funded by European\nUnion – NextGenerationEU, the FoReLab project (Departments of\nExcellence), and the NEREO PRIN project funded by the Italian Min-\nistry of Education and Research Grant no. 2022AEFHAZ. This work\nwas carried out while Florin Cuconasu was enrolled in the Italian\nNational Doctorate on Artificial Intelligence run by the Sapienza\nUniversity of Rome.\n"}
{"page": 10, "bbox": [{"x": 0.7656427621841431, "y": 0.07780219614505768}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.7656427621841431, "y": 0.08571428805589676}], "text": "Cuconasu and Trappolini, et al.\n"}
{"page": 10, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.07912088185548782}, {"x": 0.3213879466056824, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 10, "bbox": [{"x": 0.08816837519407272, "y": 0.10813187062740326}, {"x": 0.20079636573791504, "y": 0.10857142508029938}, {"x": 0.20079636573791504, "y": 0.11824175715446472}, {"x": 0.08816837519407272, "y": 0.117802195250988}], "text": "REFERENCES\n"}
{"page": 10, "bbox": [{"x": 0.5187713503837585, "y": 0.1120879128575325}, {"x": 0.9141069650650024, "y": 0.11164835095405579}, {"x": 0.914675772190094, "y": 0.8940659165382385}, {"x": 0.5193401575088501, "y": 0.894505500793457}], "text": "Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,\n6769-6781. https://doi.org/10.18653/v1/2020.emnlp-main.550\n[21] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael\nBendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.\narXiv preprint arXiv:2401.06954 (2024).\n[22] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of naacL-HLT, Vol. 1. Association for Computational Linguistic, Min-\nneapolis, 2.\n[23] Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby,\nfuzzy far away: How neural language models use context.\n[24] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage\nsearch via contextualized late interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research and development in Information\nRetrieval. ACM, Xi'an, 39-48.\n[25] Bevan Koopman and Guido Zuccon. 2023. Dr ChatGPT tell me what I want to\nhear: How different prompts impact health answer correctness. In Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing,\nHouda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational\nLinguistics, Singapore, 15012-15022. https://doi.org/10.18653/v1/2023.emnlp-\nmain.928\n[26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\nKristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.\nDai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A\nBenchmark for Question Answering Research. Transactions of the Association for\nComputational Linguistics 7 (2019), 452-466. https://doi.org/10.1162/tacl_a_00276\n[27] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for\nWeakly Supervised Open Domain Question Answering. In Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R.\nTraum, and Lluís Màrquez (Eds.). Association for Computational Linguistics,\nFlorence, 6086-6096. https://doi.org/10.18653/V1/P19-1612\n[28] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459-9474.\n[29] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar,\nand Yin Tat Lee. 2023. Textbooks are all you need ii: phi-1.5 technical report.\n[30] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models\nuse long contexts.\n[31] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022.\nFantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot\nPrompt Order Sensitivity. In Proceedings of the 60th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio (Eds.). Association for Computational\nLinguistics, Dublin, Ireland, 8086-8098. https://doi.org/10.18653/v1/2022.acl-\nlong.556\n[32] C Manning, P Raghavan, and H Schutze. 2008. Term weighting, and the vector\nspace model. Cambridge University Press Cambridge, Cambridge. 109-133 pages.\n[33] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram\nPasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language models: a survey.\n[34] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020.\nAmbigQA: Answering Ambiguous Open-domain Questions. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\nBonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for\nComputational Linguistics, Online, 5783-5797. https://doi.org/10.18653/v1/2020.\nemnlp-main.466\n[35] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming\nCurated Corpora with Web Data, and Web Data Only. arXiv:2306.01116 [cs.CL]\n[36] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention\nwith Linear Biases Enables Input Length Extrapolation. https://openreview.net/\nforum?id=R8sQPPGCv0\n[37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.\nImproving language understanding by generative pre-training.\n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1,8 (2019), 9.\n[39] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin\nLeyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language\nmodels.\n[40] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and TrendsⓇ in Information Retrieval\n3, 4 (2009), 333-389.\n"}
{"page": 10, "bbox": [{"x": 0.08930602669715881, "y": 0.12483516335487366}, {"x": 0.4846416413784027, "y": 0.1257142871618271}, {"x": 0.48236632347106934, "y": 0.8896703124046326}, {"x": 0.08703071624040604, "y": 0.8887912034988403}], "text": "[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,\nRuxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien\nLaunay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier,\nand Guilherme Penedo. 2023. The Falcon Series of Open Language Models.\narXiv:2311.16867 [cs.CL]\n[2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.\nSelf-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.\narXiv:2310.11511 [cs.CL]\n[3] Giuseppe Attanasio, Debora Nozza, Dirk Hovy, and Elena Baralis. 2022. Entropy-\nbased attention regularization frees unintended bias mitigation from lists.\n[4] Andrea Bacciu, Florin Cuconasu, Federico Siciliano, Fabrizio Silvestri, Nicola\nTonellotto, and Giovanni Trappolini. 2023. RRAML: Reinforced Retrieval Aug-\nmented Machine Learning. In Proceedings of the Discussion Papers - 22nd Interna-\ntional Conference of the Italian Association for Artificial Intelligence (AIxIA 2023 DP)\nco-located with 22nd International Conference of the Italian Association for Artificial\nIntelligence (AIxIA 2023), Rome, Italy, November 6-9, 2023 (CEUR Workshop Proceed-\nings, Vol. 3537), Roberto Basili, Domenico Lembo, Carla Limongelli, and Andrea\nOrlandini (Eds.). CEUR-WS.org, 29-37. https://ceur-ws.org/Vol-3537/paper4.pdf\n[5] Andrea Bacciu, Giovanni Trappolini, Andrea Santilli, Emanuele Rodolà, and\nFabrizio Silvestri. 2023. Fauno: The Italian Large Language Model that will leave\nyou senza parole!. In Proceedings of the 13th Italian Information Retrieval Workshop\n(IIR 2023), Pisa, Italy, June 8-9, 2023 (CEUR Workshop Proceedings, Vol. 3448),\nFranco Maria Nardini, Nicola Tonellotto, Guglielmo Faggioli, and Antonio Ferrara\n(Eds.). CEUR-WS.org, 9-17. https://ceur-ws.org/Vol-3448/paper-24.pdf\n[6] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lam-\nbert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.\nOpen LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_\nIlm_leaderboard.\n[7] Parishad BehnamGhader, Santiago Miret, and Siva Reddy. 2023. Can Retriever-\nAugmented Language Models Reason? The Blame Game Between the Retriever\nand the Language Model. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.).\nAssociation for Computational Linguistics, Singapore, 15492-15509. https:\n//doi.org/10.18653/v1/2023.findings-emnlp.1036\n[8] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\nford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-\ndan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving\nfrom trillions of tokens. In International conference on machine learning. PMLR,\nBaltimora, 2206-2240.\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877-1901.\n[10] Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and Effective Text Encoding\nfor Chinese LLAMA and Alpaca. arXiv preprint arXiv:2304.08177 (2023). https:\n//arxiv.org/abs/2304.08177\n[11] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,\nPierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.\nThe Faiss library. (2024). arXiv:2401.08281 [cs.LG]\n[12] Garrachonr. 2023. LlamaDos. https://github.com/Garrachonr/LlamaDos.\n[13] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.\nRetrieval augmented language model pre-training. In International conference on\nmachine learning. PMLR, Vienna, 3929-3938.\n[14] David T Hoffmann, Simon Schrodi, Nadine Behrmann, Volker Fischer, and\nThomas Brox. 2023. Eureka-Moments in Transformers: Multi-Step Tasks Reveal\nSoftmax Induced Optimization Problems.\n[15] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense infor-\nmation retrieval with contrastive learning.\n[16] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck,\nCaio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan,\nSivakanth Gopi, et al. 2023. Phi-2: The surprising power of small language\nmodels.\n[17] jphme. 2023. Llama-2-13b-chat-german. https://huggingface.co/jphme/Llama-2-\n13b-chat-german.\n[18] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.\n2023. Large language models struggle to learn long-tail knowledge. In Proceedings\nof the 40th International Conference on Machine Learning (ICML'23). JMLR.org,\nHonolulu, Hawaii, USA, Article 641, 12 pages.\n[19] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\n[20] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn,\n"}
{"page": 11, "bbox": [{"x": 0.08703071624040604, "y": 0.0782417580485344}, {"x": 0.361774742603302, "y": 0.07912088185548782}, {"x": 0.361774742603302, "y": 0.08659340441226959}, {"x": 0.08703071624040604, "y": 0.08571428805589676}], "text": "The Power of Noise: Redefining Retrieval for RAG Systems\n"}
{"page": 11, "bbox": [{"x": 0.6780432462692261, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.6780432462692261, "y": 0.08659340441226959}], "text": "SIGIR '24, July 14-18, 2024, Washington, DC, USA\n"}
{"page": 11, "bbox": [{"x": 0.5443686246871948, "y": 0.1120879128575325}, {"x": 0.9129692912101746, "y": 0.1120879128575325}, {"x": 0.9129692912101746, "y": 0.1402197778224945}, {"x": 0.5443686246871948, "y": 0.1402197778224945}], "text": "Conference on Research and Development in Information Retrieval in the Asia Pacific\nRegion (Beijing, China) (SIGIR-AP '23). Association for Computing Machinery,\nNew York, NY, USA, 46-51. https://doi.org/10.1145/3624918.3625329\n"}
{"page": 11, "bbox": [{"x": 0.08759954571723938, "y": 0.11164835095405579}, {"x": 0.4829351603984833, "y": 0.11164835095405579}, {"x": 0.4829351603984833, "y": 0.8949450254440308}, {"x": 0.08759954571723938, "y": 0.8949450254440308}], "text": "[41] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan\nLiu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston.\n2021. Recipes for Building an Open-Domain Chatbot. In Proceedings of the\n16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.).\nAssociation for Computational Linguistics, Online, 300-325. https://doi.org/10.\n18653/v1/2021.eacl-main.24\n[42] Gerard Salton and Michael J. McGill. 1983. Introduction to modern information\nretrieval. McGraw-Hill (1983).\n[43] Andrea Santilli and Emanuele Rodolà. 2023. Camoscio: an Italian Instruction-\ntuned LLAMA. arXiv:2307.16456 [cs.CL]\n[44] Artsiom Sauchuk, James Thorne, Alon Halevy, Nicola Tonellotto, and Fabrizio\nSilvestri. 2022. On the Role of Relevance in Natural Language Processing Tasks.\nIn Proceedings of the 45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. ACM, Madrid, 1785–1789.\n[45] Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You\nNeed. arXiv:1911.02150 [cs.NE]\n[46] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021.\nDo long-range language models actually use long-range context?\n[47] MosaicML NLP Team et al. 2023. Introducing mpt-7b: A new standard for open-\nsource, ly usable llms.\n[48] Gabriele Tolomei, Cesare Campagnano, Fabrizio Silvestri, and Giovanni Trap-\npolini. 2023. Prompt-to-OS (P2OS): Revolutionizing Operating Systems and\nHuman-Computer Interaction with Integrated AI Generative Models. In 5th\nIEEE International Conference on Cognitive Machine Intelligence, CogMI 2023, At-\nlanta, GA, USA, November 1-4, 2023. IEEE, 128–134. https://doi.org/10.1109/\nCOGMI58952.2023.00027\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models.\n[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\n[51] Giovanni Trappolini, Andrea Santilli, Emanuele Rodolà, Alon Y. Halevy, and\nFabrizio Silvestri. 2023. Multimodal Neural Databases. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Ed-\nward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete\n(Eds.). ACM, 2619-2628. https://doi.org/10.1145/3539618.3591930\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc., Long Beach. https://proceedings.neurips.cc/\npaper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[53] Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. 2017. TL;DR:\nMining Reddit to Learn Automatic Summarization. In Proceedings of the Workshop\non New Frontiers in Summarization, Lu Wang, Jackie Chi Kit Cheung, Giuseppe\nCarenini, and Fei Liu (Eds.). Association for Computational Linguistics, Copen-\nhagen, Denmark, 59-63. https://doi.org/10.18653/v1/W17-4508\n[54] Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, and Dacheng Tao. 2024.\nOOP: Object-Oriented Programming Evaluation Benchmark for Large Language\nModels. arXiv preprint arXiv:2401.06628 (2024).\n[55] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive\nchameleon or stubborn sloth: Revealing the behavior of large language mod-\nels in knowledge conflicts. In The Twelfth International Conference on Learning\nRepresentations.\n[56] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng\nCao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng\nXu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu.\n2024. OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in\nReal Computer Environments. arXiv:2404.07972 [cs.AI]\n[57] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers\nfor Text Ranking: BERT and Beyond. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies: Tutorials, Greg Kondrak, Kalina Bontcheva, and Dan\nGillick (Eds.). Association for Computational Linguistics, Online, 1-4. https:\n//doi.org/10.18653/v1/2021.naacl-tutorials.1\n[58] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Rama-\npuram, Yizhe Zhang, Jiatao Gu, and Joshua M Susskind. 2023. Stabilizing trans-\nformer training by preventing attention entropy collapse. In International Con-\nference on Machine Learning. PMLR, PMLR, Hawaii, 40770-40803.\n[59] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping\nMa. 2021. Optimizing Dense Retrieval Model Training with Hard Negatives.\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR '21). New York, NY, USA, 1503–1512.\n[60] Guido Zuccon, Bevan Koopman, and Razia Shaik. 2023. ChatGPT Hallucinates\nwhen Attributing Answers. In Proceedings of the Annual International ACM SIGIR\n"}
{"page": 1, "bbox": [{"x": 0.2718620002269745, "y": 0.08957106620073318}, {"x": 0.7281380295753479, "y": 0.08957106620073318}, {"x": 0.7281380295753479, "y": 0.10386879742145538}, {"x": 0.2718620002269745, "y": 0.10386879742145538}], "text": "Corrective Retrieval Augmented Generation\n"}
{"page": 1, "bbox": [{"x": 0.1427721530199051, "y": 0.11858704686164856}, {"x": 0.8607971668243408, "y": 0.11858704686164856}, {"x": 0.8607971668243408, "y": 0.182085782289505}, {"x": 0.1427721530199051, "y": 0.182085782289505}], "text": "Shi-Qi Yan¹*, Jia-Chen Gu²*, Yun Zhu³, Zhen-Hua Ling¹\n¹National Engineering Research Center of Speech and Language Information Processing,\nUniversity of Science and Technology of China, Hefei, China\n2Department of Computer Science, University of California, Los Angeles\n"}
{"page": 1, "bbox": [{"x": 0.4217727482318878, "y": 0.18502943217754364}, {"x": 0.5817965269088745, "y": 0.18502943217754364}, {"x": 0.5817965269088745, "y": 0.1984861195087433}, {"x": 0.4217727482318878, "y": 0.1984861195087433}], "text": "3Google DeepMind\n"}
{"page": 1, "bbox": [{"x": 0.1255205273628235, "y": 0.20311185717582703}, {"x": 0.8786436915397644, "y": 0.20311185717582703}, {"x": 0.8786436915397644, "y": 0.21530698239803314}, {"x": 0.1255205273628235, "y": 0.21530698239803314}], "text": "yansiki@mail.ustc.edu.cn, gujc@ucla.edu, yunzhu@google.com, zhling@ustc.edu.cn\n"}
{"page": 1, "bbox": [{"x": 0.26412850618362427, "y": 0.24264086782932281}, {"x": 0.3402736485004425, "y": 0.24264086782932281}, {"x": 0.3402736485004425, "y": 0.2514718174934387}, {"x": 0.26412850618362427, "y": 0.2514718174934387}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.6960142850875854, "y": 0.25063079595565796}, {"x": 0.8667460083961487, "y": 0.25063079595565796}, {"x": 0.8667460083961487, "y": 0.2586206793785095}, {"x": 0.6960142850875854, "y": 0.2586206793785095}], "text": "Q: Who was the screenwriter\n"}
{"page": 1, "bbox": [{"x": 0.5306365489959717, "y": 0.24894869327545166}, {"x": 0.6567519307136536, "y": 0.2502102553844452}, {"x": 0.6561570763587952, "y": 0.27249789237976074}, {"x": 0.5300416350364685, "y": 0.2712363302707672}], "text": "Q: What is Henry\nFeilden's occupation?\n"}
{"page": 1, "bbox": [{"x": 0.7120761275291443, "y": 0.26366695761680603}, {"x": 0.8483045697212219, "y": 0.26366695761680603}, {"x": 0.8483045697212219, "y": 0.27039527893066406}, {"x": 0.7120761275291443, "y": 0.27039527893066406}], "text": "for Death of a Batman?\n"}
{"page": 1, "bbox": [{"x": 0.6680546998977661, "y": 0.30950379371643066}, {"x": 0.7287328839302063, "y": 0.30950379371643066}, {"x": 0.7287328839302063, "y": 0.31665265560150146}, {"x": 0.6680546998977661, "y": 0.31665265560150146}], "text": "Retriever\n"}
{"page": 1, "bbox": [{"x": 0.5306365489959717, "y": 0.3322119414806366}, {"x": 0.6686496138572693, "y": 0.3322119414806366}, {"x": 0.6686496138572693, "y": 0.338940292596817}, {"x": 0.5306365489959717, "y": 0.338940292596817}], "text": "Accurate Documents\n"}
{"page": 1, "bbox": [{"x": 0.7162403464317322, "y": 0.3322119414806366}, {"x": 0.8643664717674255, "y": 0.3322119414806366}, {"x": 0.8643664717674255, "y": 0.338940292596817}, {"x": 0.7162403464317322, "y": 0.338940292596817}], "text": "Inaccurate Documents\n"}
{"page": 1, "bbox": [{"x": 0.715645432472229, "y": 0.36711522936820984}, {"x": 0.8506841063499451, "y": 0.3675357401371002}, {"x": 0.8500892519950867, "y": 0.42598822712898254}, {"x": 0.7150505781173706, "y": 0.42556771636009216}], "text": "Batman (1989 film):\nof the murder of Bruce\nWayne's parents. When\nHamm's script was\nrewritten, ...\n"}
{"page": 1, "bbox": [{"x": 0.5306365489959717, "y": 0.3675357401371002}, {"x": 0.6799523830413818, "y": 0.3662741780281067}, {"x": 0.680547297000885, "y": 0.4264087378978729}, {"x": 0.5312314033508301, "y": 0.42767030000686646}], "text": "Henry Feilden\n(Conservative politician):\nHenry Master Feilden\nwas an Conservative\nParty politician...\n"}
{"page": 1, "bbox": [{"x": 0.5966686606407166, "y": 0.4625735878944397}, {"x": 0.6549673080444336, "y": 0.4625735878944397}, {"x": 0.6549673080444336, "y": 0.4697224497795105}, {"x": 0.5966686606407166, "y": 0.4697224497795105}], "text": "Politician.\n"}
{"page": 1, "bbox": [{"x": 0.7787031531333923, "y": 0.4638351500034332}, {"x": 0.8298631906509399, "y": 0.4646762013435364}, {"x": 0.8292682766914368, "y": 0.4764508008956909}, {"x": 0.7781082391738892, "y": 0.47560974955558777}], "text": "Hamm.x\n"}
{"page": 1, "bbox": [{"x": 0.14574657380580902, "y": 0.2737594544887543}, {"x": 0.45984533429145813, "y": 0.2733389437198639}, {"x": 0.4604402184486389, "y": 0.6955424547195435}, {"x": 0.1463414579629898, "y": 0.6959629654884338}], "text": "Large language models (LLMs) inevitably\nexhibit hallucinations since the accuracy of\ngenerated texts cannot be secured solely by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily on the relevance of retrieved docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose the Corrective Retrieval Augmented\nGeneration (CRAG) to improve the robustness\nof generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the\noverall quality of retrieved documents for a\nquery, returning a confidence degree based\non which different knowledge retrieval ac-\ntions can be triggered. Since retrieval from\nstatic and limited corpora can only return sub-\noptimal documents, large-scale web searches\nare utilized as an extension for augmenting the\nretrieval results. Besides, a decompose-then-\nrecompose algorithm is designed for retrieved\ndocuments to selectively focus on key infor-\nmation and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be\nseamlessly coupled with various RAG-based\napproaches. Experiments on four datasets\ncovering short- and long-form generation tasks\nshow that CRAG can significantly improve the\nperformance of RAG-based approaches.\n"}
{"page": 1, "bbox": [{"x": 0.7245687246322632, "y": 0.4819175899028778}, {"x": 0.7620463967323303, "y": 0.4819175899028778}, {"x": 0.7620463967323303, "y": 0.49705633521080017}, {"x": 0.7245687246322632, "y": 0.49705633521080017}], "text": "100\n"}
{"page": 1, "bbox": [{"x": 0.028554432094097137, "y": 0.7169890403747559}, {"x": 0.030339084565639496, "y": 0.32043734192848206}, {"x": 0.06067816913127899, "y": 0.32043734192848206}, {"x": 0.05889351665973663, "y": 0.7169890403747559}], "text": "arXiv:2401.15884v3 [cs.CL] 7 Oct 2024\n"}
{"page": 1, "bbox": [{"x": 0.7144556641578674, "y": 0.5159798264503479}, {"x": 0.7828673124313354, "y": 0.5164003372192383}, {"x": 0.7828673124313354, "y": 0.5235491991043091}, {"x": 0.7144556641578674, "y": 0.5231286883354187}], "text": "Generator\n"}
{"page": 1, "bbox": [{"x": 0.5288518667221069, "y": 0.5168208479881287}, {"x": 0.5966686606407166, "y": 0.5168208479881287}, {"x": 0.5966686606407166, "y": 0.5235491991043091}, {"x": 0.5288518667221069, "y": 0.5235491991043091}], "text": "Generator\n"}
{"page": 1, "bbox": [{"x": 0.5139797925949097, "y": 0.542052149772644}, {"x": 0.8822129964828491, "y": 0.5416316390037537}, {"x": 0.8822129964828491, "y": 0.5954583883285522}, {"x": 0.5139797925949097, "y": 0.5958788990974426}], "text": "Figure 1: The examples show that a low-quality retriever\nis prone to introducing a substantial amount of irrelevant\ninformation, impeding the generators from acquiring\naccurate knowledge and potentially misleading them.\n"}
{"page": 1, "bbox": [{"x": 0.4217727482318878, "y": 0.6837678551673889}, {"x": 0.4271267056465149, "y": 0.6837678551673889}, {"x": 0.4271267056465149, "y": 0.688814103603363}, {"x": 0.4217727482318878, "y": 0.688814103603363}], "text": "1\n"}
{"page": 1, "bbox": [{"x": 0.11897680163383484, "y": 0.7157275080680847}, {"x": 0.2581796646118164, "y": 0.7157275080680847}, {"x": 0.2581796646118164, "y": 0.7253994941711426}, {"x": 0.11897680163383484, "y": 0.7253994941711426}], "text": "1 Introduction\n"}
{"page": 1, "bbox": [{"x": 0.5133848786354065, "y": 0.6190075874328613}, {"x": 0.8839976191520691, "y": 0.6190075874328613}, {"x": 0.8839976191520691, "y": 0.920941948890686}, {"x": 0.5133848786354065, "y": 0.920941948890686}], "text": "the parametric knowledge they encapsulate (Zhang\net al., 2023b; Muhlgay et al., 2023).\nPrior research has introduced the retrieval tech-\nniques to incorporate the knowledge relevant to\ninput and augment generation, as exemplified\nby retrieval-augmented generation (RAG) (Lewis\net al., 2020). In this framework, the input to models\nis augmented by prepending relevant documents\nthat are retrieved from an external knowledge\ncorpus (Guu et al., 2020). While RAG serves as a\npracticable complement to LLMs, its effectiveness\nis contingent upon the relevance and accuracy of\nthe retrieved documents (Li et al., 2022; Tan et al.,\n2022). The heavy reliance of generation on the\nretrieved knowledge raises significant concerns\nabout the model's behavior and performance in\nscenarios where retrieval may fail or return inaccu-\nrate results (Shi et al., 2023). As Figure 1 shows\nthat a low-quality retriever is prone to introducing\n"}
{"page": 1, "bbox": [{"x": 0.11778703331947327, "y": 0.7417998313903809}, {"x": 0.4878048896789551, "y": 0.7417998313903809}, {"x": 0.4878048896789551, "y": 0.8830950260162354}, {"x": 0.11778703331947327, "y": 0.8830950260162354}], "text": "Large language models (LLMs) have attracted\nincreasing attention and exhibited impressive abili-\nties to understand instructions and generate fluent\nlanguage texts (Brown et al., 2020; Ouyang et al.,\n2022; Touvron et al., 2023a). Nevertheless, LLMs\ninevitably manifest hallucinations (Ji et al., 2023)\ndue to their struggle with factual errors (Mallen\net al., 2023; Min et al., 2023) and inability to\nsecure the accuracy of generated texts solely by\n"}
{"page": 1, "bbox": [{"x": 0.13979773223400116, "y": 0.897392749786377}, {"x": 0.2629387378692627, "y": 0.8965517282485962}, {"x": 0.2629387378692627, "y": 0.9058032035827637}, {"x": 0.13979773223400116, "y": 0.9066442251205444}], "text": "* Equal contribution.\n"}
{"page": 1, "bbox": [{"x": 0.14039261639118195, "y": 0.9087468385696411}, {"x": 0.4830458164215088, "y": 0.910008430480957}, {"x": 0.4830458164215088, "y": 0.9205214381217957}, {"x": 0.14039261639118195, "y": 0.9192599058151245}], "text": "'The code is available at github.com/HuskyInSalt/CRAG\n"}
{"page": 2, "bbox": [{"x": 0.11778703331947327, "y": 0.08788898587226868}, {"x": 0.48899465799331665, "y": 0.08788898587226868}, {"x": 0.48899465799331665, "y": 0.3086627423763275}, {"x": 0.11778703331947327, "y": 0.3086627423763275}], "text": "a substantial amount of irrelevant information,\nimpeding the models from acquiring accurate\nknowledge and potentially misleading them, result-\ning in issues such as hallucinations (Zhang et al.,\n2023b). However, most conventional RAG ap-\nproaches indiscriminately incorporate the retrieved\ndocuments, regardless of whether these documents\nare relevant or not (Rony et al., 2022). Furthermore,\ncurrent methods mostly treat complete documents\nas reference knowledge both during retrieval and\nutilization. But a considerable portion of the text\nwithin these retrieved documents is often non-\nessential for generation, which should not have\nbeen equally referred to and involved in RAG.\n"}
{"page": 2, "bbox": [{"x": 0.5133848786354065, "y": 0.08704794198274612}, {"x": 0.8834027647972107, "y": 0.08704794198274612}, {"x": 0.8834027647972107, "y": 0.42220354080200195}, {"x": 0.5133848786354065, "y": 0.42220354080200195}], "text": "four datasets of PopQA (Mallen et al., 2023), Biog-\nraphy (Min et al., 2023), Pub Health (Zhang et al.,\n2023a), and Arc-Challenge (Bhakthavatsalam et al.,\n2021) show that CRAG can significantly improve\nthe performance of standard RAG and state-of-the-\nart Self-RAG, demonstrating its generalizability\nacross both short- and long-form generation tasks.\nTo facilitate others to reproduce our results, we will\npublish all source code later.\nIn summary, our contributions in this paper are\nthree-fold: 1) This paper studies the scenarios\nwhere the retriever returns inaccurate results and,\nto the best of our knowledge, makes the first\nattempt to design corrective strategies for RAG to\nimprove its robustness. 2) A plug-and-play method\nnamed CRAG is proposed to improve the ability of\nautomatic self-correction and efficient utilization\nof retrieved documents. 3) Experimental results\nextensively demonstrate CRAG's adaptability to\nRAG-based approaches and its generalizability\nacross short- and long-form generation tasks.\n"}
{"page": 2, "bbox": [{"x": 0.47233790159225464, "y": 0.4108494520187378}, {"x": 0.4860202372074127, "y": 0.4108494520187378}, {"x": 0.4860202372074127, "y": 0.41883936524391174}, {"x": 0.47233790159225464, "y": 0.41883936524391174}], "text": "A\n"}
{"page": 2, "bbox": [{"x": 0.5127900242805481, "y": 0.44028595089912415}, {"x": 0.663890540599823, "y": 0.44028595089912415}, {"x": 0.663890540599823, "y": 0.4495374262332916}, {"x": 0.5127900242805481, "y": 0.4495374262332916}], "text": "2 Related Work\n"}
{"page": 2, "bbox": [{"x": 0.47233790159225464, "y": 0.5071488618850708}, {"x": 0.4872100055217743, "y": 0.5071488618850708}, {"x": 0.4872100055217743, "y": 0.5155593156814575}, {"x": 0.47233790159225464, "y": 0.5155593156814575}], "text": "A\n"}
{"page": 2, "bbox": [{"x": 0.11778703331947327, "y": 0.3120269179344177}, {"x": 0.48839977383613586, "y": 0.3120269179344177}, {"x": 0.48899465799331665, "y": 0.9213625192642212}, {"x": 0.11838191747665405, "y": 0.9213625192642212}], "text": "On account of the above issues, this paper\nparticularly studies the scenarios where\nthe retriever returns inaccurate results. A\nmethod named Corrective Retrieval-Augmented\nGeneration (CRAG) is proposed to self-correct\nthe results of retriever and improve the utilization\nof documents for augmenting generation.\nlightweight retrieval evaluator is designed to\nassess the overall quality of retrieved documents\nfor a query. This serves as a crucial component\nin RAG, contributing to informative generation\nby reviewing and evaluating the relevance\nand reliability of the retrieved documents.\nconfidence degree is quantified based on which\ndifferent knowledge retrieval actions of {Correct,\nIncorrect, Ambiguous} can be triggered. For the\nlatter two actions, large-scale web searches (Piktus\net al., 2021; Komeili et al., 2022) are integrated as\na strategic extension, since retrieval from static\nand limited corpora can only return sub-optimal\ndocuments in terms of scope and diversity. This\naugmentation is implemented to broaden the\nspectrum of retrieved information, harnessing\nthe expansive and dynamic nature of the web\nto complement and enrich the initially obtained\ndocuments. Furthermore, to eliminate redundant\ncontexts contained in retrieved documents that are\nunhelpful for RAG, a decompose-then-recompose\nalgorithm is meticulously crafted throughout the\nretrieval and utilization process. This algorithm\nensures the refinement of retrieved information,\noptimizing the extraction of key insights and\nminimizing the inclusion of non-essential elements,\nthereby enhancing the utilization of retrieved data.\nCRAG is plug-and-play and experimentally\nimplemented into RAG (Lewis et al., 2020) and\nSelf-RAG (Asai et al., 2024) for demonstrating its\nadaptability to RAG-based approaches. Results on\n"}
{"page": 2, "bbox": [{"x": 0.5127900242805481, "y": 0.4659377634525299}, {"x": 0.8845925331115723, "y": 0.4663582742214203}, {"x": 0.8834027647972107, "y": 0.9213625192642212}, {"x": 0.5116002559661865, "y": 0.920941948890686}], "text": "Hallucinations of LLMS Although LLMs have\nexhibited impressive abilities to understand instruc-\ntions and generate fluent language texts (Bang et al.,\n2023; Qin et al., 2023; Zhong et al., 2023), one of\nthe most severe issues that LLMs have still been\nstruggling with is hallucinations. As many studies\nfound (Tonmoy et al., 2024; Zhang et al., 2023b;\nShuster et al., 2021), either outdated information\nor incorrect knowledge that is activated would\nseriously result in hallucinations. Large-scale\nunregulated training data collection, low proportion\nof high-quality sampling data, imperfection of\ndata allocation in the input space, and many\nother realistic factors could impact the LLMs and\nexacerbate the problems. Thus, it is obvious that\nthe lack of accurate and specific knowledge can\nlead to misleading or even inaccurate generation,\nwhich will severely hurt the experience of users in\nmost practical applications.\nRetrieval-Augmented Generation RAG (Lewis\net al., 2020; Guu et al., 2020) is regarded as a\nuseful method to address the issues above, which\nenhances the input questions of generative LMs\nwith retrieved documents. It usually provides an\nextra knowledge source from a specific corpus,\ni.e., Wikipedia, which greatly improves the per-\nformance of LMs in a variety of tasks, especially\nin the knowledge-intensive ones. The proposed\n"}
{"page": 3, "bbox": [{"x": 0.7132658958435059, "y": 0.12405382841825485}, {"x": 0.7132658958435059, "y": 0.12783852219581604}, {"x": 0.7025579810142517, "y": 0.12783852219581604}, {"x": 0.7025579810142517, "y": 0.12405382841825485}], "text": "=\n"}
{"page": 3, "bbox": [{"x": 0.5127900242805481, "y": 0.08788898587226868}, {"x": 0.8810232281684875, "y": 0.08704794198274612}, {"x": 0.881618082523346, "y": 0.19596299529075623}, {"x": 0.5133848786354065, "y": 0.19680403172969818}], "text": "framework is usually divided into a retriever R\nand a generator G. The retriever R aims to retrieve\nthe top-K documents D {dr1,..., dr} that are\nrelevant to the input X from the corpus C. Based\non the input X and the retrieved results D, the\ngenerator G is responsible for generating the output\ny. This framework can be formulated as:\n"}
{"page": 3, "bbox": [{"x": 0.8607971668243408, "y": 0.21278385818004608}, {"x": 0.8810232281684875, "y": 0.21278385818004608}, {"x": 0.8810232281684875, "y": 0.22413793206214905}, {"x": 0.8607971668243408, "y": 0.22413793206214905}], "text": "(1)\n"}
{"page": 3, "bbox": [{"x": 0.5758476853370667, "y": 0.21152228116989136}, {"x": 0.8179655075073242, "y": 0.21152228116989136}, {"x": 0.8179655075073242, "y": 0.22582001984119415}, {"x": 0.5758476853370667, "y": 0.22582001984119415}], "text": "P(Y|X) = P(D|X)P(Y,D|X).\n"}
{"page": 3, "bbox": [{"x": 0.5127900242805481, "y": 0.24011774361133575}, {"x": 0.8839976191520691, "y": 0.2392767071723938}, {"x": 0.8845925331115723, "y": 0.332632452249527}, {"x": 0.5133848786354065, "y": 0.3334735035896301}], "text": "It shows that the retriever and generator are seam-\nlessly coupled, exhibiting low risk tolerance. Any\nunsuccessful retrieval can result in an unsatisfac-\ntory response, regardless of the impressive abilities\nof the generator. This is exactly the focus of this\npaper to improve the robustness of generation.\n"}
{"page": 3, "bbox": [{"x": 0.5127900242805481, "y": 0.34903278946876526}, {"x": 0.603807270526886, "y": 0.34903278946876526}, {"x": 0.603807270526886, "y": 0.3587048053741455}, {"x": 0.5127900242805481, "y": 0.3587048053741455}], "text": "4 CRAG\n"}
{"page": 3, "bbox": [{"x": 0.11778703331947327, "y": 0.08662741631269455}, {"x": 0.48839977383613586, "y": 0.08662741631269455}, {"x": 0.48839977383613586, "y": 0.7994112968444824}, {"x": 0.11778703331947327, "y": 0.7994112968444824}], "text": "methods generally leverage information retrieval to\nsupply documents containing relevant knowledge\nfor generative LLMs. Earlier studies adopt either\nsparse or dense retrievers at the front end of a pre-\ntrained language model that specializes in response\ngeneration. Despite this, the methods above usually\nignore a question, what if the retrieval goes wrong?\nSince the purpose of introducing a retrieval is to\nsecure that generative LMs can obtain relevant and\naccurate knowledge. If retrieved documents are\nirrelevant, the retrieval system can even exacerbate\nthe factual error that LMs make.\nAdvanced RAG Many advanced approaches\nhave been developed from the original RAG in\nrecent years (Zhang et al., 2024; Kim et al., 2024;\nWang et al., 2024; Liu et al., 2024). Considering\nthat retrieval is sometimes unnecessary for some\nqueries, conversely, responses without retrieval\nare even more accurate in many situations. Self-\nRAG (Asai et al., 2024) is proposed to selectively\nretrieve knowledge and introduce a critic model\nto decide whether to retrieve. Yoran et al. (2024)\ndesigned an NLI model to identify the irrelevant\ncontext and improve robustness. SAIL (Luo\net al., 2023) is tuned on instructions to insert\nretrieved documents before instructions. While\nToolformer (Schick et al., 2023) is pre-trained for\ncalling APIs such as Wikipedia. In addition, in\nsome long-text generation tasks, external knowl-\nedge is needed more than once, and when to\nretrieve should be concerned. Jiang et al. (2023)\nactively anticipate future content and decide when\nand what to retrieve in long-form generation.\nCompared with recent studies (Schick et al.,\n2023; Luo et al., 2023; Asai et al., 2024) that are\nthe most relevant to our work, a main difference\nshould be highlighted. These approaches target\non exploiting retrieval as a useful tool to augment\ngeneration or whether retrieval is necessary, while\nthis study particularly studies the scenarios where\nthe retriever returns inaccurate results. To the best\nof our knowledge, this paper makes the first attempt\nto explore and design corrective strategies for RAG\nto improve its robustness of generation.\n"}
{"page": 3, "bbox": [{"x": 0.5127900242805481, "y": 0.37426409125328064}, {"x": 0.8851873874664307, "y": 0.374684602022171}, {"x": 0.8845925331115723, "y": 0.9213625192642212}, {"x": 0.5121951103210449, "y": 0.920941948890686}], "text": "4.1 Overview of Model Inference\nFigure 2 and Algorithm 1 present an overview\nof CRAG at inference, which designs corrective\nstrategies to improve the robustness of generation.\nGiven an input query and the retrieved documents\nfrom any retriever, a lightweight retrieval evaluator\nis constructed to estimate the relevance score\nof retrieved documents to the input query (Sec-\ntion 4.2). The relevance score is quantified into a\ntotal of three confidence degrees and then triggered\nthe corresponding actions: {Correct, Incorrect,\nAmbiguous} (Section 4.3). If the action Correct\nis triggered, the retrieved documents will be re-\nfined into more precise knowledge strips. This\nrefinement operation involves knowledge decom-\nposition, filter, and recomposition (Section 4.4).\nIf the action Incorrect is triggered, the retrieved\ndocuments will be discarded. Instead, web searches\nare resorted to and regarded as complementary\nknowledge sources for corrections (Section 4.5).\nEventually, when it cannot confidently make a\ncorrect or incorrect judgment, a soft and balanced\naction Ambiguous which combines both of them is\ntriggered. After optimizing the retrieval results, an\narbitrary generative model can be adopted.\n4.2 Retrieval Evaluator\nIt is natural to wonder whether the retrieved docu-\nments are accurate or not before using them, which\nis significant since irrelevant or misleading mes-\nsages can be identified in this way. The accuracy\nof the retrieval evaluator undeniably plays a pivotal\nrole in shaping the overall system performance, as\nit influences the outcomes of subsequent processes.\n"}
{"page": 3, "bbox": [{"x": 0.11838191747665405, "y": 0.8174936771392822}, {"x": 0.3022010624408722, "y": 0.8174936771392822}, {"x": 0.3022010624408722, "y": 0.8271656632423401}, {"x": 0.11838191747665405, "y": 0.8271656632423401}], "text": "3 Task Formulation\n"}
{"page": 3, "bbox": [{"x": 0.11838191747665405, "y": 0.8439865708351135}, {"x": 0.48839977383613586, "y": 0.843145489692688}, {"x": 0.48899465799331665, "y": 0.9205214381217957}, {"x": 0.11897680163383484, "y": 0.9213625192642212}], "text": "Following previous work (Lewis et al., 2020; Asai\net al., 2024), given input X and an accessible\ncorpus containing a large amount of knowledge\ndocuments C = {d1, ..., dN}, the system is ex-\npected to generate the output . The entire\n"}
{"page": 4, "bbox": [{"x": 0.605591893196106, "y": 0.09209419786930084}, {"x": 0.7757287621498108, "y": 0.09209419786930084}, {"x": 0.7757287621498108, "y": 0.10092514753341675}, {"x": 0.605591893196106, "y": 0.10092514753341675}], "text": "Retrieved Documents\n"}
{"page": 4, "bbox": [{"x": 0.1374182105064392, "y": 0.09587889164686203}, {"x": 0.21653777360916138, "y": 0.09587889164686203}, {"x": 0.21653777360916138, "y": 0.10555088520050049}, {"x": 0.1374182105064392, "y": 0.10555088520050049}], "text": "Retrieval\n"}
{"page": 4, "bbox": [{"x": 0.2629387378692627, "y": 0.09840201586484909}, {"x": 0.5752528309822083, "y": 0.09840201586484909}, {"x": 0.5752528309822083, "y": 0.107232965528965}, {"x": 0.2629387378692627, "y": 0.107232965528965}], "text": "X: Who was the screenwriter for Death of a Batman?\n"}
{"page": 4, "bbox": [{"x": 0.7935752272605896, "y": 0.09377628564834595}, {"x": 0.863176703453064, "y": 0.09377628564834595}, {"x": 0.863176703453064, "y": 0.11185870319604874}, {"x": 0.7935752272605896, "y": 0.11185870319604874}], "text": "d, dz\n"}
{"page": 4, "bbox": [{"x": 0.474122554063797, "y": 0.14760303497314453}, {"x": 0.655562162399292, "y": 0.14592094719409943}, {"x": 0.655562162399292, "y": 0.15685449540615082}, {"x": 0.474122554063797, "y": 0.15853658318519592}], "text": "Knowledge Refinement\n"}
{"page": 4, "bbox": [{"x": 0.3414634168148041, "y": 0.14928510785102844}, {"x": 0.4342653155326843, "y": 0.14886459708213806}, {"x": 0.4342653155326843, "y": 0.15643398463726044}, {"x": 0.3414634168148041, "y": 0.15685449540615082}], "text": "Ask: If retrieved\n"}
{"page": 4, "bbox": [{"x": 0.23973825573921204, "y": 0.14886459708213806}, {"x": 0.3140987455844879, "y": 0.14886459708213806}, {"x": 0.3140987455844879, "y": 0.17367535829544067}, {"x": 0.23973825573921204, "y": 0.17367535829544067}], "text": "Retrieval\nEvaluator\n"}
{"page": 4, "bbox": [{"x": 0.3450327217578888, "y": 0.16148023307323456}, {"x": 0.43307554721832275, "y": 0.16190075874328613}, {"x": 0.43307554721832275, "y": 0.16904962062835693}, {"x": 0.3450327217578888, "y": 0.16862909495830536}], "text": "documents are\n"}
{"page": 4, "bbox": [{"x": 0.5901249051094055, "y": 0.1698906570672989}, {"x": 0.6210588812828064, "y": 0.17073170840740204}, {"x": 0.620464026927948, "y": 0.17998318374156952}, {"x": 0.5895300507545471, "y": 0.17914213240146637}], "text": "stripi\n"}
{"page": 4, "bbox": [{"x": 0.35157644748687744, "y": 0.17451639473438263}, {"x": 0.42415228486061096, "y": 0.17409588396549225}, {"x": 0.42415228486061096, "y": 0.18082422018051147}, {"x": 0.35157644748687744, "y": 0.18124474585056305}], "text": "correct to x?\n"}
{"page": 4, "bbox": [{"x": 0.4949434995651245, "y": 0.1766190081834793}, {"x": 0.5086258053779602, "y": 0.1766190081834793}, {"x": 0.5086258053779602, "y": 0.18671151995658875}, {"x": 0.4949434995651245, "y": 0.18671151995658875}], "text": "d₁\n"}
{"page": 4, "bbox": [{"x": 0.7091017365455627, "y": 0.17830109596252441}, {"x": 0.740630567073822, "y": 0.18082422018051147}, {"x": 0.738845944404602, "y": 0.1909167319536209}, {"x": 0.707317054271698, "y": 0.18839360773563385}], "text": "stripi\n"}
{"page": 4, "bbox": [{"x": 0.5913146734237671, "y": 0.18965516984462738}, {"x": 0.6216537952423096, "y": 0.19133725762367249}, {"x": 0.620464026927948, "y": 0.19974768161773682}, {"x": 0.5901249051094055, "y": 0.1980656087398529}], "text": "strip2\n"}
{"page": 4, "bbox": [{"x": 0.8322427272796631, "y": 0.19133725762367249}, {"x": 0.8483045697212219, "y": 0.1930193454027176}, {"x": 0.846519947052002, "y": 0.20100925862789154}, {"x": 0.8304580450057983, "y": 0.19932717084884644}], "text": "kin\n"}
{"page": 4, "bbox": [{"x": 0.7091017365455627, "y": 0.2052144706249237}, {"x": 0.7400357127189636, "y": 0.20605550706386566}, {"x": 0.7394407987594604, "y": 0.21446593105793}, {"x": 0.7085068225860596, "y": 0.21362489461898804}], "text": "stripk\n"}
{"page": 4, "bbox": [{"x": 0.7566924691200256, "y": 0.20815812051296234}, {"x": 0.8143962025642395, "y": 0.20857863128185272}, {"x": 0.8143962025642395, "y": 0.21530698239803314}, {"x": 0.7566924691200256, "y": 0.21488645672798157}], "text": "Recompose\n"}
{"page": 4, "bbox": [{"x": 0.4949434995651245, "y": 0.20773759484291077}, {"x": 0.5794169902801514, "y": 0.20815812051296234}, {"x": 0.5794169902801514, "y": 0.21740958094596863}, {"x": 0.4949434995651245, "y": 0.21698907017707825}], "text": "d₂ Decompose\n"}
{"page": 4, "bbox": [{"x": 0.6513980031013489, "y": 0.21026071906089783}, {"x": 0.6769779920578003, "y": 0.21026071906089783}, {"x": 0.6769779920578003, "y": 0.21572750806808472}, {"x": 0.6513980031013489, "y": 0.21572750806808472}], "text": "Filter\n"}
{"page": 4, "bbox": [{"x": 0.5907198190689087, "y": 0.21320436894893646}, {"x": 0.622248649597168, "y": 0.21572750806808472}, {"x": 0.620464026927948, "y": 0.224978968501091}, {"x": 0.5895300507545471, "y": 0.22245584428310394}], "text": "stripk\n"}
{"page": 4, "bbox": [{"x": 0.34741225838661194, "y": 0.2266610562801361}, {"x": 0.3914336562156677, "y": 0.2266610562801361}, {"x": 0.3914336562156677, "y": 0.23296888172626495}, {"x": 0.34741225838661194, "y": 0.23296888172626495}], "text": "Correct\n"}
{"page": 4, "bbox": [{"x": 0.13265913724899292, "y": 0.23338940739631653}, {"x": 0.23200476169586182, "y": 0.23338940739631653}, {"x": 0.23200476169586182, "y": 0.26156434416770935}, {"x": 0.13265913724899292, "y": 0.26156434416770935}], "text": "Knowledge\nCorrection\n"}
{"page": 4, "bbox": [{"x": 0.474122554063797, "y": 0.2590412199497223}, {"x": 0.6418798565864563, "y": 0.2590412199497223}, {"x": 0.6418798565864563, "y": 0.2699747681617737}, {"x": 0.474122554063797, "y": 0.2699747681617737}], "text": "Knowledge Searching\n"}
{"page": 4, "bbox": [{"x": 0.33432480692863464, "y": 0.26913371682167053}, {"x": 0.40273645520210266, "y": 0.26913371682167053}, {"x": 0.40273645520210266, "y": 0.27754414081573486}, {"x": 0.33432480692863464, "y": 0.27754414081573486}], "text": "Ambiguous\n"}
{"page": 4, "bbox": [{"x": 0.7543129324913025, "y": 0.2733389437198639}, {"x": 0.7662106156349182, "y": 0.2733389437198639}, {"x": 0.7662106156349182, "y": 0.2817493677139282}, {"x": 0.7543129324913025, "y": 0.2817493677139282}], "text": "k₁\n"}
{"page": 4, "bbox": [{"x": 0.7537180185317993, "y": 0.29394447803497314}, {"x": 0.7662106156349182, "y": 0.29394447803497314}, {"x": 0.7662106156349182, "y": 0.3019343912601471}, {"x": 0.7537180185317993, "y": 0.3019343912601471}], "text": "k,\n"}
{"page": 4, "bbox": [{"x": 0.490779310464859, "y": 0.2964676320552826}, {"x": 0.502082109451294, "y": 0.2964676320552826}, {"x": 0.502082109451294, "y": 0.30277544260025024}, {"x": 0.490779310464859, "y": 0.30277544260025024}], "text": "X\n"}
{"page": 4, "bbox": [{"x": 0.8310529589653015, "y": 0.29520606994628906}, {"x": 0.8488994836807251, "y": 0.2964676320552826}, {"x": 0.8477097153663635, "y": 0.3057190775871277}, {"x": 0.8298631906509399, "y": 0.30445751547813416}], "text": "kex\n"}
{"page": 4, "bbox": [{"x": 0.5651397705078125, "y": 0.29184189438819885}, {"x": 0.690660297870636, "y": 0.29226240515708923}, {"x": 0.690660297870636, "y": 0.3111858665943146}, {"x": 0.5651397705078125, "y": 0.3107653558254242}], "text": "q: Death of a Batman;\nscreenwriter; Wikipedia\n"}
{"page": 4, "bbox": [{"x": 0.757287323474884, "y": 0.30950379371643066}, {"x": 0.7650208473205566, "y": 0.30950379371643066}, {"x": 0.7650208473205566, "y": 0.3107653558254242}, {"x": 0.757287323474884, "y": 0.3107653558254242}], "text": "...\n"}
{"page": 4, "bbox": [{"x": 0.34265318512916565, "y": 0.31160637736320496}, {"x": 0.396192729473114, "y": 0.3120269179344177}, {"x": 0.396192729473114, "y": 0.3183347284793854}, {"x": 0.34265318512916565, "y": 0.317914217710495}], "text": "Incorrect\n"}
{"page": 4, "bbox": [{"x": 0.5157644152641296, "y": 0.3124474287033081}, {"x": 0.5544319152832031, "y": 0.3124474287033081}, {"x": 0.5544319152832031, "y": 0.317914217710495}, {"x": 0.5157644152641296, "y": 0.317914217710495}], "text": "Rewrite\n"}
{"page": 4, "bbox": [{"x": 0.7894110679626465, "y": 0.3124474287033081}, {"x": 0.8197501301765442, "y": 0.3128679692745209}, {"x": 0.8197501301765442, "y": 0.31875526905059814}, {"x": 0.7894110679626465, "y": 0.3183347284793854}], "text": "Select\n"}
{"page": 4, "bbox": [{"x": 0.6983938217163086, "y": 0.31160637736320496}, {"x": 0.7317073345184326, "y": 0.31160637736320496}, {"x": 0.7317073345184326, "y": 0.32758620381355286}, {"x": 0.6983938217163086, "y": 0.32758620381355286}], "text": "Web\nSearch\n"}
{"page": 4, "bbox": [{"x": 0.7519333958625793, "y": 0.37089991569519043}, {"x": 0.8048780560493469, "y": 0.3713204264640808}, {"x": 0.8048780560493469, "y": 0.3784692883491516}, {"x": 0.7519333958625793, "y": 0.37804877758026123}], "text": "Incorrect\n"}
{"page": 4, "bbox": [{"x": 0.3093396723270416, "y": 0.3713204264640808}, {"x": 0.3539559841156006, "y": 0.3717409670352936}, {"x": 0.3539559841156006, "y": 0.37804877758026123}, {"x": 0.3093396723270416, "y": 0.37762826681137085}], "text": "Correct\n"}
{"page": 4, "bbox": [{"x": 0.5193337202072144, "y": 0.3717409670352936}, {"x": 0.5877453684806824, "y": 0.3717409670352936}, {"x": 0.5877453684806824, "y": 0.37931033968925476}, {"x": 0.5193337202072144, "y": 0.37931033968925476}], "text": "Ambiguous\n"}
{"page": 4, "bbox": [{"x": 0.32540154457092285, "y": 0.40285953879356384}, {"x": 0.37656158208847046, "y": 0.4032800793647766}, {"x": 0.37656158208847046, "y": 0.4125315248966217}, {"x": 0.32540154457092285, "y": 0.4121110141277313}], "text": "+ Kin\n"}
{"page": 4, "bbox": [{"x": 0.7340868711471558, "y": 0.4049621522426605}, {"x": 0.7459845542907715, "y": 0.4049621522426605}, {"x": 0.7459845542907715, "y": 0.4108494520187378}, {"x": 0.7340868711471558, "y": 0.4108494520187378}], "text": "X\n"}
{"page": 4, "bbox": [{"x": 0.474122554063797, "y": 0.4049621522426605}, {"x": 0.4866151213645935, "y": 0.4049621522426605}, {"x": 0.4866151213645935, "y": 0.4112699627876282}, {"x": 0.474122554063797, "y": 0.4112699627876282}], "text": "X\n"}
{"page": 4, "bbox": [{"x": 0.2849494218826294, "y": 0.4053826630115509}, {"x": 0.29625222086906433, "y": 0.4053826630115509}, {"x": 0.29625222086906433, "y": 0.4112699627876282}, {"x": 0.2849494218826294, "y": 0.4112699627876282}], "text": "X\n"}
{"page": 4, "bbox": [{"x": 0.5139797925949097, "y": 0.4053826630115509}, {"x": 0.5229030251502991, "y": 0.4053826630115509}, {"x": 0.5229030251502991, "y": 0.4112699627876282}, {"x": 0.5139797925949097, "y": 0.4112699627876282}], "text": "+\n"}
{"page": 4, "bbox": [{"x": 0.5502676963806152, "y": 0.403700590133667}, {"x": 0.5663295388221741, "y": 0.403700590133667}, {"x": 0.5663295388221741, "y": 0.4129520654678345}, {"x": 0.5502676963806152, "y": 0.4129520654678345}], "text": "Kin\n"}
{"page": 4, "bbox": [{"x": 0.5936942100524902, "y": 0.40580320358276367}, {"x": 0.6020225882530212, "y": 0.40580320358276367}, {"x": 0.6020225882530212, "y": 0.4108494520187378}, {"x": 0.5936942100524902, "y": 0.4108494520187378}], "text": "+\n"}
{"page": 4, "bbox": [{"x": 0.6299821734428406, "y": 0.403700590133667}, {"x": 0.6478286981582642, "y": 0.4049621522426605}, {"x": 0.6466389298439026, "y": 0.41337257623672485}, {"x": 0.628792405128479, "y": 0.4121110141277313}], "text": "kex\n"}
{"page": 4, "bbox": [{"x": 0.7745389938354492, "y": 0.40580320358276367}, {"x": 0.7828673124313354, "y": 0.40580320358276367}, {"x": 0.7828673124313354, "y": 0.4112699627876282}, {"x": 0.7745389938354492, "y": 0.4112699627876282}], "text": "+\n"}
{"page": 4, "bbox": [{"x": 0.8096371293067932, "y": 0.403700590133667}, {"x": 0.8274836540222168, "y": 0.4041211009025574}, {"x": 0.8274836540222168, "y": 0.4137931168079376}, {"x": 0.8096371293067932, "y": 0.41337257623672485}], "text": "kex\n"}
{"page": 4, "bbox": [{"x": 0.129684716463089, "y": 0.4230445623397827}, {"x": 0.23081499338150024, "y": 0.4230445623397827}, {"x": 0.23081499338150024, "y": 0.43271657824516296}, {"x": 0.129684716463089, "y": 0.43271657824516296}], "text": "Generation\n"}
{"page": 4, "bbox": [{"x": 0.5139797925949097, "y": 0.4533221125602722}, {"x": 0.5960737466812134, "y": 0.453742653131485}, {"x": 0.5960737466812134, "y": 0.4621530771255493}, {"x": 0.5139797925949097, "y": 0.46173253655433655}], "text": "Generator\n"}
{"page": 4, "bbox": [{"x": 0.11838191747665405, "y": 0.48654332756996155}, {"x": 0.8822129964828491, "y": 0.48654332756996155}, {"x": 0.8822129964828491, "y": 0.5256518125534058}, {"x": 0.11838191747665405, "y": 0.5256518125534058}], "text": "Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the\nrelevance of the retrieved documents to the input, and estimate a confidence degree based on which different\nknowledge retrieval actions of {Correct, Incorrect, Ambiguous} can be triggered.\n"}
{"page": 4, "bbox": [{"x": 0.5127900242805481, "y": 0.5483599901199341}, {"x": 0.8839976191520691, "y": 0.5479394197463989}, {"x": 0.8845925331115723, "y": 0.8654331564903259}, {"x": 0.5133848786354065, "y": 0.8658536672592163}], "text": "not relevant. More details about this fine-tuning\nstep can be referred to in Appendix B.3. For\nevery question, there are generally 10 documents\nretrieved. The question is concatenated with each\nsingle document as the input, and the evaluator\npredicts the relevance score for each question-\ndocument pair individually. We also tried to prompt\nChatGPT to identify the retrieval relevance for\ncomparison, but it underperforms as elaborated in\nSection 5.5. Based on these calculated relevance\nscores, a final judgment is made as to whether\nthe retrieval is correct or not associated with the\naction trigger. In our proposed framework, the\nretrieval quality is evaluated at a relatively low\ncost without the need to have access to large and\nexpensive LLMs. Compared with the critic model\nof Self-RAG (Asai et al., 2024) that instruction-\ntuned LLAMA-2 (7B), the evaluator designed in\nCRAG demonstrates the advantages of being quite\nlightweight (0.77B).\n"}
{"page": 4, "bbox": [{"x": 0.11778703331947327, "y": 0.5483599901199341}, {"x": 0.4878048896789551, "y": 0.5483599901199341}, {"x": 0.4878048896789551, "y": 0.9201009273529053}, {"x": 0.11778703331947327, "y": 0.9201009273529053}], "text": "Our objective is to correct the retrieved documents\nif they are irrelevant. Specifically, T5-large (Raffel\net al., 2020) is adopted for initializing the retrieval\nevaluator and fine-tuned. Its parameter size is much\nsmaller than the most current LLMs (Touvron et al.,\n2023a,b; Chowdhery et al., 2023; Anil et al., 2023;\nBrown et al., 2020; Ouyang et al., 2022; OpenAI,\n2023). To ensure all experimental results were\ncomparable with Self-RAG (Asai et al., 2024), the\nsame retrieval results through Contriever (Izacard\net al., 2022) provided by Self-RAG were also\nadopted in our experiments. The relevance signals\nfor fine-tuning the evaluator can be collected from\nthe existing datasets. For example, PopQA (Mallen\net al., 2023) provides the golden subject wiki title\nfrom wikipedia for each question. We can use that\nto track a not 100% relevant but rather high-quality\npassage. We utilized that as the relevance signals\nfor fine-tuning the retrieval evaluator.² On the other\nhand, the negative samples for fine-tuning were\nall randomly sampled from the retrieval results,\nwhich are rather similar to the input query but\n2https://huggingface.co/datasets/akariasai/PopQA\n"}
{"page": 5, "bbox": [{"x": 0.12611541152000427, "y": 0.09125315397977829}, {"x": 0.36109459400177, "y": 0.08999159187078476}, {"x": 0.36109459400177, "y": 0.10050462931394577}, {"x": 0.12611541152000427, "y": 0.1017661914229393}], "text": "Algorithm 1: CRAG Inference\n"}
{"page": 5, "bbox": [{"x": 0.13682332634925842, "y": 0.10807400941848755}, {"x": 0.6751933097839355, "y": 0.10807400941848755}, {"x": 0.6751933097839355, "y": 0.13666947185993195}, {"x": 0.13682332634925842, "y": 0.13666947185993195}], "text": "Require: E (Retrieval Evaluator), W (Query Rewriter), G (Generator)\nInput :x (Input question), D = {d1, d2, ..., dk} (Retrieved documents)\n"}
{"page": 5, "bbox": [{"x": 0.1207614541053772, "y": 0.1408746838569641}, {"x": 0.8132064342498779, "y": 0.1408746838569641}, {"x": 0.8132064342498779, "y": 0.23254835605621338}, {"x": 0.1207614541053772, "y": 0.23254835605621338}], "text": "Outputy (Generated response)\n1 scorei = E evaluates the relevance of each pair (x, di), di € D\n2 Confidence = Calculate and give a final judgment based on {score1, score2, ...\n...\nscorek}\n// Confidence has 3 optional values: [CORRECT], [INCORRECT] or [AMBIGUOUS]\n3 if Confidence == [CORRECT] then\nInternal_Knowledge = Knowledge_Refine(x, D)\n"}
{"page": 5, "bbox": [{"x": 0.12195122241973877, "y": 0.22455845773220062}, {"x": 0.1290898323059082, "y": 0.22455845773220062}, {"x": 0.1290898323059082, "y": 0.22960470616817474}, {"x": 0.12195122241973877, "y": 0.22960470616817474}], "text": "4\n"}
{"page": 5, "bbox": [{"x": 0.16478286683559418, "y": 0.23549200594425201}, {"x": 0.3456276059150696, "y": 0.23717409372329712}, {"x": 0.3456276059150696, "y": 0.2481076568365097}, {"x": 0.16478286683559418, "y": 0.2464255690574646}], "text": "k = Internal_Knowledge\n"}
{"page": 5, "bbox": [{"x": 0.12195122241973877, "y": 0.24011774361133575}, {"x": 0.1290898323059082, "y": 0.24011774361133575}, {"x": 0.1290898323059082, "y": 0.24558451771736145}, {"x": 0.12195122241973877, "y": 0.24558451771736145}], "text": "5\n"}
{"page": 5, "bbox": [{"x": 0.12135633826255798, "y": 0.25231286883354187}, {"x": 0.6490184664726257, "y": 0.2539949417114258}, {"x": 0.6490184664726257, "y": 0.28301092982292175}, {"x": 0.12135633826255798, "y": 0.28132885694503784}], "text": "6 else if Confidence == [INCORRECT] then\nExternal_Knowledge = Web_Search(W Rewrites x for searching)\n"}
{"page": 5, "bbox": [{"x": 0.12195122241973877, "y": 0.2733389437198639}, {"x": 0.12849494814872742, "y": 0.2733389437198639}, {"x": 0.12849494814872742, "y": 0.278385192155838}, {"x": 0.12195122241973877, "y": 0.278385192155838}], "text": "7\n"}
{"page": 5, "bbox": [{"x": 0.12254610657691956, "y": 0.29015979170799255}, {"x": 0.12849494814872742, "y": 0.29015979170799255}, {"x": 0.12849494814872742, "y": 0.29520606994628906}, {"x": 0.12254610657691956, "y": 0.29520606994628906}], "text": "8\n"}
{"page": 5, "bbox": [{"x": 0.16478286683559418, "y": 0.28679561614990234}, {"x": 0.3521713316440582, "y": 0.28679561614990234}, {"x": 0.3521713316440582, "y": 0.29899075627326965}, {"x": 0.16478286683559418, "y": 0.29899075627326965}], "text": "k = External_Knowledge\n"}
{"page": 5, "bbox": [{"x": 0.1207614541053772, "y": 0.30235493183135986}, {"x": 0.44140392541885376, "y": 0.3036164939403534}, {"x": 0.44140392541885376, "y": 0.3158116042613983}, {"x": 0.1207614541053772, "y": 0.3145500421524048}], "text": "9 else if Confidence == [AMBIGUOUS] then\n"}
{"page": 5, "bbox": [{"x": 0.1165972650051117, "y": 0.3233810067176819}, {"x": 0.12790006399154663, "y": 0.3233810067176819}, {"x": 0.12790006399154663, "y": 0.328427255153656}, {"x": 0.1165972650051117, "y": 0.328427255153656}], "text": "10\n"}
{"page": 5, "bbox": [{"x": 0.1641879826784134, "y": 0.3200168311595917}, {"x": 0.6496133208274841, "y": 0.3200168311595917}, {"x": 0.6496133208274841, "y": 0.36333054304122925}, {"x": 0.1641879826784134, "y": 0.36333054304122925}], "text": "Internal_Knowledge = Knowledge_Refine(x, D)\nExternal_Knowledge = Web_Search(W Rewrites x for searching)\nk = Internal_Knowledge + External_Knowledge\n"}
{"page": 5, "bbox": [{"x": 0.1165972650051117, "y": 0.338940292596817}, {"x": 0.12790006399154663, "y": 0.338940292596817}, {"x": 0.12790006399154663, "y": 0.3448275923728943}, {"x": 0.1165972650051117, "y": 0.3448275923728943}], "text": "11\n"}
{"page": 5, "bbox": [{"x": 0.1165972650051117, "y": 0.3557611405849457}, {"x": 0.12730517983436584, "y": 0.3557611405849457}, {"x": 0.12730517983436584, "y": 0.3603868782520294}, {"x": 0.1165972650051117, "y": 0.3603868782520294}], "text": "12\n"}
{"page": 5, "bbox": [{"x": 0.11600238084793091, "y": 0.37089991569519043}, {"x": 0.16478286683559418, "y": 0.3692178428173065}, {"x": 0.16537775099277496, "y": 0.3772077262401581}, {"x": 0.1165972650051117, "y": 0.3788898289203644}], "text": "13 end\n"}
{"page": 5, "bbox": [{"x": 0.1165972650051117, "y": 0.3856181800365448}, {"x": 0.33432480692863464, "y": 0.3843565881252289}, {"x": 0.33432480692863464, "y": 0.3965517282485962}, {"x": 0.1165972650051117, "y": 0.3978132903575897}], "text": "14 G predicts y given x and k\n"}
{"page": 5, "bbox": [{"x": 0.11778703331947327, "y": 0.4318755269050598}, {"x": 0.2754313051700592, "y": 0.4339781403541565}, {"x": 0.2748364210128784, "y": 0.44533219933509827}, {"x": 0.11778703331947327, "y": 0.443229615688324}], "text": "4.3 Action Trigger\n"}
{"page": 5, "bbox": [{"x": 0.510410487651825, "y": 0.4322960376739502}, {"x": 0.8839976191520691, "y": 0.4318755269050598}, {"x": 0.8851873874664307, "y": 0.9205214381217957}, {"x": 0.5116002559661865, "y": 0.920941948890686}], "text": "considered irrelevant, which are unhelpful for\ngeneration. Once the knowledge from the retrieval\nresults is judged to be inaccurate, it is unwise to\nstill get stuck in it, which is likely to result in\nfabricated facts. Therefore, we need to seek new\nsources of knowledge for correction. Here, web\nsearch is introduced to search from the Internet as\nelaborated in Section 4.5. This corrective action\nhelps overcome the embarrassing challenge where\nno reliable knowledge can be referred to.\nAmbiguous Except for the above two situations,\nthe remaining will be assigned to an intermediate\naction of Ambiguous. This generally occurs when\nthe accuracy of the retrieval is hard to distinguish\nand the evaluator gives an intermediate score.\nSince the retrieval evaluator is not confident in its\njudgment, both types of processed knowledge in\nCorrect and Incorrect are combined to comple-\nment each other. Implementing such a moderating\nand soft strategy can significantly contribute to\nstrengthening the robustness and resilience of the\nsystem, fostering a more adaptable framework for\noptimal performance.\nDiscussion Preliminary experiments of employ-\ning only the Correct and Incorrect actions show\nthat the efficacy of CRAG was easily affected by\nthe accuracy of the retrieval evaluator. The reason\nmight be the distinct knowledge switch for all input\ncases, regardless of the level of confidence in their\njudgment. The design of the Ambiguous action\n"}
{"page": 5, "bbox": [{"x": 0.11778703331947327, "y": 0.45458367466926575}, {"x": 0.48899465799331665, "y": 0.45458367466926575}, {"x": 0.48899465799331665, "y": 0.9188393354415894}, {"x": 0.11778703331947327, "y": 0.9188393354415894}], "text": "To correct the irrelevant documents and refine the\ntarget documents as needed, actions should be exe-\ncuted discriminately. Based on the aforementioned\nconfidence score for each retrieved document, three\ntypes of actions are designed and triggered accord-\ningly where the upper and lower thresholds are set.\nIf the confidence score is higher than the upper\nthreshold, the retrieved document is identified as\nCorrect, while identified as Incorrect if below\nthe lower threshold. Otherwise, a more soft and\nintermediate action, i.e., Ambiguous is executed.\nEach retrieved document is conducted individually\nand integrated eventually.\nCorrect Here, a retrieval is assumed Correct\nwhen the confidence score of at least one retrieved\ndocument is higher than the upper threshold. If\nso, it means that there are relevant documents in\nthe retrieved results, and the knowledge from the\nretrieval results is supposed to be more reliable and\naccurate. However, even if a relevant document can\nbe found, there is inevitably some noisy knowledge\nstrips in this document. To extract the most\ncritical knowledge strips within this document, a\nknowledge refinement method is further designed\nwhich will be elaborated in Section 4.4.\nIncorrect Besides, a retrieval is assumed\nIncorrect when the confidence scores of all\nretrieved documents are below the lower threshold.\nThis indicates that all retrieved documents are\n"}
{"page": 6, "bbox": [{"x": 0.647233784198761, "y": 0.08620689809322357}, {"x": 0.6549673080444336, "y": 0.08620689809322357}, {"x": 0.6549673080444336, "y": 0.09167367219924927}, {"x": 0.647233784198761, "y": 0.09167367219924927}], "text": "3\n"}
{"page": 6, "bbox": [{"x": 0.11838191747665405, "y": 0.08830950409173965}, {"x": 0.4860202372074127, "y": 0.08662741631269455}, {"x": 0.4860202372074127, "y": 0.1143818348646164}, {"x": 0.11838191747665405, "y": 0.1160639226436615}], "text": "significantly helps to mitigate the dependence on\nthe accuracy of the retrieval evaluator.\n"}
{"page": 6, "bbox": [{"x": 0.5127900242805481, "y": 0.08368376642465591}, {"x": 0.8839976191520691, "y": 0.08452481031417847}, {"x": 0.8834027647972107, "y": 0.2447434812784195}, {"x": 0.5121951103210449, "y": 0.24390244483947754}], "text": "for every query. Considering that knowledge\nfrom large-scale web searches could introduce\nbiases or unreliable information, authoritative and\nregulated web pages like Wikipedia are preferred,\nwhich can significantly help mitigate these issues.\nMoreover, we utilize the URL links to navigate\nweb pages, transcribe their content, and employ the\nsame knowledge refinement method as Section 4.4\nto derive the relevant web knowledge, namely\nexternal knowledge.\n"}
{"page": 6, "bbox": [{"x": 0.5133848786354065, "y": 0.25946173071861267}, {"x": 0.6549673080444336, "y": 0.2607232928276062}, {"x": 0.6549673080444336, "y": 0.2737594544887543}, {"x": 0.5133848786354065, "y": 0.27249789237976074}], "text": "5 Experiments\n"}
{"page": 6, "bbox": [{"x": 0.11719214916229248, "y": 0.1408746838569641}, {"x": 0.48899465799331665, "y": 0.14045415818691254}, {"x": 0.4901844263076782, "y": 0.453742653131485}, {"x": 0.11838191747665405, "y": 0.45416316390037537}], "text": "4.4 Knowledge Refinement\nGiven a retrieved relevant document, a decompose-\nthen-recompose knowledge refinement method\nis designed to further extract the most critical\nknowledge strips in it. To obtain fine-grained\nretrieval results, we segmented the retrieved results\ninto internal strips. If a retrieved result is as short as\none or two sentences, it is regarded as an individual\nstrip, otherwise, retrieval documents are required to\nbe split into smaller units which generally consist\nof a few sentences according to the total length.\nThe scale is assumed to include an independent\npiece of information, and the filtering is based on\nthe segments. Then, the retrieval evaluator fine-\ntuned in Section 4.2 is employed to calculate the\nrelevance score of each knowledge strip. Based\non these scores, irrelevant knowledge strips are\nfiltered out, while relevant ones are recomposed via\nconcatenation in order, namely internal knowledge.\n"}
{"page": 6, "bbox": [{"x": 0.5127900242805481, "y": 0.28637510538101196}, {"x": 0.8845925331115723, "y": 0.28637510538101196}, {"x": 0.8845925331115723, "y": 0.34524810314178467}, {"x": 0.5127900242805481, "y": 0.34524810314178467}], "text": "We conducted experiments to extensively demon-\nstrate CRAG's adaptability to RAG-based ap-\nproaches and its generalizability across both short-\nand long-form generation tasks.\n"}
{"page": 6, "bbox": [{"x": 0.11778703331947327, "y": 0.47981497645378113}, {"x": 0.2540154755115509, "y": 0.47981497645378113}, {"x": 0.2540154755115509, "y": 0.48864591121673584}, {"x": 0.11778703331947327, "y": 0.48864591121673584}], "text": "4.5 Web Search\n"}
{"page": 6, "bbox": [{"x": 0.5127900242805481, "y": 0.3608073890209198}, {"x": 0.8845925331115723, "y": 0.36122792959213257}, {"x": 0.8839976191520691, "y": 0.714886486530304}, {"x": 0.5121951103210449, "y": 0.7144659161567688}], "text": "5.1 Tasks, Datasets and Metrics\nCRAG was evaluated on four datasets, including\nPopQA (Mallen et al., 2023) (short-form gener-\nation), Biography (Min et al., 2023) (long-form\ngeneration), PubHealth (Zhang et al., 2023a) (true-\nor-false question), and Arc-Challenge (Bhaktha-\nvatsalam et al., 2021) (multiple-choice question).\nFollowing previous work, accuracy was adopted\nas the evaluation metric for PopQA, PubHealth,\nand Arc-Challenge. FactScore (Min et al., 2023)\nwas adopted as the evaluation metric for Biography.\nReaders can refer to Appendix B.1 for more details.\nThe same metrics are used because our proposed\nmethod is comparable to previous studies, since\nwe used the same retrieval results as previous\nwork. The difference lies in that our motivation\nis to improve the retrieval quality by correcting\nthe retrieval results that the system judges to\nbe of low quality. This can be analogous to\nRAG's augmentation to standalone parameterized\nlanguage models and we further augment RAG\nwith corrective strategies.\n"}
{"page": 6, "bbox": [{"x": 0.11778703331947327, "y": 0.50630784034729}, {"x": 0.48839977383613586, "y": 0.50630784034729}, {"x": 0.48839977383613586, "y": 0.920941948890686}, {"x": 0.11778703331947327, "y": 0.920941948890686}], "text": "It would be more intelligent if a system itself\ncould determine that its existing knowledge corpus\ncould not solve the problem well and turn to\nadditional external knowledge for help. On the\ncontrary, even if a system knows that the existing\nknowledge cannot solve the problem, but still\nsticks to the limited knowledge corpus, it will only\ngive a fabricated fact in the end, which is called\nhallucination.. Therefore, it is extremely important\nto seek complementary external knowledge if\nthe retrieved results are all assumed irrelevant,\nand we consider a system that knows what it\ndoesn't know and what it cannot answer to be\nmore intelligent than one that clings to limited\nknowledge and is incapable of seeking external\nknowledge. Since retrieval from static and limited\ncorpora can only return sub-optimal documents\nin terms of scope and diversity, large-scale web\nsearches (Piktus et al., 2021; Komeili et al., 2022)\nare integrated as a strategic extension of RAG.\nSpecifically, the inputs are rewritten into queries\ncomposed of keywords by ChatGPT to mimic the\ndaily usage of search engine. The prompt for\nrewriting is shown in Appendix A. In CRAG,\na public and accessible commercial web search\nAPI is adopted to generate a series of URL links\n"}
{"page": 6, "bbox": [{"x": 0.5139797925949097, "y": 0.7296047210693359}, {"x": 0.6276026368141174, "y": 0.7291842103004456}, {"x": 0.6276026368141174, "y": 0.7388561964035034}, {"x": 0.5139797925949097, "y": 0.7392767071723938}], "text": "5.2 Baselines\n"}
{"page": 6, "bbox": [{"x": 0.5127900242805481, "y": 0.750630795955658}, {"x": 0.8834027647972107, "y": 0.7510513067245483}, {"x": 0.8834027647972107, "y": 0.9201009273529053}, {"x": 0.5127900242805481, "y": 0.9196804165840149}], "text": "We primarily compared CRAG with both ap-\nproaches with and without retrieval, where the\nlatter can be further split into standard RAG and\nlatest advanced RAG, including:\nBaselines without retrieval. We evaluated some\npublic LLMs, LLaMA2-7B,13B (Touvron et al.,\n2023b), instruction-tuned models, Alpaca-7B,13B\n(Dubois et al., 2023), and CoVE65B (Dhuliawala\net al., 2024) which introduces iterative engineering\n3 In this study, Google Search API is utilized for searching.\n"}
{"page": 7, "bbox": [{"x": 0.612135648727417, "y": 0.09041211009025574}, {"x": 0.63890540599823, "y": 0.09083263576030731}, {"x": 0.6383105516433716, "y": 0.09924305975437164}, {"x": 0.6115407347679138, "y": 0.09882254153490067}], "text": "Pub\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.10597140341997147}, {"x": 0.29208803176879883, "y": 0.10555088520050049}, {"x": 0.29208803176879883, "y": 0.11396130919456482}, {"x": 0.23914337158203125, "y": 0.1143818348646164}], "text": "Method\n"}
{"page": 7, "bbox": [{"x": 0.3890541195869446, "y": 0.08915054798126221}, {"x": 0.7608566284179688, "y": 0.08999159187078476}, {"x": 0.7608566284179688, "y": 0.13877207040786743}, {"x": 0.3890541195869446, "y": 0.13793103396892548}], "text": "PopQA\nBio\nARC\n(Accuracy) (FactScore) (Accuracy) (Accuracy)\nLMs trained with propriety data\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.14970563352108002}, {"x": 0.7352766394615173, "y": 0.14928510785102844}, {"x": 0.7358714938163757, "y": 0.15685449540615082}, {"x": 0.707317054271698, "y": 0.1572750210762024}], "text": "38.4\n"}
{"page": 7, "bbox": [{"x": 0.5121951103210449, "y": 0.14970563352108002}, {"x": 0.5413444638252258, "y": 0.14970563352108002}, {"x": 0.5413444638252258, "y": 0.1572750210762024}, {"x": 0.5121951103210449, "y": 0.1572750210762024}], "text": "55.9\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.1501261591911316}, {"x": 0.44556811451911926, "y": 0.1501261591911316}, {"x": 0.44556811451911926, "y": 0.1572750210762024}, {"x": 0.4146341383457184, "y": 0.1572750210762024}], "text": "20.0\n"}
{"page": 7, "bbox": [{"x": 0.6097561120986938, "y": 0.1501261591911316}, {"x": 0.6400951743125916, "y": 0.1501261591911316}, {"x": 0.6400951743125916, "y": 0.1572750210762024}, {"x": 0.6097561120986938, "y": 0.1572750210762024}], "text": "49.4\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.1484440714120865}, {"x": 0.34205830097198486, "y": 0.14970563352108002}, {"x": 0.34205830097198486, "y": 0.15937763452529907}, {"x": 0.23973825573921204, "y": 0.15811605751514435}], "text": "LLAMA2-C13B\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.16400335729122162}, {"x": 0.4431885778903961, "y": 0.1644238829612732}, {"x": 0.44259369373321533, "y": 0.17241379618644714}, {"x": 0.4140392541885376, "y": 0.17199327051639557}], "text": "51.8\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.16400335729122162}, {"x": 0.5419393181800842, "y": 0.16484440863132477}, {"x": 0.5413444638252258, "y": 0.1732548326253891}, {"x": 0.5121951103210449, "y": 0.17241379618644714}], "text": "79.9\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.1644238829612732}, {"x": 0.63890540599823, "y": 0.16484440863132477}, {"x": 0.63890540599823, "y": 0.17283432185649872}, {"x": 0.6103509664535522, "y": 0.17241379618644714}], "text": "52.1\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.16526493430137634}, {"x": 0.7346817255020142, "y": 0.16526493430137634}, {"x": 0.7346817255020142, "y": 0.17241379618644714}, {"x": 0.7067221999168396, "y": 0.17241379618644714}], "text": "37.9\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.1644238829612732}, {"x": 0.37239739298820496, "y": 0.16526493430137634}, {"x": 0.37239739298820496, "y": 0.17451639473438263}, {"x": 0.23973825573921204, "y": 0.17367535829544067}], "text": "Ret-LLAMA2-C13B\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.17914213240146637}, {"x": 0.5419393181800842, "y": 0.17956265807151794}, {"x": 0.5419393181800842, "y": 0.1875525712966919}, {"x": 0.5127900242805481, "y": 0.18713204562664032}], "text": "71.8\n"}
{"page": 7, "bbox": [{"x": 0.4140392541885376, "y": 0.17914213240146637}, {"x": 0.4443783462047577, "y": 0.17998318374156952}, {"x": 0.4437834620475769, "y": 0.18797308206558228}, {"x": 0.4134443700313568, "y": 0.1875525712966919}], "text": "29.3\n"}
{"page": 7, "bbox": [{"x": 0.23854848742485046, "y": 0.17956265807151794}, {"x": 0.30339083075523376, "y": 0.17914213240146637}, {"x": 0.30339083075523376, "y": 0.18797308206558228}, {"x": 0.23854848742485046, "y": 0.18839360773563385}], "text": "ChatGPT\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.17956265807151794}, {"x": 0.7376561760902405, "y": 0.17956265807151794}, {"x": 0.7376561760902405, "y": 0.18797308206558228}, {"x": 0.7067221999168396, "y": 0.18797308206558228}], "text": "75.3\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.17998318374156952}, {"x": 0.6383105516433716, "y": 0.17956265807151794}, {"x": 0.6383105516433716, "y": 0.1875525712966919}, {"x": 0.6103509664535522, "y": 0.18797308206558228}], "text": "70.1\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.1947014331817627}, {"x": 0.33432480692863464, "y": 0.19386038184165955}, {"x": 0.33432480692863464, "y": 0.20269133150577545}, {"x": 0.23914337158203125, "y": 0.2035323828458786}], "text": "Ret-ChatGPT\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.1947014331817627}, {"x": 0.7370612621307373, "y": 0.19428090751171112}, {"x": 0.7370612621307373, "y": 0.20269133150577545}, {"x": 0.7061272859573364, "y": 0.20311185717582703}], "text": "75.3\n"}
{"page": 7, "bbox": [{"x": 0.6097561120986938, "y": 0.19512194395065308}, {"x": 0.63890540599823, "y": 0.1947014331817627}, {"x": 0.63890540599823, "y": 0.20269133150577545}, {"x": 0.6097561120986938, "y": 0.20311185717582703}], "text": "54.7\n"}
{"page": 7, "bbox": [{"x": 0.41522902250289917, "y": 0.19554246962070465}, {"x": 0.4437834620475769, "y": 0.19554246962070465}, {"x": 0.4437834620475769, "y": 0.20311185717582703}, {"x": 0.41522902250289917, "y": 0.20311185717582703}], "text": "50.8\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.2106812447309494}, {"x": 0.5425342321395874, "y": 0.2106812447309494}, {"x": 0.5425342321395874, "y": 0.21825063228607178}, {"x": 0.5127900242805481, "y": 0.21825063228607178}], "text": "71.2\n"}
{"page": 7, "bbox": [{"x": 0.23854848742485046, "y": 0.20941968262195587}, {"x": 0.3242117762565613, "y": 0.2089991569519043}, {"x": 0.3242117762565613, "y": 0.22035323083400726}, {"x": 0.23854848742485046, "y": 0.22077375650405884}], "text": "Perplexity.ai\n"}
{"page": 7, "bbox": [{"x": 0.40749552845954895, "y": 0.23128679394721985}, {"x": 0.5936942100524902, "y": 0.23128679394721985}, {"x": 0.5936942100524902, "y": 0.23969721794128418}, {"x": 0.40749552845954895, "y": 0.23969721794128418}], "text": "Baselines without retrieval\n"}
{"page": 7, "bbox": [{"x": 0.5116002559661865, "y": 0.25273337960243225}, {"x": 0.5413444638252258, "y": 0.253153920173645}, {"x": 0.5407495498657227, "y": 0.2611438035964966}, {"x": 0.5110053420066833, "y": 0.2607232928276062}], "text": "44.5\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.25231286883354187}, {"x": 0.7358714938163757, "y": 0.25273337960243225}, {"x": 0.7352766394615173, "y": 0.26156434416770935}, {"x": 0.705532431602478, "y": 0.2611438035964966}], "text": "21.8\n"}
{"page": 7, "bbox": [{"x": 0.41641879081726074, "y": 0.253153920173645}, {"x": 0.4449732303619385, "y": 0.253153920173645}, {"x": 0.4449732303619385, "y": 0.2607232928276062}, {"x": 0.41641879081726074, "y": 0.2607232928276062}], "text": "14.7\n"}
{"page": 7, "bbox": [{"x": 0.6109458804130554, "y": 0.253153920173645}, {"x": 0.6400951743125916, "y": 0.253153920173645}, {"x": 0.6400951743125916, "y": 0.2611438035964966}, {"x": 0.6109458804130554, "y": 0.2611438035964966}], "text": "34.2\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.25273337960243225}, {"x": 0.32183223962783813, "y": 0.253153920173645}, {"x": 0.32183223962783813, "y": 0.26324641704559326}, {"x": 0.23973825573921204, "y": 0.2628259062767029}], "text": "LLAMA27B\n"}
{"page": 7, "bbox": [{"x": 0.705532431602478, "y": 0.26829269528388977}, {"x": 0.7358714938163757, "y": 0.267872154712677}, {"x": 0.7358714938163757, "y": 0.27544155716896057}, {"x": 0.705532431602478, "y": 0.27586206793785095}], "text": "45.0\n"}
{"page": 7, "bbox": [{"x": 0.5116002559661865, "y": 0.267872154712677}, {"x": 0.5413444638252258, "y": 0.26829269528388977}, {"x": 0.5407495498657227, "y": 0.27628257870674133}, {"x": 0.5110053420066833, "y": 0.27586206793785095}], "text": "45.8\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.26829269528388977}, {"x": 0.44556811451911926, "y": 0.26829269528388977}, {"x": 0.44556811451911926, "y": 0.27586206793785095}, {"x": 0.4146341383457184, "y": 0.27586206793785095}], "text": "23.6\n"}
{"page": 7, "bbox": [{"x": 0.6091611981391907, "y": 0.26871320605278015}, {"x": 0.6395003199577332, "y": 0.26871320605278015}, {"x": 0.6395003199577332, "y": 0.27586206793785095}, {"x": 0.6091611981391907, "y": 0.27586206793785095}], "text": "49.8\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.2674516439437866}, {"x": 0.3051754832267761, "y": 0.26829269528388977}, {"x": 0.3051754832267761, "y": 0.2788057327270508}, {"x": 0.23914337158203125, "y": 0.27796468138694763}], "text": "Alpaca7B\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.28259041905403137}, {"x": 0.7370612621307373, "y": 0.28301092982292175}, {"x": 0.7370612621307373, "y": 0.2910008430480957}, {"x": 0.7061272859573364, "y": 0.2905803322792053}], "text": "29.4\n"}
{"page": 7, "bbox": [{"x": 0.6097561120986938, "y": 0.28343144059181213}, {"x": 0.6400951743125916, "y": 0.28301092982292175}, {"x": 0.6400951743125916, "y": 0.2905803322792053}, {"x": 0.6097561120986938, "y": 0.2910008430480957}], "text": "29.4\n"}
{"page": 7, "bbox": [{"x": 0.41641879081726074, "y": 0.28343144059181213}, {"x": 0.4437834620475769, "y": 0.28343144059181213}, {"x": 0.4437834620475769, "y": 0.2910008430480957}, {"x": 0.41641879081726074, "y": 0.2910008430480957}], "text": "14.7\n"}
{"page": 7, "bbox": [{"x": 0.5121951103210449, "y": 0.28343144059181213}, {"x": 0.5419393181800842, "y": 0.28301092982292175}, {"x": 0.5419393181800842, "y": 0.2910008430480957}, {"x": 0.5121951103210449, "y": 0.2914213538169861}], "text": "53.4\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.2821698784828186}, {"x": 0.327781081199646, "y": 0.28343144059181213}, {"x": 0.327781081199646, "y": 0.29352396726608276}, {"x": 0.23914337158203125, "y": 0.29226240515708923}], "text": "LLAMA213B\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.2977291941642761}, {"x": 0.5425342321395874, "y": 0.2985702157020569}, {"x": 0.5419393181800842, "y": 0.30656012892723083}, {"x": 0.5121951103210449, "y": 0.3057190775871277}], "text": "50.2\n"}
{"page": 7, "bbox": [{"x": 0.6097561120986938, "y": 0.2977291941642761}, {"x": 0.6400951743125916, "y": 0.2981497049331665}, {"x": 0.6395003199577332, "y": 0.3069806694984436}, {"x": 0.6091611981391907, "y": 0.30656012892723083}], "text": "55.5\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.2985702157020569}, {"x": 0.7352766394615173, "y": 0.29899075627326965}, {"x": 0.7352766394615173, "y": 0.30656012892723083}, {"x": 0.7067221999168396, "y": 0.30613961815834045}], "text": "54.9\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.29899075627326965}, {"x": 0.4443783462047577, "y": 0.29899075627326965}, {"x": 0.4443783462047577, "y": 0.30613961815834045}, {"x": 0.4146341383457184, "y": 0.30613961815834045}], "text": "24.4\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.2977291941642761}, {"x": 0.31171920895576477, "y": 0.2985702157020569}, {"x": 0.31171920895576477, "y": 0.3090832531452179}, {"x": 0.23914337158203125, "y": 0.30824223160743713}], "text": "Alpaca 13B\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.3128679692745209}, {"x": 0.5425342321395874, "y": 0.31328848004341125}, {"x": 0.5419393181800842, "y": 0.32211941480636597}, {"x": 0.5127900242805481, "y": 0.3216989040374756}], "text": "71.2\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.3124474287033081}, {"x": 0.3063652515411377, "y": 0.3141295313835144}, {"x": 0.3057703673839569, "y": 0.32422202825546265}, {"x": 0.23854848742485046, "y": 0.32253995537757874}], "text": "COVE65B\n"}
{"page": 7, "bbox": [{"x": 0.4182034432888031, "y": 0.33473506569862366}, {"x": 0.5823914408683777, "y": 0.33473506569862366}, {"x": 0.5823914408683777, "y": 0.343145489692688}, {"x": 0.4182034432888031, "y": 0.343145489692688}], "text": "Baselines with retrieval\n"}
{"page": 7, "bbox": [{"x": 0.41522902250289917, "y": 0.35660219192504883}, {"x": 0.4431885778903961, "y": 0.35618165135383606}, {"x": 0.4431885778903961, "y": 0.36417156457901}, {"x": 0.41522902250289917, "y": 0.3645921051502228}], "text": "38.2\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.3570227026939392}, {"x": 0.5425342321395874, "y": 0.3570227026939392}, {"x": 0.5425342321395874, "y": 0.3645921051502228}, {"x": 0.5127900242805481, "y": 0.3645921051502228}], "text": "78.0\n"}
{"page": 7, "bbox": [{"x": 0.705532431602478, "y": 0.3570227026939392}, {"x": 0.7358714938163757, "y": 0.3570227026939392}, {"x": 0.7358714938163757, "y": 0.3645921051502228}, {"x": 0.705532431602478, "y": 0.3645921051502228}], "text": "48.0\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.357443243265152}, {"x": 0.6400951743125916, "y": 0.357443243265152}, {"x": 0.6400951743125916, "y": 0.3645921051502228}, {"x": 0.6103509664535522, "y": 0.3645921051502228}], "text": "30.0\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.35660219192504883}, {"x": 0.32183223962783813, "y": 0.3570227026939392}, {"x": 0.32183223962783813, "y": 0.36711522936820984}, {"x": 0.23914337158203125, "y": 0.36669468879699707}], "text": "LLAMA27B\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.3713204264640808}, {"x": 0.4443783462047577, "y": 0.37216147780418396}, {"x": 0.4437834620475769, "y": 0.3801513910293579}, {"x": 0.4140392541885376, "y": 0.37931033968925476}], "text": "46.7\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.3713204264640808}, {"x": 0.5431290864944458, "y": 0.3717409670352936}, {"x": 0.5431290864944458, "y": 0.3801513910293579}, {"x": 0.5127900242805481, "y": 0.37973088026046753}], "text": "76.6\n"}
{"page": 7, "bbox": [{"x": 0.705532431602478, "y": 0.37216147780418396}, {"x": 0.7364664077758789, "y": 0.37216147780418396}, {"x": 0.7364664077758789, "y": 0.37973088026046753}, {"x": 0.705532431602478, "y": 0.37973088026046753}], "text": "48.0\n"}
{"page": 7, "bbox": [{"x": 0.6097561120986938, "y": 0.37216147780418396}, {"x": 0.6406900882720947, "y": 0.37258198857307434}, {"x": 0.6406900882720947, "y": 0.3801513910293579}, {"x": 0.6097561120986938, "y": 0.37973088026046753}], "text": "40.2\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.3713204264640808}, {"x": 0.3063652515411377, "y": 0.37216147780418396}, {"x": 0.3063652515411377, "y": 0.3835155665874481}, {"x": 0.23914337158203125, "y": 0.38267451524734497}], "text": "Alpaca7B\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.3873002529144287}, {"x": 0.7358714938163757, "y": 0.38687974214553833}, {"x": 0.7358714938163757, "y": 0.3944491147994995}, {"x": 0.7061272859573364, "y": 0.3948696255683899}], "text": "48.4\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.3873002529144287}, {"x": 0.27602618932724, "y": 0.3873002529144287}, {"x": 0.27602618932724, "y": 0.3948696255683899}, {"x": 0.23973825573921204, "y": 0.3948696255683899}], "text": "SAIL\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.3873002529144287}, {"x": 0.63890540599823, "y": 0.38687974214553833}, {"x": 0.63890540599823, "y": 0.3948696255683899}, {"x": 0.6103509664535522, "y": 0.39529016613960266}], "text": "69.2\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.40243902802467346}, {"x": 0.6395003199577332, "y": 0.4020185172557831}, {"x": 0.6395003199577332, "y": 0.40958788990974426}, {"x": 0.6103509664535522, "y": 0.41000840067863464}], "text": "30.2\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.40285953879356384}, {"x": 0.7370612621307373, "y": 0.40285953879356384}, {"x": 0.7370612621307373, "y": 0.41000840067863464}, {"x": 0.7061272859573364, "y": 0.41000840067863464}], "text": "26.0\n"}
{"page": 7, "bbox": [{"x": 0.4140392541885376, "y": 0.4020185172557831}, {"x": 0.4443783462047577, "y": 0.40243902802467346}, {"x": 0.4443783462047577, "y": 0.4108494520187378}, {"x": 0.4140392541885376, "y": 0.4104289412498474}], "text": "45.7\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.40243902802467346}, {"x": 0.5413444638252258, "y": 0.40243902802467346}, {"x": 0.5413444638252258, "y": 0.4104289412498474}, {"x": 0.5127900242805481, "y": 0.4104289412498474}], "text": "77.5\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.40117746591567993}, {"x": 0.327781081199646, "y": 0.4020185172557831}, {"x": 0.327781081199646, "y": 0.4125315248966217}, {"x": 0.23973825573921204, "y": 0.41169050335884094}], "text": "LLAMA213B\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.41673675179481506}, {"x": 0.4437834620475769, "y": 0.41715726256370544}, {"x": 0.4437834620475769, "y": 0.4251471757888794}, {"x": 0.4146341383457184, "y": 0.424726665019989}], "text": "46.1\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.4175778031349182}, {"x": 0.5419393181800842, "y": 0.4175778031349182}, {"x": 0.5419393181800842, "y": 0.4251471757888794}, {"x": 0.5127900242805481, "y": 0.4251471757888794}], "text": "77.7\n"}
{"page": 7, "bbox": [{"x": 0.6097561120986938, "y": 0.4179983139038086}, {"x": 0.6383105516433716, "y": 0.4179983139038086}, {"x": 0.6383105516433716, "y": 0.42556771636009216}, {"x": 0.6097561120986938, "y": 0.42556771636009216}], "text": "51.1\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.4179983139038086}, {"x": 0.7370612621307373, "y": 0.4179983139038086}, {"x": 0.7370612621307373, "y": 0.42556771636009216}, {"x": 0.7061272859573364, "y": 0.42556771636009216}], "text": "57.6\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.41673675179481506}, {"x": 0.31290897727012634, "y": 0.4179983139038086}, {"x": 0.31231409311294556, "y": 0.4285113513469696}, {"x": 0.23854848742485046, "y": 0.4272497892379761}], "text": "Alpaca13B\n"}
{"page": 7, "bbox": [{"x": 0.4473527669906616, "y": 0.4318755269050598}, {"x": 0.5526472330093384, "y": 0.4322960376739502}, {"x": 0.5526472330093384, "y": 0.4423885643482208}, {"x": 0.4473527669906616, "y": 0.44196805357933044}], "text": "LLAMA2-hf-7b\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.44743481278419495}, {"x": 0.27305176854133606, "y": 0.4478553533554077}, {"x": 0.27305176854133606, "y": 0.4558452367782593}, {"x": 0.23914337158203125, "y": 0.4554247260093689}], "text": "RAG\n"}
{"page": 7, "bbox": [{"x": 0.6097561120986938, "y": 0.4482758641242981}, {"x": 0.6395003199577332, "y": 0.4482758641242981}, {"x": 0.6395003199577332, "y": 0.4558452367782593}, {"x": 0.6097561120986938, "y": 0.4558452367782593}], "text": "48.9\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.4482758641242981}, {"x": 0.7358714938163757, "y": 0.44743481278419495}, {"x": 0.7364664077758789, "y": 0.4558452367782593}, {"x": 0.7067221999168396, "y": 0.4566862881183624}], "text": "43.4\n"}
{"page": 7, "bbox": [{"x": 0.5121951103210449, "y": 0.4478553533554077}, {"x": 0.5413444638252258, "y": 0.4482758641242981}, {"x": 0.5407495498657227, "y": 0.45626577734947205}, {"x": 0.5116002559661865, "y": 0.4558452367782593}], "text": "44.9\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.4482758641242981}, {"x": 0.44556811451911926, "y": 0.4482758641242981}, {"x": 0.44556811451911926, "y": 0.45626577734947205}, {"x": 0.4146341383457184, "y": 0.45626577734947205}], "text": "50.5\n"}
{"page": 7, "bbox": [{"x": 0.6091611981391907, "y": 0.4629940986633301}, {"x": 0.6395003199577332, "y": 0.4625735878944397}, {"x": 0.6395003199577332, "y": 0.47056350111961365}, {"x": 0.6091611981391907, "y": 0.47098401188850403}], "text": "59.5\n"}
{"page": 7, "bbox": [{"x": 0.5121951103210449, "y": 0.4625735878944397}, {"x": 0.5425342321395874, "y": 0.46341463923454285}, {"x": 0.5419393181800842, "y": 0.4714045524597168}, {"x": 0.5116002559661865, "y": 0.47056350111961365}], "text": "47.7\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.46341463923454285}, {"x": 0.7370612621307373, "y": 0.46341463923454285}, {"x": 0.7370612621307373, "y": 0.47098401188850403}, {"x": 0.7067221999168396, "y": 0.47098401188850403}], "text": "53.7\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.4629940986633301}, {"x": 0.2879238426685333, "y": 0.46341463923454285}, {"x": 0.2879238426685333, "y": 0.4718250632286072}, {"x": 0.23973825573921204, "y": 0.4714045524597168}], "text": "CRAG\n"}
{"page": 7, "bbox": [{"x": 0.4140392541885376, "y": 0.46341463923454285}, {"x": 0.4437834620475769, "y": 0.46341463923454285}, {"x": 0.4437834620475769, "y": 0.4714045524597168}, {"x": 0.4140392541885376, "y": 0.4714045524597168}], "text": "54.9\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.47813287377357483}, {"x": 0.3152885138988495, "y": 0.47813287377357483}, {"x": 0.3152885138988495, "y": 0.4861227869987488}, {"x": 0.23914337158203125, "y": 0.4861227869987488}], "text": "Self-RAG*\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.47813287377357483}, {"x": 0.4449732303619385, "y": 0.4785534143447876}, {"x": 0.4449732303619385, "y": 0.48654332756996155}, {"x": 0.4146341383457184, "y": 0.4861227869987488}], "text": "29.0\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.4785534143447876}, {"x": 0.5419393181800842, "y": 0.4785534143447876}, {"x": 0.5419393181800842, "y": 0.4861227869987488}, {"x": 0.5133848786354065, "y": 0.4861227869987488}], "text": "32.2\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.47813287377357483}, {"x": 0.7358714938163757, "y": 0.4785534143447876}, {"x": 0.7358714938163757, "y": 0.48696383833885193}, {"x": 0.7061272859573364, "y": 0.48654332756996155}], "text": "23.9\n"}
{"page": 7, "bbox": [{"x": 0.613920271396637, "y": 0.4785534143447876}, {"x": 0.6365258693695068, "y": 0.4785534143447876}, {"x": 0.6365258693695068, "y": 0.48654332756996155}, {"x": 0.613920271396637, "y": 0.48654332756996155}], "text": "0.7\n"}
{"page": 7, "bbox": [{"x": 0.613920271396637, "y": 0.4932716488838196}, {"x": 0.63712078332901, "y": 0.4932716488838196}, {"x": 0.63712078332901, "y": 0.5012615919113159}, {"x": 0.613920271396637, "y": 0.5012615919113159}], "text": "0.6\n"}
{"page": 7, "bbox": [{"x": 0.4140392541885376, "y": 0.4932716488838196}, {"x": 0.4443783462047577, "y": 0.49411270022392273}, {"x": 0.4437834620475769, "y": 0.5016821026802063}, {"x": 0.4134443700313568, "y": 0.5008410215377808}], "text": "49.0\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.49369218945503235}, {"x": 0.3200475871562958, "y": 0.49369218945503235}, {"x": 0.3200475871562958, "y": 0.5016821026802063}, {"x": 0.23914337158203125, "y": 0.5016821026802063}], "text": "Self-CRAG\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.49369218945503235}, {"x": 0.5407495498657227, "y": 0.49369218945503235}, {"x": 0.5407495498657227, "y": 0.5016821026802063}, {"x": 0.5127900242805481, "y": 0.5016821026802063}], "text": "69.1\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.49369218945503235}, {"x": 0.7352766394615173, "y": 0.49369218945503235}, {"x": 0.7352766394615173, "y": 0.5016821026802063}, {"x": 0.7061272859573364, "y": 0.5016821026802063}], "text": "27.9\n"}
{"page": 7, "bbox": [{"x": 0.4259369373321533, "y": 0.5088309645652771}, {"x": 0.5746579170227051, "y": 0.5075693726539612}, {"x": 0.5746579170227051, "y": 0.517241358757019}, {"x": 0.4259369373321533, "y": 0.518502950668335}], "text": "SelfRAG-LLAMA2-7b\n"}
{"page": 7, "bbox": [{"x": 0.7061272859573364, "y": 0.5239697098731995}, {"x": 0.7358714938163757, "y": 0.5239697098731995}, {"x": 0.7358714938163757, "y": 0.5319596529006958}, {"x": 0.7061272859573364, "y": 0.5319596529006958}], "text": "53.2\n"}
{"page": 7, "bbox": [{"x": 0.23914337158203125, "y": 0.5243902206420898}, {"x": 0.27364665269851685, "y": 0.5243902206420898}, {"x": 0.27364665269851685, "y": 0.5319596529006958}, {"x": 0.23914337158203125, "y": 0.5319596529006958}], "text": "RAG\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.5243902206420898}, {"x": 0.4443783462047577, "y": 0.5239697098731995}, {"x": 0.4443783462047577, "y": 0.5319596529006958}, {"x": 0.4146341383457184, "y": 0.5323801636695862}], "text": "52.8\n"}
{"page": 7, "bbox": [{"x": 0.5121951103210449, "y": 0.5243902206420898}, {"x": 0.5419393181800842, "y": 0.5243902206420898}, {"x": 0.5419393181800842, "y": 0.5319596529006958}, {"x": 0.5121951103210449, "y": 0.5319596529006958}], "text": "59.2\n"}
{"page": 7, "bbox": [{"x": 0.6109458804130554, "y": 0.5243902206420898}, {"x": 0.6395003199577332, "y": 0.5243902206420898}, {"x": 0.6395003199577332, "y": 0.5319596529006958}, {"x": 0.6109458804130554, "y": 0.5319596529006958}], "text": "39.0\n"}
{"page": 7, "bbox": [{"x": 0.24033313989639282, "y": 0.5386879444122314}, {"x": 0.2879238426685333, "y": 0.5391085147857666}, {"x": 0.2879238426685333, "y": 0.5470983982086182}, {"x": 0.24033313989639282, "y": 0.5466778874397278}], "text": "CRAG\n"}
{"page": 7, "bbox": [{"x": 0.4140392541885376, "y": 0.539529025554657}, {"x": 0.4437834620475769, "y": 0.539529025554657}, {"x": 0.4437834620475769, "y": 0.5466778874397278}, {"x": 0.4140392541885376, "y": 0.5466778874397278}], "text": "59.8\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.5391085147857666}, {"x": 0.7376561760902405, "y": 0.539529025554657}, {"x": 0.7376561760902405, "y": 0.5470983982086182}, {"x": 0.7067221999168396, "y": 0.5466778874397278}], "text": "68.6\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.539529025554657}, {"x": 0.5413444638252258, "y": 0.539529025554657}, {"x": 0.5413444638252258, "y": 0.5475189089775085}, {"x": 0.5133848786354065, "y": 0.5475189089775085}], "text": "74.1\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.539529025554657}, {"x": 0.6412849426269531, "y": 0.539529025554657}, {"x": 0.6412849426269531, "y": 0.5475189089775085}, {"x": 0.6103509664535522, "y": 0.5475189089775085}], "text": "75.6\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.554247260093689}, {"x": 0.7358714938163757, "y": 0.554247260093689}, {"x": 0.7358714938163757, "y": 0.5618166327476501}, {"x": 0.7067221999168396, "y": 0.5618166327476501}], "text": "67.3\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.554247260093689}, {"x": 0.6395003199577332, "y": 0.5538267493247986}, {"x": 0.6395003199577332, "y": 0.5618166327476501}, {"x": 0.6103509664535522, "y": 0.5622372031211853}], "text": "72.4\n"}
{"page": 7, "bbox": [{"x": 0.4146341383457184, "y": 0.5546677708625793}, {"x": 0.4431885778903961, "y": 0.554247260093689}, {"x": 0.4431885778903961, "y": 0.5618166327476501}, {"x": 0.4146341383457184, "y": 0.5622372031211853}], "text": "54.9\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.5546677708625793}, {"x": 0.30814990401268005, "y": 0.5546677708625793}, {"x": 0.30814990401268005, "y": 0.5622372031211853}, {"x": 0.23973825573921204, "y": 0.5622372031211853}], "text": "Self-RAG\n"}
{"page": 7, "bbox": [{"x": 0.5127900242805481, "y": 0.5546677708625793}, {"x": 0.5419393181800842, "y": 0.5546677708625793}, {"x": 0.5419393181800842, "y": 0.5622372031211853}, {"x": 0.5127900242805481, "y": 0.5622372031211853}], "text": "81.2\n"}
{"page": 7, "bbox": [{"x": 0.5121951103210449, "y": 0.5698065757751465}, {"x": 0.5413444638252258, "y": 0.5698065757751465}, {"x": 0.5413444638252258, "y": 0.5769554376602173}, {"x": 0.5121951103210449, "y": 0.5769554376602173}], "text": "86.2\n"}
{"page": 7, "bbox": [{"x": 0.7067221999168396, "y": 0.5698065757751465}, {"x": 0.7352766394615173, "y": 0.5698065757751465}, {"x": 0.7352766394615173, "y": 0.5769554376602173}, {"x": 0.7067221999168396, "y": 0.5769554376602173}], "text": "67.2\n"}
{"page": 7, "bbox": [{"x": 0.23973825573921204, "y": 0.5693860650062561}, {"x": 0.319452702999115, "y": 0.5693860650062561}, {"x": 0.319452702999115, "y": 0.577796459197998}, {"x": 0.23973825573921204, "y": 0.577796459197998}], "text": "Self-CRAG\n"}
{"page": 7, "bbox": [{"x": 0.41522902250289917, "y": 0.5698065757751465}, {"x": 0.4449732303619385, "y": 0.5698065757751465}, {"x": 0.4449732303619385, "y": 0.577796459197998}, {"x": 0.41522902250289917, "y": 0.577796459197998}], "text": "61.8\n"}
{"page": 7, "bbox": [{"x": 0.6103509664535522, "y": 0.5702270865440369}, {"x": 0.6400951743125916, "y": 0.5702270865440369}, {"x": 0.6400951743125916, "y": 0.5773759484291077}, {"x": 0.6103509664535522, "y": 0.5773759484291077}], "text": "74.8\n"}
{"page": 7, "bbox": [{"x": 0.11838191747665405, "y": 0.5988225340843201}, {"x": 0.8810232281684875, "y": 0.5988225340843201}, {"x": 0.8810232281684875, "y": 0.6526492834091187}, {"x": 0.11838191747665405, "y": 0.6526492834091187}], "text": "Table 1: Overall evaluation results on the test sets of four datasets. Results are separated based on the generation\nLLMs. Bold numbers indicate the best performance among all methods and LLMs. Gray-colored bold scores\nindicate the best performance using a specific LLM. * indicates the results reproduced by us, otherwise results\nexcept ours are cited from their original papers.\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.6749369502067566}, {"x": 0.8839976191520691, "y": 0.675357460975647}, {"x": 0.8839976191520691, "y": 0.8326324820518494}, {"x": 0.5133848786354065, "y": 0.832211971282959}], "text": "before instructions. (2) Self-RAG (Asai et al.,\n2024) that tuned the LLaMA2 on the instruction-\ntuning data comtaining several sets of reflection\ntokens which were labeled by GPT-4 (OpenAI,\n2023). (3) Following Asai et al. (2024), we also\ncited the results of retrieval-augmented baselines\ntrained with private data: Ret-ChatGPT and Ret-\nLLAMA-chat, which deploy the same augmenta-\ntion technique above, as well as perplexity.ai, an\nInstructGPT-based production search system.\n"}
{"page": 7, "bbox": [{"x": 0.11838191747665405, "y": 0.6749369502067566}, {"x": 0.48839977383613586, "y": 0.6745163798332214}, {"x": 0.48899465799331665, "y": 0.9205214381217957}, {"x": 0.11897680163383484, "y": 0.920941948890686}], "text": "to improve the factuality of LLM generations.\nPropriety LLMs such as LLAMA2-chat13B and\nChatGPT are also included.\nStandard RAG. We evaluated the standard\nRAG (Lewis et al., 2020) where an LM generates\noutput given the query prepended with the top\nretrieved documents using the same retriever as\nin our system. Here we adopted several pub-\nlic instruction-tuned LLMs, including LLaMA2-\n7B, 13B (Touvron et al., 2023b), Alpaca-7B, 13B\n(Dubois et al., 2023), as well as LLaMA2-7B\ninstruction-tuned in Self-RAG (Asai et al., 2024).\nAdvanced RAG. (1) SAIL (Luo et al., 2023) that\ninstruction-tuned an LM on the Alpaca instruction-\ntuning data with top retrieved documents inserted\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.8439865708351135}, {"x": 0.6127305030822754, "y": 0.8439865708351135}, {"x": 0.6127305030822754, "y": 0.8519764542579651}, {"x": 0.5133848786354065, "y": 0.8519764542579651}], "text": "5.3 Results\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.8599663376808167}, {"x": 0.8828078508377075, "y": 0.8595458269119263}, {"x": 0.8828078508377075, "y": 0.918418824672699}, {"x": 0.5133848786354065, "y": 0.9188393354415894}], "text": "Table 1 presents the results on four datasets. The\nmodel coupling the proposed method with standard\nRAG is named CRAG and that coupling with Self-\nRAG is named Self-CRAG. Readers can refer to\n"}
{"page": 8, "bbox": [{"x": 0.6502082347869873, "y": 0.09209419786930084}, {"x": 0.8697203993797302, "y": 0.09209419786930084}, {"x": 0.8697203993797302, "y": 0.09882254153490067}, {"x": 0.6502082347869873, "y": 0.09882254153490067}], "text": "LLAMA2-hf-7b SelfRAG-LLAMA2-7b\n"}
{"page": 8, "bbox": [{"x": 0.6793575286865234, "y": 0.11143818497657776}, {"x": 0.707317054271698, "y": 0.11185870319604874}, {"x": 0.7067221999168396, "y": 0.11984860897064209}, {"x": 0.6787626147270203, "y": 0.11942809075117111}], "text": "54.9\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.11227922886610031}, {"x": 0.8203450441360474, "y": 0.11185870319604874}, {"x": 0.8203450441360474, "y": 0.11900757253170013}, {"x": 0.7917906045913696, "y": 0.11942809075117111}], "text": "59.8\n"}
{"page": 8, "bbox": [{"x": 0.5258774757385254, "y": 0.11185870319604874}, {"x": 0.5734681487083435, "y": 0.11227922886610031}, {"x": 0.5734681487083435, "y": 0.12026913464069366}, {"x": 0.5258774757385254, "y": 0.11984860897064209}], "text": "CRAG\n"}
{"page": 8, "bbox": [{"x": 0.6787626147270203, "y": 0.12741799652576447}, {"x": 0.7079119682312012, "y": 0.12741799652576447}, {"x": 0.7079119682312012, "y": 0.13498738408088684}, {"x": 0.6787626147270203, "y": 0.13498738408088684}], "text": "53.2\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.12783852219581604}, {"x": 0.8209398984909058, "y": 0.12783852219581604}, {"x": 0.8209398984909058, "y": 0.13498738408088684}, {"x": 0.7917906045913696, "y": 0.13498738408088684}], "text": "58.3\n"}
{"page": 8, "bbox": [{"x": 0.5336109399795532, "y": 0.12783852219581604}, {"x": 0.6264128684997559, "y": 0.12825904786586761}, {"x": 0.6264128684997559, "y": 0.1358284205198288}, {"x": 0.5336109399795532, "y": 0.13540790975093842}], "text": "w/o. Correct\n"}
{"page": 8, "bbox": [{"x": 0.7911956906318665, "y": 0.14171572029590607}, {"x": 0.8203450441360474, "y": 0.1408746838569641}, {"x": 0.8209398984909058, "y": 0.14928510785102844}, {"x": 0.7917906045913696, "y": 0.1501261591911316}], "text": "59.5\n"}
{"page": 8, "bbox": [{"x": 0.6787626147270203, "y": 0.14255677163600922}, {"x": 0.7079119682312012, "y": 0.14255677163600922}, {"x": 0.7079119682312012, "y": 0.14970563352108002}, {"x": 0.6787626147270203, "y": 0.14970563352108002}], "text": "54.4\n"}
{"page": 8, "bbox": [{"x": 0.5336109399795532, "y": 0.14213624596595764}, {"x": 0.6430696249008179, "y": 0.14255677163600922}, {"x": 0.6430696249008179, "y": 0.15054668486118317}, {"x": 0.5336109399795532, "y": 0.1501261591911316}], "text": "w/o. Incorrect\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.1572750210762024}, {"x": 0.8209398984909058, "y": 0.1572750210762024}, {"x": 0.8209398984909058, "y": 0.16400335729122162}, {"x": 0.7917906045913696, "y": 0.16400335729122162}], "text": "59.0\n"}
{"page": 8, "bbox": [{"x": 0.6793575286865234, "y": 0.15643398463726044}, {"x": 0.7079119682312012, "y": 0.15685449540615082}, {"x": 0.707317054271698, "y": 0.16484440863132477}, {"x": 0.6787626147270203, "y": 0.1644238829612732}], "text": "54.0\n"}
{"page": 8, "bbox": [{"x": 0.5336109399795532, "y": 0.15601345896720886}, {"x": 0.6436644792556763, "y": 0.1572750210762024}, {"x": 0.6436644792556763, "y": 0.16652649641036987}, {"x": 0.5336109399795532, "y": 0.16526493430137634}], "text": "w/o. Ambiguous\n"}
{"page": 8, "bbox": [{"x": 0.5252825617790222, "y": 0.17746004462242126}, {"x": 0.6026175022125244, "y": 0.17703953385353088}, {"x": 0.6026175022125244, "y": 0.18544995784759521}, {"x": 0.5252825617790222, "y": 0.1858704835176468}], "text": "Self-CRAG\n"}
{"page": 8, "bbox": [{"x": 0.6781677603721619, "y": 0.17746004462242126}, {"x": 0.7085068225860596, "y": 0.17788057029247284}, {"x": 0.7079119682312012, "y": 0.1858704835176468}, {"x": 0.6781677603721619, "y": 0.18544995784759521}], "text": "49.0\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.17830109596252441}, {"x": 0.8209398984909058, "y": 0.17788057029247284}, {"x": 0.8209398984909058, "y": 0.18502943217754364}, {"x": 0.7917906045913696, "y": 0.18544995784759521}], "text": "61.8\n"}
{"page": 8, "bbox": [{"x": 0.6781677603721619, "y": 0.19259881973266602}, {"x": 0.7096965909004211, "y": 0.19343987107276917}, {"x": 0.7091017365455627, "y": 0.20100925862789154}, {"x": 0.6775728464126587, "y": 0.2001682072877884}], "text": "43.6\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.19343987107276917}, {"x": 0.8209398984909058, "y": 0.1930193454027176}, {"x": 0.8209398984909058, "y": 0.20058873295783997}, {"x": 0.7917906045913696, "y": 0.20100925862789154}], "text": "59.6\n"}
{"page": 8, "bbox": [{"x": 0.5336109399795532, "y": 0.1930193454027176}, {"x": 0.6264128684997559, "y": 0.19343987107276917}, {"x": 0.6264128684997559, "y": 0.20142976939678192}, {"x": 0.5336109399795532, "y": 0.20100925862789154}], "text": "w/o. Correct\n"}
{"page": 8, "bbox": [{"x": 0.792385458946228, "y": 0.20815812051296234}, {"x": 0.8197501301765442, "y": 0.20815812051296234}, {"x": 0.8197501301765442, "y": 0.21572750806808472}, {"x": 0.792385458946228, "y": 0.21572750806808472}], "text": "60.8\n"}
{"page": 8, "bbox": [{"x": 0.6781677603721619, "y": 0.20815812051296234}, {"x": 0.7079119682312012, "y": 0.20857863128185272}, {"x": 0.7079119682312012, "y": 0.2161480188369751}, {"x": 0.6781677603721619, "y": 0.21572750806808472}], "text": "47.7\n"}
{"page": 8, "bbox": [{"x": 0.5330160856246948, "y": 0.20773759484291077}, {"x": 0.6442593932151794, "y": 0.20857863128185272}, {"x": 0.6436644792556763, "y": 0.23338940739631653}, {"x": 0.5324211716651917, "y": 0.23254835605621338}], "text": "w/o. Incorrect\nw/o. Ambiguous\n"}
{"page": 8, "bbox": [{"x": 0.6781677603721619, "y": 0.2232968807220459}, {"x": 0.7067221999168396, "y": 0.2232968807220459}, {"x": 0.7067221999168396, "y": 0.2304457575082779}, {"x": 0.6781677603721619, "y": 0.2304457575082779}], "text": "48.1\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.2232968807220459}, {"x": 0.8203450441360474, "y": 0.2232968807220459}, {"x": 0.8203450441360474, "y": 0.2304457575082779}, {"x": 0.7917906045913696, "y": 0.2304457575082779}], "text": "61.5\n"}
{"page": 8, "bbox": [{"x": 0.5133848786354065, "y": 0.2514718174934387}, {"x": 0.8804283142089844, "y": 0.2518923580646515}, {"x": 0.8804283142089844, "y": 0.2771236300468445}, {"x": 0.5133848786354065, "y": 0.2767031192779541}], "text": "Table 2: Ablation study for removing each single action\non the PopQA dataset in terms of accuracy.\n"}
{"page": 8, "bbox": [{"x": 0.647233784198761, "y": 0.3124474287033081}, {"x": 0.8691255450248718, "y": 0.3124474287033081}, {"x": 0.8691255450248718, "y": 0.3191757798194885}, {"x": 0.647233784198761, "y": 0.3191757798194885}], "text": "LLAMA2-hf-7b SelfRAG-LLAMA2-7b\n"}
{"page": 8, "bbox": [{"x": 0.7906008362770081, "y": 0.3322119414806366}, {"x": 0.8197501301765442, "y": 0.332632452249527}, {"x": 0.8191552758216858, "y": 0.3406223654747009}, {"x": 0.7906008362770081, "y": 0.34020185470581055}], "text": "59.8\n"}
{"page": 8, "bbox": [{"x": 0.6769779920578003, "y": 0.332632452249527}, {"x": 0.705532431602478, "y": 0.332632452249527}, {"x": 0.705532431602478, "y": 0.3406223654747009}, {"x": 0.6769779920578003, "y": 0.3406223654747009}], "text": "54.9\n"}
{"page": 8, "bbox": [{"x": 0.5258774757385254, "y": 0.332632452249527}, {"x": 0.5734681487083435, "y": 0.33305299282073975}, {"x": 0.5734681487083435, "y": 0.3406223654747009}, {"x": 0.5258774757385254, "y": 0.34020185470581055}], "text": "CRAG\n"}
{"page": 8, "bbox": [{"x": 0.6757882237434387, "y": 0.34777122735977173}, {"x": 0.7043426632881165, "y": 0.34735071659088135}, {"x": 0.7043426632881165, "y": 0.35492008924484253}, {"x": 0.6757882237434387, "y": 0.3553406298160553}], "text": "49.8\n"}
{"page": 8, "bbox": [{"x": 0.7906008362770081, "y": 0.3481917679309845}, {"x": 0.8197501301765442, "y": 0.3481917679309845}, {"x": 0.8197501301765442, "y": 0.3553406298160553}, {"x": 0.7906008362770081, "y": 0.3553406298160553}], "text": "54.2\n"}
{"page": 8, "bbox": [{"x": 0.7906008362770081, "y": 0.3629100024700165}, {"x": 0.8197501301765442, "y": 0.3624894917011261}, {"x": 0.8197501301765442, "y": 0.37047940492630005}, {"x": 0.7906008362770081, "y": 0.37089991569519043}], "text": "56.2\n"}
{"page": 8, "bbox": [{"x": 0.5336109399795532, "y": 0.34735071659088135}, {"x": 0.6424747109413147, "y": 0.3481917679309845}, {"x": 0.6418798565864563, "y": 0.38645920157432556}, {"x": 0.5330160856246948, "y": 0.3856181800365448}], "text": "w/o. refinement\nw/o. rewriting\nw/o. selection\n"}
{"page": 8, "bbox": [{"x": 0.6769779920578003, "y": 0.36333054304122925}, {"x": 0.7061272859573364, "y": 0.36333054304122925}, {"x": 0.7061272859573364, "y": 0.37089991569519043}, {"x": 0.6769779920578003, "y": 0.37089991569519043}], "text": "51.7\n"}
{"page": 8, "bbox": [{"x": 0.6763830780982971, "y": 0.3772077262401581}, {"x": 0.7043426632881165, "y": 0.3767872154712677}, {"x": 0.7049375176429749, "y": 0.38519763946533203}, {"x": 0.6769779920578003, "y": 0.3856181800365448}], "text": "50.9\n"}
{"page": 8, "bbox": [{"x": 0.7906008362770081, "y": 0.37804877758026123}, {"x": 0.8209398984909058, "y": 0.37804877758026123}, {"x": 0.8209398984909058, "y": 0.38519763946533203}, {"x": 0.7906008362770081, "y": 0.38519763946533203}], "text": "58.6\n"}
{"page": 8, "bbox": [{"x": 0.5252825617790222, "y": 0.39865434169769287}, {"x": 0.603807270526886, "y": 0.3982338011264801}, {"x": 0.603807270526886, "y": 0.40664422512054443}, {"x": 0.5252825617790222, "y": 0.4070647656917572}], "text": "Self-CRAG\n"}
{"page": 8, "bbox": [{"x": 0.7911956906318665, "y": 0.3982338011264801}, {"x": 0.8197501301765442, "y": 0.39907485246658325}, {"x": 0.8191552758216858, "y": 0.4070647656917572}, {"x": 0.7906008362770081, "y": 0.40622371435165405}], "text": "61.8\n"}
{"page": 8, "bbox": [{"x": 0.6763830780982971, "y": 0.39865434169769287}, {"x": 0.7067221999168396, "y": 0.39907485246658325}, {"x": 0.7067221999168396, "y": 0.4070647656917572}, {"x": 0.6763830780982971, "y": 0.40664422512054443}], "text": "49.0\n"}
{"page": 8, "bbox": [{"x": 0.6775728464126587, "y": 0.414213627576828}, {"x": 0.7049375176429749, "y": 0.4146341383457184}, {"x": 0.7043426632881165, "y": 0.42262405157089233}, {"x": 0.6769779920578003, "y": 0.42220354080200195}], "text": "35.9\n"}
{"page": 8, "bbox": [{"x": 0.7906008362770081, "y": 0.4146341383457184}, {"x": 0.8203450441360474, "y": 0.414213627576828}, {"x": 0.8203450441360474, "y": 0.42220354080200195}, {"x": 0.7906008362770081, "y": 0.42262405157089233}], "text": "52.2\n"}
{"page": 8, "bbox": [{"x": 0.5336109399795532, "y": 0.4146341383457184}, {"x": 0.6424747109413147, "y": 0.41505467891693115}, {"x": 0.6424747109413147, "y": 0.42262405157089233}, {"x": 0.5336109399795532, "y": 0.42220354080200195}], "text": "w/o. refinement\n"}
{"page": 8, "bbox": [{"x": 0.7906008362770081, "y": 0.42893186211586}, {"x": 0.8191552758216858, "y": 0.4285113513469696}, {"x": 0.8191552758216858, "y": 0.43650126457214355}, {"x": 0.7906008362770081, "y": 0.43692177534103394}], "text": "58.4\n"}
{"page": 8, "bbox": [{"x": 0.6769779920578003, "y": 0.42935240268707275}, {"x": 0.705532431602478, "y": 0.42935240268707275}, {"x": 0.705532431602478, "y": 0.43692177534103394}, {"x": 0.6769779920578003, "y": 0.43692177534103394}], "text": "37.2\n"}
{"page": 8, "bbox": [{"x": 0.5336109399795532, "y": 0.4280908405780792}, {"x": 0.6311719417572021, "y": 0.4285113513469696}, {"x": 0.6311719417572021, "y": 0.45248109102249146}, {"x": 0.5336109399795532, "y": 0.4520605504512787}], "text": "w/o. rewriting\nw/o. selection\n"}
{"page": 8, "bbox": [{"x": 0.6763830780982971, "y": 0.4444911777973175}, {"x": 0.7049375176429749, "y": 0.4444911777973175}, {"x": 0.7049375176429749, "y": 0.4520605504512787}, {"x": 0.6763830780982971, "y": 0.4520605504512787}], "text": "24.9\n"}
{"page": 8, "bbox": [{"x": 0.7906008362770081, "y": 0.4444911777973175}, {"x": 0.8197501301765442, "y": 0.4444911777973175}, {"x": 0.8197501301765442, "y": 0.45248109102249146}, {"x": 0.7906008362770081, "y": 0.45248109102249146}], "text": "57.9\n"}
{"page": 8, "bbox": [{"x": 0.5133848786354065, "y": 0.47224557399749756}, {"x": 0.8804283142089844, "y": 0.4735071361064911}, {"x": 0.8804283142089844, "y": 0.49915894865989685}, {"x": 0.5133848786354065, "y": 0.4978973865509033}], "text": "Table 3: Ablation study for removing each knowledge\nutilization operation on the PopQA in terms of accuracy.\n"}
{"page": 8, "bbox": [{"x": 0.1165972650051117, "y": 0.08704794198274612}, {"x": 0.4901844263076782, "y": 0.0874684602022171}, {"x": 0.48899465799331665, "y": 0.920941948890686}, {"x": 0.11540749669075012, "y": 0.9205214381217957}], "text": "Appendix B.3 for more implementation details of\nour proposed methods. From these results, we can\nconclude the following findings:\nFirst, the proposed method can significantly\nimprove the performance of RAG and Self-RAG.\nSpecifically, as shown in table 1, CRAG outper-\nformed RAG by margins of 7.0% accuracy on\nPopQA, 14.9% FactScore on Biography, 36.6%\naccuracy on PubHealth, and 15.4% accuracy on\nArc-Challenge when based on SelfRAG-LLAMA2-\n7b, as well as by margins of 4.4% accuracy\non PopQA, 2.8% FactScore on Biography, and\n10.3% on Arc-Challenge when based on LLAMA2-\nhf-7b. Compared with the current state-of-the-\nart Self-RAG, Self-CRAG outperformed it by\nmargins of 20.0% accuracy on PopQA, 36.9%\nFactScore on Biography, and 4.0% accuracy on\nArc-Challenge when based on LLAMA2-hf-7b, as\nwell as by margins of 6.9% accuracy on PopQA,\n5.0% FactScore on Biography, and 2.4% accuracy\non PubHealth, when based on SelfRAG-LLaMA2-\n7b. These results demonstrated the adaptability\nof CRAG which is plug-and-play and can be\nimplemented into RAG-based approaches.\nSecond, the proposed method demonstrated\ngreat generalizability across a variety of gen-\neration tasks. In particular, these benchmarks\nreported in Table 1 respectively represent different\npractical scenarios including short-form entity\ngeneration (PopQA), long-form generation (Bi-\nography), and closed-set tasks (PubHealth, Arc-\nChallenge). These results verified the consistent\neffectiveness of CRAG. Its versatility across a spec-\ntrum of tasks underscores its robust capabilities and\ngeneralizability across diverse scenarios.\nThird, the proposed method exhibited greater\nflexibility in replacing the underlying LLM gen-\nerator. It can be seen that CRAG still showed\ncompetitive performance when the underlying\nLLMs was changed from SelfRAG-LLaMA2-7b\nto LLAMA2-hf-7b, while the performance of Self-\nRAG dropped significantly, even underperforming\nthe standard RAG on several benchmarks. The\nreason for these results is that Self-RAG needs to be\ninstruction-tuned using human or LLM annotated\ndata to learn to output special critic tokens as\nneeded, while this ability is not learned in common\nLLMS. CRAG does not have any requirements\nfor this ability. As you can imagine, when more\nadvanced LLMs are available in the future, they\ncan be coupled with CRAG easily, while additional\ninstruction tuning is still necessary for Self-RAG.\n"}
{"page": 8, "bbox": [{"x": 0.5139797925949097, "y": 0.5340622663497925}, {"x": 0.6751933097839355, "y": 0.5361648201942444}, {"x": 0.6745984554290771, "y": 0.5470983982086182}, {"x": 0.5133848786354065, "y": 0.5449957847595215}], "text": "5.4 Ablation Study\n"}
{"page": 8, "bbox": [{"x": 0.5121951103210449, "y": 0.5681244730949402}, {"x": 0.8834027647972107, "y": 0.5681244730949402}, {"x": 0.8834027647972107, "y": 0.9171572923660278}, {"x": 0.5121951103210449, "y": 0.9171572923660278}], "text": "The impact of each triggered action. To fur-\nther verify the effectiveness of triggered actions\ndesigned in the retrieval evaluator, ablation tests\nfor removing each single action in the proposed\nmethod were conducted as shown in Table 2.\nEvaluations on the PopQA dataset were conducted\nto demonstrate the performance change in terms of\naccuracy. Specifically, when the action Correct\nor Incorrect was removed, it was merged with\nAmbiguous so that the proportion that originally\ntriggered Correct or Incorrect would trigger\nAmbiguous. On the other hand, when the action\nAmbiguous was removed, there was only one\nthreshold against which all input queries clearly\ntriggered Correct or Incorrect. From these\nresults, it can be seen that there was a performance\ndrop no matter which action was removed, illustrat-\ning that each action contributed to improving the\nrobustness of generation. To further illustrate the\nstudy, experiments are also conducted by triggering\nonly one action once, and the results shown in the\nappendix also prove the consistency.\n"}
{"page": 9, "bbox": [{"x": 0.4015466868877411, "y": 0.09167367219924927}, {"x": 0.46757882833480835, "y": 0.09167367219924927}, {"x": 0.46757882833480835, "y": 0.10092514753341675}, {"x": 0.4015466868877411, "y": 0.10092514753341675}], "text": "Accuracy\n"}
{"page": 9, "bbox": [{"x": 0.7376561760902405, "y": 0.10639192909002304}, {"x": 0.7715645432472229, "y": 0.10639192909002304}, {"x": 0.7715645432472229, "y": 0.11143818497657776}, {"x": 0.7376561760902405, "y": 0.11143818497657776}], "text": "Self-RAG\n"}
{"page": 9, "bbox": [{"x": 0.8102319836616516, "y": 0.107232965528965}, {"x": 0.8488994836807251, "y": 0.107232965528965}, {"x": 0.8488994836807251, "y": 0.11101765930652618}, {"x": 0.8102319836616516, "y": 0.11101765930652618}], "text": "Self-CRAG\n"}
{"page": 9, "bbox": [{"x": 0.4193932116031647, "y": 0.11312027275562286}, {"x": 0.449137419462204, "y": 0.11312027275562286}, {"x": 0.449137419462204, "y": 0.12026913464069366}, {"x": 0.4193932116031647, "y": 0.12026913464069366}], "text": "84.3\n"}
{"page": 9, "bbox": [{"x": 0.13622844219207764, "y": 0.11143818497657776}, {"x": 0.3825104236602783, "y": 0.11269974708557129}, {"x": 0.3825104236602783, "y": 0.1232127845287323}, {"x": 0.13622844219207764, "y": 0.12195122241973877}], "text": "Our Retrieval Evaluator (T5-based)\n"}
{"page": 9, "bbox": [{"x": 0.4193932116031647, "y": 0.128679558634758}, {"x": 0.449137419462204, "y": 0.128679558634758}, {"x": 0.449137419462204, "y": 0.13624894618988037}, {"x": 0.4193932116031647, "y": 0.13624894618988037}], "text": "58.0\n"}
{"page": 9, "bbox": [{"x": 0.13622844219207764, "y": 0.12825904786586761}, {"x": 0.20166566967964172, "y": 0.12825904786586761}, {"x": 0.20166566967964172, "y": 0.13666947185993195}, {"x": 0.13622844219207764, "y": 0.13666947185993195}], "text": "ChatGPT\n"}
{"page": 9, "bbox": [{"x": 0.13622844219207764, "y": 0.14381833374500275}, {"x": 0.2373587191104889, "y": 0.14339780807495117}, {"x": 0.2373587191104889, "y": 0.15222875773906708}, {"x": 0.13622844219207764, "y": 0.15264928340911865}], "text": "ChatGPT-COT\n"}
{"page": 9, "bbox": [{"x": 0.4193932116031647, "y": 0.14423885941505432}, {"x": 0.45032718777656555, "y": 0.14423885941505432}, {"x": 0.45032718777656555, "y": 0.1518082469701767}, {"x": 0.4193932116031647, "y": 0.1518082469701767}], "text": "62.4\n"}
{"page": 9, "bbox": [{"x": 0.5318263173103333, "y": 0.20227082073688507}, {"x": 0.5324211716651917, "y": 0.10470984131097794}, {"x": 0.543724000453949, "y": 0.10470984131097794}, {"x": 0.5431290864944458, "y": 0.20227082073688507}], "text": "Accuracy of generation\n"}
{"page": 9, "bbox": [{"x": 0.41998809576034546, "y": 0.15937763452529907}, {"x": 0.449137419462204, "y": 0.15937763452529907}, {"x": 0.449137419462204, "y": 0.16652649641036987}, {"x": 0.41998809576034546, "y": 0.16652649641036987}], "text": "64.7\n"}
{"page": 9, "bbox": [{"x": 0.13622844219207764, "y": 0.1589571088552475}, {"x": 0.2671029269695282, "y": 0.1589571088552475}, {"x": 0.2671029269695282, "y": 0.16736753284931183}, {"x": 0.13622844219207764, "y": 0.16736753284931183}], "text": "ChatGPT-few-shot\n"}
{"page": 9, "bbox": [{"x": 0.5960737466812134, "y": 0.18797308206558228}, {"x": 0.6311719417572021, "y": 0.19049622118473053}, {"x": 0.630577027797699, "y": 0.19512194395065308}, {"x": 0.595478892326355, "y": 0.19259881973266602}], "text": "etrieval\n"}
{"page": 9, "bbox": [{"x": 0.578822135925293, "y": 0.1909167319536209}, {"x": 0.5901249051094055, "y": 0.1909167319536209}, {"x": 0.5901249051094055, "y": 0.19428090751171112}, {"x": 0.578822135925293, "y": 0.19428090751171112}], "text": "no\n"}
{"page": 9, "bbox": [{"x": 0.11838191747665405, "y": 0.18881413340568542}, {"x": 0.4860202372074127, "y": 0.18881413340568542}, {"x": 0.4860202372074127, "y": 0.21320436894893646}, {"x": 0.11838191747665405, "y": 0.21320436894893646}], "text": "Table 4: Evaluation of our retrieval evaluator and\nChatGPT for the retrieval results on the PopQA dataset.\n"}
{"page": 9, "bbox": [{"x": 0.5461035370826721, "y": 0.2018502950668335}, {"x": 0.5591909289360046, "y": 0.20227082073688507}, {"x": 0.5591909289360046, "y": 0.20773759484291077}, {"x": 0.5461035370826721, "y": 0.2073170691728592}], "text": "20\n"}
{"page": 9, "bbox": [{"x": 0.613920271396637, "y": 0.20605550706386566}, {"x": 0.6234384179115295, "y": 0.20605550706386566}, {"x": 0.6234384179115295, "y": 0.20984020829200745}, {"x": 0.613920271396637, "y": 0.20984020829200745}], "text": "60\n"}
{"page": 9, "bbox": [{"x": 0.6591314673423767, "y": 0.20605550706386566}, {"x": 0.6680546998977661, "y": 0.20605550706386566}, {"x": 0.6680546998977661, "y": 0.20984020829200745}, {"x": 0.6591314673423767, "y": 0.20984020829200745}], "text": "50\n"}
{"page": 9, "bbox": [{"x": 0.748958945274353, "y": 0.20605550706386566}, {"x": 0.7578822374343872, "y": 0.20605550706386566}, {"x": 0.7578822374343872, "y": 0.20984020829200745}, {"x": 0.748958945274353, "y": 0.20984020829200745}], "text": "30\n"}
{"page": 9, "bbox": [{"x": 0.7935752272605896, "y": 0.20605550706386566}, {"x": 0.8036882877349854, "y": 0.20605550706386566}, {"x": 0.8036882877349854, "y": 0.20984020829200745}, {"x": 0.7935752272605896, "y": 0.20984020829200745}], "text": "20\n"}
{"page": 9, "bbox": [{"x": 0.7037477493286133, "y": 0.20647603273391724}, {"x": 0.7132658958435059, "y": 0.20647603273391724}, {"x": 0.7132658958435059, "y": 0.20984020829200745}, {"x": 0.7037477493286133, "y": 0.20984020829200745}], "text": "40\n"}
{"page": 9, "bbox": [{"x": 0.8393813371658325, "y": 0.20647603273391724}, {"x": 0.8471148014068604, "y": 0.20647603273391724}, {"x": 0.8471148014068604, "y": 0.20984020829200745}, {"x": 0.8393813371658325, "y": 0.20984020829200745}], "text": "10\n"}
{"page": 9, "bbox": [{"x": 0.5597858428955078, "y": 0.20605550706386566}, {"x": 0.5907198190689087, "y": 0.20605550706386566}, {"x": 0.5907198190689087, "y": 0.21698907017707825}, {"x": 0.5597858428955078, "y": 0.21698907017707825}], "text": "69.8\n(Actual)\n"}
{"page": 9, "bbox": [{"x": 0.6466389298439026, "y": 0.21825063228607178}, {"x": 0.7703747749328613, "y": 0.21698907017707825}, {"x": 0.7703747749328613, "y": 0.224978968501091}, {"x": 0.6466389298439026, "y": 0.22624054551124573}], "text": "Accuracy of retrieval\n"}
{"page": 9, "bbox": [{"x": 0.5133848786354065, "y": 0.24516400694847107}, {"x": 0.8822129964828491, "y": 0.24432295560836792}, {"x": 0.8822129964828491, "y": 0.3120269179344177}, {"x": 0.5133848786354065, "y": 0.3128679692745209}], "text": "Figure 3: The generation performance of Self-RAG\nand Self-CRAG given different retrieval performance\non the PopQA dataset with SelfRAG-LLaMA-7b. The\nlower horizontal line demonstrates the performance of\nthe generator without retrieval.\n"}
{"page": 9, "bbox": [{"x": 0.6561570763587952, "y": 0.33851975202560425}, {"x": 0.8697203993797302, "y": 0.33851975202560425}, {"x": 0.8697203993797302, "y": 0.3448275923728943}, {"x": 0.6561570763587952, "y": 0.3448275923728943}], "text": "LLAMA2-hf-7b SelfRAG-LLAMA2-7b\n"}
{"page": 9, "bbox": [{"x": 0.5246877074241638, "y": 0.3570227026939392}, {"x": 0.5794169902801514, "y": 0.3570227026939392}, {"x": 0.5794169902801514, "y": 0.3801513910293579}, {"x": 0.5246877074241638, "y": 0.3801513910293579}], "text": "PopQA\nCRAG\n"}
{"page": 9, "bbox": [{"x": 0.6841166019439697, "y": 0.37258198857307434}, {"x": 0.7126710414886475, "y": 0.37258198857307434}, {"x": 0.7126710414886475, "y": 0.3801513910293579}, {"x": 0.6841166019439697, "y": 0.3801513910293579}], "text": "54.9\n"}
{"page": 9, "bbox": [{"x": 0.7935752272605896, "y": 0.3730025291442871}, {"x": 0.8215348124504089, "y": 0.3730025291442871}, {"x": 0.8215348124504089, "y": 0.3801513910293579}, {"x": 0.7935752272605896, "y": 0.3801513910293579}], "text": "59.8\n"}
{"page": 9, "bbox": [{"x": 0.6841166019439697, "y": 0.38645920157432556}, {"x": 0.7120761275291443, "y": 0.38645920157432556}, {"x": 0.7120761275291443, "y": 0.3944491147994995}, {"x": 0.6841166019439697, "y": 0.3944491147994995}], "text": "50.5\n"}
{"page": 9, "bbox": [{"x": 0.5330160856246948, "y": 0.38687974214553833}, {"x": 0.5669244527816772, "y": 0.38687974214553833}, {"x": 0.5669244527816772, "y": 0.39402860403060913}, {"x": 0.5330160856246948, "y": 0.39402860403060913}], "text": "RAG\n"}
{"page": 9, "bbox": [{"x": 0.7935752272605896, "y": 0.38687974214553833}, {"x": 0.8215348124504089, "y": 0.38687974214553833}, {"x": 0.8215348124504089, "y": 0.39402860403060913}, {"x": 0.7935752272605896, "y": 0.39402860403060913}], "text": "52.8\n"}
{"page": 9, "bbox": [{"x": 0.5330160856246948, "y": 0.40117746591567993}, {"x": 0.6180844902992249, "y": 0.40075692534446716}, {"x": 0.6180844902992249, "y": 0.40832632780075073}, {"x": 0.5330160856246948, "y": 0.4087468385696411}], "text": "RAG w. web\n"}
{"page": 9, "bbox": [{"x": 0.7941701412200928, "y": 0.40117746591567993}, {"x": 0.8215348124504089, "y": 0.40075692534446716}, {"x": 0.8215348124504089, "y": 0.40832632780075073}, {"x": 0.7941701412200928, "y": 0.4087468385696411}], "text": "53.8\n"}
{"page": 9, "bbox": [{"x": 0.6841166019439697, "y": 0.4015979766845703}, {"x": 0.7126710414886475, "y": 0.4015979766845703}, {"x": 0.7126710414886475, "y": 0.4087468385696411}, {"x": 0.6841166019439697, "y": 0.4087468385696411}], "text": "52.2\n"}
{"page": 9, "bbox": [{"x": 0.5330160856246948, "y": 0.41547518968582153}, {"x": 0.6085663437843323, "y": 0.4146341383457184}, {"x": 0.6085663437843323, "y": 0.4234651029109955}, {"x": 0.5330160856246948, "y": 0.42430615425109863}], "text": "Self-CRAG\n"}
{"page": 9, "bbox": [{"x": 0.6835216879844666, "y": 0.4163162410259247}, {"x": 0.7120761275291443, "y": 0.4163162410259247}, {"x": 0.7120761275291443, "y": 0.4234651029109955}, {"x": 0.6835216879844666, "y": 0.4234651029109955}], "text": "49.0\n"}
{"page": 9, "bbox": [{"x": 0.7941701412200928, "y": 0.4163162410259247}, {"x": 0.8215348124504089, "y": 0.4163162410259247}, {"x": 0.8215348124504089, "y": 0.4234651029109955}, {"x": 0.7941701412200928, "y": 0.4234651029109955}], "text": "61.8\n"}
{"page": 9, "bbox": [{"x": 0.7935752272605896, "y": 0.42977291345596313}, {"x": 0.8215348124504089, "y": 0.4301934540271759}, {"x": 0.8215348124504089, "y": 0.43818333745002747}, {"x": 0.7935752272605896, "y": 0.4377628266811371}], "text": "54.9\n"}
{"page": 9, "bbox": [{"x": 0.5330160856246948, "y": 0.4301934540271759}, {"x": 0.5984532833099365, "y": 0.4301934540271759}, {"x": 0.5984532833099365, "y": 0.43818333745002747}, {"x": 0.5330160856246948, "y": 0.43818333745002747}], "text": "Self-RAG\n"}
{"page": 9, "bbox": [{"x": 0.6835216879844666, "y": 0.4306139647960663}, {"x": 0.7126710414886475, "y": 0.4306139647960663}, {"x": 0.7126710414886475, "y": 0.4377628266811371}, {"x": 0.6835216879844666, "y": 0.4377628266811371}], "text": "29.0\n"}
{"page": 9, "bbox": [{"x": 0.7935752272605896, "y": 0.44533219933509827}, {"x": 0.8203450441360474, "y": 0.4444911777973175}, {"x": 0.8209398984909058, "y": 0.4520605504512787}, {"x": 0.7941701412200928, "y": 0.45290160179138184}], "text": "57.9\n"}
{"page": 9, "bbox": [{"x": 0.5330160856246948, "y": 0.4449116885662079}, {"x": 0.6508030891418457, "y": 0.4449116885662079}, {"x": 0.6508030891418457, "y": 0.45290160179138184}, {"x": 0.5330160856246948, "y": 0.45290160179138184}], "text": "Self-RAG w. web\n"}
{"page": 9, "bbox": [{"x": 0.6835216879844666, "y": 0.44533219933509827}, {"x": 0.7114812731742859, "y": 0.4449116885662079}, {"x": 0.7114812731742859, "y": 0.45248109102249146}, {"x": 0.6835216879844666, "y": 0.45290160179138184}], "text": "24.9\n"}
{"page": 9, "bbox": [{"x": 0.5139797925949097, "y": 0.4714045524597168}, {"x": 0.8839976191520691, "y": 0.4735071361064911}, {"x": 0.8834027647972107, "y": 0.5138772130012512}, {"x": 0.5133848786354065, "y": 0.5117745995521545}], "text": "Table 5: Comparison results between CRAG, Self-\nCRAG and RAG, Self-RAG with the same input in\nterms of accuracy.\n"}
{"page": 9, "bbox": [{"x": 0.11719214916229248, "y": 0.23423044383525848}, {"x": 0.48958954215049744, "y": 0.23423044383525848}, {"x": 0.48899465799331665, "y": 0.920941948890686}, {"x": 0.1165972650051117, "y": 0.920941948890686}], "text": "The impact of each knowledge utilization oper-\nation. Table 3 illustrated how the performance\nchanged if a key knowledge utilization operation\nwas ablated. Evaluations on the PopQA dataset in\nterms of accuracy were conducted by individually\nremoving the knowledge utilization operations of\ndocument refinement, search query rewriting, and\nexternal knowledge selection. Removing document\nrefinement denoted that the original retrieved docu-\nments were directly fed to the following generator,\nas in most existing works. Additionally, removing\nsearch query rewriting denoted that questions were\nnot rewritten into queries consisting of keywords\nduring knowledge searching. Eventually, removing\nknowledge selection denoted that all searched con-\ntent of web pages was all regarded as the external\nknowledge without selection. These results help\nderive the findings that the performance of the\nfinal system degraded no matter which knowledge\nutilization operation was removed, revealing that\neach knowledge utilization operation contributed\nto improving the utilization of knowledge.\n5.5 Accuracy of the Retrieval Evaluator\nThe quality of the retrieval evaluator significantly\ndetermined the performance of the entire system.\nGiven the document retrieval results, we assessed\nwhether the retrieval evaluator can accurately\ndetermine the overall quality of these results. The\nassessment accuracy on the PopQA dataset of\nour retrieval evaluator and the commercial LLM\nChatGPT on the document retrieval results was\nshown in Table 4. The prompts of ChatGPT,\nChatGPT-COT, and ChatGPT-few-shot used in our\nexperiments can be referred to in Appendix A.\nResults reveal that the lightweight T5-based re-\ntrieval evaluator significantly outperformed the\ncompetitive ChatGPT in all settings.\n5.6 Robustness to Retrieval Performance\nTo further verify the robustness of the proposed\nmethod to retrieval performance, we studied how\nthe generation performance changed given different\n"}
{"page": 9, "bbox": [{"x": 0.5133848786354065, "y": 0.5361648201942444}, {"x": 0.8839976191520691, "y": 0.535744309425354}, {"x": 0.8845925331115723, "y": 0.7737594842910767}, {"x": 0.5139797925949097, "y": 0.774179995059967}], "text": "retrieval performance. A part of accurate retrieval\nresults were deliberately removed at random to\nimitate a low-quality retriever and evaluate how\nthe performance changed. Figure 3 demonstrated\nthe performance change of Self-RAG and Self-\nCRAG on the PopQA dataset. It can be seen\nthat the generation performance of Self-RAG and\nSelf-CRAG dropped as the retrieval performance\ndropped, indicating that the generator relied heavily\non the quality of the retriever. Furthermore, as\nthe retrieval performance dropped, the generation\nperformance of Self-CRAG dropped more slightly\nthan that of Self-RAG. These results imply the\nsuperiority of Self-CRAG over Self-RAG on en-\nhancing the robustness to retrieval performance.\n"}
{"page": 9, "bbox": [{"x": 0.5133848786354065, "y": 0.7893187403678894}, {"x": 0.8364068865776062, "y": 0.7901598215103149}, {"x": 0.8364068865776062, "y": 0.8023549318313599}, {"x": 0.5133848786354065, "y": 0.8015138506889343}], "text": "5.7 Consistent Supplementation of Web\n"}
{"page": 9, "bbox": [{"x": 0.5550267696380615, "y": 0.8048780560493469}, {"x": 0.7019631266593933, "y": 0.8069806694984436}, {"x": 0.7013682126998901, "y": 0.8174936771392822}, {"x": 0.5544319152832031, "y": 0.8153910636901855}], "text": "Search Knowledge\n"}
{"page": 9, "bbox": [{"x": 0.5133848786354065, "y": 0.828427255153656}, {"x": 0.8839976191520691, "y": 0.8288477659225464}, {"x": 0.8839976191520691, "y": 0.9188393354415894}, {"x": 0.5133848786354065, "y": 0.918418824672699}], "text": "This paper highlights the necessity of enhancing\nthe retrieved context by incorporating additional\ninformation when the initial retrieval results are\nirrelevant and unreliable. Meanwhile, it is also\ncrucial to confirm that the primary improvements\nin our method stem from the self-correction mech-\n"}
{"page": 10, "bbox": [{"x": 0.24449732899665833, "y": 0.09209419786930084}, {"x": 0.3438429534435272, "y": 0.09209419786930084}, {"x": 0.3438429534435272, "y": 0.09966358542442322}, {"x": 0.24449732899665833, "y": 0.09966358542442322}], "text": "TFLOPS per token\n"}
{"page": 10, "bbox": [{"x": 0.36109459400177, "y": 0.09209419786930084}, {"x": 0.4533016085624695, "y": 0.09125315397977829}, {"x": 0.4533016085624695, "y": 0.09966358542442322}, {"x": 0.36109459400177, "y": 0.10050462931394577}], "text": "executing time(s)\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.08830950409173965}, {"x": 0.8822129964828491, "y": 0.08830950409173965}, {"x": 0.8822129964828491, "y": 0.11522287875413895}, {"x": 0.5133848786354065, "y": 0.11522287875413895}], "text": "while significantly enhancing performance, thereby\nvalidating its lightweight nature.\n"}
{"page": 10, "bbox": [{"x": 0.3890541195869446, "y": 0.1105971410870552}, {"x": 0.4259369373321533, "y": 0.1105971410870552}, {"x": 0.4259369373321533, "y": 0.11816652864217758}, {"x": 0.3890541195869446, "y": 0.11816652864217758}], "text": "0.363\n"}
{"page": 10, "bbox": [{"x": 0.14991076290607452, "y": 0.11101765930652618}, {"x": 0.18322427570819855, "y": 0.11101765930652618}, {"x": 0.18322427570819855, "y": 0.11816652864217758}, {"x": 0.14991076290607452, "y": 0.11816652864217758}], "text": "RAG\n"}
{"page": 10, "bbox": [{"x": 0.27900058031082153, "y": 0.11101765930652618}, {"x": 0.30814990401268005, "y": 0.11101765930652618}, {"x": 0.30814990401268005, "y": 0.11858704686164856}, {"x": 0.27900058031082153, "y": 0.11858704686164856}], "text": "26.5\n"}
{"page": 10, "bbox": [{"x": 0.27900058031082153, "y": 0.12531539797782898}, {"x": 0.30755501985549927, "y": 0.1248948723077774}, {"x": 0.30755501985549927, "y": 0.13246425986289978}, {"x": 0.27900058031082153, "y": 0.13288477063179016}], "text": "27.2\n"}
{"page": 10, "bbox": [{"x": 0.1505056470632553, "y": 0.12531539797782898}, {"x": 0.19690659642219543, "y": 0.12531539797782898}, {"x": 0.19690659642219543, "y": 0.13288477063179016}, {"x": 0.1505056470632553, "y": 0.13288477063179016}], "text": "CRAG\n"}
{"page": 10, "bbox": [{"x": 0.3890541195869446, "y": 0.12531539797782898}, {"x": 0.42534205317497253, "y": 0.12531539797782898}, {"x": 0.42534205317497253, "y": 0.13288477063179016}, {"x": 0.3890541195869446, "y": 0.13288477063179016}], "text": "0.512\n"}
{"page": 10, "bbox": [{"x": 0.2546103596687317, "y": 0.14003364741802216}, {"x": 0.3325401544570923, "y": 0.14003364741802216}, {"x": 0.3325401544570923, "y": 0.14718250930309296}, {"x": 0.2546103596687317, "y": 0.14718250930309296}], "text": "26.5~132.4\n"}
{"page": 10, "bbox": [{"x": 0.3890541195869446, "y": 0.14003364741802216}, {"x": 0.42474716901779175, "y": 0.14003364741802216}, {"x": 0.42474716901779175, "y": 0.14760303497314453}, {"x": 0.3890541195869446, "y": 0.14760303497314453}], "text": "0.741\n"}
{"page": 10, "bbox": [{"x": 0.14991076290607452, "y": 0.14003364741802216}, {"x": 0.22546103596687317, "y": 0.14003364741802216}, {"x": 0.22546103596687317, "y": 0.16190075874328613}, {"x": 0.14991076290607452, "y": 0.16190075874328613}], "text": "Self-RAG\nSelf-CRAG\n"}
{"page": 10, "bbox": [{"x": 0.3890541195869446, "y": 0.15433137118816376}, {"x": 0.42534205317497253, "y": 0.15433137118816376}, {"x": 0.42534205317497253, "y": 0.16148023307323456}, {"x": 0.3890541195869446, "y": 0.16148023307323456}], "text": "0.908\n"}
{"page": 10, "bbox": [{"x": 0.2587745487689972, "y": 0.15433137118816376}, {"x": 0.3283759653568268, "y": 0.15475189685821533}, {"x": 0.3283759653568268, "y": 0.16190075874328613}, {"x": 0.2587745487689972, "y": 0.16148023307323456}], "text": "27.2~80.2\n"}
{"page": 10, "bbox": [{"x": 0.11778703331947327, "y": 0.18292683362960815}, {"x": 0.4878048896789551, "y": 0.182085782289505}, {"x": 0.48839977383613586, "y": 0.2926829159259796}, {"x": 0.11838191747665405, "y": 0.29352396726608276}], "text": "Table 6: computational overhead assessment of RAG,\nCRAG, Self-CRAG, and Self-RAG about FLOPs per\ntoken on GPUs and executing time instance. The\nper\nupper bound of Self-CRAG is lower because only three\npassages are provided as input (correct, incorrect and\nambiguous content). All the data in the table only\nrepresents a rough estimate of the generation phase, the\nretrieval and data-processing stages are not included.\n"}
{"page": 10, "bbox": [{"x": 0.5127900242805481, "y": 0.13162320852279663}, {"x": 0.8839976191520691, "y": 0.13162320852279663}, {"x": 0.8839976191520691, "y": 0.5206055641174316}, {"x": 0.5127900242805481, "y": 0.5206055641174316}], "text": "6 Conclusion & Limitation\nThis paper studies the problem where RAG-based\napproaches are challenged if retrieval goes wrong,\nthereby exposing inaccurate and misleading knowl-\nedge to generative LMs. Corrective Retrieval\nAugmented Generation is proposed to improve the\nrobustness of generation. Essentially, a lightweight\nretrieval evaluator is to estimate and trigger three\nknowledge retrieval actions discriminately. With\nthe further leverage of web search and optimized\nknowledge utilization, CRAG has significantly\nimproved the ability of automatic self-correction\nand efficient utilization of retrieved documents.\nExperiments extensively demonstrate its adaptabil-\nity to RAG-based approaches as well as general-\nizability across short- and long-form generation\ntasks. While we primarily proposed to improve the\nRAG framework from a corrective perspective and\nCRAG can be seamlessly coupled with various\nRAG-based approaches, fine-tuning an external\nretrieval evaluator is inevitable. How to eliminate\nthis external evaluator and equip LLMs with better\nretrieval evaluation capabilities will be our future\nwork.\n"}
{"page": 10, "bbox": [{"x": 0.11778703331947327, "y": 0.3183347284793854}, {"x": 0.48899465799331665, "y": 0.3183347284793854}, {"x": 0.48899465799331665, "y": 0.6206896305084229}, {"x": 0.11778703331947327, "y": 0.6206896305084229}], "text": "anism, rather than solely from the supplementary\ninformation obtained through web searches. To\nfurther demonstrate the effectiveness of the pro-\nposed self-correction mechanism, both RAG and\nSelf-RAG were consistently supplemented with\nweb search knowledge to ensure they had access\nto the same scope of the retrieved knowledge.\nThe results in Table 5 show that consistently\nsupplementing RAG or Self-RAG with web search\nknowledge can improve the performance in most\ncases (except Self-RAG w. web using the original\nLLAMA2 model), though the improvement remains\nlimited. Furthermore, augmenting RAG or Self-\nRAG with the proposed self-correction mechanism\nsignificantly outperformed the models consistently\nsupplemented with web search knowledge in all\ncases. This finding confirms that the observed\nadvancements are primarily attributable to the\nproposed self-correction mechanism.\n"}
{"page": 10, "bbox": [{"x": 0.5145746469497681, "y": 0.5550882816314697}, {"x": 0.6073765754699707, "y": 0.5550882816314697}, {"x": 0.6073765754699707, "y": 0.5643397569656372}, {"x": 0.5145746469497681, "y": 0.5643397569656372}], "text": "References\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.577796459197998}, {"x": 0.8834027647972107, "y": 0.577796459197998}, {"x": 0.8834027647972107, "y": 0.6791421175003052}, {"x": 0.5139797925949097, "y": 0.6791421175003052}], "text": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern,\nGaurav Mishra, Erica Moreira, Mark Omernick,\nKevin Robinson, Sebastian Ruder, et al. 2023. PALM\n2 technical report. CORR, abs/2305.10403.\n"}
{"page": 10, "bbox": [{"x": 0.5133848786354065, "y": 0.6917577981948853}, {"x": 0.8828078508377075, "y": 0.6921783089637756}, {"x": 0.8828078508377075, "y": 0.7687131762504578}, {"x": 0.5133848786354065, "y": 0.7682926654815674}], "text": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\n"}
{"page": 10, "bbox": [{"x": 0.11778703331947327, "y": 0.6425567865371704}, {"x": 0.48839977383613586, "y": 0.6425567865371704}, {"x": 0.48839977383613586, "y": 0.920941948890686}, {"x": 0.11778703331947327, "y": 0.920941948890686}], "text": "5.8 Computational Overhead Analysis\nTo illustrate that our self-correction mechanism\nserves as a lightweight, plug-and-play solution\nfor various RAG-based frameworks, we measured\nthe computational overhead. FLOPs prediction\nformulas in Narayanan et al. (2021) were employed,\nwith the results presented in Table 6 which shows\nthe predicted FLOPs per token on GPUs. Due to\nthe adaptive nature of Self-RAG, which varies its\ngeneration strategies based on input, the compu-\ntational overhead cannot be precisely determined.\nTherefore, we present an estimated range instead.\nAdditionally, we conducted the experiments on\nPopQA to assess the average execution time per\ninstance in practice, as detailed in Table 6. The\nfindings indicate that the self-correction mecha-\nnism incurs only modest computational overhead\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.7813288569450378}, {"x": 0.8822129964828491, "y": 0.7817493677139282}, {"x": 0.8822129964828491, "y": 0.8687973022460938}, {"x": 0.5139797925949097, "y": 0.8683767914772034}], "text": "Yejin Bang, Samuel Cahyawijaya, Nayeon Lee,\nWenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,\nZiwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do,\nYan Xu, and Pascale Fung. 2023. A multitask,\nmultilingual, multimodal evaluation of chatgpt on\nreasoning, hallucination, and interactivity. pages\n675-718.\n"}
{"page": 10, "bbox": [{"x": 0.5139797925949097, "y": 0.8830950260162354}, {"x": 0.8822129964828491, "y": 0.8835155367851257}, {"x": 0.8822129964828491, "y": 0.920941948890686}, {"x": 0.5139797925949097, "y": 0.9205214381217957}], "text": "Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar\nKhot, Bhavana Dalvi Mishra, Kyle Richardson,\nAshish Sabharwal, Carissa Schoenick, Oyvind\n"}
{"page": 11, "bbox": [{"x": 0.13622844219207764, "y": 0.08873002231121063}, {"x": 0.4878048896789551, "y": 0.08915054798126221}, {"x": 0.4878048896789551, "y": 0.13666947185993195}, {"x": 0.13622844219207764, "y": 0.13624894618988037}], "text": "Tafjord, and Peter Clark. 2021. Think you have\nsolved direct-answer question answering? try arc-da,\nthe direct-answer AI2 reasoning challenge. CORR,\nabs/2102.03315.\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.1501261591911316}, {"x": 0.4878048896789551, "y": 0.1480235457420349}, {"x": 0.48839977383613586, "y": 0.1980656087398529}, {"x": 0.11897680163383484, "y": 0.2001682072877884}], "text": "Tom B Brown, Benjamin Mann, Nick Ryder, et al.\n2020. Language models are few-shot learners. In\nAdvances in neural information processing systems,\npages 1877-1901.\n"}
{"page": 11, "bbox": [{"x": 0.5133848786354065, "y": 0.08873002231121063}, {"x": 0.8834027647972107, "y": 0.08873002231121063}, {"x": 0.8828078508377075, "y": 0.5290159583091736}, {"x": 0.5127900242805481, "y": 0.5290159583091736}], "text": "Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Active\nretrieval augmented generation. In Proceedings\nof the 2023 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023, pages 7969-7992.\nAssociation for Computational Linguistics.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin\nPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo\nHa, and Jinwoo Shin. 2024. Sure: Summarizing\nretrievals using answer candidates for open-domain\nQA of Ilms. In The Twelfth International Conference\non Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2022. Internet-augmented dialogue generation. In\nProceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 8460-8478. Association for\nComputational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra\nPiktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in\nNeural Information Processing Systems 33: Annual\nConference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.21152228116989136}, {"x": 0.4878048896789551, "y": 0.21152228116989136}, {"x": 0.4878048896789551, "y": 0.5193439722061157}, {"x": 0.11838191747665405, "y": 0.5193439722061157}], "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer,\nVinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2023. Palm: Scaling language\nmodeling with pathways. J. Mach. Learn. Res.,\n24:240:1-240:113.\n"}
{"page": 11, "bbox": [{"x": 0.11897680163383484, "y": 0.5332211852073669}, {"x": 0.48899465799331665, "y": 0.5336416959762573}, {"x": 0.48899465799331665, "y": 0.5841042995452881}, {"x": 0.11897680163383484, "y": 0.5836837887763977}], "text": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. 2024. Chain-of-verification reduces\nhallucination in large language models. pages 3563-\n"}
{"page": 11, "bbox": [{"x": 0.1374182105064392, "y": 0.5866274237632751}, {"x": 0.17430101335048676, "y": 0.5866274237632751}, {"x": 0.17430101335048676, "y": 0.593776285648346}, {"x": 0.1374182105064392, "y": 0.593776285648346}], "text": "3578.\n"}
{"page": 11, "bbox": [{"x": 0.5139797925949097, "y": 0.5428931713104248}, {"x": 0.8845925331115723, "y": 0.5428931713104248}, {"x": 0.8845925331115723, "y": 0.7939444780349731}, {"x": 0.5139797925949097, "y": 0.7939444780349731}], "text": "Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation. CORR, abs/2202.01110.\nYanming Liu, Xinyue Peng, Xuhong Zhang, Weihao\nLiu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024.\nRA-ISF: learning to answer and understand from\nretrieval augmentation via iterative self-feedback.\nIn Findings of the Association for Computational\nLinguistics, ACL 2024, Bangkok, Thailand and\nvirtual meeting, August 11-16, 2024, pages 4730-\n4749. Association for Computational Linguistics.\nHongyin Luo, Tianhua Zhang, Yung-Sung Chuang,\nYuan Gong, Yoon Kim, Xixin Wu, Helen Meng, and\nJames R. Glass. 2023. Search augmented instruction\nlearning. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, Singapore,\nDecember 6-10, 2023, pages 3717-3729. Association\nfor Computational Linguistics.\n"}
{"page": 11, "bbox": [{"x": 0.11838191747665405, "y": 0.6063919067382812}, {"x": 0.4878048896789551, "y": 0.6063919067382812}, {"x": 0.4878048896789551, "y": 0.8448275923728943}, {"x": 0.11838191747665405, "y": 0.8448275923728943}], "text": "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback. CoRR, abs/2305.14387.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 3929-3938. PMLR.\nGautier Izacard, Mathilde Caron, Lucas Hosseini,\nSebastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2022. Unsupervised dense\ninformation retrieval with contrastive learning. Trans.\nMach. Learn. Res., 2022.\n"}
{"page": 11, "bbox": [{"x": 0.5318263173103333, "y": 0.832211971282959}, {"x": 0.570493757724762, "y": 0.832211971282959}, {"x": 0.570493757724762, "y": 0.8393608331680298}, {"x": 0.5318263173103333, "y": 0.8393608331680298}], "text": "2023.\n"}
{"page": 11, "bbox": [{"x": 0.5133848786354065, "y": 0.8040370345115662}, {"x": 0.8834027647972107, "y": 0.8044575452804565}, {"x": 0.8834027647972107, "y": 0.9217830300331116}, {"x": 0.5133848786354065, "y": 0.9213625192642212}], "text": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Daniel Khashabi, and Hannaneh Hajishirzi.\nWhen not to trust language models:\nInvestigating effectiveness of parametric and non-\nparametric memories. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023,\nToronto, Canada, July 9-14, 2023, pages 9802-9822.\nAssociation for Computational Linguistics.\n"}
{"page": 11, "bbox": [{"x": 0.11778703331947327, "y": 0.8574432134628296}, {"x": 0.4866151213645935, "y": 0.8570227026939392}, {"x": 0.4866151213645935, "y": 0.9196804165840149}, {"x": 0.11778703331947327, "y": 0.9201009273529053}], "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of\nhallucination in natural language generation. ACM\nComput. Surv., 55(12):248:1-248:38.\n"}
{"page": 12, "bbox": [{"x": 0.5312314033508301, "y": 0.08915054798126221}, {"x": 0.8822129964828491, "y": 0.08915054798126221}, {"x": 0.8822129964828491, "y": 0.1248948723077774}, {"x": 0.5312314033508301, "y": 0.1248948723077774}], "text": "NAACL 2022, Seattle, WA, United States, July\n10-15, 2022, pages 2557-2571. Association for\nComputational Linguistics.\n"}
{"page": 12, "bbox": [{"x": 0.5145746469497681, "y": 0.13835155963897705}, {"x": 0.8828078508377075, "y": 0.13835155963897705}, {"x": 0.8828078508377075, "y": 0.19890664517879486}, {"x": 0.5145746469497681, "y": 0.19890664517879486}], "text": "Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom.\n2023. Toolformer: Language models can teach\nthemselves to use tools.\n"}
{"page": 12, "bbox": [{"x": 0.11838191747665405, "y": 0.08873002231121063}, {"x": 0.4901844263076782, "y": 0.08873002231121063}, {"x": 0.4901844263076782, "y": 0.4230445623397827}, {"x": 0.11838191747665405, "y": 0.4230445623397827}], "text": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2023, Singapore, December 6-10, 2023, pages 12076-\n12100. Association for Computational Linguistics.\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\nNir Ratner, Yonatan Belinkov, Omri Abend, Kevin\nLeyton-Brown, Amnon Shashua, and Yoav Shoham.\n2023. Generating benchmarks for factuality evalua-\ntion of language models. CoRR, abs/2307.06908.\nDeepak Narayanan, Mohammad Shoeybi, Jared\nCasper, Patrick LeGresley, Mostofa Patwary, Vijay\nKorthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, Amar Phanishayee,\nand Matei Zaharia. 2021. Efficient large-scale\nlanguage model training on GPU clusters using\nmegatron-lm. In International Conference for\nHigh Performance Computing, Networking, Storage\nand Analysis, SC 2021, St. Louis, Missouri, USA,\nNovember 14-19, 2021, page 58. ACM.\n"}
{"page": 12, "bbox": [{"x": 0.5133848786354065, "y": 0.21320436894893646}, {"x": 0.8834027647972107, "y": 0.21320436894893646}, {"x": 0.8834027647972107, "y": 0.3128679692745209}, {"x": 0.5133848786354065, "y": 0.3128679692745209}], "text": "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H. Chi, Nathanael Schärli,\nand Denny Zhou. 2023. Large language models\ncan be easily distracted by irrelevant context. In\nProceedings of the 40th International Conference\non Machine Learning, volume 202 of Proceedings\nof Machine Learning Research, pages 31210-31227.\nPMLR.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.32800671458244324}, {"x": 0.8822129964828491, "y": 0.32758620381355286}, {"x": 0.8822129964828491, "y": 0.4285113513469696}, {"x": 0.5139797925949097, "y": 0.42893186211586}], "text": "Kurt Shuster, Spencer Poff, Moya Chen, Douwe\nKiela, and Jason Weston. 2021. Retrieval\naugmentation reduces hallucination in conversation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, Virtual Event / Punta\nCana, Dominican Republic, 16-20 November, 2021,\npages 3784-3803. Association for Computational\nLinguistics.\n"}
{"page": 12, "bbox": [{"x": 0.24925640225410461, "y": 0.4339781403541565}, {"x": 0.4193932116031647, "y": 0.43650126457214355}, {"x": 0.4187983274459839, "y": 0.44701430201530457}, {"x": 0.24866151809692383, "y": 0.4444911777973175}], "text": "GPT-4 technical report.\n"}
{"page": 12, "bbox": [{"x": 0.4431885778903961, "y": 0.43650126457214355}, {"x": 0.4872100055217743, "y": 0.43650126457214355}, {"x": 0.4872100055217743, "y": 0.4449116885662079}, {"x": 0.4431885778903961, "y": 0.4449116885662079}], "text": "CORR,\n"}
{"page": 12, "bbox": [{"x": 0.11957168579101562, "y": 0.43481916189193726}, {"x": 0.24747174978256226, "y": 0.4339781403541565}, {"x": 0.24747174978256226, "y": 0.4571067988872528}, {"x": 0.11957168579101562, "y": 0.45794785022735596}], "text": "OpenAI. 2023.\nabs/2303.08774.\n"}
{"page": 12, "bbox": [{"x": 0.11838191747665405, "y": 0.4735071361064911}, {"x": 0.4872100055217743, "y": 0.4735071361064911}, {"x": 0.4872100055217743, "y": 0.5731707215309143}, {"x": 0.11838191747665405, "y": 0.5731707215309143}], "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter\nWelinder, Paul F. Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow\ninstructions with human feedback. In NeurIPS.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.44196805357933044}, {"x": 0.8828078508377075, "y": 0.4423885643482208}, {"x": 0.8822129964828491, "y": 0.618587076663971}, {"x": 0.5133848786354065, "y": 0.6181665062904358}], "text": "Chao-Hong Tan, Jia-Chen Gu, Chongyang Tao, Zhen-\nHua Ling, Can Xu, Huang Hu, Xiubo Geng,\nand Daxin Jiang. 2022. Tegtok: Augmenting\ntext generation via task-specific and open-world\nknowledge. In Findings of the Association for\nComputational Linguistics: ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 1597-1609.\nAssociation for Computational Linguistics.\nS. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman,\nVinija Jain, Anku Rani, Vipula Rawte, Aman Chadha,\nand Amitava Das. 2024. A comprehensive survey of\nhallucination mitigation techniques in large language\nmodels. CoRR, abs/2401.01313.\n"}
{"page": 12, "bbox": [{"x": 0.11838191747665405, "y": 0.5891505479812622}, {"x": 0.48839977383613586, "y": 0.5887300372123718}, {"x": 0.48839977383613586, "y": 0.665264904499054}, {"x": 0.11838191747665405, "y": 0.6656854748725891}], "text": "Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick S. H. Lewis, Barlas Oguz, Edouard Grave,\nWen-tau Yih, and Sebastian Riedel. 2021. The web\nis your oyster-knowledge-intensive NLP against a\nvery large web corpus. CORR, abs/2112.09924.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.6316232085227966}, {"x": 0.8828078508377075, "y": 0.6316232085227966}, {"x": 0.8828078508377075, "y": 0.7186711430549622}, {"x": 0.5139797925949097, "y": 0.7186711430549622}], "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\n"}
{"page": 12, "bbox": [{"x": 0.11778703331947327, "y": 0.6783010959625244}, {"x": 0.4878048896789551, "y": 0.677880585193634}, {"x": 0.4878048896789551, "y": 0.7800672650337219}, {"x": 0.11778703331947327, "y": 0.7804877758026123}], "text": "Chengwei Qin, Aston Zhang, Zhuosheng Zhang,\nJiaao Chen, Michihiro Yasunaga, and Diyi Yang.\n2023. Is chatgpt a general-purpose natural language\nprocessing task solver? In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore,\nDecember 6-10, 2023, pages 1339-1384. Association\nfor Computational Linguistics.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.732127845287323}, {"x": 0.8828078508377075, "y": 0.732127845287323}, {"x": 0.8828078508377075, "y": 0.8078216910362244}, {"x": 0.5139797925949097, "y": 0.8078216910362244}], "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, et al. 2023b.\nLlama 2: Open foundation and fine-tuned chat\nmodels. CoRR, abs/2307.09288.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.7943649888038635}, {"x": 0.4878048896789551, "y": 0.7943649888038635}, {"x": 0.4878048896789551, "y": 0.8549200892448425}, {"x": 0.11897680163383484, "y": 0.8549200892448425}], "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21:140:1-140:67.\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.8216989040374756}, {"x": 0.881618082523346, "y": 0.8216989040374756}, {"x": 0.881618082523346, "y": 0.9188393354415894}, {"x": 0.5139797925949097, "y": 0.9188393354415894}], "text": "Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian\nMa, and Yitao Liang. 2024. RAT: retrieval\naugmented thoughts elicit context-aware reasoning\nin long-horizon generation. CORR, abs/2403.05313.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\nBerant. 2024. Making retrieval-augmented language\nmodels robust to irrelevant context.\n"}
{"page": 12, "bbox": [{"x": 0.11897680163383484, "y": 0.8696383237838745}, {"x": 0.48839977383613586, "y": 0.8704794049263}, {"x": 0.48839977383613586, "y": 0.9213625192642212}, {"x": 0.11897680163383484, "y": 0.9205214381217957}], "text": "Md. Rashad Al Hasan Rony, Ricardo Usbeck, and\nJens Lehmann. 2022. Dialokg: Knowledge-structure\naware task-oriented dialogue generation. In Findings\nof the Association for Computational Linguistics:\n"}
{"page": 13, "bbox": [{"x": 0.11897680163383484, "y": 0.08788898587226868}, {"x": 0.4872100055217743, "y": 0.08788898587226868}, {"x": 0.4872100055217743, "y": 0.14928510785102844}, {"x": 0.11897680163383484, "y": 0.14928510785102844}], "text": "Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\nDanny Fox, Helen Meng, and James R. Glass. 2023a.\nInterpretable unified language checking.\nabs/2304.03728.\n"}
{"page": 13, "bbox": [{"x": 0.4431885778903961, "y": 0.12825904786586761}, {"x": 0.4866151213645935, "y": 0.12783852219581604}, {"x": 0.4866151213645935, "y": 0.13666947185993195}, {"x": 0.4431885778903961, "y": 0.13708999752998352}], "text": "CORR,\n"}
{"page": 13, "bbox": [{"x": 0.11957168579101562, "y": 0.1644238829612732}, {"x": 0.4872100055217743, "y": 0.1644238829612732}, {"x": 0.4872100055217743, "y": 0.2140454202890396}, {"x": 0.11957168579101562, "y": 0.2140454202890396}], "text": "Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E.\nGonzalez. 2024. RAFT: adapting language model to\ndomain specific RAG. CORR, abs/2403.10131.\n"}
{"page": 13, "bbox": [{"x": 0.11838191747665405, "y": 0.22750210762023926}, {"x": 0.4878048896789551, "y": 0.22750210762023926}, {"x": 0.4878048896789551, "y": 0.30277544260025024}, {"x": 0.11838191747665405, "y": 0.30277544260025024}], "text": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\nBi, Freda Shi, and Shuming Shi. 2023b. Siren's song\nin the AI ocean: A survey on hallucination in large\nlanguage models. CORR, abs/2309.01219.\n"}
{"page": 13, "bbox": [{"x": 0.11897680163383484, "y": 0.3158116042613983}, {"x": 0.4872100055217743, "y": 0.3145500421524048}, {"x": 0.4878048896789551, "y": 0.36333054304122925}, {"x": 0.11957168579101562, "y": 0.3645921051502228}], "text": "Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too? A\ncomparative study on chatgpt and fine-tuned BERT.\nCORR, abs/2302.10198.\n"}
{"page": 14, "bbox": [{"x": 0.11778703331947327, "y": 0.085786372423172}, {"x": 0.27305176854133606, "y": 0.08830950409173965}, {"x": 0.2724568843841553, "y": 0.1000841036438942}, {"x": 0.11719214916229248, "y": 0.09756097197532654}], "text": "A Task Prompts\n"}
{"page": 14, "bbox": [{"x": 0.5139797925949097, "y": 0.08620689809322357}, {"x": 0.8804283142089844, "y": 0.08620689809322357}, {"x": 0.8804283142089844, "y": 0.10849453508853912}, {"x": 0.5139797925949097, "y": 0.10849453508853912}], "text": "Table 10: The few-shot prompt to GPT-3.5 Turbo as the\nevaluator.\n"}
{"page": 14, "bbox": [{"x": 0.11838191747665405, "y": 0.11312027275562286}, {"x": 0.48542535305023193, "y": 0.11312027275562286}, {"x": 0.48542535305023193, "y": 0.14003364741802216}, {"x": 0.11838191747665405, "y": 0.14003364741802216}], "text": "The prompts for generating knowledge keywords\nas web search queries were illustrated in Table 7.\n"}
{"page": 14, "bbox": [{"x": 0.5234979391098022, "y": 0.12952060997486115}, {"x": 0.8732897043228149, "y": 0.12994112074375153}, {"x": 0.8732897043228149, "y": 0.16316232085227966}, {"x": 0.5234979391098022, "y": 0.1627417951822281}], "text": "Given a question, does the following document have exact\ninformation to answer the question? Answer yes or no\nonly.\n"}
{"page": 14, "bbox": [{"x": 0.11838191747665405, "y": 0.1572750210762024}, {"x": 0.4872100055217743, "y": 0.15643398463726044}, {"x": 0.4872100055217743, "y": 0.18166527152061462}, {"x": 0.11838191747665405, "y": 0.18250630795955658}], "text": "Table 7: The few-shot prompt to GPT-3.5 Turbo for\ngenerating knowledge keywords as web search queries.\n"}
{"page": 14, "bbox": [{"x": 0.12790006399154663, "y": 0.20100925862789154}, {"x": 0.4782867431640625, "y": 0.20142976939678192}, {"x": 0.4782867431640625, "y": 0.24600504338741302}, {"x": 0.12790006399154663, "y": 0.24558451771736145}], "text": "Extract at most three keywords separated by comma from\nthe following dialogues and questions as queries for the\nweb search, including topic background within dialogues\nand main intent within questions.\n"}
{"page": 14, "bbox": [{"x": 0.12849494814872742, "y": 0.25988224148750305}, {"x": 0.41225460171699524, "y": 0.2590412199497223}, {"x": 0.41225460171699524, "y": 0.2817493677139282}, {"x": 0.12849494814872742, "y": 0.28259041905403137}], "text": "question: What is Henry Feilden's occupation?\nquery: Henry Feilden, occupation\n"}
{"page": 14, "bbox": [{"x": 0.5223081707954407, "y": 0.17788057029247284}, {"x": 0.8744794726371765, "y": 0.17788057029247284}, {"x": 0.8744794726371765, "y": 0.3982338011264801}, {"x": 0.5223081707954407, "y": 0.3982338011264801}], "text": "Question: In what city was Abraham Raimbach born?\nDocument: Bancroft was born on November 25, 1839\nin New Ipswich, New Hampshire to James Bancroft and\nSarah Kimball. At an early age he was cared for by Mr.\nand Mrs. Patch of Ashby, Massachusetts, the neighboring\ntown. While not legally adopted, they named him Cecil\nFranklin Patch Bancroft, adding Franklin Patch after the\nson Mr. and Mrs. Patch had who recently died. He\nattended public schools in Ashby as well as the Appleton\nAcademy in New Ipswich. He entered Dartmouth College\nin 1856 at the age of sixteen and graduated in 1860 near\nthe top of his class. Bancroft continued his education as he\nbegan his career in teaching. He took classes at the Union\nTheological Seminary in New York City during the 1864-\n65 academic year. While there he was a member of the\nUnited States Christian Commission, traveling to support\nsoldiers during the Civil War. He then transferred to the\nAndover Theological Seminary where he would graduate\nin 1867.\n"}
{"page": 14, "bbox": [{"x": 0.12790006399154663, "y": 0.29562658071517944}, {"x": 0.4098750650882721, "y": 0.2931034564971924}, {"x": 0.4104699492454529, "y": 0.31539109349250793}, {"x": 0.12849494814872742, "y": 0.317914217710495}], "text": "question: In what city was Billy Carlson born?\nquery: city, Billy Carlson, born\n"}
{"page": 14, "bbox": [{"x": 0.12790006399154663, "y": 0.33095037937164307}, {"x": 0.41165971755981445, "y": 0.32968881726264954}, {"x": 0.41165971755981445, "y": 0.3515559434890747}, {"x": 0.12790006399154663, "y": 0.35281750559806824}], "text": "question: What is the religion of John Gwynn?\nquery: religion of John Gwynn\n"}
{"page": 14, "bbox": [{"x": 0.12849494814872742, "y": 0.36375105381011963}, {"x": 0.4776918590068817, "y": 0.36375105381011963}, {"x": 0.4776918590068817, "y": 0.4015979766845703}, {"x": 0.12849494814872742, "y": 0.4015979766845703}], "text": "question: What sport does Kiribati men's national\nbasketball team play?\nquery: sport, Kiribati men's national basketball team play\n"}
{"page": 14, "bbox": [{"x": 0.5234979391098022, "y": 0.40243902802467346}, {"x": 0.6008328199386597, "y": 0.40243902802467346}, {"x": 0.6008328199386597, "y": 0.40958788990974426}, {"x": 0.5234979391098022, "y": 0.40958788990974426}], "text": "Answer: No.\n"}
{"page": 14, "bbox": [{"x": 0.12849494814872742, "y": 0.414213627576828}, {"x": 0.2504461705684662, "y": 0.4137931168079376}, {"x": 0.2504461705684662, "y": 0.43481916189193726}, {"x": 0.12849494814872742, "y": 0.43523970246315}], "text": "question: [question]\nquery:\n"}
{"page": 14, "bbox": [{"x": 0.5223081707954407, "y": 0.424726665019989}, {"x": 0.8744794726371765, "y": 0.42388561367988586}, {"x": 0.8750743865966797, "y": 0.5046257376670837}, {"x": 0.5229030251502991, "y": 0.5054667592048645}], "text": "Question: In what country is Wilcza Jama, Sokółka\nCounty?\nDocument: Wilcza Jama is a village in the administrative\ndistrict of Gmina Sokółka, within Sokółka County,\nPodlaskie Voivodeship, in north-eastern Poland, close to\nthe border with Belarus.\nAnswer: Yes.\n"}
{"page": 14, "bbox": [{"x": 0.11838191747665405, "y": 0.4587889015674591}, {"x": 0.48839977383613586, "y": 0.4583683907985687}, {"x": 0.48839977383613586, "y": 0.5033641457557678}, {"x": 0.11838191747665405, "y": 0.503784716129303}], "text": "The prompts to instruct ChatGPT as the evalua-\ntor were illustrated in Table 8, Table 9, and Table 10\nrespectively.\n"}
{"page": 14, "bbox": [{"x": 0.11838191747665405, "y": 0.5189234614372253}, {"x": 0.4866151213645935, "y": 0.5197644829750061}, {"x": 0.4866151213645935, "y": 0.5428931713104248}, {"x": 0.11838191747665405, "y": 0.542052149772644}], "text": "Table 8: The direct prompt to GPT-3.5 Turbo as the\nevaluator.\n"}
{"page": 14, "bbox": [{"x": 0.5229030251502991, "y": 0.5168208479881287}, {"x": 0.8762641549110413, "y": 0.5189234614372253}, {"x": 0.8750743865966797, "y": 0.6248948574066162}, {"x": 0.5217132568359375, "y": 0.6227922439575195}], "text": "Question: What sport does 2004 Legg Mason Tennis\nClassic play?\nDocument: The 2004 Legg Mason Tenis Classic was the\n36th edition of this tennis tournament and was played\non outdoor hard courts. The tournament was part of the\nInternational Series of the 2004 ATP Tour. It was held at\nthe William H.G. FitzGerald Tennis Center in Washington,\nD.C. from August 16 through August 22, 2004.\nAnswer: Yes.\n"}
{"page": 14, "bbox": [{"x": 0.12849494814872742, "y": 0.5639192461967468}, {"x": 0.4776918590068817, "y": 0.5639192461967468}, {"x": 0.4776918590068817, "y": 0.5971404314041138}, {"x": 0.12849494814872742, "y": 0.5971404314041138}], "text": "Given a question, does the following document have exact\ninformation to answer the question? Answer yes or no\nonly.\n"}
{"page": 14, "bbox": [{"x": 0.12849494814872742, "y": 0.5984020233154297}, {"x": 0.2712671160697937, "y": 0.5992430448532104}, {"x": 0.2712671160697937, "y": 0.6206896305084229}, {"x": 0.12849494814872742, "y": 0.6198486089706421}], "text": "Question: [question]\nDocument: [document]\n"}
{"page": 14, "bbox": [{"x": 0.11838191747665405, "y": 0.6572750210762024}, {"x": 0.4878048896789551, "y": 0.6551724076271057}, {"x": 0.4878048896789551, "y": 0.6795626282691956}, {"x": 0.11838191747665405, "y": 0.6816652417182922}], "text": "Table 9: The prompt to GPT-3.5 Turbo with Chain-of-\nThought as the evaluator.\n"}
{"page": 14, "bbox": [{"x": 0.12849494814872742, "y": 0.6993271708488464}, {"x": 0.4776918590068817, "y": 0.7010092735290527}, {"x": 0.4770969748497009, "y": 0.7354919910430908}, {"x": 0.12849494814872742, "y": 0.7338099479675293}], "text": "Given a question, does the following document have exact\ninformation to answer the question?\nQuestion: [question]\n"}
{"page": 14, "bbox": [{"x": 0.5229030251502991, "y": 0.6379310488700867}, {"x": 0.8750743865966797, "y": 0.6375105381011963}, {"x": 0.8756692409515381, "y": 0.825904130935669}, {"x": 0.5234979391098022, "y": 0.8263246417045593}], "text": "Question: Who is the author of Skin?\nDocument: The Skin We're In: A Year of Black Resistance\nand Power is a book by Desmond Cole published by\nDoubleday Canada in 2020. The Skin We're In describes\nthe struggle against racism in Canada during the year 2017,\nchronicling Cole's role as an anti-racist activist and the\nimpact of systemic racism in Canadian society. Among\nthe events it discusses are the aftermath of the assault of\nDafonte Miller in late 2016 and Canada 150. The work\nargues that Canada is not immune to the anti-Black racism\nthat characterizes American society. Due to an error by the\npublisher, the initial printing of the book's cover did not\ninclude word Blackïn the subtitle. The mistake was later\ncorrected. The book won the Toronto Book Award for 2020.\nIn 2021, the book was nominated for the Shaughnessy\nCohen Prize for Political Writing.\n"}
{"page": 14, "bbox": [{"x": 0.12849494814872742, "y": 0.7363330721855164}, {"x": 0.2706722319126129, "y": 0.7375946044921875}, {"x": 0.2706722319126129, "y": 0.746846079826355}, {"x": 0.12849494814872742, "y": 0.7455845475196838}], "text": "Document: [document]\n"}
{"page": 14, "bbox": [{"x": 0.12790006399154663, "y": 0.7489486932754517}, {"x": 0.440809041261673, "y": 0.7489486932754517}, {"x": 0.440809041261673, "y": 0.7582001686096191}, {"x": 0.12790006399154663, "y": 0.7582001686096191}], "text": "Think Step by step, and answer with yes or no only.\n"}
{"page": 14, "bbox": [{"x": 0.5234979391098022, "y": 0.828427255153656}, {"x": 0.6008328199386597, "y": 0.828427255153656}, {"x": 0.6008328199386597, "y": 0.8355761170387268}, {"x": 0.5234979391098022, "y": 0.8355761170387268}], "text": "Answer: No.\n"}
{"page": 14, "bbox": [{"x": 0.5240927934646606, "y": 0.8511354327201843}, {"x": 0.6656752228736877, "y": 0.8519764542579651}, {"x": 0.6656752228736877, "y": 0.8730025291442871}, {"x": 0.5240927934646606, "y": 0.8721615076065063}], "text": "Question: [question]\nDocument: [document]\n"}
{"page": 14, "bbox": [{"x": 0.5234979391098022, "y": 0.8759461641311646}, {"x": 0.5734681487083435, "y": 0.8755256533622742}, {"x": 0.5734681487083435, "y": 0.8830950260162354}, {"x": 0.5234979391098022, "y": 0.8835155367851257}], "text": "Answer:\n"}
{"page": 15, "bbox": [{"x": 0.11897680163383484, "y": 0.08704794198274612}, {"x": 0.2623438537120819, "y": 0.08788898587226868}, {"x": 0.2623438537120819, "y": 0.10050462931394577}, {"x": 0.11897680163383484, "y": 0.09966358542442322}], "text": "B Experiments\n"}
{"page": 15, "bbox": [{"x": 0.5110053420066833, "y": 0.08788898587226868}, {"x": 0.8839976191520691, "y": 0.08788898587226868}, {"x": 0.8839976191520691, "y": 0.6312026977539062}, {"x": 0.5110053420066833, "y": 0.6312026977539062}], "text": "utilized on the Bio, Pub and ARC datasets during\ninference. The label of positive samples was 1,\nwhile that of negative ones was -1. At inference,\nthe evaluator scored the relevance from -1 to 1 for\neach document. The two confidence thresholds\nfor triggering one of the three actions were set\nempirically. Specifically, they were set as (0.59,\n-0.99) in PopQA, (0.5, -0.91) in PubQA and Arc-\nChallenge, as well as (0.95, -0.91) in Biography.\nInternal Knowledge: To obtain fine-grained\nretrieval results, we segmented the retrieved results\ninto internal strips. If a retrieved result is as short as\none or two sentences, it is regarded as an individual\nstrip, otherwise, retrieval documents are required to\nbe split into smaller units which generally consist\nof a few sentences according to the total length.\nThe scale is assumed to include an independent\npiece of information, and the filtering is based on\nthe segments. We directly adopted the evaluator\nagain for knowledge strips filtering, and the top-k\nis set to 5, filter threshold as -0.5.\nExternal Knowledge: Google Search API was\nadopted to search for the relevant URLs, top-k\nis set to 5, and pages from Wikipedia will be\nadded preferentially. The searched web pages\nare generally in the form of HTML files, where\ncontent is split with special tokens like <p>\nand </p>. Thus an extra segmentation like the\nknowledge refinement is not required, related\nknowledge paragraphs can be directly selected with\nthe evaluator similar to internal knowledge. In\nthis way, the accuracy of the search outcomes can\nbe ensured without compromising the quality and\nrelevance of the information used for generation.\n"}
{"page": 15, "bbox": [{"x": 0.1165972650051117, "y": 0.11396130919456482}, {"x": 0.48899465799331665, "y": 0.11396130919456482}, {"x": 0.48899465799331665, "y": 0.6114381551742554}, {"x": 0.1165972650051117, "y": 0.6114381551742554}], "text": "B.1 Tasks, Datasets and Metrics\nCRAG was evaluated on four datasets, which are in\npublic domain and licensed for research purposes,\nincluding:\nPopQA (Mallen et al., 2023) is a short-form\ngeneration task. Generally, only one entity of\nfactual knowledge is expected to be answered for\neach single question. In our experiments, we\nexactly followed the setting in Self-RAG (Asai\net al., 2024) which evaluated methods on a long-tail\nsubset consisting of 1,399 rare entity queries whose\nmonthly Wikipedia page views are less than 100.\nAccuracy was adopted as the evaluation metric.\nBiography (Min et al., 2023) is a long-form\ngeneration task that is tasked to generate a detailed\nbiography about a certain entity. Following previ-\nous work, FactScore (Min et al., 2023) was adopted\nto evaluate the generated biographies.\nPub Health (Zhang et al., 2023a) is a task\nin health care domain consisting of true-or-false\nquestions. Claims are represented about health\nwith factual information, and the model is tasked\nto verify the authenticity and give the judgment.\nAccuracy was adopted as the evaluation metric.\nArc-Challenge (Bhakthavatsalam et al., 2021)\nis a multiple-choice question task about some\ndaily commonsense science phenomena. Given\na scientific event that occurs in daily life, the model\nis required to select the correct description among\n3 or 4 optional choices. Accuracy was adopted as\nthe evaluation metric as well.\n"}
{"page": 15, "bbox": [{"x": 0.8613920211791992, "y": 0.6682085990905762}, {"x": 0.8822129964828491, "y": 0.6682085990905762}, {"x": 0.8822129964828491, "y": 0.6766189932823181}, {"x": 0.8613920211791992, "y": 0.6766189932823181}], "text": "To\n"}
{"page": 15, "bbox": [{"x": 0.11719214916229248, "y": 0.6299411058425903}, {"x": 0.48839977383613586, "y": 0.6299411058425903}, {"x": 0.48839977383613586, "y": 0.7241379022598267}, {"x": 0.11719214916229248, "y": 0.7241379022598267}], "text": "B.2 Experiments compute Resources\nWe used NVIDIA A800 80GB GPU for experi-\nments. For LLAMA-2 (7B) generation, it occupies\nover 40GB memory during inference. For T5-large\n(0.77B) fine-tuning, it takes much less compared\nwith LLAMA-2.\n"}
{"page": 15, "bbox": [{"x": 0.5133848786354065, "y": 0.6333053112030029}, {"x": 0.8845925331115723, "y": 0.6337258219718933}, {"x": 0.8839976191520691, "y": 0.9188393354415894}, {"x": 0.5127900242805481, "y": 0.918418824672699}], "text": "Generator: As CRAG is a plug-and-play\nmethod, all generation models that can be uti-\nlized in RAG fit our approach as well.\nbe consistent with baselines for comparison, we\nadopted LLAMA2 (Touvron et al., 2023b) for the\ngeneration. We first introduced the LLAMA2-hf-\n7b from huggingface to generate responses. Since\nSelf-RAG (Asai et al., 2024) fine-tuned LLAMA2\nand reached a new state-of-the-art performance\non several tasks, we further utilized the launched\nmodel, SelfRAG-LLAMA2-7b, as a new generator to\nbe consistent with their work and study the specific\nimprovement of our method.\nSelf-CRAG: To demonstrate that our plug-and-\nplay approach can be utilized in other concurrent\nstudies, we specifically designed to insert our\nCRAG into the Self-RAG (Asai et al., 2024)\nframework and named it Self-CRAG. Self-RAG\n"}
{"page": 15, "bbox": [{"x": 0.11778703331947327, "y": 0.7426408529281616}, {"x": 0.4878048896789551, "y": 0.7417998313903809}, {"x": 0.48839977383613586, "y": 0.918418824672699}, {"x": 0.11838191747665405, "y": 0.9192599058151245}], "text": "B.3 Implementation Details\nRetrieval Evaluator: We fine-tuned the retrieval\nevaluator based on the lightweight T5-large (Raffel\net al., 2020) pre-trained model. The dataset we\nused is the version provided by Self-RAG (Asai\net al., 2024). Specifically, the original PopQA\ndataset consists of 14k samples, 1,399 of which\nwere used for testing following Self-RAG (Asai\net al., 2024), and the remaining were used for\nfine-tuning to avoid information leakage. Besides,\nthe fine-tuned evaluator was transferred and also\n"}
{"page": 16, "bbox": [{"x": 0.11838191747665405, "y": 0.08452481031417847}, {"x": 0.48542535305023193, "y": 0.08620689809322357}, {"x": 0.48542535305023193, "y": 0.11185870319604874}, {"x": 0.11838191747665405, "y": 0.11017661541700363}], "text": "Table 11: Ablation study for removing only a single\naction on the PopQA dataset in terms of accuracy.\n"}
{"page": 16, "bbox": [{"x": 0.25223082304000854, "y": 0.13246425986289978}, {"x": 0.47233790159225464, "y": 0.13246425986289978}, {"x": 0.47233790159225464, "y": 0.139192596077919}, {"x": 0.25223082304000854, "y": 0.139192596077919}], "text": "LLAMA2-hf-7b SelfRAG-LLAMA2-7b\n"}
{"page": 16, "bbox": [{"x": 0.39381319284439087, "y": 0.1518082469701767}, {"x": 0.4229625165462494, "y": 0.15222875773906708}, {"x": 0.4229625165462494, "y": 0.15979814529418945}, {"x": 0.39381319284439087, "y": 0.15937763452529907}], "text": "59.8\n"}
{"page": 16, "bbox": [{"x": 0.12790006399154663, "y": 0.15222875773906708}, {"x": 0.1760856658220291, "y": 0.15222875773906708}, {"x": 0.1760856658220291, "y": 0.15979814529418945}, {"x": 0.12790006399154663, "y": 0.15979814529418945}], "text": "CRAG\n"}
{"page": 16, "bbox": [{"x": 0.2807852327823639, "y": 0.15264928340911865}, {"x": 0.3093396723270416, "y": 0.15264928340911865}, {"x": 0.3093396723270416, "y": 0.15979814529418945}, {"x": 0.2807852327823639, "y": 0.15979814529418945}], "text": "54.9\n"}
{"page": 16, "bbox": [{"x": 0.39381319284439087, "y": 0.16652649641036987}, {"x": 0.4229625165462494, "y": 0.16694700717926025}, {"x": 0.4223676323890686, "y": 0.17535744607448578}, {"x": 0.3932183086872101, "y": 0.1749369204044342}], "text": "56.7\n"}
{"page": 16, "bbox": [{"x": 0.2807852327823639, "y": 0.1677880585193634}, {"x": 0.3099345564842224, "y": 0.1677880585193634}, {"x": 0.3099345564842224, "y": 0.1749369204044342}, {"x": 0.2807852327823639, "y": 0.1749369204044342}], "text": "52.4\n"}
{"page": 16, "bbox": [{"x": 0.13563355803489685, "y": 0.16736753284931183}, {"x": 0.2278405725955963, "y": 0.16652649641036987}, {"x": 0.2278405725955963, "y": 0.17619848251342773}, {"x": 0.13563355803489685, "y": 0.17703953385353088}], "text": "only Correct\n"}
{"page": 16, "bbox": [{"x": 0.3926234245300293, "y": 0.18250630795955658}, {"x": 0.4217727482318878, "y": 0.18166527152061462}, {"x": 0.4223676323890686, "y": 0.18965516984462738}, {"x": 0.3932183086872101, "y": 0.19049622118473053}], "text": "48.5\n"}
{"page": 16, "bbox": [{"x": 0.2801903486251831, "y": 0.182085782289505}, {"x": 0.3105294406414032, "y": 0.18250630795955658}, {"x": 0.3099345564842224, "y": 0.19049622118473053}, {"x": 0.2801903486251831, "y": 0.19007569551467896}], "text": "47.0\n"}
{"page": 16, "bbox": [{"x": 0.13503867387771606, "y": 0.18292683362960815}, {"x": 0.2450922131538391, "y": 0.18166527152061462}, {"x": 0.2450922131538391, "y": 0.19133725762367249}, {"x": 0.13503867387771606, "y": 0.19259881973266602}], "text": "only Incorrect\n"}
{"page": 16, "bbox": [{"x": 0.2807852327823639, "y": 0.19722455739974976}, {"x": 0.3093396723270416, "y": 0.19722455739974976}, {"x": 0.3093396723270416, "y": 0.20479394495487213}, {"x": 0.2807852327823639, "y": 0.20479394495487213}], "text": "52.7\n"}
{"page": 16, "bbox": [{"x": 0.39381319284439087, "y": 0.19764508306980133}, {"x": 0.4235574007034302, "y": 0.19764508306980133}, {"x": 0.4235574007034302, "y": 0.20437341928482056}, {"x": 0.39381319284439087, "y": 0.20437341928482056}], "text": "58.0\n"}
{"page": 16, "bbox": [{"x": 0.13563355803489685, "y": 0.19722455739974976}, {"x": 0.2456870973110199, "y": 0.19722455739974976}, {"x": 0.2456870973110199, "y": 0.2068965584039688}, {"x": 0.13563355803489685, "y": 0.2068965584039688}], "text": "only Ambiguous\n"}
{"page": 16, "bbox": [{"x": 0.12790006399154663, "y": 0.2178301066160202}, {"x": 0.20523497462272644, "y": 0.21740958094596863}, {"x": 0.20523497462272644, "y": 0.22582001984119415}, {"x": 0.12790006399154663, "y": 0.22624054551124573}], "text": "Self-CRAG\n"}
{"page": 16, "bbox": [{"x": 0.39440807700157166, "y": 0.21825063228607178}, {"x": 0.4217727482318878, "y": 0.21825063228607178}, {"x": 0.4217727482318878, "y": 0.22539949417114258}, {"x": 0.39440807700157166, "y": 0.22539949417114258}], "text": "61.8\n"}
{"page": 16, "bbox": [{"x": 0.2801903486251831, "y": 0.21867115795612335}, {"x": 0.3093396723270416, "y": 0.21867115795612335}, {"x": 0.3093396723270416, "y": 0.22582001984119415}, {"x": 0.2801903486251831, "y": 0.22582001984119415}], "text": "49.0\n"}
{"page": 16, "bbox": [{"x": 0.2801903486251831, "y": 0.23296888172626495}, {"x": 0.3105294406414032, "y": 0.23338940739631653}, {"x": 0.3105294406414032, "y": 0.24137930572032928}, {"x": 0.2801903486251831, "y": 0.2409587949514389}], "text": "48.6\n"}
{"page": 16, "bbox": [{"x": 0.39381319284439087, "y": 0.23296888172626495}, {"x": 0.4229625165462494, "y": 0.23338940739631653}, {"x": 0.4223676323890686, "y": 0.24137930572032928}, {"x": 0.3932183086872101, "y": 0.2409587949514389}], "text": "57.2\n"}
{"page": 16, "bbox": [{"x": 0.13563355803489685, "y": 0.23338940739631653}, {"x": 0.2284354567527771, "y": 0.23254835605621338}, {"x": 0.2284354567527771, "y": 0.24222035706043243}, {"x": 0.13563355803489685, "y": 0.2430613934993744}], "text": "only Correct\n"}
{"page": 16, "bbox": [{"x": 0.39381319284439087, "y": 0.24852816760540009}, {"x": 0.4235574007034302, "y": 0.24852816760540009}, {"x": 0.4235574007034302, "y": 0.25609755516052246}, {"x": 0.39381319284439087, "y": 0.25609755516052246}], "text": "53.3\n"}
{"page": 16, "bbox": [{"x": 0.2801903486251831, "y": 0.24894869327545166}, {"x": 0.3099345564842224, "y": 0.24894869327545166}, {"x": 0.3099345564842224, "y": 0.25609755516052246}, {"x": 0.2801903486251831, "y": 0.25609755516052246}], "text": "40.8\n"}
{"page": 16, "bbox": [{"x": 0.13563355803489685, "y": 0.2481076568365097}, {"x": 0.2450922131538391, "y": 0.24684609472751617}, {"x": 0.2450922131538391, "y": 0.257359117269516}, {"x": 0.13563355803489685, "y": 0.2586206793785095}], "text": "only Incorrect\n"}
{"page": 16, "bbox": [{"x": 0.39381319284439087, "y": 0.26366695761680603}, {"x": 0.4223676323890686, "y": 0.26366695761680603}, {"x": 0.4223676323890686, "y": 0.2712363302707672}, {"x": 0.39381319284439087, "y": 0.2712363302707672}], "text": "59.8\n"}
{"page": 16, "bbox": [{"x": 0.13622844219207764, "y": 0.26366695761680603}, {"x": 0.3099345564842224, "y": 0.26366695761680603}, {"x": 0.3099345564842224, "y": 0.2729184329509735}, {"x": 0.13622844219207764, "y": 0.2729184329509735}], "text": "only Ambiguous 44.9\n"}
{"page": 16, "bbox": [{"x": 0.11719214916229248, "y": 0.30613961815834045}, {"x": 0.48839977383613586, "y": 0.30613961815834045}, {"x": 0.48839977383613586, "y": 0.7018502950668335}, {"x": 0.11719214916229248, "y": 0.7018502950668335}], "text": "is an advanced RAG approach that introduces a\ncritic model to decide whether to retrieve and which\nretrieved document to be referred for generation. It\nmeets our demand for deciding which action to be\ntriggered, thus we replaced the retrieved items in\nSelf-RAG with our processed internal knowledge\nfor Correct, external knowledge for Incorrect,\nand combined knowledge for Ambiguous.\nB.4 More Detailed Results\nAblation Study: The following results in Table 11\ndemonstrate the ablation study by triggering one\naction only for all instances.\nB.5 Results on PubHealth and Arc-Challenge\nIt is worth mentioning that the performance on\nPubHealth based on LLaMA2-hf-7b was much\nworse than others. We studied these cases and\nfound that LLaMA2-hf-7b is relatively weak in\ninstruction comprehension. Most of the cases\nfail to generate True or False in such a binary-\nquestion task, resulting in a low accuracy during\nthe evaluation. This situation somewhat happens in\nArc-Challenge as well, when the model is tasked\nto generate the index of a candidate.\n"}
{"page": 1, "bbox": [{"x": 0.10125142335891724, "y": 0.11032967269420624}, {"x": 0.8981797695159912, "y": 0.11340659111738205}, {"x": 0.8976109027862549, "y": 0.1934065967798233}, {"x": 0.10068259388208389, "y": 0.1903296709060669}], "text": "When Large Language Models Meet Vector Databases: A Survey\nZhi Jing*1, Yongye Su*², Yikun Han*³, Bo Yuan*4, Chunjiang Liu³, Haiyun Xu, Kehai Chen³\n¹Carnegie Mellon University\n2Purdue University\n"}
{"page": 1, "bbox": [{"x": 0.3737201392650604, "y": 0.19252747297286987}, {"x": 0.6245733499526978, "y": 0.19472527503967285}, {"x": 0.6240045428276062, "y": 0.2272527515888214}, {"x": 0.3731513023376465, "y": 0.22505494952201843}], "text": "3 University of Michigan\n4Harbin Institute of Technology\n"}
{"page": 1, "bbox": [{"x": 0.2286689430475235, "y": 0.2294505536556244}, {"x": 0.7696245908737183, "y": 0.2294505536556244}, {"x": 0.7696245908737183, "y": 0.2610988914966583}, {"x": 0.2286689430475235, "y": 0.2610988914966583}], "text": "5 National Science Library (Chengdu), Chinese Academy of Sciences\n6Shandong University of Technology\n"}
{"page": 1, "bbox": [{"x": 0.13879407942295074, "y": 0.26153847575187683}, {"x": 0.8600682616233826, "y": 0.26153847575187683}, {"x": 0.8600682616233826, "y": 0.2887912094593048}, {"x": 0.13879407942295074, "y": 0.2887912094593048}], "text": "zjing2@cs.cmu.edu, su311@purdue.edu, yikunhan@umich.edu, 23s051006@stu.hit.edu.cn,\nliucj@clas.ac.cn, chenkehai@hit.edu.cn, xuhy@sdut.edu.cn\n"}
{"page": 1, "bbox": [{"x": 0.24971558153629303, "y": 0.30989012122154236}, {"x": 0.3230944275856018, "y": 0.3103296756744385}, {"x": 0.3230944275856018, "y": 0.3199999928474426}, {"x": 0.24971558153629303, "y": 0.3195604383945465}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.11945392191410065, "y": 0.3353846073150635}, {"x": 0.45392492413520813, "y": 0.33494505286216736}, {"x": 0.4544937312602997, "y": 0.5815384387969971}, {"x": 0.12002275139093399, "y": 0.5819780230522156}], "text": "The recent burst in Large Language Models has\nopened new frontiers in human-like text processing\nand generation. However, alongside their remark-\nable growth, Large Language Models have encoun-\ntered critical challenges including issues of hallu-\ncination, bias, real-time knowledge updates, and\nthe high costs of implementation and maintenance\nin commercial settings. Vector Databases, another\nincreasingly popular tool, offer potential solutions\nto these challenges. These databases are adept at\nhandling high-dimensional data and are crucial for\ntasks such as efficient information retrieval and se-\nmantic search. By integrating with Large Language\nModels, they significantly enhance AI systems'\nability to manage and utilize diverse data more ef-\nfectively. This survey paper provides an in-depth\nand unique analysis of the intersection between\nLarge Language Models and Vector Databases.\n"}
{"page": 1, "bbox": [{"x": 0.027303753420710564, "y": 0.7054945230484009}, {"x": 0.029010238125920296, "y": 0.27076923847198486}, {"x": 0.05915813520550728, "y": 0.27076923847198486}, {"x": 0.057451650500297546, "y": 0.7054945230484009}], "text": "arXiv:2402.01763v1 [cs.DB] 30 Jan 2024\n"}
{"page": 1, "bbox": [{"x": 0.5130830407142639, "y": 0.3112087845802307}, {"x": 0.9141069650650024, "y": 0.3112087845802307}, {"x": 0.9135380983352661, "y": 0.8887912034988403}, {"x": 0.5125142335891724, "y": 0.8887912034988403}], "text": "are still under the shadows of doubt in many aspects. One ma-\njor shortcoming is the problem of hallucination, where LLMs\ngenerate plausible but factually incorrect or faithfully non-\nsensical information [Huang et al., 2023]. Causes behind\nthis problem include 1. Lack of domain knowledge. The fact\nthat LLMs are primarily trained on public datasets [Penedo et\nal., 2023] inevitably leads to a limited ability to answer do-\nmain-specific questions that are out of the scope of their inter-\nnal knowledge. 2. Real-time knowledge updates. Even if the\nquestions are within the learning corpus of LLMs, their an-\nswers may still exhibit limitations because the internal knowl-\nedge may be outdated when the outside world is dynamic and\nkeeps changing [Onoe et al., 2022]. 3. Biases. The datasets\nused to train LLMs are large which may introduce systematic\nerrors [Bender et al., 2021]. And essentially every dataset can\nbe questioned with biases issues, including imitative false-\nhoods [Bender et al., 2021], duplicating biases [Lee et al.,\n2022], and social biases [Venkit et al., 2023]. Moreover, a\ndisadvantage of incorporating LLMs commercially is the ex-\npensive cost of maintenance. For an average business entity,\napplying LLMs for business use is barely feasible. It is almost\nimpossible for a non-tech company to customize and train a\nGPT model of its own because they don't have the resources\nand talents to conduct such big of a project [Musser, 2023],\nwhile frequent API calls to third-party LLM providers like\nOpenAI can be extremely expensive, not to mention there is\na very limited number of such providers in certain areas. Ad-\nditionally, the oblivion problem of LLMs has been of contro-\nversial because LLMs are proven to tend to forget.\nAnother increasingly popular sub-field of AI is AI\ndatabases that support vector data storage and its efficient re-\ntrieval at scale, also called vector databases (VecDBs). What-\never kind of multi-modal data that LLMs deal with, searching\nover many vectors is time-consuming, it is recommended by\nOpenAI and other LLM vendors to use VecDBs, which al-\nways provide LLMs and their applications with cost-effective\nretrievals and scalable data management.\nThe intersection of Large Language Models and Vector\nDatabases has been and will still be a burgeoning area of\nstudy and application, offering exciting possibilities. This\nsynergy allows for the creation of more powerful, efficient,\n"}
{"page": 1, "bbox": [{"x": 0.11717861145734787, "y": 0.6070329546928406}, {"x": 0.22298066318035126, "y": 0.6070329546928406}, {"x": 0.22298066318035126, "y": 0.6171428561210632}, {"x": 0.11717861145734787, "y": 0.6171428561210632}], "text": "Introduction\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.6074725389480591}, {"x": 0.09783845394849777, "y": 0.6074725389480591}, {"x": 0.09783845394849777, "y": 0.6171428561210632}, {"x": 0.08816837519407272, "y": 0.6171428561210632}], "text": "1\n"}
{"page": 1, "bbox": [{"x": 0.08759954571723938, "y": 0.6281318664550781}, {"x": 0.48634812235832214, "y": 0.6281318664550781}, {"x": 0.48634812235832214, "y": 0.8892307877540588}, {"x": 0.08759954571723938, "y": 0.8892307877540588}], "text": "The concept of Artificial Intelligence (AI) was envisioned\ndecades ago, but it is not until ChatGPT unlocked the power\nof interactive smart AI assistants that people are offered with\nthe unprecedentedly tangible experience of talking to a ma-\nchine program that \"seems\" more knowledgeable than an or-\ndinary person. The phenomenal heat of ChatGPT inherits\nfrom the success of Large Language Models (LLMs), one\nmajor element of AI. LLMs, exemplified by systems like GPT\n[Brown and et al., 2020; Achiam and et al., 2023], BERT [De-\nvlin et al., 2018], and Llama [Touvron and et al., 2023], can\nprocess, understand, and generate human-like text. Thanks to\nthe power of pre-training which involves training a language\nmodel on a massive corpus of text data, LLMs can capture the\ncomplexities of human languages, including context, idioms,\nand even cultural references. Their applications range from\nsimple text completion to complex tasks like translation, un-\nderstanding, and generation, making them invaluable assets\nin both commercial and research domains.\nHowever, despite the impressive capabilities of LLMs, they\n"}
{"page": 2, "bbox": [{"x": 0.11149033159017563, "y": 0.08659340441226959}, {"x": 0.1706484705209732, "y": 0.08703296631574631}, {"x": 0.1706484705209732, "y": 0.0953846126794815}, {"x": 0.11149033159017563, "y": 0.09494505822658539}], "text": "Structure\n"}
{"page": 2, "bbox": [{"x": 0.3862343430519104, "y": 0.08703296631574631}, {"x": 0.5017064809799194, "y": 0.08703296631574631}, {"x": 0.5017064809799194, "y": 0.0953846126794815}, {"x": 0.3862343430519104, "y": 0.0953846126794815}], "text": "Encoder-Decoder\n"}
{"page": 2, "bbox": [{"x": 0.20136518776416779, "y": 0.08571428805589676}, {"x": 0.2929465174674988, "y": 0.08747252821922302}, {"x": 0.2923777103424072, "y": 0.09670329838991165}, {"x": 0.20136518776416779, "y": 0.09494505822658539}], "text": "Encoder Only\n"}
{"page": 2, "bbox": [{"x": 0.5699658989906311, "y": 0.08527472615242004}, {"x": 0.6621160507202148, "y": 0.08703296631574631}, {"x": 0.6615471839904785, "y": 0.09758241474628448}, {"x": 0.5693970322608948, "y": 0.09582417458295822}], "text": "Decoder Only\n"}
{"page": 2, "bbox": [{"x": 0.12059158086776733, "y": 0.12835164368152618}, {"x": 0.16211603581905365, "y": 0.1287912130355835}, {"x": 0.16211603581905365, "y": 0.1375824213027954}, {"x": 0.12059158086776733, "y": 0.1371428519487381}], "text": "LLMs\n"}
{"page": 2, "bbox": [{"x": 0.20136518776416779, "y": 0.12791208922863007}, {"x": 0.36746302247047424, "y": 0.12791208922863007}, {"x": 0.36746302247047424, "y": 0.24835164844989777}, {"x": 0.20136518776416779, "y": 0.24835164844989777}], "text": "BERT [Devlin et al.,\n2018], ROBERTa [Liu et\nal., 2019], XLM [Lam-\nple and Conneau, 2019],\nALBERT [Lan and\net al., 2019], ELEC-\nTRA [Clark and et al.,\n2020], DeBERTa [He\nand et al., 2020]\n"}
{"page": 2, "bbox": [{"x": 0.38566553592681885, "y": 0.12791208922863007}, {"x": 0.5517633557319641, "y": 0.12835164368152618}, {"x": 0.5511945486068726, "y": 0.24879120290279388}, {"x": 0.3850966989994049, "y": 0.24835164844989777}], "text": "T5 [Raffel et al., 2020],\nBART [Lewis and et\nal., 2019], mT5 [Xue\nand et al., 2020], M2M-\n100 [Fan and et al.,\n2021], BigBird [Zaheer\nand et al., 2020], Chat-\nGLM [Zeng and et al.,\n2022]\n"}
{"page": 2, "bbox": [{"x": 0.5693970322608948, "y": 0.12747253477573395}, {"x": 0.9010238647460938, "y": 0.12791208922863007}, {"x": 0.9010238647460938, "y": 0.2764835059642792}, {"x": 0.5693970322608948, "y": 0.27604395151138306}], "text": "GPT-2 [Radford and et al., 2019], GPT-\n3 [Brown and et al., 2020], OPT [Zhang\nand et al., 2022], PaLM [Chowdhery and et\nal., 2023], BLOOM [Workshop and et al.,\n2022], MT-NLG [Smith and et al., 2022],\nGLAM [Du and et al., 2022], Gopher [Rae\nand et al., 2021], chinchilla [Hoffmann and\net al., 2022], LaMDA [Thoppilan and et al.,\n2022], LLAMA [Touvron and et al., 2023], GPT-\n4 [Achiam and et al., 2023], BloombergGPT [Wu\nand et al., 2023]\n"}
{"page": 2, "bbox": [{"x": 0.4175198972225189, "y": 0.2971428632736206}, {"x": 0.5824800729751587, "y": 0.2971428632736206}, {"x": 0.5824800729751587, "y": 0.30637362599372864}, {"x": 0.4175198972225189, "y": 0.30637362599372864}], "text": "Table 1: Summary of LLMs\n"}
{"page": 2, "bbox": [{"x": 0.08646188676357269, "y": 0.3336263597011566}, {"x": 0.48748576641082764, "y": 0.3336263597011566}, {"x": 0.4880546033382416, "y": 0.7692307829856873}, {"x": 0.08703071624040604, "y": 0.7692307829856873}], "text": "and versatile AI systems, capable of handling a broad spec-\ntrum of tasks with enhanced accuracy and speed. It also paves\nthe way for innovative applications in fields like healthcare,\nfinance, education, and entertainment, where AI can deliver\ntailored solutions and insights.\nIn the light of lacking papers that introduce LLMs in the\nview of Vector Databases, this survey aims to picture how\nVector Databases can be potential solutions to refine Large\nLanguage Models' known shortcomings in previous works\nand hope to offer a unique perspective of future directions\nin the intersection that is fertile of research potentials. This\npaper develops as below:\n• Section 2 is presented as introduction to the background\nknowledge of the two major characters of this paper,\nLLMs and Vector Databases. Section 2.1 offers a brief\nview of the popular LLMs and accents on their underly-\ning shortcomings, while Section 3.1 depicts the picture\nof Vector Databases and their unique capabilities in in-\ntegration with LLMs.\n• In Section 3, we provide a comprehensive summary of\nhow previous research has effectively combined LLMs\nwith Vector Databases. Section 3.2 delves into a de-\ntailed blueprint of the Retrieval-Augmented Generation\nparadigm in which a unique role that Vector Databases\nplay. Section 3.3 and 3.4 showcases other memory-wise\nbenefits that vector databases can bring for LLMs.\n• Section 4 is dedicated to our analysis of the limitations\nand unexplored areas at the intersection of LLMs and\nVector Databases and highlights the potential future re-\nsearch opportunities in this domain.\n"}
{"page": 2, "bbox": [{"x": 0.5125142335891724, "y": 0.3336263597011566}, {"x": 0.9141069650650024, "y": 0.3331868052482605}, {"x": 0.914675772190094, "y": 0.8892307877540588}, {"x": 0.5130830407142639, "y": 0.8896703124046326}], "text": "the approaches to understanding and generating human lan-\nguages by machines. There are two key factors in the evo-\nlution of Large Language Models, the development of Neu-\nral Network architectures and the superpower of pre-training\nmodels on extensive data.\nDevelopment of Language Models\nWith the advent of neural networks, the field of NLP has\nundergone a transformative shift, which began with the in-\ntroduction of Recurrent Neural Networks (RNNs) [Zaremba\net al., 2014]. RNNs provide a way to process sequences of\nwords and capture temporal dependencies in text. And 2 of\nits famous variants, Gated Recurrent Units (GRUs) [Chung et\nal., 2014] and Long Short-Term Memory networks (LSTMs)\n[Shi et al., 2015] address the limitations of RNNs in handling\nproblems of vanishing and exploding gradients.\nThe next pivotal milestone for NLP is the widespread adop-\ntion of the Transformer architecture [Vaswani et al., 2017]\nwhich has set a new standard for language models. The multi-\nhead self-attention modules and cross-attention modules in\nthe encoders and decoders enable the model to capture long-\nrange dependencies, parallel processing, and contextual un-\nderstanding. Furthermore, the model starts to rapidly grow\nin size in the Transformer era. More importantly, the Trans-\nformer represents a paradigm shift in NLP field, and the suc-\ncess of the Transformer architecture becomes the significant\nbackbone of LLMs, upon which the encoder-only, encoder-\ndecoder, and decoder-only models are built.\nEncoder-Only Models These models are designed to an-\nalyze and understand input text. They process the input to\ncreate representations, in the form of embedding vectors, that\ncapture the nuances and context of the language. Exam-\nples of the models in this category include BERT [Devlin et\nal., 2018] and its derivatives, such as ROBERTA [Liu et al.,\n2019]. They excel in tasks that require a deep understanding\nof language context, such as sentiment analysis, named entity\nrecognition, and question answering where the answers are\ncontained within the given text.\nEncoder-Decoder Models This architecture consists of\ntwo parts: an encoder that processes the input text and a de-\n"}
{"page": 2, "bbox": [{"x": 0.08646188676357269, "y": 0.7898901104927063}, {"x": 0.4857792854309082, "y": 0.7903296947479248}, {"x": 0.4857792854309082, "y": 0.8892307877540588}, {"x": 0.08646188676357269, "y": 0.8887912034988403}], "text": "2 Background\n2.1 Large Language Models\nOver the last half-decade, we have witnessed the ground-\nbreaking success of Large Language Models (LLMs), mark-\ning a significant milestone in the field of Natural Language\nProcessing (NLP). LLMs have revolutionized our views on\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.07208791375160217}, {"x": 0.4857792854309082, "y": 0.07164835184812546}, {"x": 0.48634812235832214, "y": 0.3393406569957733}, {"x": 0.08759954571723938, "y": 0.33978021144866943}], "text": "coder that generates the output. These models are particularly\neffective in tasks that involve transforming an input into a\ndifferent output, such as machine translation or text summa-\nrizing. The Transformer model itself, as we have discussed\nbefore, is an example of this category, which has brought as-\ntonishing breakthroughs in machine translation. Another sig-\nnificant model is T5 [Raffel et al., 2020], which frames all\nlanguage tasks as a text-to-text problem, showcasing the ver-\nsatility of the encoder-decoder framework.\nDecoder-Only Models Decoder-only architectures are op-\ntimized for generating text. These models take an input and\nthen continue to generate language based on that input. GPT-\n3 [Brown and et al., 2020] and its successor, GPT-4 [Achiam\nand et al., 2023], are quintessential examples of this cate-\ngory. They are particularly adept at tasks that require creative\nand coherent text generation, such as content creation, story-\ntelling, and even code generation.\nWe show the LLMs that have received notable attention\nduring the development of language models in Table 1.\n"}
{"page": 3, "bbox": [{"x": 0.5136518478393555, "y": 0.07164835184812546}, {"x": 0.9135380983352661, "y": 0.07164835184812546}, {"x": 0.9135380983352661, "y": 0.40131866931915283}, {"x": 0.5136518478393555, "y": 0.40131866931915283}], "text": "privacy problems [Yao et al., 2024] and the potential for mis-\nuse of generative content [Ganguli et al., 2022]. Training lan-\nguage models to follow instructions with human feedback,\nlike toxic content, also receives quite a bit of debate. Re-\nsearchers are arguing that the power of LLMs is very super-\nficial, and it is the large pre-training data [Schaeffer, 2023]\nthat helps the models gain excellent performance on most of\nthe benchmarks. Hallucinations [Ji and et al., 2023] are also\nfound to be a major problem with LLMs, casting doubt on the\nreliability of their outputs. Additionally, the computational\nresources required for training and running these models are\nsubstantial, posing environmental and accessibility questions\n[Strubell and et al., 2019]. As the sizes of both LLMs and the\ntraining datasets keep getting bigger, the cost of training and\ninferring is heavily influenced. It is estimated that training the\n11-billion parameter version of T5 [Raffel et al., 2020] costs\nover 1.3 million dollars for a single run, whereas one round\nof training GPT-3 [Brown and et al., 2020] of 175 billion pa-\nrameters using a Tesla V100 cloud instance requires costs 4.6\nmillion dollars. On top of that, training a BERT-based lan-\nguage model [Devlin et al., 2018] using 8 V100 GPUs for 36\nhours and used a total of 37.3 kWh which is estimated to be\nmore energy-consuming than a gallon of gasoline in terms of\nCO2 emission [Dodge and et al., 2022].\n"}
{"page": 3, "bbox": [{"x": 0.08816837519407272, "y": 0.3582417666912079}, {"x": 0.2542662024497986, "y": 0.3582417666912079}, {"x": 0.2542662024497986, "y": 0.36835163831710815}, {"x": 0.08816837519407272, "y": 0.36835163831710815}], "text": "3 LLM+VectorDB\n"}
{"page": 3, "bbox": [{"x": 0.08759954571723938, "y": 0.37890109419822693}, {"x": 0.23947668075561523, "y": 0.38021978735923767}, {"x": 0.23947668075561523, "y": 0.39076924324035645}, {"x": 0.08759954571723938, "y": 0.3894505500793457}], "text": "Power of Pre-training\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.3947252631187439}, {"x": 0.4869169592857361, "y": 0.3947252631187439}, {"x": 0.48634812235832214, "y": 0.8887912034988403}, {"x": 0.08646188676357269, "y": 0.8887912034988403}], "text": "The scaling laws proposed by OpenAI [Kaplan and et al.,\n2020] highlight a critical trend: the scaling of Pretrained Lan-\nguage Models (PLMs), in terms of both model size and data\nvolume, leads to significant improvement on downstream\ntasks. This is evidenced by the development of increas-\ningly larger PLMs, such as the 1.76-trillion-parameter GPT-\n4 [Achiam and et al., 2023] and the 540-billion-parameter\nPaLM [Chowdhery and et al., 2023]. Unlike their smaller\npredecessors, like the 330-million-parameter BERT [Devlin\net al., 2018] and 1.5-billion-parameter GPT-2 [Radford and\net al., 2019], these large-scale models demonstrate unique be-\nhaviors and emergent abilities [Wei and et al., 2022] in var-\nious tasks like zero-shot learning, which is extremely chal-\nlenging to the small-scale models. Researchers started to\ncall the large-scale models in the NLP field “Large Language\nModels\" or \"LLMs\" to denote these significantly scaled\nPLMs, in order to distinguish the outstanding performance\nand gain more traction in academia.\nThe superpower of pre-training lies in its ability to pro-\nvide models with a general understanding of the languages,\nwhich is then tailored through additional training, known as\nfine-tuning, for specific tasks unlike traditional deep learning\nmethods [Dong et al., 2021; Liu and et al., 2024]. The advan-\ntage is that the model doesn't start from scratch when learn-\ning a new task; it builds upon a rich, pre-existing foundation\nof language understanding. And this enables what is known\nas transfer learning [Yosinski and et al., 2014]. Knowledge\ngained during this initial phase can be transferred to a wide\nrange of tasks, even those that the model was not explicitly\ntrained on.\nAre we there yet? (Challenge faced by pure LLMs)\nHowever, the success of LLMs does not come without chal-\nlenges. Issues such as data biases [Raffel et al., 2020], which\ncan lead to skewed or unfair outcomes [Li and et al., 2023],\nare of significant concern. Ethical considerations, including\n"}
{"page": 3, "bbox": [{"x": 0.5136518478393555, "y": 0.4153846204280853}, {"x": 0.912400484085083, "y": 0.4153846204280853}, {"x": 0.912400484085083, "y": 0.8887912034988403}, {"x": 0.5136518478393555, "y": 0.8887912034988403}], "text": "3.1 Vector Databases, the V-factor\nWhile LLMs like ChatGPT are relatively new concepts,\ndatabase management systems (DBMS) have been thor-\noughly developed and applied in many aspects in the last 60\nyears of history, well-recognized for their consistent stabil-\nity and universality for structured data with fixed formats that\nexcel well with computer storage. However, the development\nand wide application of deep learning models such as con-\nvolutional neural networks [He and et al., 2016] and trans-\nformers [Devlin et al., 2018; Su and et al., 2022], enables\nthe embedding of unstructured multi-modal data like images\nand text mapped into corresponding fixed-length vector rep-\nresentations, which include the high-dimensional semantic\nfeatures of original data and the semantic similarities are na-\ntively represented by distances between vectors, requiring a\nnew type of DBMS that is specifically designed for handling\nvector data operations, especially vector search and storage.\nThere are many kinds of databases, as shown in Table 2,\nwhereas vector databases are the only category of databases\nthat natively support diverse unstructured data with efficient\nstorage, indexing, and retrieval. All these operations could\nbe done based on the vector indexes, which are optimally\ndesigned collections of vectors deployed together for ANN\nsearch. As a result of various blooming applications on the\ncloud, various data sources are in many different formats and\ndiverse places. Unlike traditional databases that require struc-\ntured or semi-structured data that must convey a few restric-\ntions and formats, vector databases are purpose-designed for\nstoring the deep learning embedding of various unstructured\ndata that emerged in real-world applications. On the other\nhand, distinct from traditional DBMS that searches for exact\nvalues within databases, vector databases heavily rely on ap-\nproximate nearest neighbor (ANN) search for vectors, which\nsearches approximate top-k nearest distance neighbors within\n"}
{"page": 4, "bbox": [{"x": 0.49317407608032227, "y": 0.07252747565507889}, {"x": 0.5705347061157227, "y": 0.07208791375160217}, {"x": 0.5705347061157227, "y": 0.0782417580485344}, {"x": 0.49317407608032227, "y": 0.07868131995201111}], "text": "Vector database\n"}
{"page": 4, "bbox": [{"x": 0.11092150211334229, "y": 0.072967030107975}, {"x": 0.19738338887691498, "y": 0.072967030107975}, {"x": 0.19738338887691498, "y": 0.0782417580485344}, {"x": 0.11092150211334229, "y": 0.0782417580485344}], "text": "Unstructured data\n"}
{"page": 4, "bbox": [{"x": 0.7485779523849487, "y": 0.07208791375160217}, {"x": 0.861774742603302, "y": 0.07120878994464874}, {"x": 0.861774742603302, "y": 0.07912088185548782}, {"x": 0.7485779523849487, "y": 0.07999999821186066}], "text": "Large Language Models\n"}
{"page": 4, "bbox": [{"x": 0.2878270745277405, "y": 0.07252747565507889}, {"x": 0.37428897619247437, "y": 0.07164835184812546}, {"x": 0.37428897619247437, "y": 0.07912088185548782}, {"x": 0.2878270745277405, "y": 0.07999999821186066}], "text": "Embedding model\n"}
{"page": 4, "bbox": [{"x": 0.6029579043388367, "y": 0.09582417458295822}, {"x": 0.7320818901062012, "y": 0.09582417458295822}, {"x": 0.7320818901062012, "y": 0.11252747476100922}, {"x": 0.6029579043388367, "y": 0.11252747476100922}], "text": "Please answer the question\nusing the given context.\n"}
{"page": 4, "bbox": [{"x": 0.7684869170188904, "y": 0.09406593441963196}, {"x": 0.8452787399291992, "y": 0.09362637251615524}, {"x": 0.8458475470542908, "y": 0.12703296542167664}, {"x": 0.7690557241439819, "y": 0.12747253477573395}], "text": "ChatGPT\n∞ Llama\n"}
{"page": 4, "bbox": [{"x": 0.30887371301651, "y": 0.1063736230134964}, {"x": 0.35324230790138245, "y": 0.10593406856060028}, {"x": 0.35324230790138245, "y": 0.11516483873128891}, {"x": 0.30887371301651, "y": 0.11560439318418503}], "text": "OpenAI\n"}
{"page": 4, "bbox": [{"x": 0.14562001824378967, "y": 0.11956044286489487}, {"x": 0.16723549365997314, "y": 0.11956044286489487}, {"x": 0.16723549365997314, "y": 0.12483516335487366}, {"x": 0.14562001824378967, "y": 0.12483516335487366}], "text": "Text\n"}
{"page": 4, "bbox": [{"x": 0.40273037552833557, "y": 0.12703296542167664}, {"x": 0.4544937312602997, "y": 0.12791208922863007}, {"x": 0.4544937312602997, "y": 0.134505495429039}, {"x": 0.40273037552833557, "y": 0.13362637162208557}], "text": "Vectorized\n"}
{"page": 4, "bbox": [{"x": 0.21899886429309845, "y": 0.12747253477573395}, {"x": 0.26393628120422363, "y": 0.12835164368152618}, {"x": 0.26393628120422363, "y": 0.134505495429039}, {"x": 0.21899886429309845, "y": 0.13362637162208557}], "text": "Chunking\n"}
{"page": 4, "bbox": [{"x": 0.6029579043388367, "y": 0.12351648509502411}, {"x": 0.6541524529457092, "y": 0.1230769231915474}, {"x": 0.6541524529457092, "y": 0.13846154510974884}, {"x": 0.6029579043388367, "y": 0.13890109956264496}], "text": "Context:\n\"{Context}\"\n"}
{"page": 4, "bbox": [{"x": 0.30546075105667114, "y": 0.12219779938459396}, {"x": 0.37428897619247437, "y": 0.12175824493169785}, {"x": 0.37428897619247437, "y": 0.1454945057630539}, {"x": 0.30546075105667114, "y": 0.14593406021595}], "text": "cohere\nHugging Face\n"}
{"page": 4, "bbox": [{"x": 0.11717861145734787, "y": 0.1402197778224945}, {"x": 0.12229806929826736, "y": 0.1402197778224945}, {"x": 0.12229806929826736, "y": 0.14461538195610046}, {"x": 0.11717861145734787, "y": 0.14461538195610046}], "text": "D\n"}
{"page": 4, "bbox": [{"x": 0.10352673381567001, "y": 0.15032966434955597}, {"x": 0.2081911265850067, "y": 0.1516483575105667}, {"x": 0.2081911265850067, "y": 0.15824176371097565}, {"x": 0.10352673381567001, "y": 0.1569230705499649}], "text": "Videos Audio Images\n"}
{"page": 4, "bbox": [{"x": 0.5056883096694946, "y": 0.15296703577041626}, {"x": 0.564846396446228, "y": 0.15340659022331238}, {"x": 0.564846396446228, "y": 0.16043956577777863}, {"x": 0.5056883096694946, "y": 0.1599999964237213}], "text": "Embeddings\n"}
{"page": 4, "bbox": [{"x": 0.6029579043388367, "y": 0.15076923370361328}, {"x": 0.6604095697402954, "y": 0.15076923370361328}, {"x": 0.6604095697402954, "y": 0.1652747243642807}, {"x": 0.6029579043388367, "y": 0.1652747243642807}], "text": "Question:\n\"{Question}\"\n"}
{"page": 4, "bbox": [{"x": 0.21501706540584564, "y": 0.1709890067577362}, {"x": 0.26166099309921265, "y": 0.1705494523048401}, {"x": 0.26166099309921265, "y": 0.17714285850524902}, {"x": 0.21501706540584564, "y": 0.17758241295814514}], "text": "Question\n"}
{"page": 4, "bbox": [{"x": 0.7775881886482239, "y": 0.18065933883190155}, {"x": 0.8367462754249573, "y": 0.18065933883190155}, {"x": 0.8367462754249573, "y": 0.18593406677246094}, {"x": 0.7775881886482239, "y": 0.18593406677246094}], "text": "Get Answers\n"}
{"page": 4, "bbox": [{"x": 0.21501706540584564, "y": 0.2017582356929779}, {"x": 0.26279863715171814, "y": 0.2017582356929779}, {"x": 0.26279863715171814, "y": 0.20791208744049072}, {"x": 0.21501706540584564, "y": 0.20791208744049072}], "text": "Question\n"}
{"page": 4, "bbox": [{"x": 0.4004550576210022, "y": 0.20219780504703522}, {"x": 0.46587032079696655, "y": 0.2013186812400818}, {"x": 0.46587032079696655, "y": 0.2074725329875946}, {"x": 0.4004550576210022, "y": 0.20835164189338684}], "text": "Vector Search\n"}
{"page": 4, "bbox": [{"x": 0.5972696542739868, "y": 0.20219780504703522}, {"x": 0.6348122954368591, "y": 0.20263735949993134}, {"x": 0.6348122954368591, "y": 0.20791208744049072}, {"x": 0.5972696542739868, "y": 0.2074725329875946}], "text": "Context\n"}
{"page": 4, "bbox": [{"x": 0.5028441548347473, "y": 0.20307692885398865}, {"x": 0.5642775893211365, "y": 0.20351648330688477}, {"x": 0.5642775893211365, "y": 0.21802197396755219}, {"x": 0.5028441548347473, "y": 0.21758241951465607}], "text": "Get Relevant\nDocuments\n"}
{"page": 4, "bbox": [{"x": 0.7781569957733154, "y": 0.20791208744049072}, {"x": 0.8373151421546936, "y": 0.20791208744049072}, {"x": 0.8373151421546936, "y": 0.2131868153810501}, {"x": 0.7781569957733154, "y": 0.2131868153810501}], "text": "Get Answers\n"}
{"page": 4, "bbox": [{"x": 0.3037542700767517, "y": 0.20263735949993134}, {"x": 0.36234357953071594, "y": 0.20351648330688477}, {"x": 0.361774742603302, "y": 0.21934065222740173}, {"x": 0.30318543314933777, "y": 0.2184615433216095}], "text": "Create\nEmbeddings\n"}
{"page": 4, "bbox": [{"x": 0.6564277410507202, "y": 0.20351648330688477}, {"x": 0.7025028467178345, "y": 0.20351648330688477}, {"x": 0.7025028467178345, "y": 0.21934065222740173}, {"x": 0.6564277410507202, "y": 0.21934065222740173}], "text": "Prompt\nTemplate\n"}
{"page": 4, "bbox": [{"x": 0.6399317383766174, "y": 0.23956044018268585}, {"x": 0.7053470015525818, "y": 0.23912088572978973}, {"x": 0.7053470015525818, "y": 0.24527472257614136}, {"x": 0.6399317383766174, "y": 0.24571429193019867}], "text": "Answer (RAG):\n"}
{"page": 4, "bbox": [{"x": 0.28100115060806274, "y": 0.24175824224948883}, {"x": 0.3202502727508545, "y": 0.24219779670238495}, {"x": 0.3202502727508545, "y": 0.24747252464294434}, {"x": 0.28100115060806274, "y": 0.24703297019004822}], "text": "Context:\n"}
{"page": 4, "bbox": [{"x": 0.10295790433883667, "y": 0.24219779670238495}, {"x": 0.1473265141248703, "y": 0.24219779670238495}, {"x": 0.1473265141248703, "y": 0.24791209399700165}, {"x": 0.10295790433883667, "y": 0.24791209399700165}], "text": "Question:\n"}
{"page": 4, "bbox": [{"x": 0.640500545501709, "y": 0.24791209399700165}, {"x": 0.8799772262573242, "y": 0.24747252464294434}, {"x": 0.8799772262573242, "y": 0.26461538672447205}, {"x": 0.640500545501709, "y": 0.26505494117736816}], "text": "In Oppenheimer, Cillian Murphy starred as J. Robert\nOppenheimer.\n"}
{"page": 4, "bbox": [{"x": 0.10238907486200333, "y": 0.25054946541786194}, {"x": 0.2519908845424652, "y": 0.25098901987075806}, {"x": 0.2519908845424652, "y": 0.26725274324417114}, {"x": 0.10238907486200333, "y": 0.266813188791275}], "text": "Who starred in Oppenheimer as\nOppenheimer\n"}
{"page": 4, "bbox": [{"x": 0.28100115060806274, "y": 0.271208792924881}, {"x": 0.2844141125679016, "y": 0.271208792924881}, {"x": 0.2844141125679016, "y": 0.27296704053878784}, {"x": 0.28100115060806274, "y": 0.27296704053878784}], "text": "•\n"}
{"page": 4, "bbox": [{"x": 0.6399317383766174, "y": 0.27076923847198486}, {"x": 0.7224118113517761, "y": 0.27032968401908875}, {"x": 0.7224118113517761, "y": 0.2769230902194977}, {"x": 0.6399317383766174, "y": 0.2773626446723938}], "text": "Answer (No RAG):\n"}
{"page": 4, "bbox": [{"x": 0.2969283163547516, "y": 0.25098901987075806}, {"x": 0.5705347061157227, "y": 0.2518681287765503}, {"x": 0.5705347061157227, "y": 0.30417582392692566}, {"x": 0.2969283163547516, "y": 0.3032967150211334}], "text": "In Oppenheimer Cillian Murphy plays J. Robert\nOppenheimer ...\nIn Oppenheimer Robert Downey Jr. plays Lewis Strauss...\nIn Oppenheimer Alden Ehrenreich plays Senate AideAlexis\nMac ...\nIn Oppenheimer Kurt Koehler plays Thomas Morgan ...\n"}
{"page": 4, "bbox": [{"x": 0.6393629312515259, "y": 0.2800000011920929}, {"x": 0.8964732885360718, "y": 0.280439555644989}, {"x": 0.8964732885360718, "y": 0.31516483426094055}, {"x": 0.6393629312515259, "y": 0.31472527980804443}], "text": "As of my last knowledge update in January 2022, there\nhadn't been a widely released film or television\nproduction about J. Robert Oppenheimer with a well-\nknown actor portraying him.\n"}
{"page": 4, "bbox": [{"x": 0.286689430475235, "y": 0.3331868052482605}, {"x": 0.7127417325973511, "y": 0.3331868052482605}, {"x": 0.7127417325973511, "y": 0.34329670667648315}, {"x": 0.286689430475235, "y": 0.34329670667648315}], "text": "Figure 1: Sample: A sample RAG framework that uses vector databases.\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.37142857909202576}, {"x": 0.4869169592857361, "y": 0.37054944038391113}, {"x": 0.48748576641082764, "y": 0.5217582583427429}, {"x": 0.08759954571723938, "y": 0.5226373672485352}], "text": "the high-dimensional base vector data space that do not nec-\nessarily require exact matches.\nWith the unstructured base data embedded into vectors\nwith high dimensionality, calculating k-nearest neighbors of\na given query vector data can be expensive, since it requires\ndistance computation to every point in the dataset and main-\ntaining the top-k results. Such operation would direct to a\ntime complexity of O(dN + N log k), where d is the dimen-\nsionality and N is the number of vectors, for searching top-k\nresults using pair-wise distance calculation and a heap to keep\ntop-k results.\n"}
{"page": 4, "bbox": [{"x": 0.3538111448287964, "y": 0.5389010906219482}, {"x": 0.3589306175708771, "y": 0.5389010906219482}, {"x": 0.3589306175708771, "y": 0.5450549721717834}, {"x": 0.3538111448287964, "y": 0.5450549721717834}], "text": "1\n"}
{"page": 4, "bbox": [{"x": 0.5136518478393555, "y": 0.37098902463912964}, {"x": 0.9129692912101746, "y": 0.37142857909202576}, {"x": 0.912400484085083, "y": 0.8092307448387146}, {"x": 0.5130830407142639, "y": 0.8087912201881409}], "text": "ing systems and their multi-modal applications. An intuitive\napplication using vector search is searching images with an\nimage on search engines built on vector databases [Wang and\net al., 2021] like Google Image Search 2, which uses one in-\nput image to find similar images on the internet. While vector\ndatabases boast their unique search capability and efficiency\nfor unstructured data, they are just a backend factor for mani-\nfold applications. To maximize their abilities, this leads us to\nan interesting question, since LLMs use embeddings to repre-\nsent text as vectors, can developers combine vector databases\nand LLMs to overcome the aforementioned challenges that\ninherit in LLM applications? The answer is yes.\nAs a kind of database encapsulated with vector search\nin vector data that represents real-world information within\nhigh dimensionalities, vector databases are well-capable for\nretrieval applications [Asai and et al., 2023] incorporat-\ning LLMs because LLM applications are generally read-\nintensive, not requiring many write-related changes, espe-\ncially data deletes [Pan et al., 2023]. On the other hand, vec-\ntor databases can efficiently manage and warehouse vector\ndata required and generated by LLMs, thus providing a solid\ndata cornerstone for both LLMs and their applications. While\nLLMs are often limited by domain knowledge that cannot be\nuploaded or distributed due to security and privacy concerns,\ndomain knowledge is often unstructured data that can easily\nembedded into vector databases for efficient local retrieval\nand further integration with generative AIs. Moreover, the\ncomputing and storage resources of vector databases are way\ncheaper than LLMs, since it does require costly GPUs and\nTPUs, thus achieving a cost-effective way of fast retrieval and\ndurable (non-volatile) storage.\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.5270329713821411}, {"x": 0.4869169592857361, "y": 0.5270329713821411}, {"x": 0.4869169592857361, "y": 0.8584615588188171}, {"x": 0.08703071624040604, "y": 0.8584615588188171}], "text": "This calls for a more efficient search technique with sat-\nisfying accuracy, the ANN Benchmarks ¹ has showcased the\ngreat performance gap between brute-force ANN search and\nindex-enhanced ANN search. Since brute force search is both\ntime-consuming and computationally expensive, vector in-\ndexing can solve both of these problems, which optimized for\nANN search within VDBMS include tree-based [Muja and\nLowe, 2009; Tao et al., 2009], hash-based [Andoni and Indyk,\n2008], Product Quantization (PQ) [Jégou et al., 2011], and\ngraph-based methods [Malkov and Yashunin, 2018]. Among\nall these indexes, the graph-based Hierarchical Navigable\nSmall Worlds (HNSW) provides state-of-the-art performance\nand capability with great universality and is widely used\nwithin most vector database management systems, including\nthe examples we mentioned in Table 3.\nTraditional full-text search engines, such as Elastic Search\nand Amazon OpenSearch, are based on term frequency met-\nrics like BM25, which has proved practical for many search\napplications. However, such search techniques require signif-\nicant investment in time and expertise to tune them to account\nfor the meaning or relevance of the terms searched. On the\nother hand, vector databases successfully solved the afore-\nmentioned problem, thus the emergence of vector databases\n(as shown in Table 3) has greatly influenced machine learn-\n"}
{"page": 4, "bbox": [{"x": 0.5335608720779419, "y": 0.8641757965087891}, {"x": 0.8623435497283936, "y": 0.8632966876029968}, {"x": 0.8623435497283936, "y": 0.8756043910980225}, {"x": 0.5335608720779419, "y": 0.8764835000038147}], "text": "2https://cloud.google.com/vertex-ai/docs/vector-search/\n"}
{"page": 4, "bbox": [{"x": 0.10864619165658951, "y": 0.8769230842590332}, {"x": 0.286689430475235, "y": 0.876043975353241}, {"x": 0.286689430475235, "y": 0.8874725103378296}, {"x": 0.10864619165658951, "y": 0.8883516192436218}], "text": "'https://ann-benchmarks.com/\n"}
{"page": 4, "bbox": [{"x": 0.5142207145690918, "y": 0.8786813020706177}, {"x": 0.5676905512809753, "y": 0.8786813020706177}, {"x": 0.5676905512809753, "y": 0.8861538171768188}, {"x": 0.5142207145690918, "y": 0.8861538171768188}], "text": "overview\n"}
{"page": 5, "bbox": [{"x": 0.20705346763134003, "y": 0.072967030107975}, {"x": 0.26791808009147644, "y": 0.072967030107975}, {"x": 0.26791808009147644, "y": 0.08307692408561707}, {"x": 0.20705346763134003, "y": 0.08307692408561707}], "text": "Category\n"}
{"page": 5, "bbox": [{"x": 0.436860054731369, "y": 0.072967030107975}, {"x": 0.6535836458206177, "y": 0.072967030107975}, {"x": 0.6535836458206177, "y": 0.0839560404419899}, {"x": 0.436860054731369, "y": 0.0839560404419899}], "text": "Supported Data type Examples\n"}
{"page": 5, "bbox": [{"x": 0.436860054731369, "y": 0.08703296631574631}, {"x": 0.5028441548347473, "y": 0.08659340441226959}, {"x": 0.5028441548347473, "y": 0.0953846126794815}, {"x": 0.436860054731369, "y": 0.09582417458295822}], "text": "Structured\n"}
{"page": 5, "bbox": [{"x": 0.5887371897697449, "y": 0.08703296631574631}, {"x": 0.7821387648582458, "y": 0.08527472615242004}, {"x": 0.7827076315879822, "y": 0.12263736128807068}, {"x": 0.5893060564994812, "y": 0.12439560145139694}], "text": "MySQL, PostgreSQL, Oracle\nMongoDB, Couchbase\nInfluxDB, TimescaleDB\n"}
{"page": 5, "bbox": [{"x": 0.20762230455875397, "y": 0.08615384250879288}, {"x": 0.4175198972225189, "y": 0.08659340441226959}, {"x": 0.4175198972225189, "y": 0.12395604699850082}, {"x": 0.20762230455875397, "y": 0.12351648509502411}], "text": "Relational Databases (RDBMS)\nDocument Databases\nTime-Series Databases\n"}
{"page": 5, "bbox": [{"x": 0.43629124760627747, "y": 0.1006593406200409}, {"x": 0.5437997579574585, "y": 0.1006593406200409}, {"x": 0.5437997579574585, "y": 0.10945054888725281}, {"x": 0.43629124760627747, "y": 0.10945054888725281}], "text": "Semi-Structured\n"}
{"page": 5, "bbox": [{"x": 0.436860054731369, "y": 0.11516483873128891}, {"x": 0.5039817690849304, "y": 0.11516483873128891}, {"x": 0.5039817690849304, "y": 0.12351648509502411}, {"x": 0.436860054731369, "y": 0.12351648509502411}], "text": "Structured\n"}
{"page": 5, "bbox": [{"x": 0.436860054731369, "y": 0.12835164368152618}, {"x": 0.5028441548347473, "y": 0.12791208922863007}, {"x": 0.5028441548347473, "y": 0.13670329749584198}, {"x": 0.436860054731369, "y": 0.1371428519487381}], "text": "Structured\n"}
{"page": 5, "bbox": [{"x": 0.2081911265850067, "y": 0.12835164368152618}, {"x": 0.31854379177093506, "y": 0.1261538416147232}, {"x": 0.319112628698349, "y": 0.1375824213027954}, {"x": 0.20875994861125946, "y": 0.1397802233695984}], "text": "Graph Databases\n"}
{"page": 5, "bbox": [{"x": 0.5893060564994812, "y": 0.1287912130355835}, {"x": 0.7531285285949707, "y": 0.1287912130355835}, {"x": 0.7531285285949707, "y": 0.13934065401554108}, {"x": 0.5893060564994812, "y": 0.13934065401554108}], "text": "Neo4j, Amazon Neptune\n"}
{"page": 5, "bbox": [{"x": 0.436860054731369, "y": 0.14197802543640137}, {"x": 0.5295790433883667, "y": 0.14241757988929749}, {"x": 0.5295790433883667, "y": 0.1512087881565094}, {"x": 0.436860054731369, "y": 0.15076923370361328}], "text": "Unstructured\n"}
{"page": 5, "bbox": [{"x": 0.2081911265850067, "y": 0.14153845608234406}, {"x": 0.3930602967739105, "y": 0.1428571492433548}, {"x": 0.3930602967739105, "y": 0.15296703577041626}, {"x": 0.2081911265850067, "y": 0.1516483575105667}], "text": "Vector Databases (VDBMS)\n"}
{"page": 5, "bbox": [{"x": 0.5893060564994812, "y": 0.14153845608234406}, {"x": 0.7923777103424072, "y": 0.14241757988929749}, {"x": 0.7923777103424072, "y": 0.15340659022331238}, {"x": 0.5893060564994812, "y": 0.15252746641635895}], "text": "Pinecone, Milvus, Jina, Qdrant\n"}
{"page": 5, "bbox": [{"x": 0.3111490309238434, "y": 0.17230768501758575}, {"x": 0.6882821321487427, "y": 0.17230768501758575}, {"x": 0.6882821321487427, "y": 0.18065933883190155}, {"x": 0.3111490309238434, "y": 0.18065933883190155}], "text": "Table 2: Different kinds of mainstream business-level databases.\n"}
{"page": 5, "bbox": [{"x": 0.27189987897872925, "y": 0.2074725329875946}, {"x": 0.30659839510917664, "y": 0.2074725329875946}, {"x": 0.30659839510917664, "y": 0.21450549364089966}, {"x": 0.27189987897872925, "y": 0.21450549364089966}], "text": "Name\n"}
{"page": 5, "bbox": [{"x": 0.4493742883205414, "y": 0.2070329636335373}, {"x": 0.5409556031227112, "y": 0.20527473092079163}, {"x": 0.5415244698524475, "y": 0.2153846174478531}, {"x": 0.4499431252479553, "y": 0.21714285016059875}], "text": "Supported Data\n"}
{"page": 5, "bbox": [{"x": 0.7372013926506042, "y": 0.2070329636335373}, {"x": 0.8111490607261658, "y": 0.2074725329875946}, {"x": 0.8111490607261658, "y": 0.2153846174478531}, {"x": 0.7372013926506042, "y": 0.21494504809379578}], "text": "Vector Index\n"}
{"page": 5, "bbox": [{"x": 0.6035267114639282, "y": 0.2070329636335373}, {"x": 0.7042093276977539, "y": 0.2070329636335373}, {"x": 0.7042093276977539, "y": 0.21714285016059875}, {"x": 0.6035267114639282, "y": 0.21714285016059875}], "text": "Supported Query\n"}
{"page": 5, "bbox": [{"x": 0.4209328889846802, "y": 0.23296703398227692}, {"x": 0.48009100556373596, "y": 0.23340658843517303}, {"x": 0.48009100556373596, "y": 0.2413186877965927}, {"x": 0.4209328889846802, "y": 0.2408791184425354}], "text": "Vec. Dim.\n"}
{"page": 5, "bbox": [{"x": 0.5904436707496643, "y": 0.23340658843517303}, {"x": 0.6222980618476868, "y": 0.23384615778923035}, {"x": 0.6222980618476868, "y": 0.2413186877965927}, {"x": 0.5904436707496643, "y": 0.2408791184425354}], "text": "Filter\n"}
{"page": 5, "bbox": [{"x": 0.6416382193565369, "y": 0.23384615778923035}, {"x": 0.7172923684120178, "y": 0.23384615778923035}, {"x": 0.7172923684120178, "y": 0.2413186877965927}, {"x": 0.6416382193565369, "y": 0.2413186877965927}], "text": "Multi-Vector\n"}
{"page": 5, "bbox": [{"x": 0.7952218651771545, "y": 0.23384615778923035}, {"x": 0.8191125988960266, "y": 0.23384615778923035}, {"x": 0.8191125988960266, "y": 0.2413186877965927}, {"x": 0.7952218651771545, "y": 0.2413186877965927}], "text": "IVF\n"}
{"page": 5, "bbox": [{"x": 0.23321956396102905, "y": 0.23164835572242737}, {"x": 0.34698522090911865, "y": 0.23296703398227692}, {"x": 0.34698522090911865, "y": 0.2439560443162918}, {"x": 0.23321956396102905, "y": 0.24263736605644226}], "text": "(Version with year)\n"}
{"page": 5, "bbox": [{"x": 0.7372013926506042, "y": 0.23340658843517303}, {"x": 0.7718998789787292, "y": 0.23296703398227692}, {"x": 0.7718998789787292, "y": 0.24307692050933838}, {"x": 0.7372013926506042, "y": 0.2435164898633957}], "text": "Graph\n"}
{"page": 5, "bbox": [{"x": 0.520477831363678, "y": 0.23384615778923035}, {"x": 0.5494880676269531, "y": 0.23384615778923035}, {"x": 0.5494880676269531, "y": 0.2439560443162918}, {"x": 0.520477831363678, "y": 0.2439560443162918}], "text": "Туре\n"}
{"page": 5, "bbox": [{"x": 0.17804323136806488, "y": 0.255384624004364}, {"x": 0.30034130811691284, "y": 0.25626373291015625}, {"x": 0.30034130811691284, "y": 0.2663736343383789}, {"x": 0.17804323136806488, "y": 0.2654944956302643}], "text": "ChormaDB (2022)\n"}
{"page": 5, "bbox": [{"x": 0.5005688071250916, "y": 0.25626373291015625}, {"x": 0.5278725624084473, "y": 0.2571428716182709}, {"x": 0.5273037552833557, "y": 0.2654944956302643}, {"x": 0.5, "y": 0.26461538672447205}], "text": "Vec.\n"}
{"page": 5, "bbox": [{"x": 0.42150169610977173, "y": 0.2571428716182709}, {"x": 0.45278725028038025, "y": 0.2571428716182709}, {"x": 0.45278725028038025, "y": 0.26505494117736816}, {"x": 0.42150169610977173, "y": 0.26505494117736816}], "text": "1536\n"}
{"page": 5, "bbox": [{"x": 0.42036405205726624, "y": 0.27032968401908875}, {"x": 0.45961320400238037, "y": 0.26989009976387024}, {"x": 0.45961320400238037, "y": 0.2778021991252899}, {"x": 0.42036405205726624, "y": 0.27824175357818604}], "text": "32768\n"}
{"page": 5, "bbox": [{"x": 0.5005688071250916, "y": 0.27032968401908875}, {"x": 0.5290102362632751, "y": 0.27076923847198486}, {"x": 0.5284414291381836, "y": 0.27912089228630066}, {"x": 0.5, "y": 0.27868130803108215}], "text": "Vec.\n"}
{"page": 5, "bbox": [{"x": 0.17747440934181213, "y": 0.269010990858078}, {"x": 0.2633674740791321, "y": 0.27032968401908875}, {"x": 0.26279863715171814, "y": 0.28131869435310364}, {"x": 0.17747440934181213, "y": 0.2800000011920929}], "text": "Manu (2022)\n"}
{"page": 5, "bbox": [{"x": 0.42036405205726624, "y": 0.28395605087280273}, {"x": 0.4601820111274719, "y": 0.28395605087280273}, {"x": 0.4601820111274719, "y": 0.2918681204319}, {"x": 0.42036405205726624, "y": 0.2918681204319}], "text": "32768\n"}
{"page": 5, "bbox": [{"x": 0.17861205339431763, "y": 0.2821978032588959}, {"x": 0.27189987897872925, "y": 0.2835164964199066}, {"x": 0.2713310718536377, "y": 0.2936263680458069}, {"x": 0.17861205339431763, "y": 0.29230770468711853}], "text": "Milvus (2021)\n"}
{"page": 5, "bbox": [{"x": 0.5005688071250916, "y": 0.2835164964199066}, {"x": 0.5278725624084473, "y": 0.28439560532569885}, {"x": 0.5273037552833557, "y": 0.29274725914001465}, {"x": 0.5, "y": 0.2918681204319}], "text": "Vec.\n"}
{"page": 5, "bbox": [{"x": 0.17804323136806488, "y": 0.29582417011260986}, {"x": 0.28498294949531555, "y": 0.2971428632736206}, {"x": 0.28498294949531555, "y": 0.3081318736076355}, {"x": 0.17804323136806488, "y": 0.30681318044662476}], "text": "Pinecone (2019)\n"}
{"page": 5, "bbox": [{"x": 0.42036405205726624, "y": 0.29802197217941284}, {"x": 0.4613196849822998, "y": 0.29802197217941284}, {"x": 0.4613196849822998, "y": 0.3059340715408325}, {"x": 0.42036405205726624, "y": 0.3059340715408325}], "text": "20000\n"}
{"page": 5, "bbox": [{"x": 0.5005688071250916, "y": 0.2975824177265167}, {"x": 0.5278725624084473, "y": 0.29890111088752747}, {"x": 0.5273037552833557, "y": 0.3072527348995209}, {"x": 0.5, "y": 0.3059340715408325}], "text": "Vec.\n"}
{"page": 5, "bbox": [{"x": 0.42036405205726624, "y": 0.31164833903312683}, {"x": 0.46075084805488586, "y": 0.31164833903312683}, {"x": 0.46075084805488586, "y": 0.3195604383945465}, {"x": 0.42036405205726624, "y": 0.3195604383945465}], "text": "65535\n"}
{"page": 5, "bbox": [{"x": 0.5005688071250916, "y": 0.3112087845802307}, {"x": 0.5278725624084473, "y": 0.31208792328834534}, {"x": 0.5273037552833557, "y": 0.3199999928474426}, {"x": 0.5, "y": 0.3191208839416504}], "text": "Vec.\n"}
{"page": 5, "bbox": [{"x": 0.17747440934181213, "y": 0.30989012122154236}, {"x": 0.2855517566204071, "y": 0.3112087845802307}, {"x": 0.2855517566204071, "y": 0.3221977949142456}, {"x": 0.17747440934181213, "y": 0.32087913155555725}], "text": "Weaviate (2019)\n"}
{"page": 5, "bbox": [{"x": 0.4197952151298523, "y": 0.32615384459495544}, {"x": 0.45278725028038025, "y": 0.32615384459495544}, {"x": 0.45278725028038025, "y": 0.3336263597011566}, {"x": 0.4197952151298523, "y": 0.3336263597011566}], "text": "4096\n"}
{"page": 5, "bbox": [{"x": 0.5005688071250916, "y": 0.3257142901420593}, {"x": 0.5284414291381836, "y": 0.32659339904785156}, {"x": 0.5278725624084473, "y": 0.33450549840927124}, {"x": 0.5, "y": 0.3336263597011566}], "text": "Vec.\n"}
{"page": 5, "bbox": [{"x": 0.17804323136806488, "y": 0.3257142901420593}, {"x": 0.27076223492622375, "y": 0.3257142901420593}, {"x": 0.27076223492622375, "y": 0.3358241617679596}, {"x": 0.17804323136806488, "y": 0.3358241617679596}], "text": "Qdrant (2021)\n"}
{"page": 5, "bbox": [{"x": 0.5, "y": 0.34549450874328613}, {"x": 0.5255972743034363, "y": 0.34637361764907837}, {"x": 0.5250284671783447, "y": 0.35428571701049805}, {"x": 0.49943116307258606, "y": 0.3534066081047058}], "text": "Ftx.\n"}
{"page": 5, "bbox": [{"x": 0.42150169610977173, "y": 0.34505495429039}, {"x": 0.46473264694213867, "y": 0.34549450874328613}, {"x": 0.46473264694213867, "y": 0.3551648259162903}, {"x": 0.42150169610977173, "y": 0.35472527146339417}], "text": "16,000\n"}
{"page": 5, "bbox": [{"x": 0.17747440934181213, "y": 0.34593406319618225}, {"x": 0.4004550576210022, "y": 0.34593406319618225}, {"x": 0.4004550576210022, "y": 0.356483519077301}, {"x": 0.17747440934181213, "y": 0.356483519077301}], "text": "Amazon OpenSearch (v2.9, 2023)\n"}
{"page": 5, "bbox": [{"x": 0.17804323136806488, "y": 0.35780221223831177}, {"x": 0.35494881868362427, "y": 0.3591208755970001}, {"x": 0.35494881868362427, "y": 0.3692307770252228}, {"x": 0.17804323136806488, "y": 0.36791208386421204}], "text": "Elastic Search (v8.0, 2022)\n"}
{"page": 5, "bbox": [{"x": 0.41922640800476074, "y": 0.36000001430511475}, {"x": 0.45278725028038025, "y": 0.36000001430511475}, {"x": 0.45278725028038025, "y": 0.3674725294113159}, {"x": 0.41922640800476074, "y": 0.3674725294113159}], "text": "4096\n"}
{"page": 5, "bbox": [{"x": 0.5, "y": 0.36000001430511475}, {"x": 0.5255972743034363, "y": 0.36000001430511475}, {"x": 0.5255972743034363, "y": 0.36791208386421204}, {"x": 0.5, "y": 0.36791208386421204}], "text": "Ftx.\n"}
{"page": 5, "bbox": [{"x": 0.4209328889846802, "y": 0.37978023290634155}, {"x": 0.45676904916763306, "y": 0.37890109419822693}, {"x": 0.457337886095047, "y": 0.38813185691833496}, {"x": 0.42150169610977173, "y": 0.3890109956264496}], "text": "≥512\n"}
{"page": 5, "bbox": [{"x": 0.49943116307258606, "y": 0.38021978735923767}, {"x": 0.5255972743034363, "y": 0.38021978735923767}, {"x": 0.5255972743034363, "y": 0.38813185691833496}, {"x": 0.49943116307258606, "y": 0.38813185691833496}], "text": "Rel.\n"}
{"page": 5, "bbox": [{"x": 0.17747440934181213, "y": 0.37934064865112305}, {"x": 0.32081910967826843, "y": 0.37890109419822693}, {"x": 0.32081910967826843, "y": 0.3898901045322418}, {"x": 0.17747440934181213, "y": 0.39032965898513794}], "text": "AnalyticDB-V (2020)\n"}
{"page": 5, "bbox": [{"x": 0.42036405205726624, "y": 0.39340659976005554}, {"x": 0.4522184431552887, "y": 0.3929670453071594}, {"x": 0.4522184431552887, "y": 0.40131866931915283}, {"x": 0.42036405205726624, "y": 0.40175825357437134}], "text": "2000\n"}
{"page": 5, "bbox": [{"x": 0.49943116307258606, "y": 0.39384615421295166}, {"x": 0.5693970322608948, "y": 0.39384615421295166}, {"x": 0.5693970322608948, "y": 0.40219780802726746}, {"x": 0.49943116307258606, "y": 0.40219780802726746}], "text": "Rel. + Ftx.\n"}
{"page": 5, "bbox": [{"x": 0.17804323136806488, "y": 0.39384615421295166}, {"x": 0.36746302247047424, "y": 0.39384615421295166}, {"x": 0.36746302247047424, "y": 0.40439561009407043}, {"x": 0.17804323136806488, "y": 0.40439561009407043}], "text": "PostgreSQL-pgvector (2021)\n"}
{"page": 5, "bbox": [{"x": 0.42036405205726624, "y": 0.40703296661376953}, {"x": 0.4522184431552887, "y": 0.40747252106666565}, {"x": 0.4522184431552887, "y": 0.41582417488098145}, {"x": 0.42036405205726624, "y": 0.4153846204280853}], "text": "2048\n"}
{"page": 5, "bbox": [{"x": 0.17747440934181213, "y": 0.40703296661376953}, {"x": 0.37030717730522156, "y": 0.4061538577079773}, {"x": 0.37030717730522156, "y": 0.4167032837867737}, {"x": 0.17747440934181213, "y": 0.4175824224948883}], "text": "MongoDB Atlas (v6.0, 2023)\n"}
{"page": 5, "bbox": [{"x": 0.49943116307258606, "y": 0.4065934121608734}, {"x": 0.5506256818771362, "y": 0.40703296661376953}, {"x": 0.5506256818771362, "y": 0.4171428680419922}, {"x": 0.49943116307258606, "y": 0.4167032837867737}], "text": "NoSQL\n"}
{"page": 5, "bbox": [{"x": 0.42150169610977173, "y": 0.42153847217559814}, {"x": 0.45278725028038025, "y": 0.42153847217559814}, {"x": 0.45278725028038025, "y": 0.42945054173469543}, {"x": 0.42150169610977173, "y": 0.42945054173469543}], "text": "1536\n"}
{"page": 5, "bbox": [{"x": 0.5, "y": 0.42197802662849426}, {"x": 0.5255972743034363, "y": 0.42197802662849426}, {"x": 0.5255972743034363, "y": 0.42989009618759155}, {"x": 0.5, "y": 0.42989009618759155}], "text": "Rel.\n"}
{"page": 5, "bbox": [{"x": 0.17747440934181213, "y": 0.42153847217559814}, {"x": 0.2827076315879822, "y": 0.4206593334674835}, {"x": 0.2827076315879822, "y": 0.4312087893486023}, {"x": 0.17747440934181213, "y": 0.43208789825439453}], "text": "MyScale (2023)\n"}
{"page": 5, "bbox": [{"x": 0.08759954571723938, "y": 0.4509890079498291}, {"x": 0.9118316173553467, "y": 0.4509890079498291}, {"x": 0.9118316173553467, "y": 0.47252747416496277}, {"x": 0.08759954571723938, "y": 0.47252747416496277}], "text": "Table 3: Comparison of mainstream all-level databases supporting vector data (the version number represents the first version that supports\nvector search and its release date)\n"}
{"page": 5, "bbox": [{"x": 0.5136518478393555, "y": 0.5015384554862976}, {"x": 0.9129692912101746, "y": 0.5015384554862976}, {"x": 0.9129692912101746, "y": 0.8865934014320374}, {"x": 0.5136518478393555, "y": 0.8865934014320374}], "text": "Data storage means establishing reliable external memory\nlike vector databases, and there are detailed recipes for this\nprocess [Han et al., 2023]. It starts with the data preprocess-\ning, during which the original data is collected, cleaned, inte-\ngrated, transformed, normalized and standardized. The pro-\ncessed data is chunked into smaller segments because of the\ncontextual limit of LLMs. These segments of data are then\nconverted by an embedding model into vectors, the seman-\ntic representations of the data, which are stored in a vector\ndatabase and will be used for vector search in later steps. A\nwell-developed vector database will properly index the data\nand optimize the retrieval process. The retrieval part starts\nby a user asking a question in the form of prompts to the\nsame embedding model, which has generated the vector rep-\nresentation of the stored data, and gaining the vector embed-\ndings of the question. The next step of the process is vector\nsearching inside the vector databases The vector search is es-\nsentially computing similarity scores among vectors, and the\ndatabase then identifies and retrieves the data segments that\nhave the highest similarity scores (top K in most RAG sys-\ntems) compared to the query vector. These retrieved segments\nare then converted back from their vector format to their orig-\ninal form, which is typically text of documents. In the gener-\ntation part, LLMs are involved to generate the final answers.\nThe retrieved documents, along with the user's question, are\nincorporated into a specifically chosen and designed prompt\ntemplate. This template selection is based on the task type\nand aims to effectively merge the context with the question to\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.5006593465805054}, {"x": 0.48634812235832214, "y": 0.5006593465805054}, {"x": 0.48634812235832214, "y": 0.8883516192436218}, {"x": 0.08703071624040604, "y": 0.8883516192436218}], "text": "3.2 Retrieval-Augmented Generation (RAG):\nVecDB as an External Knowledge Base\nDevelopment of RAG Paradigm\nAs the release of ChatGPT casts spotlight on LLMs to the\npublic world, there is an increasing need of using AI chat-\nbots as query and retrieval agents. But simply loading users'\nprivate data as input to LLMs is found to be incompetent\nin real-world applications. Because LLMs have been con-\nstrained by their limited token counts and the high costs as-\nsociated with training and fine-tuning for every alterations\nof data, especially pronounced when dealing with person-\nalized or business-specific responses that require real-time\ndata updates. To address such concerns and need, retrieval-\naugmented generation (RAG) emerges as a novel solution\nthat addresses the challenges faced by LLMs in integrating\nand processing large and dynamic data in external databases,\nwhere vector databases offer a solution by acting as exter-\nnal memory for LLMs. They allow for the segmentation\nof private data by converting them into vectors and storing\nthese vectors efficiently for quick retrieval process. Integrat-\ning with vector databases enables LLMs to access and synthe-\nsize enormous amount of data without the need for constant\nre-training, thereby overcoming their inherent limitations.\nThe concept of RAG is devised to be a paradigm, and a\ncommon workflow of RAG is illustrated in Figure 1. There\nare essentially 3 main parts to complete one full run of the\nsystems: data storage, retrieval, and generation.\n"}
{"page": 6, "bbox": [{"x": 0.5335608720779419, "y": 0.07516483217477798}, {"x": 0.5381115078926086, "y": 0.07516483217477798}, {"x": 0.5381115078926086, "y": 0.07868131995201111}, {"x": 0.5335608720779419, "y": 0.07868131995201111}], "text": "•\n"}
{"page": 6, "bbox": [{"x": 0.08816837519407272, "y": 0.07164835184812546}, {"x": 0.48521047830581665, "y": 0.07164835184812546}, {"x": 0.48521047830581665, "y": 0.09714286029338837}, {"x": 0.08816837519407272, "y": 0.09714286029338837}], "text": "form a coherent prompt. The selected LLM is provided with\nthe prompt and generates the final answer.\n"}
{"page": 6, "bbox": [{"x": 0.08873720467090607, "y": 0.10945054888725281}, {"x": 0.2827076315879822, "y": 0.10945054888725281}, {"x": 0.2827076315879822, "y": 0.117802195250988}, {"x": 0.08873720467090607, "y": 0.117802195250988}], "text": "RAG with Vector Databases\n"}
{"page": 6, "bbox": [{"x": 0.5142207145690918, "y": 0.07208791375160217}, {"x": 0.912400484085083, "y": 0.07120878994464874}, {"x": 0.9129692912101746, "y": 0.16043956577777863}, {"x": 0.5147895216941833, "y": 0.16131867468357086}], "text": "Embedding evaluation comprehensively assess all\nessential capabilities of text embeddings, including\nBEIR [Thakur and et al., 2021], C-MTEB [Xiao and et\nal., 2023].\nMultimodality of RAG RAG has now evolved to handle a\nrange of data types by lending the power of multimodal\n"}
{"page": 6, "bbox": [{"x": 0.5142207145690918, "y": 0.14989010989665985}, {"x": 0.564846396446228, "y": 0.14989010989665985}, {"x": 0.564846396446228, "y": 0.17186813056468964}, {"x": 0.5142207145690918, "y": 0.17186813056468964}], "text": "wide\nmodels.\n"}
{"page": 6, "bbox": [{"x": 0.08703071624040604, "y": 0.12439560145139694}, {"x": 0.4869169592857361, "y": 0.12527473270893097}, {"x": 0.4857792854309082, "y": 0.45802196860313416}, {"x": 0.08589305728673935, "y": 0.4571428596973419}], "text": "In the prototype of the RAG paradigm, we use such an idea\nto overcome the hallucination problems of LLMs by provid-\ning domain-specific data with accurate instruction, where the\nvector database serves as an external knowledge base to ware-\nhouse domain-specific data, then LLMs can easily handle\nmassive data owned by users.\nEven though LLMs such as ChatGPT now have user-\nspecified GPTs for specific uses, with just prompt engineer-\ning, their knowledge bases are still limited to the training data\nprovided by OpenAI. However, by using a vector database,\nusers can pre-filter whatever they want it to look at, whereas\nthat is difficult with prompt-engineering GPT assistants.\nExpanding Horizons: Multi-modal Integration and\nRetrieval Innovations in RAG Systems\nNumerous studies have been undertaken to enhance the per-\nformance and functionality of RAG systems.\nDatasets RAG was originally developed and evaluated on\ntext-based question-answering (QA) tasks. There are numer-\nous datasets and benchmarks available for evaluating the per-\nformance of various RAG systems, where different prompt\ntemplates need to be designed for different tasks. Therefore,\nwe classified datasets based on the type of tasks and summa-\nrized them below.\n"}
{"page": 6, "bbox": [{"x": 0.10693970322608948, "y": 0.473406583070755}, {"x": 0.11149033159017563, "y": 0.473406583070755}, {"x": 0.11149033159017563, "y": 0.4769230782985687}, {"x": 0.10693970322608948, "y": 0.4769230782985687}], "text": "•\n"}
{"page": 6, "bbox": [{"x": 0.5130830407142639, "y": 0.17714285850524902}, {"x": 0.9129692912101746, "y": 0.17714285850524902}, {"x": 0.9129692912101746, "y": 0.8887912034988403}, {"x": 0.5130830407142639, "y": 0.8887912034988403}], "text": "The impressive achievements of LLMs have inspired\nsignificant advancements in vision-and-language research.\nDALL-E from OpenAI introduced a Transformer-based ap-\nproach for converting text to images, treating images as se-\nquences of discrete tokens. Subsequent improvements in the\ntext-to-image area [Zhang et al., 2023] have been achieved\nthrough methods like model scaling, pretraining, and en-\nhanced image quantization models. BLIP-2 [Li et al., 2023]\nuses static image encoders with LLMs for efficient visual lan-\nguage pre-training, facilitating direct image-to-text transfor-\nmations. Flamingo [Alayrac et al., 2022] presented a visual\nlanguage model for text generation, showcasing remarkable\nadaptability and leading performance across various vision-\nand-language tasks. CM3 [Aghajanyan et al., 2022] trained\na randomly masked model on a large HTML corpus, and\nshowed that the model is capable of generating images and\ntext. FROMAGE [Koh et al., 2023] gains robust multimodal\ncapabilities for few-shot learning solely from image-caption\npairs, unlike other models that necessitate large-scale, inter-\nwoven image-text data from the websites.\nTo import speech data to RAG systems, Wav2Seq [Wu et\nal., 2022] allows for efficient pre-training without the need\nfor transcriptions, using techniques like k-means clustering\nand byte-pair encoding to generate pseudo subwords from\nspeech. The Large Language and Speech Model (LLaSM)\n[Shu et al., 2023] is an end-to-end trained, large multi-modal\nspeech-language model equipped with cross-modal conver-\nsational skills and proficient in understanding and responding\nto combined speech-and-language directives. Videos are also\nmade available to certain types of RAG systems. Vid2Seq\n[Yang et al., 2023] enhances language models with specific\ntemporal indicators for predicting event limits and textual de-\nscriptions in a single output sequence.\nRetrieval Optimizations of RAG To better harness the\nknowledge from various types of data, kNN-LMs [Khandel-\nwal et al., 2020] explores how incorporating nearest neigh-\nbor search into language models can enhance their ability\nto generalize by effectively leveraging memorized examples.\nEASE [Nishikawa et al., 2022] is distinctive in its use of en-\ntities as a strong indicator of text semantics, providing rich\ntraining signals for sentence embedding. In-context RALM\n[Ram et al., 2023] proves that with the language model archi-\ntecture unchanged, simply appending grounding documents\nto the input will improve the performance. SURGE [Kang\net al., 2023] enhances dialogue generation by incorporating\ncontext-relevant sub-graphs from a knowledge graph. An-\nother work that combines knowledge graphs and LLMs is\nRET-LLM [Modarressi et al., 2023], which is designed to\nequip large language models with a general write-read mem-\nory unit. These studies have focused on retrieval granularity\nand data structuring levels, with coarse granularity providing\n"}
{"page": 6, "bbox": [{"x": 0.1188850998878479, "y": 0.4703296720981598}, {"x": 0.48634812235832214, "y": 0.46989011764526367}, {"x": 0.4869169592857361, "y": 0.8861538171768188}, {"x": 0.11945392191410065, "y": 0.8865934014320374}], "text": "Single-hop QA involves simple questions where the an-\nswer can be obtained through a single-step reasoning\nprocess, including TriviaQA [Joshi and et al., 2017],\nNatural Questions [Kwiatkowski and et al., 2019], We-\nbQuestions [Berant and et al., 2013]. Multi-hop QA\nrequires multiple intermediate reasoning steps to reach\nthe final answer. For example, for a question like ”In\nwhich state did Obama go to high school?\", one needs\nto first infer which school Obama attended and then\ndetermine the location of that school to obtain the fi-\nnal answer. Datasets for multi-hop QA include Hot-\npotQA [Yang and et al., 2018], 2WikiMultiHopQA [Ho\nand et al., 2020], MuSiQue [Trivedi and et al., 2022],\nStrategyQA [Geva and et al., 2021]. In Multiple-choice\nQA, the model does not need to generate an answer in-\ndependently but rather select the correct answer from\nprovided options. Multiple-choice QA datasets include\nMMLU [Hendrycks and et al., 2020], SQUAD [Ra-\njpurkar and et al., 2016]. Open-domain QA involves\nthe task of searching for answers to information-seeking\nqueries within a large pool of knowledge sources.\nRelevant datasets for open-domain QA include MS\nMARCO [Nguyen and et al., 2016], PopQA [Mallen\nand et al., 2022], CommonsenseQA [Talmor and et al.,\n2018].\nFact-checking requires the model to first select relevant\nsentences from a given document as evidence and then\nvalidate statements based on that evidence. Datasets in-\nclude FEVER [Thorne and et al., 2018], FM2 [Eisensch-\nlos and et al., 2021].\n"}
{"page": 6, "bbox": [{"x": 0.10693970322608948, "y": 0.8254945278167725}, {"x": 0.11149033159017563, "y": 0.8254945278167725}, {"x": 0.11149033159017563, "y": 0.8285714387893677}, {"x": 0.10693970322608948, "y": 0.8285714387893677}], "text": "•\n"}
{"page": 7, "bbox": [{"x": 0.6342434287071228, "y": 0.07340659201145172}, {"x": 0.6973834037780762, "y": 0.07428571581840515}, {"x": 0.6968145370483398, "y": 0.09142857044935226}, {"x": 0.6336746215820312, "y": 0.09054945409297943}], "text": "Embedding\nModels\n"}
{"page": 7, "bbox": [{"x": 0.7974971532821655, "y": 0.07472527772188187}, {"x": 0.8452787399291992, "y": 0.07428571581840515}, {"x": 0.8458475470542908, "y": 0.09890110045671463}, {"x": 0.7980659604072571, "y": 0.09934066236019135}], "text": "Natural\nLanguage\nQuery (Q)\n"}
{"page": 7, "bbox": [{"x": 0.6109215021133423, "y": 0.09406593441963196}, {"x": 0.7201365232467651, "y": 0.09450549632310867}, {"x": 0.7201365232467651, "y": 0.11032967269420624}, {"x": 0.6109215021133423, "y": 0.10989011079072952}], "text": "text-embedding-3-small\ntext-embedding-3-large\n"}
{"page": 7, "bbox": [{"x": 0.6131967902183533, "y": 0.1257142871618271}, {"x": 0.6825938820838928, "y": 0.1257142871618271}, {"x": 0.6825938820838928, "y": 0.13098901510238647}, {"x": 0.6131967902183533, "y": 0.13098901510238647}], "text": "Vector Search\n"}
{"page": 7, "bbox": [{"x": 0.8168373107910156, "y": 0.13098901510238647}, {"x": 0.8651877045631409, "y": 0.13010989129543304}, {"x": 0.8651877045631409, "y": 0.1371428519487381}, {"x": 0.8168373107910156, "y": 0.13802197575569153}], "text": "Query IDs\n"}
{"page": 7, "bbox": [{"x": 0.7360637187957764, "y": 0.1512087881565094}, {"x": 0.7696245908737183, "y": 0.1516483575105667}, {"x": 0.7696245908737183, "y": 0.16923077404499054}, {"x": 0.7360637187957764, "y": 0.16879120469093323}], "text": "Cache\nHit\n"}
{"page": 7, "bbox": [{"x": 0.5409556031227112, "y": 0.15736263990402222}, {"x": 0.5944254994392395, "y": 0.1569230705499649}, {"x": 0.5944254994392395, "y": 0.16351647675037384}, {"x": 0.5409556031227112, "y": 0.16395604610443115}], "text": "GPT Cache\n"}
{"page": 7, "bbox": [{"x": 0.6063708662986755, "y": 0.15516483783721924}, {"x": 0.6882821321487427, "y": 0.15560439229011536}, {"x": 0.6882821321487427, "y": 0.1736263781785965}, {"x": 0.6063708662986755, "y": 0.17318680882453918}], "text": "Vector Storage\n(e.g. DynamoDB)\n"}
{"page": 7, "bbox": [{"x": 0.8253697156906128, "y": 0.15780219435691833}, {"x": 0.8481228947639465, "y": 0.15780219435691833}, {"x": 0.8481228947639465, "y": 0.17230768501758575}, {"x": 0.8253697156906128, "y": 0.17230768501758575}], "text": "Q&A\nData\n"}
{"page": 7, "bbox": [{"x": 0.786120593547821, "y": 0.16967032849788666}, {"x": 0.8037542700767517, "y": 0.17010988295078278}, {"x": 0.8037542700767517, "y": 0.17538461089134216}, {"x": 0.786120593547821, "y": 0.17494505643844604}], "text": "Yes\n"}
{"page": 7, "bbox": [{"x": 0.6951080560684204, "y": 0.1876923143863678}, {"x": 0.7093287706375122, "y": 0.1876923143863678}, {"x": 0.7093287706375122, "y": 0.19252747297286987}, {"x": 0.6951080560684204, "y": 0.19252747297286987}], "text": "No\n"}
{"page": 7, "bbox": [{"x": 0.8037542700767517, "y": 0.18857142329216003}, {"x": 0.8395904302597046, "y": 0.18813186883926392}, {"x": 0.8395904302597046, "y": 0.19384615123271942}, {"x": 0.8037542700767517, "y": 0.19428572058677673}], "text": "Cached\n"}
{"page": 7, "bbox": [{"x": 0.08703071624040604, "y": 0.07164835184812546}, {"x": 0.48634812235832214, "y": 0.07208791375160217}, {"x": 0.4857792854309082, "y": 0.33450549840927124}, {"x": 0.08646188676357269, "y": 0.3340659439563751}], "text": "more, but less precise, information. Conversely, structured\ntext retrieval offers detailed information at the cost of effi-\nciency.\nTo utilize both internal knowledge and external resources,\nSKR [Yu et al., 2023] improves LLMs' ability by enabling\nthem to assess what they know and identify when to seek ex-\nternal information to answer questions more effectively. Self-\nmem [Cheng et al., 2023] enhances retrieval-augmented text\ngeneration by creating an unbounded memory pool using the\nmodel's own output and selecting the best output as memory\nfor subsequent rounds. FLARE [Jiang et al., 2023] uses pre-\ndictions of upcoming sentences to anticipate future content\nand retrieve relevant documents for regenerating sentences,\nespecially when they contain low-confidence tokens. Atlas\n[Izacard et al., 2022] demonstrates impressive performance\nin tasks like question-answering and fact-checking, outper-\nforming much larger models despite having fewer parame-\nters. Many other works like these also aim to make RAG\nsystem more efficient and competent.\n"}
{"page": 7, "bbox": [{"x": 0.8151308298110962, "y": 0.20879121124744415}, {"x": 0.8663253784179688, "y": 0.20879121124744415}, {"x": 0.8663253784179688, "y": 0.2237362563610077}, {"x": 0.8151308298110962, "y": 0.2237362563610077}], "text": "Generated\nAnswer (A)\n"}
{"page": 7, "bbox": [{"x": 0.7428896427154541, "y": 0.2158241719007492}, {"x": 0.7935153841972351, "y": 0.2158241719007492}, {"x": 0.7935153841972351, "y": 0.2210988998413086}, {"x": 0.7428896427154541, "y": 0.2210988998413086}], "text": "Generated\n"}
{"page": 7, "bbox": [{"x": 0.6626848578453064, "y": 0.21142856776714325}, {"x": 0.7167235612869263, "y": 0.21186813712120056}, {"x": 0.7161546945571899, "y": 0.237802192568779}, {"x": 0.6621160507202148, "y": 0.23736263811588287}], "text": "Large\nLanguage\nModels\n"}
{"page": 7, "bbox": [{"x": 0.5142207145690918, "y": 0.2580219805240631}, {"x": 0.9118316173553467, "y": 0.255384624004364}, {"x": 0.9118316173553467, "y": 0.2764835059642792}, {"x": 0.5142207145690918, "y": 0.27912089228630066}], "text": "Figure 2: An overview of semantic cache for GPTs that utilizes vec-\ntor database\n"}
{"page": 7, "bbox": [{"x": 0.5125142335891724, "y": 0.30637362599372864}, {"x": 0.9129692912101746, "y": 0.3059340715408325}, {"x": 0.9135380983352661, "y": 0.7771428823471069}, {"x": 0.5130830407142639, "y": 0.7775824069976807}], "text": "3.4 VecDB as a Cost-effective Semantic Cache\nThe combination of vector databases and LLM not only fa-\ncilitates the profound application of LLM with RAG but also\nprovides a new frontier for cost-effective end-to-end LLM ap-\nplications:\nOutrageous API Costs LLM-based chatbots and agent\nsystems heavily rely on LLM's output from API vendors,\nconsiderable repeated or similar inquiries may lead to out-\nrageous API costs.\nAPI Bandwidth Limitations Such chatbots and agent sys-\ntems could also experience bursty inquiries workload that\nmay drown the system's bandwidth with explosive API calls\ncoming within seconds, leading to the system's outage and\nreconfiguration.\nOne of the primary benefits of integrating vector databases\nwith LLMs is the significant reduction in data operational\ncosts. GPT-Cache, as shown in Figure 2, for instance, stores\nresponses to previously asked queries, and works as a cache\nbefore calling LLM APIs. This caching mechanism means\nthat the system doesn't really need to have API calls to wait\nfor generated responses from scratch every time, reducing the\nnumber of costly API calls to the LLM. Moreover, this ap-\nproach also speeds up the response time, enhancing user ex-\nperience.\nVector databases enhance the LLMs' ability to retrieve and\nutilize relevant information by indexing vast amounts of pre-\nvious Q&A data and mapping them into a vector space, in-\nstead of caches in computer systems that require exact hash-\nmatch, this allows more precise semantic matching of queries\nwith existing knowledge and results in responses that are not\nonly generated based on the LLM's training data but also in-\nformed by the most relevant and recent information available\nin the vector database.\n"}
{"page": 7, "bbox": [{"x": 0.08703071624040604, "y": 0.35120880603790283}, {"x": 0.48634812235832214, "y": 0.35120880603790283}, {"x": 0.4869169592857361, "y": 0.8857142925262451}, {"x": 0.08759954571723938, "y": 0.8857142925262451}], "text": "3.3 VecDB as a Reliable Memory of GPTS\nOblivion When using LLM Q&A applications like Chat-\nGPT, LLMS are likely to completely forget the content and\ninformation of your previous conversation, even within the\nsame chat tab.\nThe integration of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) with vector\ndatabases (VecDB) heralds a transformative leap in the field\nof information retrieval and artificial intelligence. This sub-\nsection delves into the compelling synergy between VecDB as\na persistent, reliable memory layer and the dynamic, contex-\ntually aware computation layer provided by LLMs. In recent\nworks of vector databases that integrate with LLMs, [Zhang\nand et al., 2023] showcases that vector databases' capability\nof storing vectors for GPT's memory.\nVector databases serve as a robust memory layer that ad-\ndresses one of the intrinsic limitations of LLMs: the static\nnature of their knowledge. While LLMs excel in generat-\ning human-like text based on patterns learned during training,\nthey cannot inherently update their knowledge base dynami-\ncally. VecDBs bridge this gap by offering a storage solution\nthat can be continually updated with new information, en-\nsuring that the LLM's responses are informed by the most\ncurrent and relevant data available.\nThe VecDB and LLM combination brings forth a synergy\nwhere the LLM provides context and understanding for user\nqueries, while the VecDB offers a precise mechanism for stor-\ning and retrieving the relevant vectors. This collaborative ap-\nproach allows for more accurate, relevant, and efficient re-\nsponses to complex queries, which would be challenging for\neither system to address independently.\nA VecDB integrated with an LLM facilitates real-time\nlearning and adaptation. As new data is ingested into the\nVecDB, the LLM can immediately leverage this updated\nrepository to refine its responses. This capability is pivotal\nfor applications requiring up-to-the-minute accuracy, such as\nfinancial analysis, news dissemination, and personalized rec-\nommendations.\n"}
{"page": 7, "bbox": [{"x": 0.5130830407142639, "y": 0.7982417345046997}, {"x": 0.912400484085083, "y": 0.7982417345046997}, {"x": 0.912400484085083, "y": 0.8887912034988403}, {"x": 0.5130830407142639, "y": 0.8887912034988403}], "text": "4 Discussion: Challenges and Future Work\nAre vector searches all you need?\nAlthough vector databases offer a cost-efficient modality for\ninformation retrieval within LLM frameworks, their utility\nin traditional relational database operations remains limited.\nSpecifically, vector-based search methodologies are often not\n"}
{"page": 8, "bbox": [{"x": 0.5142207145690918, "y": 0.07164835184812546}, {"x": 0.9118316173553467, "y": 0.07208791375160217}, {"x": 0.9118316173553467, "y": 0.12263736128807068}, {"x": 0.5142207145690918, "y": 0.12219779938459396}], "text": "perspectives and interdisciplinary studies. Our study also\ndemonstrates both the research and engineering challenges\nin this fast-growing field and suggests directions for future\nwork.\n"}
{"page": 8, "bbox": [{"x": 0.5142207145690918, "y": 0.14593406021595}, {"x": 0.605802059173584, "y": 0.14593406021595}, {"x": 0.605802059173584, "y": 0.15560439229011536}, {"x": 0.5142207145690918, "y": 0.15560439229011536}], "text": "References\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.07076922804117203}, {"x": 0.4880546033382416, "y": 0.07208791375160217}, {"x": 0.48634812235832214, "y": 0.31472527980804443}, {"x": 0.08589305728673935, "y": 0.3134065866470337}], "text": "optimized for operations such as post-query filtering, com-\nprehensive full-text searches, and nuanced keyword search\nmechanisms that are fundamental to conventional database\nsystems. This distinction underscores a potential gap in the\nfunctional alignment of vector databases with established\ndata retrieval paradigms, necessitating the development of hy-\nbrid search algorithms that can seamlessly integrate vector\nsearch with traditional relational database capabilities.\nVDBMS with multi-modality\nWhile uni-modal data may lack the prospect of providing in-\nformative and more contextually appropriate search results,\nthe multi-modal data and its hybrid processing also present\na non-trivial challenge for vector databases' storage and re-\ntrieval [Wang and et al., 2023], since integrating and merging\nmulti-modal data require efficient preprocessing with multi-\nmodal encoders, multi-modal storage indexes, but also weight\nassignment and multi-modal data fusion.\n"}
{"page": 8, "bbox": [{"x": 0.08816837519407272, "y": 0.3226373493671417}, {"x": 0.22241182625293732, "y": 0.3243955969810486}, {"x": 0.22241182625293732, "y": 0.3358241617679596}, {"x": 0.08816837519407272, "y": 0.3340659439563751}], "text": "Data preprocessing\n"}
{"page": 8, "bbox": [{"x": 0.08703071624040604, "y": 0.3380219638347626}, {"x": 0.4869169592857361, "y": 0.33758240938186646}, {"x": 0.48748576641082764, "y": 0.558681309223175}, {"x": 0.08759954571723938, "y": 0.5591208934783936}], "text": "While text embedding is considered efficient for processing\nlong text and its retrieval, it comes with a dazzling challenge\nfor building every text-based knowledge database. It is ob-\nserved that vector retrieval would have fundamentally better\nperformance when precise chunks of text with clear meaning\nare applied, this raises an interesting question: how to have a\nproper (probably unified) embedding methodology for every\nraw text?\nOn the other hand, dimension reduction for vector data\nis also considered important due to the curse of dimension-\nality theory. The current vector data dimension may be not\ncost-effective, as vectors are large: e.g., a 1536 dimensional\nvector is about 6 kilobytes, with scaling up to 1 billion, the\ndata size would be 6 terabytes. This requires a more efficient\nvector data dimension reduction algorithm that matches vec-\ntor databases and search algorithms.\n"}
{"page": 8, "bbox": [{"x": 0.5113765597343445, "y": 0.16703297197818756}, {"x": 0.9135380983352661, "y": 0.16615384817123413}, {"x": 0.9163822531700134, "y": 0.8879120945930481}, {"x": 0.5142207145690918, "y": 0.8887912034988403}], "text": "[Achiam and et al., 2023] Josh Achiam and Steven Adler\net al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[Aghajanyan et al., 2022] Armen Aghajanyan, Bernie\nHuang, Candace Ross, Vladimir Karpukhin, Hu Xu,\nNaman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi\nGhosh, Mike Lewis, and Luke Zettlemoyer. Cm3: A\ncausal masked multimodal model of the internet, 2022.\n[Alayrac et al., 2022] Jean-Baptiste Alayrac, Jeff Donahue,\nPauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm\nReynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, Andrew\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj\nBinkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zis-\nserman, and Karen Simonyan. Flamingo: a visual lan-\nguage model for few-shot learning, 2022.\n[Andoni and Indyk, 2008] Alexandr Andoni and Piotr Indyk.\nNear-Optimal Hashing Algorithms for Approximate Near-\nest Neighbor in High Dimensions. Communications of the\nACM, 51(1):117-122, 2008.\n[Asai and et al., 2023] Akari Asai and Sewon Min et al. Acl\n2023 tutorial: Retrieval-based language models and appli-\ncations. ACL 2023, 2023.\n[Bender et al., 2021] Emily M. Bender, Timnit Gebru, An-\ngelina McMillan-Major, and Shmargaret Shmitchell. On\nthe dangers of stochastic parrots: Can language models be\ntoo big? In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency, FAccT '21,\npage 610-623, New York, NY, USA, 2021. Association\nfor Computing Machinery.\n[Berant and et al., 2013] Jonathan Berant and Andrew Chou\net al. Semantic parsing on freebase from question-answer\npairs. In Proc. 2013 Conf. on Empirical Methods in Natu-\nral Language Processing, pages 1533-1544, 2013.\n[Brown and et al., 2020] Tom Brown and Benjamin Mann\net al. Language models are few-shot learners. Advances\nin Neural Information Processing Systems, 33:1877–1901,\n2020.\n[Cheng et al., 2023] Xin Cheng, Di Luo, Xiuying Chen,\nLemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself\nup: Retrieval-augmented text generation with self mem-\nory, 2023.\n[Chowdhery and et al., 2023] Aakanksha Chowdhery and\nSharan Narang et al. Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research,\n24(240):1-113, 2023.\n"}
{"page": 8, "bbox": [{"x": 0.08816837519407272, "y": 0.5683516263961792}, {"x": 0.22127418220043182, "y": 0.567472517490387}, {"x": 0.22127418220043182, "y": 0.5780220031738281}, {"x": 0.08816837519407272, "y": 0.5789011120796204}], "text": "Knowledge conflict\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.5832967162132263}, {"x": 0.4869169592857361, "y": 0.5832967162132263}, {"x": 0.4869169592857361, "y": 0.795604407787323}, {"x": 0.08759954571723938, "y": 0.795604407787323}], "text": "Conflicting knowledge from the same or different knowledge\nbases presents a non-trivial challenge for both humans and\nLLMs to distinguish the correct piece of knowledge. This\nconflict becomes particularly pronounced when integrating\nmultiple knowledge bases, each potentially carrying its own\nbiases or inaccuracies. Resolving such conflicts requires ro-\nbust conflict resolution strategies that can assess the reliabil-\nity of sources and the context of data to determine the most\naccurate information.\nData management systems for LLMs with multi-tenancy\nThis challenge is particularly in maintaining the isolation and\nsecurity of distinct tenants' data while ensuring efficiency,\nwhere isolation is crucial for privacy and security, yet it must\nbe balanced with the need for resource sharing to ensure cost-\neffectiveness.\n"}
{"page": 8, "bbox": [{"x": 0.08703071624040604, "y": 0.8153846263885498}, {"x": 0.09726962447166443, "y": 0.8153846263885498}, {"x": 0.09726962447166443, "y": 0.8259340524673462}, {"x": 0.08703071624040604, "y": 0.8259340524673462}], "text": "5\n"}
{"page": 8, "bbox": [{"x": 0.11774744093418121, "y": 0.8153846263885498}, {"x": 0.2104664444923401, "y": 0.8153846263885498}, {"x": 0.2104664444923401, "y": 0.8259340524673462}, {"x": 0.11774744093418121, "y": 0.8259340524673462}], "text": "Conclusion\n"}
{"page": 8, "bbox": [{"x": 0.08816837519407272, "y": 0.8356043696403503}, {"x": 0.4857792854309082, "y": 0.8347252607345581}, {"x": 0.4857792854309082, "y": 0.8883516192436218}, {"x": 0.08816837519407272, "y": 0.8892307877540588}], "text": "In this paper, we present a systematic review of recent ad-\nvances in combinations of LLMs and vector databases. We\nalso introduce recent VecDB+LLMs applications with dis-\ntinct prototypes that categorize existing works from various\n"}
{"page": 9, "bbox": [{"x": 0.8879408240318298, "y": 0.12087912112474442}, {"x": 0.9112628102302551, "y": 0.12087912112474442}, {"x": 0.9112628102302551, "y": 0.12835164368152618}, {"x": 0.8879408240318298, "y": 0.12835164368152618}], "text": "and\n"}
{"page": 9, "bbox": [{"x": 0.5142207145690918, "y": 0.07164835184812546}, {"x": 0.912400484085083, "y": 0.07120878994464874}, {"x": 0.912400484085083, "y": 0.17758241295814514}, {"x": 0.5142207145690918, "y": 0.17802198231220245}], "text": "[He and et al., 2020] Pengcheng He and Xiaodong Liu et al.\nDeberta: Decoding-enhanced bert with disentangled atten-\ntion. arXiv preprint arXiv:2006.03654, 2020.\n[Hendrycks and et al., 2020] Dan Hendrycks\nCollin Burns et al. Measuring massive multitask language\nunderstanding. arXiv preprint arXiv:2009.03300, 2020.\n[Ho and et al., 2020] Xanh\n"}
{"page": 9, "bbox": [{"x": 0.7281001210212708, "y": 0.16923077404499054}, {"x": 0.7485779523849487, "y": 0.16923077404499054}, {"x": 0.7485779523849487, "y": 0.1767033040523529}, {"x": 0.7281001210212708, "y": 0.1767033040523529}], "text": "Ho\n"}
{"page": 9, "bbox": [{"x": 0.7838453054428101, "y": 0.16923077404499054}, {"x": 0.806598424911499, "y": 0.16923077404499054}, {"x": 0.806598424911499, "y": 0.1767033040523529}, {"x": 0.7838453054428101, "y": 0.1767033040523529}], "text": "and\n"}
{"page": 9, "bbox": [{"x": 0.5142207145690918, "y": 0.1679120808839798}, {"x": 0.9135380983352661, "y": 0.1679120808839798}, {"x": 0.9135380983352661, "y": 0.6580219864845276}, {"x": 0.5142207145690918, "y": 0.6580219864845276}], "text": "Anh-Khoa\nDuong Nguyen et al. Constructing a multi-hop qa\ndataset for comprehensive evaluation of reasoning steps.\narXiv preprint arXiv:2011.01060, 2020.\n[Hoffmann and et al., 2022] Jordan Hoffmann and Sebas-\ntian Borgeaud et al. Training compute-optimal large lan-\nguage models. arXiv preprint arXiv:2203.15556, 2022.\n[Huang et al., 2023] Lei Huang, Weijiang Yu, Weitao Ma,\nWeihong Zhong, Zhangyin Feng, Haotian Wang, Qiang-\nlong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and\nTing Liu. A survey on hallucination in large language\nmodels: Principles, taxonomy, challenges, and open ques-\ntions, 2023.\n[Izacard et al., 2022] Gautier Izacard, Patrick Lewis, Maria\nLomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. Atlas: Few-shot learning with retrieval\naugmented language models, 2022.\n[Jégou et al., 2011] Hervé Jégou, Matthijs Douze, and\nCordelia Schmid. Product Quantization for Nearest Neigh-\nbor Search. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (TPAMI), 33(1):117–128, 2011.\n[Ji and et al., 2023] Ziwei Ji and Nayeon Lee et al. Survey of\nhallucination in natural language generation. ACM Com-\nputing Surveys, 55(12):1–38, 2023.\n[Jiang et al., 2023] Zhengbao Jiang, Frank F. Xu, Luyu Gao,\nZhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval aug-\nmented generation, 2023.\n[Joshi and et al., 2017] Mandar Joshi and Eunsol Choi et al.\nTriviaqa: A large scale distantly supervised challenge\ndataset for reading comprehension. arXiv preprint\n"}
{"page": 9, "bbox": [{"x": 0.08703071624040604, "y": 0.07120878994464874}, {"x": 0.4880546033382416, "y": 0.07076922804117203}, {"x": 0.48919227719306946, "y": 0.8883516192436218}, {"x": 0.08816837519407272, "y": 0.8887912034988403}], "text": "[Chung et al., 2014] Junyoung Chung, Caglar Gulcehre,\nKyungHyun Cho, and Yoshua Bengio. Empirical evalua-\ntion of gated recurrent neural networks on sequence mod-\neling. arXiv preprint arXiv:1412.3555, 2014.\n[Clark and et al., 2020] Kevin Clark and Minh-Thang Luong\net al. Electra: Pre-training text encoders as discriminators\nrather than generators. arXiv preprint arXiv:2003.10555,\n2020.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Dodge and et al., 2022] Jesse Dodge and Taylor Prewitt\net al. Measuring the carbon intensity of ai in cloud in-\nstances, 2022.\n[Dong et al., 2021] Hongyuan Dong, Jiaqing Xie, Zhi Jing,\nand Dexin Ren. Variational autoencoder for anti-cancer\ndrug response prediction, 2021.\n[Du and et al., 2022] Nan Du and Yanping Huang et al.\nGlam: Efficient scaling of language models with mixture-\nof-experts. In Int. Conf. on Machine Learning, pages\n5547-5569. PMLR, 2022.\n[Eisenschlos and et al., 2021] Julian Martin Eisenschlos and\nBhuwan Dhingra et al. Fool me twice: Entailment from\nwikipedia gamification. arXiv preprint arXiv:2104.04725,\n2021.\n[Fan and et al., 2021] Angela Fan and Shruti Bhosale et al.\nBeyond english-centric multilingual machine translation.\nJournal of Machine Learning Research, 22(107):1–48,\n2021.\n[Ganguli et al., 2022] Deep Ganguli, Liane Lovitt, Jackson\nKernion, Amanda Askell, Yuntao Bai, Saurav Kada-\nvath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal\nNdousse, Andy Jones, Sam Bowman, Anna Chen, Tom\nConerly, Nova DasSarma, Dawn Drain, Nelson Elhage,\nSheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom\nHenighan, Danny Hernandez, Tristan Hume, Josh Ja-\ncobson, Scott Johnston, Shauna Kravec, Catherine Ols-\nson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom\nBrown, Nicholas Joseph, Sam McCandlish, Chris Olah,\nJared Kaplan, and Jack Clark. Red teaming language\nmodels to reduce harms: Methods, scaling behaviors, and\nlessons learned, 2022.\n[Geva and et al., 2021] Mor Geva and Daniel Khashabi et al.\nDid aristotle use a laptop? a question answering bench-\nmark with implicit reasoning strategies. Transactions of\nthe Association for Computational Linguistics, 9:346–361,\n2021.\n[Han et al., 2023] Yikun Han, Chunjiang Liu, and Pengfei\nWang. A comprehensive survey on vector database: Stor-\nage and retrieval technique, challenge, 2023.\n[He and et al., 2016] Kaiming He and Xiangyu Zhang et al.\nDeep residual learning for image recognition. In Proc. of\nthe IEEE Conf. on Computer Vision and Pattern Recogni-\ntion, pages 770-778, 2016.\n"}
{"page": 9, "bbox": [{"x": 0.5335608720779419, "y": 0.6580219864845276}, {"x": 0.6968145370483398, "y": 0.6580219864845276}, {"x": 0.6968145370483398, "y": 0.6672527194023132}, {"x": 0.5335608720779419, "y": 0.6672527194023132}], "text": "arXiv:1705.03551, 2017.\n"}
{"page": 9, "bbox": [{"x": 0.5136518478393555, "y": 0.6769230961799622}, {"x": 0.912400484085083, "y": 0.6760439276695251}, {"x": 0.9129692912101746, "y": 0.8892307877540588}, {"x": 0.5142207145690918, "y": 0.8901098966598511}], "text": "[Kang et al., 2023] Minki Kang, Jin Myung Kwak, Jinheon\nBaek, and Sung Ju Hwang. Knowledge graph-augmented\nlanguage models for knowledge-grounded dialogue gener-\nation, 2023.\n[Kaplan and et al., 2020] Jared Kaplan and Sam McCandlish\net al. Scaling laws for neural language models. arXiv\npreprint arXiv:2001.08361, 2020.\n[Khandelwal et al., 2020] Urvashi Khandelwal, Omer Levy,\nDan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gen-\neralization through memorization: Nearest neighbor lan-\nguage models, 2020.\n[Koh et al., 2023] Jing Yu Koh, Ruslan Salakhutdinov, and\nDaniel Fried. Grounding language models to images for\nmultimodal inputs and outputs. 2023.\n"}
{"page": 10, "bbox": [{"x": 0.08703071624040604, "y": 0.07120878994464874}, {"x": 0.48634812235832214, "y": 0.07076922804117203}, {"x": 0.48748576641082764, "y": 0.6589010953903198}, {"x": 0.08816837519407272, "y": 0.6593406796455383}], "text": "[Kwiatkowski and et al., 2019] Tom Kwiatkowski and Jen-\nnimaria Palomaki et al. Natural questions: a benchmark\nfor question answering research. Transactions of the Asso-\nciation for Computational Linguistics, 7:453–466, 2019.\n[Lample and Conneau, 2019] Guillaume Lample and Alexis\nConneau. Cross-lingual language model pretraining. arXiv\npreprint arXiv:1901.07291, 2019.\n[Lan and et al., 2019] Zhenzhong Lan and Mingda Chen\net al. Albert: A lite bert for self-supervised learning of lan-\nguage representations. arXiv preprint arXiv:1909.11942,\n2019.\n[Lee et al., 2022] Katherine Lee, Daphne Ippolito, Andrew\nNystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-\nBurch, and Nicholas Carlini. Deduplicating training data\nmakes language models better, 2022.\n[Lewis and et al., 2019] Mike Lewis and Yinhan Liu et al.\nBart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. arXiv preprint arXiv:1910.13461, 2019.\n[Li and et al., 2023] Yingji Li and Mengnan Du et al. A sur-\nvey on fairness in large language models. arXiv preprint\narXiv:2308.10149, 2023.\n[Li et al., 2023] Junnan Li, Dongxu Li, Silvio Savarese, and\nSteven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language\nmodels, 2023.\n[Liu and et al., 2024] Chunjiang Liu and Yikun Han et al.\nA community detection and graph-neural-network-based\nlink prediction approach for scientific literature. Mathe-\nmatics, 12(3):369, 2024.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:\nA robustly optimized bert pretraining approach. arXiv\npreprint arXiv: 1907.11692, 2019.\n[Malkov and Yashunin, 2018] Yury\nDmitry A. Yashunin. Efficient and Robust Approx-\nimate Nearest Neighbor Search Using Hierarchical\nNavigable Small World Graphs. IEEE Transactions\n"}
{"page": 10, "bbox": [{"x": 0.5136518478393555, "y": 0.07120878994464874}, {"x": 0.9129692912101746, "y": 0.07076922804117203}, {"x": 0.9141069650650024, "y": 0.7353846430778503}, {"x": 0.5147895216941833, "y": 0.7358241677284241}], "text": "[Nguyen and et al., 2016] Tri Nguyen and Mir Rosenberg\net al. Ms marco: A human generated machine reading\ncomprehension dataset. Choice, 2640:660, 2016.\n[Nishikawa et al., 2022] Sosuke Nishikawa, Ryokan Ri,\nIkuya Yamada, Yoshimasa Tsuruoka, and Isao Echizen.\nEASE: Entity-aware contrastive learning of sentence em-\nbedding. In Marine Carpuat, Marie-Catherine de Marn-\neffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, pages 3870-3885, Seattle, United\nStates, July 2022. Association for Computational Linguis-\ntics.\n[Onoe et al., 2022] Yasumasa Onoe, Michael Zhang, Eunsol\nChoi, and Greg Durrett. Entity cloze by date: What LMs\nknow about unseen entities. In Marine Carpuat, Marie-\nCatherine de Marneffe, and Ivan Vladimir Meza Ruiz, ed-\nitors, Findings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 693–702, Seattle, United\nStates, July 2022. Association for Computational Linguis-\ntics.\n[Pan et al., 2023] James Jie Pan, Jianguo Wang, and Guo-\nliang Li. Survey of vector database management systems.\narXiv preprint arXiv:2310.14021, 2023.\n[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic,\nDaniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. The refinedweb dataset for falcon llm:\nOutperforming curated corpora with web data, and web\ndata only, 2023.\n[Radford and et al., 2019] Alec Radford and Jeffrey Wu\net al. Language models are unsupervised multitask learn-\ners. OpenAI Blog, 1(8):9, 2019.\n[Rae and et al., 2021] Jack W Rae and Sebastian Borgeaud\net al. Scaling language models: Methods, analy-\nsis and insights from training gopher. arXiv preprint\narXiv:2112.11446, 2021.\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam\nRoberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits\nof transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485-\n5551, 2020.\n[Rajpurkar and et al., 2016] Pranav\n"}
{"page": 10, "bbox": [{"x": 0.3464163839817047, "y": 0.6065934300422668}, {"x": 0.48350396752357483, "y": 0.6061538457870483}, {"x": 0.48350396752357483, "y": 0.6149450540542603}, {"x": 0.3464163839817047, "y": 0.6153846383094788}], "text": "A. Malkov and\n"}
{"page": 10, "bbox": [{"x": 0.7849829196929932, "y": 0.7257142663002014}, {"x": 0.9112628102302551, "y": 0.7257142663002014}, {"x": 0.9112628102302551, "y": 0.7358241677284241}, {"x": 0.7849829196929932, "y": 0.7358241677284241}], "text": "Rajpurkar and\n"}
{"page": 10, "bbox": [{"x": 0.08759954571723938, "y": 0.6606593132019043}, {"x": 0.48634812235832214, "y": 0.6606593132019043}, {"x": 0.48634812235832214, "y": 0.8879120945930481}, {"x": 0.08759954571723938, "y": 0.8879120945930481}], "text": "on Pattern Analysis and Machine Intelligence (PAMI),\n42(4):824-836, 2018.\n[Mallen and et al., 2022] Alex Mallen and Akari Asai et al.\nWhen not to trust language models: Investigating effec-\ntiveness and limitations of parametric and non-parametric\nmemories. arXiv preprint arXiv:2212.10511, 7, 2022.\n[Modarressi et al., 2023] Ali Modarressi, Ayyoob Imani,\nMohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards\na general read-write memory for large language models,\n2023.\n[Muja and Lowe, 2009] Marius Muja and David G Lowe.\nFast Approximate Nearest Neighbors with Automatic Al-\ngorithm Configuration. VISAPP, 2(331-340):2, 2009.\n[Musser, 2023] Micah Musser. A cost analysis of generative\nlanguage models and influence operations, 2023.\n"}
{"page": 10, "bbox": [{"x": 0.5147895216941833, "y": 0.7375824451446533}, {"x": 0.9135380983352661, "y": 0.738021969795227}, {"x": 0.9129692912101746, "y": 0.8892307877540588}, {"x": 0.5142207145690918, "y": 0.8887912034988403}], "text": "Jian Zhang et al. Squad: 100,000+ questions for machine\ncomprehension of text. arXiv preprint arXiv:1606.05250,\n2016.\n[Ram et al., 2023] Ori Ram, Yoav Levine, Itay Dalmedigos,\nDor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language\nmodels. Transactions of the Association for Computa-\ntional Linguistics, 11:1316–1331, 2023.\n[Schaeffer, 2023] Rylan Schaeffer. Pretraining on the test set\nis all you need. arXiv preprint arXiv:2309.08632, 2023.\n"}
{"page": 11, "bbox": [{"x": 0.08759954571723938, "y": 0.07164835184812546}, {"x": 0.4857792854309082, "y": 0.07120878994464874}, {"x": 0.48634812235832214, "y": 0.27604395151138306}, {"x": 0.08816837519407272, "y": 0.2764835059642792}], "text": "[Shi et al., 2015] Xingjian Shi, Zhourong Chen, Hao Wang,\nDit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.\nConvolutional Istm network: A machine learning approach\nfor precipitation nowcasting. Advances in neural informa-\ntion processing systems, 28, 2015.\n[Shu et al., 2023] Yu Shu, Siwei Dong, Guangyao Chen,\nWenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang,\nand Yemin Shi. Llasm: Large language and speech model,\n2023.\n[Smith and et al., 2022] Shaden Smith and Mostofa Patwary\net al. Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model.\narXiv preprint arXiv:2201.11990, 2022.\n[Strubell and et al., 2019] Emma\n"}
{"page": 11, "bbox": [{"x": 0.5142207145690918, "y": 0.0698901116847992}, {"x": 0.9141069650650024, "y": 0.0698901116847992}, {"x": 0.9141069650650024, "y": 0.2949450612068176}, {"x": 0.5142207145690918, "y": 0.2949450612068176}], "text": "[Venkit et al., 2023] Pranav Narayanan Venkit, Sanjana\nGautam, Ruchi Panchanadikar, Ting-Hao 'Kenneth'\nHuang, and Shomir Wilson. Nationality bias in text gener-\nation, 2023.\n[Wang and et al., 2021] Jianguo Wang and Xiaomeng Yi\net al. Milvus: A purpose-built vector data management\nsystem. In Proc. of the 2021 Int. Conf. on Management of\nData, pages 2614-2627, 2021.\n[Wang and et al., 2023] Mengzhao Wang and Xiangyu Ke\net al. Must: An effective and scalable framework for\nmultimodal search of target modality. arXiv preprint\narXiv:2312.06397, 2023.\n[Wei and et al., 2022] Jason Wei and Yi Tay et al. Emer-\ngent abilities of large language models. arXiv preprint\narXiv:2206.07682, 2022.\n"}
{"page": 11, "bbox": [{"x": 0.3577929437160492, "y": 0.2663736343383789}, {"x": 0.40898749232292175, "y": 0.2663736343383789}, {"x": 0.40898749232292175, "y": 0.2751648426055908}, {"x": 0.3577929437160492, "y": 0.2751648426055908}], "text": "Strubell\n"}
{"page": 11, "bbox": [{"x": 0.46188852190971375, "y": 0.266813188791275}, {"x": 0.48407280445098877, "y": 0.266813188791275}, {"x": 0.48407280445098877, "y": 0.2751648426055908}, {"x": 0.46188852190971375, "y": 0.2751648426055908}], "text": "and\n"}
{"page": 11, "bbox": [{"x": 0.8879408240318298, "y": 0.3059340715408325}, {"x": 0.9106939435005188, "y": 0.3059340715408325}, {"x": 0.9106939435005188, "y": 0.31472527980804443}, {"x": 0.8879408240318298, "y": 0.31472527980804443}], "text": "and\n"}
{"page": 11, "bbox": [{"x": 0.08759954571723938, "y": 0.27912089228630066}, {"x": 0.48634812235832214, "y": 0.27912089228630066}, {"x": 0.4869169592857361, "y": 0.8870329856872559}, {"x": 0.08816837519407272, "y": 0.8870329856872559}], "text": "Ananya Ganesh et al. Energy and policy considerations\nfor deep learning in nlp. arXiv preprint arXiv:1906.02243,\n2019.\n[Su and et al., 2022] Yongye Su and Qian Liu et al. Yolo-\nlogo: A transformer-based yolo segmentation model for\nbreast mass detection and segmentation in digital mammo-\ngrams. Computer Methods and Programs in Biomedicine,\n221:106903, 2022.\n[Talmor and et al., 2018] Alon Talmor and Jonathan Herzig\net al. Commonsenseqa: A question answering chal-\nlenge targeting commonsense knowledge. arXiv preprint\narXiv:1811.00937, 2018.\n[Tao et al., 2009] Yufei Tao, Ke Yi, Cheng Sheng, and Panos\nKalnis. Quality and efficiency in high dimensional nearest\nneighbor search. In Proceedings of the International Con-\nference on Management of Data (SIGMOD), SIGMOD\n'09, page 563-576, New York, NY, USA, 2009. Associ-\nation for Computing Machinery.\n[Thakur and et al., 2021] Nandan Thakur and Nils Reimers\net al. Beir: A heterogenous benchmark for zero-shot eval-\nuation of information retrieval models. arXiv preprint\narXiv:2104.08663, 2021.\n[Thoppilan and et al., 2022] Romal Thoppilan and Daniel\nDe Freitas et al. Lamda: Language models for dialog ap-\nplications. arXiv preprint arXiv:2201.08239, 2022.\n[Thorne and et al., 2018] James Thorne and Andreas Vla-\nchos et al. Fever: a large-scale dataset for fact extraction\nand verification. arXiv preprint arXiv:1803.05355, 2018.\n[Touvron and et al., 2023] Hugo Touvron and Thibaut Lavril\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023.\n[Trivedi and et al., 2022] Harsh Trivedi and Niranjan Bala-\nsubramanian et al. Musique: Multihop questions via\nsingle-hop question composition. Transactions of the\nAssociation for Computational Linguistics, 10:539–554,\n2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing systems,\n30, 2017.\n"}
{"page": 11, "bbox": [{"x": 0.5130830407142639, "y": 0.3046153783798218}, {"x": 0.9129692912101746, "y": 0.30417582392692566}, {"x": 0.914675772190094, "y": 0.8870329856872559}, {"x": 0.5147895216941833, "y": 0.8874725103378296}], "text": "[Workshop and et al., 2022] BigScience Workshop\nTeven Le Scao et al. Bloom: A 176b-parameter open-\naccess multilingual language model. arXiv preprint\narXiv:2211.05100, 2022.\n[Wu and et al., 2023] Shijie Wu and Ozan Irsoy et al.\nBloomberggpt: A large language model for finance. arXiv\npreprint arXiv:2303.17564, 2023.\n[Wu et al., 2022] Felix Wu, Kwangyoun Kim, Shinji Watan-\nabe, Kyu Han, Ryan McDonald, Kilian Q. Weinberger, and\nYoav Artzi. Wav2seq: Pre-training speech-to-text encoder-\ndecoder models using pseudo languages, 2022.\n[Xiao and et al., 2023] Shitao Xiao and Zheng Liu et al. C-\npack: Packaged resources to advance general chinese em-\nbedding. arXiv preprint arXiv:2309.07597, 2023.\n[Xue and et al., 2020] Linting Xue and Noah Constant et al.\nmt5: A massively multilingual pre-trained text-to-text\ntransformer. arXiv preprint arXiv: 2010.11934, 2020.\n[Yang and et al., 2018] Zhilin Yang and Peng Qi et al. Hot-\npotqa: A dataset for diverse, explainable multi-hop ques-\ntion answering. arXiv preprint arXiv:1809.09600, 2018.\n[Yang et al., 2023] Antoine Yang, Arsha Nagrani,\nPaul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset,\nIvan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq:\nLarge-scale pretraining of a visual language model for\ndense video captioning, 2023.\n[Yao et al., 2024] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuan-\nfang Cai, Zhibo Sun, and Yue Zhang. A survey on large\nlanguage model (llm) security and privacy: The good, the\nbad, and the ugly, 2024.\n[Yosinski and et al., 2014] Jason Yosinski and Jeff Clune\net al. How transferable are features in deep neural net-\nworks? Advances in Neural Information Processing Sys-\ntems, 27, 2014.\n[Yu et al., 2023] Wenhao Yu, Dan Iter, Shuohang Wang, Yi-\nchong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. Generate rather than re-\ntrieve: Large language models are strong context genera-\ntors, 2023.\n"}
{"page": 12, "bbox": [{"x": 0.08759954571723938, "y": 0.07120878994464874}, {"x": 0.48634812235832214, "y": 0.07076922804117203}, {"x": 0.4869169592857361, "y": 0.3868131935596466}, {"x": 0.08816837519407272, "y": 0.3872527480125427}], "text": "[Zaheer and et al., 2020] Manzil Zaheer and Guru Guru-\nganesh et al. Big bird: Transformers for longer se-\nquences. Advances in Neural Information Processing Sys-\ntems, 33:17283–17297, 2020.\n[Zaremba et al., 2014] Wojciech Zaremba, Ilya Sutskever,\nand Oriol Vinyals. Recurrent neural network regulariza-\ntion. arXiv preprint arXiv:1409.2329, 2014.\n[Zeng and et al., 2022] Aohan Zeng and Xiao Liu et al. Glm-\n130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414, 2022.\n[Zhang and et al., 2022] Susan Zhang and Stephen Roller\net al. Opt: Open pre-trained transformer language mod-\nels. arXiv preprint arXiv:2205.01068, 2022.\n[Zhang and et al., 2023] Yi Zhang and Zhongyang Yu et al.\nLong-term memory for large language models through\ntopic-based vector database. In 2023 International Con-\nference on Asian Language Processing (IALP), pages 258-\n264, 2023.\n[Zhang et al., 2023] Chenshuang Zhang, Chaoning Zhang,\nMengchun Zhang, and In So Kweon. Text-to-image dif-\nfusion models in generative ai: A survey, 2023.\n"}
{"page": 1, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 1, "bbox": [{"x": 0.1188850998878479, "y": 0.12483516335487366}, {"x": 0.8799772262573242, "y": 0.12483516335487366}, {"x": 0.8799772262573242, "y": 0.1402197778224945}, {"x": 0.1188850998878479, "y": 0.1402197778224945}], "text": "Benchmarking Large Language Models in Retrieval-Augmented Generation\n"}
{"page": 1, "bbox": [{"x": 0.3668941855430603, "y": 0.1595604419708252}, {"x": 0.38168373703956604, "y": 0.1595604419708252}, {"x": 0.38168373703956604, "y": 0.16615384817123413}, {"x": 0.3668941855430603, "y": 0.16615384817123413}], "text": "1,3\n"}
{"page": 1, "bbox": [{"x": 0.12969283759593964, "y": 0.15868131816387177}, {"x": 0.8703071475028992, "y": 0.1569230705499649}, {"x": 0.8703071475028992, "y": 0.22197802364826202}, {"x": 0.12969283759593964, "y": 0.2237362563610077}], "text": "Jiawei Chen¹³, Hongyu Lin¹*, Xianpei Han¹,2,*, Le Sun¹,2\n¹Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China\n2 State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China\n3 University of Chinese Academy of Sciences, Beijing, China\n"}
{"page": 1, "bbox": [{"x": 0.3424345850944519, "y": 0.224175825715065}, {"x": 0.658703088760376, "y": 0.224175825715065}, {"x": 0.658703088760376, "y": 0.23604395985603333}, {"x": 0.3424345850944519, "y": 0.23604395985603333}], "text": "{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn\n"}
{"page": 1, "bbox": [{"x": 0.25255972146987915, "y": 0.2769230902194977}, {"x": 0.31342434883117676, "y": 0.2769230902194977}, {"x": 0.31342434883117676, "y": 0.2852747142314911}, {"x": 0.25255972146987915, "y": 0.2852747142314911}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.5295790433883667, "y": 0.27560439705848694}, {"x": 0.7059158086776733, "y": 0.27604395151138306}, {"x": 0.7059158086776733, "y": 0.34329670667648315}, {"x": 0.5295790433883667, "y": 0.34285715222358704}], "text": "Noise Robustness\nQuestion\nWho was awarded the 2022\nNobel prize in literature?\nExternal documents contain noises\nFrench author Annie Ernaux\n"}
{"page": 1, "bbox": [{"x": 0.7218430042266846, "y": 0.27604395151138306}, {"x": 0.8873720169067383, "y": 0.2764835059642792}, {"x": 0.8868032097816467, "y": 0.3806593418121338}, {"x": 0.721274197101593, "y": 0.38021978735923767}], "text": "Negative Rejection\nQuestion\nWho was awarded the 2022\nNobel prize in literature?\nExternal documents are all noises\nin Literature for 2021 is...\nThe 2020 Nobel Laureate ...\nRetrieval Augmented\n"}
{"page": 1, "bbox": [{"x": 0.5267349481582642, "y": 0.3556044101715088}, {"x": 0.6973834037780762, "y": 0.3551648259162903}, {"x": 0.6973834037780762, "y": 0.38813185691833496}, {"x": 0.5267349481582642, "y": 0.38857144117355347}], "text": "in Literature for 2021 is...\nRetrieval Augmented\nGeneration\n"}
{"page": 1, "bbox": [{"x": 0.7485779523849487, "y": 0.38285714387893677}, {"x": 0.8014789819717407, "y": 0.38285714387893677}, {"x": 0.8014789819717407, "y": 0.38857144117355347}, {"x": 0.7485779523849487, "y": 0.38857144117355347}], "text": "Generation\n"}
{"page": 1, "bbox": [{"x": 0.5915813446044922, "y": 0.3956044018268585}, {"x": 0.6581342220306396, "y": 0.3956044018268585}, {"x": 0.6581342220306396, "y": 0.40131866931915283}, {"x": 0.5915813446044922, "y": 0.40131866931915283}], "text": "Annie Ernaux\n"}
{"page": 1, "bbox": [{"x": 0.7411831617355347, "y": 0.3951648473739624}, {"x": 0.8924914598464966, "y": 0.39604395627975464}, {"x": 0.8924914598464966, "y": 0.4039560556411743}, {"x": 0.7411831617355347, "y": 0.4030769169330597}], "text": "I can not answer the question...\n"}
{"page": 1, "bbox": [{"x": 0.5312855243682861, "y": 0.4171428680419922}, {"x": 0.6774743795394897, "y": 0.41890108585357666}, {"x": 0.6774743795394897, "y": 0.4281318783760071}, {"x": 0.5312855243682861, "y": 0.4263736307621002}], "text": "Information Integration\n"}
{"page": 1, "bbox": [{"x": 0.5381115078926086, "y": 0.4316483438014984}, {"x": 0.5796359777450562, "y": 0.4316483438014984}, {"x": 0.5796359777450562, "y": 0.4373626410961151}, {"x": 0.5381115078926086, "y": 0.4373626410961151}], "text": "Question\n"}
{"page": 1, "bbox": [{"x": 0.10352673381567001, "y": 0.30109891295433044}, {"x": 0.4624573290348053, "y": 0.30109891295433044}, {"x": 0.4624573290348053, "y": 0.6136263608932495}, {"x": 0.10352673381567001, "y": 0.6136263608932495}], "text": "Retrieval-Augmented Generation (RAG) is a promising ap-\nproach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\non different large language models, which make it challeng-\ning to identify the potential bottlenecks in the capabilities\nof RAG for different LLMs. In this paper, we systemati-\ncally investigate the impact of Retrieval-Augmented Gener-\nation on large language models. We analyze the performance\nof different large language models in 4 fundamental abili-\nties required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robust-\nness. To this end, we establish Retrieval-Augmented Genera-\ntion Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the afore-\nmentioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diag-\nnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\nof noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to ef-\nfectively apply RAG to LLMs.\n"}
{"page": 1, "bbox": [{"x": 0.7184300422668457, "y": 0.41846153140068054}, {"x": 0.8930602669715881, "y": 0.4175824224948883}, {"x": 0.8936291337013245, "y": 0.5221977829933167}, {"x": 0.7189988493919373, "y": 0.5230769515037537}], "text": "Counterfactual Robustness\nQuestion\nWhich city hosted the Olympic\ngames in 2004?\nCounterfactual external documents\n2004 Olympic ...to New York,\nNew York easily defeated...\nRetrieval Augmented\n"}
{"page": 1, "bbox": [{"x": 0.5273037552833557, "y": 0.44087910652160645}, {"x": 0.7093287706375122, "y": 0.44131869077682495}, {"x": 0.7087599635124207, "y": 0.5235164761543274}, {"x": 0.5267349481582642, "y": 0.5230769515037537}], "text": "When were the ChatGPT app for\niOS and api launched?\nExternal documents contain all answers\nOn May 18th, 2023, OpenAI...\nThat changed on March 1, ...\nRetrieval Augmented\n"}
{"page": 1, "bbox": [{"x": 0.7445961236953735, "y": 0.5235164761543274}, {"x": 0.796928346157074, "y": 0.5243955850601196}, {"x": 0.796928346157074, "y": 0.5309889912605286}, {"x": 0.7445961236953735, "y": 0.5301098823547363}], "text": "Generation\n"}
{"page": 1, "bbox": [{"x": 0.5523322224617004, "y": 0.5257142782211304}, {"x": 0.6763367652893066, "y": 0.5252747535705566}, {"x": 0.6763367652893066, "y": 0.5450549721717834}, {"x": 0.5523322224617004, "y": 0.5454944968223572}], "text": "Generation\nMay 18 and March 1.\n"}
{"page": 1, "bbox": [{"x": 0.7417519688606262, "y": 0.5367032885551453}, {"x": 0.894197940826416, "y": 0.5367032885551453}, {"x": 0.894197940826416, "y": 0.542417585849762}, {"x": 0.7417519688606262, "y": 0.542417585849762}], "text": "There are factual errors in the...\n"}
{"page": 1, "bbox": [{"x": 0.5216154456138611, "y": 0.5613186955451965}, {"x": 0.912400484085083, "y": 0.5600000023841858}, {"x": 0.912400484085083, "y": 0.5863736271858215}, {"x": 0.5216154456138611, "y": 0.5876923203468323}], "text": "Figure 1: Illustration of 4 kinds of abilities required for\nretrieval-augmented generation of LLMs.\n"}
{"page": 1, "bbox": [{"x": 0.22980660200119019, "y": 0.6395604610443115}, {"x": 0.3356086313724518, "y": 0.6386812925338745}, {"x": 0.3356086313724518, "y": 0.6492307782173157}, {"x": 0.22980660200119019, "y": 0.6501098871231079}], "text": "Introduction\n"}
{"page": 1, "bbox": [{"x": 0.08759954571723938, "y": 0.6610988974571228}, {"x": 0.47895336151123047, "y": 0.6606593132019043}, {"x": 0.47895336151123047, "y": 0.8386813402175903}, {"x": 0.08759954571723938, "y": 0.8391208648681641}], "text": "Recently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges includ-\ning factual hallucination (Cao et al. 2020; Raunak, Menezes,\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-\ndating (He, Zhang, and Roth 2022), and the lack of domain-\nspecific expertise (Li et al. 2023c; Shen et al. 2023).\nIncorporating external knowledge via information re-\ntrieval, i.e., Retrieval-Augmented Generation (RAG), has\nbeen regarded as a promising way to resolve the above chal-\nlenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\n"}
{"page": 1, "bbox": [{"x": 0.5216154456138611, "y": 0.6246153712272644}, {"x": 0.9135380983352661, "y": 0.6241758465766907}, {"x": 0.9141069650650024, "y": 0.8883516192436218}, {"x": 0.5221843123435974, "y": 0.8887912034988403}], "text": "2022; Izacard et al. 2022). With the help of external knowl-\nedge, LLMs can generate more accurate and reliable re-\nsponses. The most common method is to use a search engine\nas a retriever such as New Bing. Due to the vast amount of\ninformation available on the Internet, using a search engine\ncan provide more real-time information.\nHowever, Retrieval-Augmented Generation brings not\nonly positive effects to LLMs (Liu, Zhang, and Liang 2023;\nMaynez et al. 2020). On one hand, there is a significant\namount of noise information even fake news in the content\navailable on the Internet, which poses challenges for search\nengines in accurately retrieving desirable knowledge. On the\nother hand, LLMs suffer from unreliable generation chal-\nlenge. LLMs can be misled by incorrect information con-\ntained in the context (Bian et al. 2023) and also suffer from\nhallucination during the generation (Adlakha et al. 2023),\nresulting in generating content that goes beyond external in-\nformation. These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Un-\n"}
{"page": 1, "bbox": [{"x": 0.10864619165658951, "y": 0.8545054793357849}, {"x": 0.11319681257009506, "y": 0.8545054793357849}, {"x": 0.11319681257009506, "y": 0.8571428656578064}, {"x": 0.10864619165658951, "y": 0.8571428656578064}], "text": "*\n"}
{"page": 1, "bbox": [{"x": 0.12116041034460068, "y": 0.8545054793357849}, {"x": 0.25881683826446533, "y": 0.8527472615242004}, {"x": 0.25881683826446533, "y": 0.8637362718582153}, {"x": 0.12116041034460068, "y": 0.8654944896697998}], "text": "Corresponding authors.\n"}
{"page": 1, "bbox": [{"x": 0.08759954571723938, "y": 0.8654944896697998}, {"x": 0.14675767719745636, "y": 0.8654944896697998}, {"x": 0.14675767719745636, "y": 0.876043975353241}, {"x": 0.08759954571723938, "y": 0.876043975353241}], "text": "Copyright\n"}
{"page": 1, "bbox": [{"x": 0.08816837519407272, "y": 0.8646153807640076}, {"x": 0.4778156876564026, "y": 0.8637362718582153}, {"x": 0.4778156876564026, "y": 0.8879120945930481}, {"x": 0.08816837519407272, "y": 0.8887912034988403}], "text": "2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n"}
{"page": 1, "bbox": [{"x": 0.48009100556373596, "y": 0.942417562007904}, {"x": 0.5164960026741028, "y": 0.9428571462631226}, {"x": 0.5164960026741028, "y": 0.9507692456245422}, {"x": 0.48009100556373596, "y": 0.9503296613693237}], "text": "17754\n"}
{"page": 2, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 2, "bbox": [{"x": 0.08759954571723938, "y": 0.07120878994464874}, {"x": 0.47895336151123047, "y": 0.07120878994464874}, {"x": 0.47895336151123047, "y": 0.5283516645431519}, {"x": 0.08759954571723938, "y": 0.5283516645431519}], "text": "fortunately, currently there lacks of comprehensive under-\nstanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and im-\nprovement their performance via information retrieval. As a\nresult, there is a pressing need for a comprehensive evalua-\ntion of LLMs on their ability to effectively utilize retrieved\ninformation, as well as their ability to withstand the various\ndrawbacks present in information retrieval.\nTo this end, this paper conducts a comprehensive evalua-\ntion of RAG for current LLMs. Specifically, we create a new\nRetrieval-Augmented Generation Benchmark, namely RGB,\nin both English and Chinese. In order to ensure that the in-\nternal knowledge of LLMs does not introduce bias into the\nevaluation results, RGB chooses to aggregate the latest news\ninformation and constructs queries based on the news infor-\nmation. Then, based on these queries, we use Search API to\nfetch relevant documents and select most relevant snippets\nfrom the content as external retrieved documents. Finally,\nbased on different compositions of query and document-set\npairs, we expand the corpus and divided it into 4 testbeds to\nevaluate the following basic abilities of LLMs according to\nthe common challenges in RAG, as shown in Figure 1:\n• Noise Robustness, which means a LLM can extract use-\nful information from noisy documents. In this paper, we\ndefine noisy documents as those that are relevant to the\nquestion but do not contain any information of the an-\nswer. For the instance in Figure 1, the noisy documents\nrelated to the question \"Who was awarded the 2022 No-\nbel Prize in Literature” include reports about the 2021\nNobel Prize in Literature. To this end, the testbed for\nnoise robustness contains instances whose external doc-\numents contain a certain number of noisy documents\nbased on the desired noise ratio.\n"}
{"page": 2, "bbox": [{"x": 0.520477831363678, "y": 0.07164835184812546}, {"x": 0.9135380983352661, "y": 0.07164835184812546}, {"x": 0.9135380983352661, "y": 0.6241758465766907}, {"x": 0.520477831363678, "y": 0.6241758465766907}], "text": "nAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\n6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\nQwen-7B-Chat (Bai et al. 2023), BELLE-7B (BELLEGroup\n2023). We found that even though RAG can improve the re-\nsponse accuracy of LLMs, they still suffer from the above-\nmentioned challenges significantly. Specifically, we found\nthat even though LLMs demonstrate some level of noise ro-\nbustness, they tend to confuse similar information and fre-\nquently generate inaccurate answers when relevant informa-\ntion exists. For example, when faced with a question about\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\nments about the 2021 Nobel Prize in Literature in external\ndocuments, LLMs may become confused and provide inac-\ncurate answers. Besides, LLMs frequently fail to reject an-\nswering and generate incorrect answers when none of the\nexternal documents contain relevant information. Further-\nmore, LLMs lack the ability to summarize from multiple\ndocuments, and therefore if multiple documents are needed\nto answer a question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about po-\ntential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize the retrieved information\nover their own existing knowledge. The experimental results\nmentioned above highlight the need for further resolution of\nimportant issues in the existing RAG method. Therefore, it\nis crucial to exercise caution and carefully design its usage.\nGenerally speaking, the contributions of this are¹:\npaper\n• We proposed to evaluate four capabilities for retrieval-\naugmented generation of LLMs and created the\nRetrieval-Augmented Generation Benchmark in both En-\nglish and Chinese. To best of our knowledge, it is the first\nbenchmark designed to assess these four capabilities for\nretrieval-augmented generation of LLMs.\n• We evaluated the existing LLMs using RGB and found\nthe limitations of them in the four different abilities.\n• We analyzed the responses of LLMs in RGB and identi-\nfied their current shortcomings as well as suggested di-\nrections for improvement.\n"}
{"page": 2, "bbox": [{"x": 0.095563143491745, "y": 0.5389010906219482}, {"x": 0.10011376440525055, "y": 0.5389010906219482}, {"x": 0.10011376440525055, "y": 0.542417585849762}, {"x": 0.095563143491745, "y": 0.542417585849762}], "text": "•\n"}
{"page": 2, "bbox": [{"x": 0.658703088760376, "y": 0.6386812925338745}, {"x": 0.7758817076683044, "y": 0.6386812925338745}, {"x": 0.7758817076683044, "y": 0.6487911939620972}, {"x": 0.658703088760376, "y": 0.6487911939620972}], "text": "Related Work\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.5345054864883423}, {"x": 0.47895336151123047, "y": 0.5340659618377686}, {"x": 0.4795221984386444, "y": 0.8887912034988403}, {"x": 0.08759954571723938, "y": 0.8892307877540588}], "text": "Negative Rejection, which means that a LLM should re-\nject to answer the question when the required knowledge\nis not present in any retrieved document. The testbed for\nnegative rejection contains instances whose external doc-\numents are only with noisy documents. LLMs are ex-\npected to indicate “insufficient information\" or other re-\njection signals.\n• Information Integration, which evaluates whether\nLLMs can answer complex questions that require inte-\ngrating information from multiple documents. For the in-\nstance in Figure 1, for the question “When were the Chat-\nGPT app for iOS and ChatGPT api launched?”, LLMs\nare expected to provide information of the launch dates\nfor both the ChatGPT iOS app and ChatGPT API. The\ntestbed for information integration contains instances\nthat can only be answered using multiple documents.\n• Counterfactual Robustness, which evaluates whether\nLLMs can identify risks of known factual errors in the\nretrieved documents when the LLMs are given warnings\nabout potential risks in the retrieved information through\ninstruction. The testbed for counterfactual robustness in-\ncludes instances that can be answered directly by the\nLLMs, but the external documents contain factual errors.\nBased on RGB, we conduct evaluation on 6 state-of-\nthe-art large language models including ChatGPT (Ope-\n"}
{"page": 2, "bbox": [{"x": 0.5210466384887695, "y": 0.6584615111351013}, {"x": 0.912400484085083, "y": 0.6589010953903198}, {"x": 0.9118316173553467, "y": 0.8887912034988403}, {"x": 0.520477831363678, "y": 0.8883516192436218}], "text": "Retrieval-augmented models The knowledge stored in\nlarge language models is commonly out-of-date (He, Zhang,\nand Roth 2022) and they also sometimes generate hallu-\ncination (Cao et al. 2020; Raunak, Menezes, and Junczys-\nDowmunt 2021; Ji et al. 2023) i.e., they may generate ir-\nrelevant or factually incorrect contents. By using external\nknowledge as guidance, retrieval-augmented models can\ngenerate more accurate and reliable responses (Guu et al.\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-\naugmented models have achieved remarkable results in var-\nious tasks such as open-domain QA (Izacard and Grave\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\net al. 2019a,b; Peng et al. 2023), domain-specific ques-\ntion answering (Cui et al. 2023) and code generation (Zhou\n¹Our code&data: https://github.com/chen700564/RGB.\n"}
{"page": 2, "bbox": [{"x": 0.48009100556373596, "y": 0.9432967305183411}, {"x": 0.5170648694038391, "y": 0.9432967305183411}, {"x": 0.5170648694038391, "y": 0.9503296613693237}, {"x": 0.48009100556373596, "y": 0.9503296613693237}], "text": "17755\n"}
{"page": 3, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 3, "bbox": [{"x": 0.329351544380188, "y": 0.07780219614505768}, {"x": 0.3498293459415436, "y": 0.07780219614505768}, {"x": 0.3498293459415436, "y": 0.08351648598909378}, {"x": 0.329351544380188, "y": 0.08351648598909378}], "text": "Top2\n"}
{"page": 3, "bbox": [{"x": 0.42036405205726624, "y": 0.07736263424158096}, {"x": 0.44653013348579407, "y": 0.07692307978868484}, {"x": 0.447098970413208, "y": 0.09186813235282898}, {"x": 0.4209328889846802, "y": 0.0923076942563057}], "text": "Top30\nChunk\n"}
{"page": 3, "bbox": [{"x": 0.2713310718536377, "y": 0.07780219614505768}, {"x": 0.2986348271369934, "y": 0.07780219614505768}, {"x": 0.2986348271369934, "y": 0.09274725615978241}, {"x": 0.2713310718536377, "y": 0.09274725615978241}], "text": "Top1\nChunk\n"}
{"page": 3, "bbox": [{"x": 0.3265073895454407, "y": 0.08747252821922302}, {"x": 0.3538111448287964, "y": 0.08747252821922302}, {"x": 0.3538111448287964, "y": 0.09274725615978241}, {"x": 0.3265073895454407, "y": 0.09274725615978241}], "text": "Chunk\n"}
{"page": 3, "bbox": [{"x": 0.1188850998878479, "y": 0.09318681061267853}, {"x": 0.20477814972400665, "y": 0.09318681061267853}, {"x": 0.20477814972400665, "y": 0.11032967269420624}, {"x": 0.1188850998878479, "y": 0.11032967269420624}], "text": "Rerank by dense\nretrieval model\n"}
{"page": 3, "bbox": [{"x": 0.25767919421195984, "y": 0.10153846442699432}, {"x": 0.351535826921463, "y": 0.10109890252351761}, {"x": 0.351535826921463, "y": 0.10681318491697311}, {"x": 0.25767919421195984, "y": 0.10725274682044983}], "text": "Dense retrieval model\n"}
{"page": 3, "bbox": [{"x": 0.5216154456138611, "y": 0.07164835184812546}, {"x": 0.9118316173553467, "y": 0.07208791375160217}, {"x": 0.9118316173553467, "y": 0.16483516991138458}, {"x": 0.5216154456138611, "y": 0.16439560055732727}], "text": "and responsibility of LLMs, M3Exam (Zhang et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Ad-\nlakha et al. (2023) evaluate the RAG of LLMs in exist QA\ndataset. Different from their work, we focus on 4 required\nabilities of RAG and create Retrieval-Augmented Genera-\ntion Benchmark to evaluate the LLMs.\n"}
{"page": 3, "bbox": [{"x": 0.3048919141292572, "y": 0.11912088096141815}, {"x": 0.329351544380188, "y": 0.11912088096141815}, {"x": 0.329351544380188, "y": 0.1261538416147232}, {"x": 0.3048919141292572, "y": 0.1261538416147232}], "text": "Query\n"}
{"page": 3, "bbox": [{"x": 0.3964732587337494, "y": 0.12087912112474442}, {"x": 0.42434585094451904, "y": 0.12087912112474442}, {"x": 0.42434585094451904, "y": 0.1261538416147232}, {"x": 0.3964732587337494, "y": 0.1261538416147232}], "text": "Chunk\n"}
{"page": 3, "bbox": [{"x": 0.255972683429718, "y": 0.14769230782985687}, {"x": 0.4431171715259552, "y": 0.148131862282753}, {"x": 0.4431171715259552, "y": 0.15560439229011536}, {"x": 0.255972683429718, "y": 0.15516483783721924}], "text": "{\"link\":\"https://www.nobelprize.org/prizes/\n"}
{"page": 3, "bbox": [{"x": 0.25654152035713196, "y": 0.15780219435691833}, {"x": 0.3162684738636017, "y": 0.15868131816387177}, {"x": 0.3162684738636017, "y": 0.1652747243642807}, {"x": 0.25654152035713196, "y": 0.16439560055732727}], "text": "medicine/\", ...\n"}
{"page": 3, "bbox": [{"x": 0.1240045502781868, "y": 0.16747252643108368}, {"x": 0.19795222580432892, "y": 0.1683516502380371}, {"x": 0.19738338887691498, "y": 0.18725274503231049}, {"x": 0.12343572080135345, "y": 0.18637362122535706}], "text": "Retrieve using\nsearch engine\n"}
{"page": 3, "bbox": [{"x": 0.37258246541023254, "y": 0.17538461089134216}, {"x": 0.4505119323730469, "y": 0.17450548708438873}, {"x": 0.4505119323730469, "y": 0.18065933883190155}, {"x": 0.37258246541023254, "y": 0.18153846263885498}], "text": "Google Search API\n"}
{"page": 3, "bbox": [{"x": 0.2548350393772125, "y": 0.19164834916591644}, {"x": 0.4544937312602997, "y": 0.19120879471302032}, {"x": 0.4544937312602997, "y": 0.21098901331424713}, {"x": 0.2548350393772125, "y": 0.21142856776714325}], "text": "Query: Who was awarded the 2022 Nobel\nPrize for Physiology and Medicine?\",\n"}
{"page": 3, "bbox": [{"x": 0.5216154456138611, "y": 0.18417581915855408}, {"x": 0.9118316173553467, "y": 0.18329671025276184}, {"x": 0.9118316173553467, "y": 0.257582426071167}, {"x": 0.5216154456138611, "y": 0.25846153497695923}], "text": "Retrieval-Augmented Generation Benchmark\nIn this section, we first introduce the specific retrieval-\naugmented generation abilities we aim to evaluate. Next, we\noutline the process of constructing the RAG benchmark for\nevaluation. Lastly, we present the evaluation metrics.\n"}
{"page": 3, "bbox": [{"x": 0.2548350393772125, "y": 0.22857142984867096}, {"x": 0.4544937312602997, "y": 0.22857142984867096}, {"x": 0.4544937312602997, "y": 0.25670328736305237}, {"x": 0.2548350393772125, "y": 0.25670328736305237}], "text": "\"Question\": \"Who was awarded the 2022\nNobel Prize for Physiology and Medicine?\",\n\"Answer\": [\"Svante Pääbo','Svante Paabo']\n"}
{"page": 3, "bbox": [{"x": 0.11945392191410065, "y": 0.22813187539577484}, {"x": 0.20477814972400665, "y": 0.22857142984867096}, {"x": 0.20477814972400665, "y": 0.2571428716182709}, {"x": 0.11945392191410065, "y": 0.25670328736305237}], "text": "Data adjustment\nand filtering by\nHuman\n"}
{"page": 3, "bbox": [{"x": 0.25540387630462646, "y": 0.2602197825908661}, {"x": 0.26052331924438477, "y": 0.2602197825908661}, {"x": 0.26052331924438477, "y": 0.26725274324417114}, {"x": 0.25540387630462646, "y": 0.26725274324417114}], "text": "}\n"}
{"page": 3, "bbox": [{"x": 0.2582480013370514, "y": 0.2821978032588959}, {"x": 0.43856656551361084, "y": 0.282637357711792}, {"x": 0.43856656551361084, "y": 0.30065932869911194}, {"x": 0.2582480013370514, "y": 0.3002197742462158}], "text": "Related event: ... \\nQuestion:...\\nKey\ninformation:...\n"}
{"page": 3, "bbox": [{"x": 0.3748577833175659, "y": 0.30945053696632385}, {"x": 0.44766780734062195, "y": 0.30901098251342773}, {"x": 0.44766780734062195, "y": 0.31560438871383667}, {"x": 0.3748577833175659, "y": 0.3160439431667328}], "text": "gpt-3.5-turbo api\n"}
{"page": 3, "bbox": [{"x": 0.11319681257009506, "y": 0.31516483426094055}, {"x": 0.21160408854484558, "y": 0.31560438871383667}, {"x": 0.21160408854484558, "y": 0.3340659439563751}, {"x": 0.11319681257009506, "y": 0.3336263597011566}], "text": "Data generation by\nChatGPT\n"}
{"page": 3, "bbox": [{"x": 0.2542662024497986, "y": 0.32659339904785156}, {"x": 0.44141069054603577, "y": 0.32659339904785156}, {"x": 0.44141069054603577, "y": 0.34593406319618225}, {"x": 0.2542662024497986, "y": 0.34593406319618225}], "text": "We simulate the process of a user\nquerying and obtaining information.....\n"}
{"page": 3, "bbox": [{"x": 0.2542662024497986, "y": 0.36000001430511475}, {"x": 0.42150169610977173, "y": 0.36000001430511475}, {"x": 0.42150169610977173, "y": 0.3665934205055237}, {"x": 0.2542662024497986, "y": 0.3665934205055237}], "text": "News: The 2022 Nobel Prize for ...\n"}
{"page": 3, "bbox": [{"x": 0.12229806929826736, "y": 0.38813185691833496}, {"x": 0.2002275288105011, "y": 0.38769230246543884}, {"x": 0.2002275288105011, "y": 0.3942857086658478}, {"x": 0.12229806929826736, "y": 0.3947252631187439}], "text": "News Collection\n"}
{"page": 3, "bbox": [{"x": 0.2508532404899597, "y": 0.3841758370399475}, {"x": 0.4328782856464386, "y": 0.38285714387893677}, {"x": 0.4328782856464386, "y": 0.4030769169330597}, {"x": 0.2508532404899597, "y": 0.40439561009407043}], "text": "News about The 2022 Nobel Prize for\nPhysiology and Medicine\n"}
{"page": 3, "bbox": [{"x": 0.5193401575088501, "y": 0.27296704053878784}, {"x": 0.912400484085083, "y": 0.271208792924881}, {"x": 0.9141069650650024, "y": 0.5265933871269226}, {"x": 0.5210466384887695, "y": 0.5283516645431519}], "text": "Required Abilities of RAG\nExternal knowledge is the key to resolving the problems\nof LLMs such as hallucination and outdated knowledge,\nwhich can make LLMs generate more accurate and reliable\nresponses through retrieval-augmented generation (RAG).\nHowever, LLMs cannot always response as expected with\nRAG. For one thing, there are numerous irrelevant docu-\nments and false information on the Internet. Incorporating\nthese external documents into LLMs could have a detrimen-\ntal effect. For anthoer, LLMs suffer from the unreliable\neration challenge. The generation of LLMs is often unpre-\ndictable, and we cannot guarantee that they will utilize the\nuseful information entailed in the external documents. Ad-\nditionally, LLMs can easily be misled by incorrect infor-\nmation in the document. To this end, we build Retrieval-\nAugmented Generation Benchmark (RGB) to evaluate the\nretrieval-augmented generation of LLMs, and we concern\nabout 4 specific abilities:\n"}
{"page": 3, "bbox": [{"x": 0.8828213810920715, "y": 0.4083516597747803}, {"x": 0.9101251363754272, "y": 0.40703296661376953}, {"x": 0.9106939435005188, "y": 0.4149450659751892}, {"x": 0.8833901882171631, "y": 0.41626372933387756}], "text": "gen-\n"}
{"page": 3, "bbox": [{"x": 0.08816837519407272, "y": 0.4224175810813904}, {"x": 0.47838452458381653, "y": 0.4232966899871826}, {"x": 0.47838452458381653, "y": 0.49098899960517883}, {"x": 0.08816837519407272, "y": 0.4901098906993866}], "text": "Figure 2: The process of data generation. Firstly, we use\nmodels to extract (event, question, answer) from news ar-\nticles. Next, we utilize search engines to retrieve relevant\nweb pages. Finally, a dense retrieval model is employed to\nre-rank the content of these web pages.\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.5217582583427429}, {"x": 0.47838452458381653, "y": 0.5217582583427429}, {"x": 0.47838452458381653, "y": 0.8887912034988403}, {"x": 0.08703071624040604, "y": 0.8887912034988403}], "text": "et al. 2023b). Recently, with the development of large mod-\nels, a series of retrieval-enhanced tools and products have\ngained widespread attention, such as ChatGPT retrieval plu-\ngin, Langchain, New Bing, etc. However, in real-world sce-\nnarios, the retrieved text inevitably contains noise. There-\nfore, in this paper we conducted a systematic evaluation and\nanalysis of retrieval-augmented generation in LLMs.\nEvaluation of LLMs Evaluating LLMs has received sig-\nnificant attention due to their remarkable general capabil-\nity (Chang et al. 2023). It enables us to gain a deeper under-\nstanding of the specific abilities and limitations of LLMs,\nwhile also providing valuable guidance for future research.\nIn the past, benchmarks such as GLUE (Wang et al. 2019b)\nand SuperCLUE (Wang et al. 2019a) primarily focused on\nevaluating NLP tasks, particularly in natural language un-\nderstanding. However, these evaluations often fail to fully\ncapture the capabilities of LLMs. MMLU (Hendrycks et al.\n2021) was then proposed to measure the knowledge acquired\nby language models when pre-training. Recently, with the\ndevelopment of LLMs, a series of general evaluation bench-\nmarks have emerged, such as AGIEval (Zhong et al. 2023),\nC-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),\nOpenLLM Leaderboard (Edward Beeching 2023), etc. In\naddition to general abilities, there are also specific bench-\nmarks that focus on evaluating the capabilities of models.\nFor example, CValues (Xu et al. 2023a) focuses on the safety\n"}
{"page": 3, "bbox": [{"x": 0.520477831363678, "y": 0.5296703577041626}, {"x": 0.914675772190094, "y": 0.5292307734489441}, {"x": 0.9152445793151855, "y": 0.8865934014320374}, {"x": 0.5210466384887695, "y": 0.8870329856872559}], "text": "Noise Robustness is the robustness of LLMs in noisy\ndocuments. As retrievers are not perfect, the external knowl-\nedge they retrieve often contains a significant amount of\nnoise, i.e., documents which are relevant to the question but\ndo not contain any information about the answer. To effec-\ntively answer user questions, LLMs must be able to extract\nthe necessary information from documents despite there are\nnoisy documents.\nNegative Rejection is a measure of whether LLMs can\ndecline to answer a question when none of the contexts pro-\nvide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the an-\nswers. In these cases, it is important for the model to have\nthe capability to reject recognition and avoid generating mis-\nleading content.\nInformation Integration is a capacity to integrate an-\nswers from multiple documents. In many cases, the an-\nswer to a question may be contained in multiple documents.\nFor example, for the question \"Who are the champions of\nthe U.S. Open 2022 men's and women's singles?\", the two\nchampions may be mentioned in different documents. In or-\nder to provide better answers to complex questions, it is nec-\nessary for LLMs to have the ability to integrate information.\nCounterfactual Robustness refers to a capacity to han-\ndle errors in external knowledge. In the real world, there is\nan abundance of false information on the internet. Please\n"}
{"page": 3, "bbox": [{"x": 0.48009100556373596, "y": 0.9432967305183411}, {"x": 0.5164960026741028, "y": 0.9432967305183411}, {"x": 0.5164960026741028, "y": 0.9503296613693237}, {"x": 0.48009100556373596, "y": 0.9503296613693237}], "text": "17756\n"}
{"page": 4, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.07032967358827591}, {"x": 0.47838452458381653, "y": 0.06901098787784576}, {"x": 0.47895336151123047, "y": 0.16439560055732727}, {"x": 0.08759954571723938, "y": 0.16571427881717682}], "text": "note that we only evaluate the situation that LLMs are given\nwarnings about potential risks in the retrieved information\nthrough instruction.\nIn real-world scenarios, it is not possible to obtain per-\nfect documents with all the necessary external knowledge.\nTherefore, evaluating these four abilities of the model be-\ncomes essential in order to measure the RAG of LLMs.\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.18109890818595886}, {"x": 0.22923776507377625, "y": 0.18109890818595886}, {"x": 0.22923776507377625, "y": 0.190769225358963}, {"x": 0.08759954571723938, "y": 0.190769225358963}], "text": "Data Construction\n"}
{"page": 4, "bbox": [{"x": 0.5210466384887695, "y": 0.07208791375160217}, {"x": 0.912400484085083, "y": 0.07208791375160217}, {"x": 0.912400484085083, "y": 0.3160439431667328}, {"x": 0.5210466384887695, "y": 0.3160439431667328}], "text": "edge of the model. Based on the aforementioned generated\nquestions mentioned above, we adopt ChatGPT to automat-\nically generate its known knowledge. Specifically, we use\nprompts to allow the model to generate both questions and\nanswers that are already known. For example, based on the\nquestion \"Who was awarded the 2022 Nobel Prize for Phys-\niology and Medicine?”, the model will generate the known\nquestion \"Who was awarded the 2021 Nobel Prize in Lit-\nerature?\" and answer \"Abdulrazak Gurnah\". We then man-\nually verified the generated answers, and retrieve relevant\ndocuments as described above. In order to make documents\ncontain factual errors, we manually modify the answers and\nreplace the corresponding parts in the document.\nFinally, we collect totally 600 base questions in RGB,\nand 200 additional questions for the information integration\nability and 200 additional questions for counterfactual ro-\nbustness ability. Half of the instances are in English, and the\nother half are in Chinese.\n"}
{"page": 4, "bbox": [{"x": 0.5221843123435974, "y": 0.33230769634246826}, {"x": 0.6678043007850647, "y": 0.33230769634246826}, {"x": 0.6678043007850647, "y": 0.3415384590625763}, {"x": 0.5221843123435974, "y": 0.3415384590625763}], "text": "Evaluation Metrics\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.19824175536632538}, {"x": 0.47895336151123047, "y": 0.19824175536632538}, {"x": 0.47838452458381653, "y": 0.8879120945930481}, {"x": 0.08703071624040604, "y": 0.8879120945930481}], "text": "Inspired by previous benchmarks for LLMs, RGB utilizes\na question-answering format for evaluation. We evaluate the\nLLMs by judging the retrieval-augmented responses of them\nto the questions. To simulate real-world scenarios, we con-\nstruct question and answer data using actual news articles.\nDue to the abundance of knowledge contained within the\nLLMs there is a potential for bias when measuring the first\nthree abilities. To mitigate this, the instances of RGB are\nconstructed by latest news articles. Additionally, we retrieve\nexternal documents from Internet through search engines.\nFinally, we expand the corpus and divided it into 4 testbeds\nto evaluate the above basic abilities of LLMs. The overall\nprocedure of our data construction is illustrated in Figure 2.\nQA instances generation. We first collect latest news ar-\nticles and use prompts to make ChatGPT generate events,\nquestions, and answers for each articles. For example, as\nshown in the Figure 2, for a report about \"The 2022 Nobel\nPrize\", ChatGPT will generate corresponding event, ques-\ntion and provide key information for answering it. By gen-\nerating events, the model is able to preliminarily filter out\nnews articles that do not contain any events. After genera-\ntion, we manually check the answer and filter out data that\nis difficult to retrieve through search engines.\nRetrieve using search engine. For each query, we use\nGoogle's API to fetch 10 relevant web pages and extract cor-\nresponding snippets of text from them. Simultaneously, we\nread these web pages and convert their textual content into\ntext chunks with a maximum length of 300 tokens. Using an\nopen-source dense retriever, we select the top-30 text chunks\nthat match the query most effectively. These retrieved text\nchunks, along with the snippets provided by the search API,\nwill serve as our external documents. These documents will\nbe divided into positive documents and negative documents\nbased on whether they contain the answer.\nTestbeds construction for each ability. We expand the\ncorpus and divided it into 4 testbeds to evaluate the above\nbasic abilities of LLMs. To evaluate the noise robustness,\nwe sample varying numbers of negative documents ac-\ncording to the desired ratio of noises. For negative rejec-\ntion, all the external documents are sampled from negative\ndocuments. For the information integration ability, we fur-\nther construct data based on the above generated questions.\nThis involves expanding or rewriting these questions so that\ntheir answers encompass multiple aspects. For example, the\nquestion \"Who won the MVP of Super Bowl 2023?\" can\nbe rewrite as \"Who won the MVPs of Super Bowl 2022\nand 2023?\". Consequently, answering such questions re-\nquires utilizing information from various documents. Dif-\nferent from the first three abilities, the data of counterfactual\nrobustness is constructed solely based on the internal knowl-\n"}
{"page": 4, "bbox": [{"x": 0.5210466384887695, "y": 0.34857141971588135}, {"x": 0.912400484085083, "y": 0.34813186526298523}, {"x": 0.9129692912101746, "y": 0.8303296566009521}, {"x": 0.5216154456138611, "y": 0.8307692408561707}], "text": "The core of this benchmark is to evaluate whether LLMs can\nutilize the provided external documents to acquire knowl-\nedge and generate reasonable answers. We evaluate the re-\nsponses of LLMs in order to measure above-mentioned four\nabilities of them.\nAccuracy is used to measure noise robustness and infor-\nmation integration. We employ an exact matching approach\nwhere if the generated text contains an exact match to the\nanswer, it is considered as a correct answer.\nRejection rate is used to measure negative rejection.\nWhen only noisy documents are provided, LLMs should\noutput the specific content – “I can not answer the question\nbecause of the insufficient information in documents.\" (We\nuse instructions to inform the model.). If the model gener-\nates this content, it indicates a successful rejection.\nError detection rate measures whether the model can\ndetect the factual errors in the documents for counterfactual\nrobustness. When the provided documents contain factual\nerrors, the model should output the specific content – “There\nare factual errors in the provided documents.\" (We use in-\nstructions to inform the model.). If the model generates this\ncontent, it indicates that the model has detected erroneous\ninformation in the document.\nError correction rate measures whether the model can\nprovide the correct answer after identifying errors for coun-\nterfactual robustness. The model is asked to generate the cor-\nrect answer after identifying the factual errors. If the model\ngenerates the correct answer, it indicates that the model is\ncapable of correcting errors in the document.\nConsidering that LLMs may not fully adhere to instruc-\ntions, for rejection rate and error detection rate, we also use\nChatGPT to conduct additional evaluation of the responses.\nSpecifically, we prompt ChatGPT to determine if the re-\nsponses can reflect information that is not present in the doc-\nument or identify any factual errors.\n"}
{"page": 4, "bbox": [{"x": 0.6632537245750427, "y": 0.845714271068573}, {"x": 0.7701933979988098, "y": 0.8461538553237915}, {"x": 0.7701933979988098, "y": 0.8593406677246094}, {"x": 0.6632537245750427, "y": 0.8589010834693909}], "text": "Experiments\n"}
{"page": 4, "bbox": [{"x": 0.5216154456138611, "y": 0.8637362718582153}, {"x": 0.9112628102302551, "y": 0.8637362718582153}, {"x": 0.9112628102302551, "y": 0.8883516192436218}, {"x": 0.5216154456138611, "y": 0.8883516192436218}], "text": "In this section, we evaluate the performance of various\nLLMs, analyze and discuss the results, summarizing the\n"}
{"page": 4, "bbox": [{"x": 0.48009100556373596, "y": 0.9428571462631226}, {"x": 0.5164960026741028, "y": 0.9428571462631226}, {"x": 0.5164960026741028, "y": 0.9503296613693237}, {"x": 0.48009100556373596, "y": 0.9503296613693237}], "text": "17757\n"}
{"page": 5, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 5, "bbox": [{"x": 0.7201365232467651, "y": 0.072967030107975}, {"x": 0.7673492431640625, "y": 0.07340659201145172}, {"x": 0.7673492431640625, "y": 0.0813186839222908}, {"x": 0.7201365232467651, "y": 0.08087912201881409}], "text": "Chinese\n"}
{"page": 5, "bbox": [{"x": 0.21387940645217896, "y": 0.09098900854587555}, {"x": 0.28327643871307373, "y": 0.09054945409297943}, {"x": 0.28327643871307373, "y": 0.09890110045671463}, {"x": 0.21387940645217896, "y": 0.09934066236019135}], "text": "Noise Ratio\n"}
{"page": 5, "bbox": [{"x": 0.43230944871902466, "y": 0.09186813235282898}, {"x": 0.4510807693004608, "y": 0.09142857044935226}, {"x": 0.4510807693004608, "y": 0.09890110045671463}, {"x": 0.4328782856464386, "y": 0.09934066236019135}], "text": "0.2\n"}
{"page": 5, "bbox": [{"x": 0.7383390069007874, "y": 0.09186813235282898}, {"x": 0.7571103572845459, "y": 0.09186813235282898}, {"x": 0.7571103572845459, "y": 0.09890110045671463}, {"x": 0.7383390069007874, "y": 0.09890110045671463}], "text": "0.4\n"}
{"page": 5, "bbox": [{"x": 0.7895335555076599, "y": 0.09186813235282898}, {"x": 0.8538111448287964, "y": 0.09186813235282898}, {"x": 0.8538111448287964, "y": 0.09890110045671463}, {"x": 0.7895335555076599, "y": 0.09890110045671463}], "text": "0.6 0.8\n"}
{"page": 5, "bbox": [{"x": 0.3873720169067383, "y": 0.09186813235282898}, {"x": 0.3964732587337494, "y": 0.09186813235282898}, {"x": 0.3964732587337494, "y": 0.09934066236019135}, {"x": 0.3873720169067383, "y": 0.09934066236019135}], "text": "0\n"}
{"page": 5, "bbox": [{"x": 0.5346985459327698, "y": 0.09186813235282898}, {"x": 0.5540387034416199, "y": 0.09186813235282898}, {"x": 0.5540387034416199, "y": 0.09934066236019135}, {"x": 0.5346985459327698, "y": 0.09934066236019135}], "text": "0.6\n"}
{"page": 5, "bbox": [{"x": 0.6416382193565369, "y": 0.09186813235282898}, {"x": 0.6507394909858704, "y": 0.09186813235282898}, {"x": 0.6507394909858704, "y": 0.09934066236019135}, {"x": 0.6416382193565369, "y": 0.09934066236019135}], "text": "0\n"}
{"page": 5, "bbox": [{"x": 0.6877133250236511, "y": 0.09186813235282898}, {"x": 0.7064846158027649, "y": 0.09186813235282898}, {"x": 0.7064846158027649, "y": 0.09934066236019135}, {"x": 0.6877133250236511, "y": 0.09934066236019135}], "text": "0.2\n"}
{"page": 5, "bbox": [{"x": 0.585324227809906, "y": 0.0923076942563057}, {"x": 0.6040955781936646, "y": 0.0923076942563057}, {"x": 0.6040955781936646, "y": 0.09934066236019135}, {"x": 0.585324227809906, "y": 0.09934066236019135}], "text": "0.8\n"}
{"page": 5, "bbox": [{"x": 0.13253697752952576, "y": 0.10901098698377609}, {"x": 0.28327643871307373, "y": 0.11032967269420624}, {"x": 0.28327643871307373, "y": 0.12087912112474442}, {"x": 0.13253697752952576, "y": 0.11956044286489487}], "text": "ChatGPT (OpenAI 2022)\n"}
{"page": 5, "bbox": [{"x": 0.13253697752952576, "y": 0.12131868302822113}, {"x": 0.32366326451301575, "y": 0.12219779938459396}, {"x": 0.32366326451301575, "y": 0.1318681389093399}, {"x": 0.13253697752952576, "y": 0.13098901510238647}], "text": "ChatGLM-6B (THUDM 2023a)\n"}
{"page": 5, "bbox": [{"x": 0.3737201392650604, "y": 0.072967030107975}, {"x": 0.8663253784179688, "y": 0.07340659201145172}, {"x": 0.8663253784179688, "y": 0.1819780170917511}, {"x": 0.3737201392650604, "y": 0.18153846263885498}], "text": "English\n0.4\n96.33 94.67 94.00 90.00 76.00 95.67 94.67 91.00 87.67 70.67\n93.67 90.67 89.33 84.67 70.67 94.33 90.67 89.00 82.33 69.00\n91.33 89.67 83.00 77.33 57.33 86.67 82.33 76.67 72.33 54.00\n87.67 83.33 86.00 82.33 60.33 85.67 82.67 77.00 69.33 49.67\n94.33 91.67 91.00 87.67 73.67 94.00 92.33 88.00 84.33 68.67\n83.33 81.00 79.00 71.33 64.67 92.00 88.67 85.33 78.33 67.68\n"}
{"page": 5, "bbox": [{"x": 0.13253697752952576, "y": 0.134505495429039}, {"x": 0.3566552996635437, "y": 0.13494504988193512}, {"x": 0.3566552996635437, "y": 0.18329671025276184}, {"x": 0.13253697752952576, "y": 0.18285714089870453}], "text": "ChatGLM2-6B (THUDM 2023b)\nVicuna-7B-v1.3 (Chiang et al. 2023)\nQwen-7B-Chat (Bai et al. 2023)\nBELLE-7B-2M (BELLEGroup 2023)\n"}
{"page": 5, "bbox": [{"x": 0.08816837519407272, "y": 0.20000000298023224}, {"x": 0.9118316173553467, "y": 0.19824175536632538}, {"x": 0.9118316173553467, "y": 0.22505494952201843}, {"x": 0.08816837519407272, "y": 0.2268131822347641}], "text": "Table 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\nincreasing noise rate poses a challenge for RAG in LLMs.\n"}
{"page": 5, "bbox": [{"x": 0.46700796484947205, "y": 0.2461538463830948}, {"x": 0.6149032711982727, "y": 0.24791209399700165}, {"x": 0.6149032711982727, "y": 0.25846153497695923}, {"x": 0.46700796484947205, "y": 0.25670328736305237}], "text": "Evidence uncertainty.\n"}
{"page": 5, "bbox": [{"x": 0.6791808605194092, "y": 0.24703297019004822}, {"x": 0.8850967288017273, "y": 0.24527472257614136}, {"x": 0.8856655359268188, "y": 0.2892307639122009}, {"x": 0.6797497272491455, "y": 0.2909890115261078}], "text": "Concept confusion.\nWhat was Tesla's revenue in Q1\n2022?\n"}
{"page": 5, "bbox": [{"x": 0.18941979110240936, "y": 0.2461538463830948}, {"x": 0.3879408538341522, "y": 0.24571429193019867}, {"x": 0.3879408538341522, "y": 0.2914285659790039}, {"x": 0.18941979110240936, "y": 0.2918681204319}], "text": "Long-distance information.\nWho did Iga Swiatek defeat to\nwin the Qatar Open 2022?\n"}
{"page": 5, "bbox": [{"x": 0.1046643927693367, "y": 0.2751648426055908}, {"x": 0.16496017575263977, "y": 0.2747252881526947}, {"x": 0.16496017575263977, "y": 0.28483515977859497}, {"x": 0.1046643927693367, "y": 0.2852747142314911}], "text": "Question\n"}
{"page": 5, "bbox": [{"x": 0.4197952151298523, "y": 0.2751648426055908}, {"x": 0.6615471839904785, "y": 0.2751648426055908}, {"x": 0.6615471839904785, "y": 0.30945053696632385}, {"x": 0.4197952151298523, "y": 0.30945053696632385}], "text": "What is the name of Apple's headset?\nVision Pro\n"}
{"page": 5, "bbox": [{"x": 0.18941979110240936, "y": 0.30109891295433044}, {"x": 0.2986348271369934, "y": 0.30065932869911194}, {"x": 0.2986348271369934, "y": 0.30901098251342773}, {"x": 0.18941979110240936, "y": 0.30945053696632385}], "text": "Anett Kontaveit\n"}
{"page": 5, "bbox": [{"x": 0.6814562082290649, "y": 0.30153846740722656}, {"x": 0.7633674740791321, "y": 0.30153846740722656}, {"x": 0.7633674740791321, "y": 0.30901098251342773}, {"x": 0.6814562082290649, "y": 0.30901098251342773}], "text": "18.76 billion\n"}
{"page": 5, "bbox": [{"x": 0.10864619165658951, "y": 0.30153846740722656}, {"x": 0.1615472137928009, "y": 0.30153846740722656}, {"x": 0.1615472137928009, "y": 0.30945053696632385}, {"x": 0.10864619165658951, "y": 0.30945053696632385}], "text": "Answer\n"}
{"page": 5, "bbox": [{"x": 0.42036405205726624, "y": 0.32087913155555725}, {"x": 0.5375426411628723, "y": 0.32087913155555725}, {"x": 0.5375426411628723, "y": 0.32923075556755066}, {"x": 0.42036405205726624, "y": 0.32923075556755066}], "text": "Positive document\n"}
{"page": 5, "bbox": [{"x": 0.6808874011039734, "y": 0.32087913155555725}, {"x": 0.8976109027862549, "y": 0.3199999928474426}, {"x": 0.8976109027862549, "y": 0.35780221223831177}, {"x": 0.6808874011039734, "y": 0.358681321144104}], "text": "Positive document\nTesla, Inc. reported Q1 FY 2022\n... revenues of $18.76 billion\n"}
{"page": 5, "bbox": [{"x": 0.18941979110240936, "y": 0.32087913155555725}, {"x": 0.39874857664108276, "y": 0.32087913155555725}, {"x": 0.39874857664108276, "y": 0.358681321144104}, {"x": 0.18941979110240936, "y": 0.358681321144104}], "text": "Positive document\nSwiatek entered into the Qatar\nOpen...won... Anett Kontaveit\n"}
{"page": 5, "bbox": [{"x": 0.4197952151298523, "y": 0.3336263597011566}, {"x": 0.6575654149055481, "y": 0.3336263597011566}, {"x": 0.6575654149055481, "y": 0.3582417666912079}, {"x": 0.4197952151298523, "y": 0.3582417666912079}], "text": "Apple unveiled a costly augmented-\nreality headset called the Vision Pro\n"}
{"page": 5, "bbox": [{"x": 0.1188850998878479, "y": 0.3617582321166992}, {"x": 0.15187713503837585, "y": 0.36307692527770996}, {"x": 0.15130829811096191, "y": 0.37098902463912964}, {"x": 0.11831627041101456, "y": 0.3696703314781189}], "text": "Docs\n"}
{"page": 5, "bbox": [{"x": 0.6797497272491455, "y": 0.3749450445175171}, {"x": 0.9038680195808411, "y": 0.37362638115882874}, {"x": 0.9044368863105774, "y": 0.4312087893486023}, {"x": 0.6803185343742371, "y": 0.43252748250961304}], "text": "Negative document\n...earnings for 2022 ... Automotive\nrevenue reached $16.86 billion\nin Q1 2022 was $16.86 billion.\n"}
{"page": 5, "bbox": [{"x": 0.41922640800476074, "y": 0.37362638115882874}, {"x": 0.6422070264816284, "y": 0.3731868267059326}, {"x": 0.6422070264816284, "y": 0.43296703696250916}, {"x": 0.41922640800476074, "y": 0.4334065914154053}], "text": "Negative document\nis what Gurman believes will be\ncalled Apple Reality Pro. ...\nheadset is Apple Reality Pro.\n"}
{"page": 5, "bbox": [{"x": 0.0995449349284172, "y": 0.37450549006462097}, {"x": 0.41296929121017456, "y": 0.3731868267059326}, {"x": 0.4135380983352661, "y": 0.4334065914154053}, {"x": 0.10011376440525055, "y": 0.434725284576416}], "text": "Negative document\nshe defeated Ons Jabeur 6-2,\n7-6(5) to win the 2022 US Open\nResponses Iga Swiatek defeated Ons Jabeur |\n"}
{"page": 5, "bbox": [{"x": 0.08759954571723938, "y": 0.44967031478881836}, {"x": 0.9118316173553467, "y": 0.4514285624027252}, {"x": 0.9118316173553467, "y": 0.4927472472190857}, {"x": 0.08759954571723938, "y": 0.49098899960517883}], "text": "Table 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses\nare generated by ChatGLM2-6B. The bold text indicates the matching parts between the document and the question or answer,\nwhile the italicized text highlights the non-matching parts.\n"}
{"page": 5, "bbox": [{"x": 0.08816837519407272, "y": 0.5182417631149292}, {"x": 0.4778156876564026, "y": 0.5208791494369507}, {"x": 0.47724688053131104, "y": 0.5472527742385864}, {"x": 0.08759954571723938, "y": 0.5446153879165649}], "text": "main challenges that existing LLMs encounter when using\nexternal knowledge.\n"}
{"page": 5, "bbox": [{"x": 0.08816837519407272, "y": 0.5643956065177917}, {"x": 0.1484641581773758, "y": 0.5648351907730103}, {"x": 0.1484641581773758, "y": 0.5758242011070251}, {"x": 0.08816837519407272, "y": 0.5753846168518066}], "text": "Settings\n"}
{"page": 5, "bbox": [{"x": 0.08759954571723938, "y": 0.5863736271858215}, {"x": 0.47838452458381653, "y": 0.5863736271858215}, {"x": 0.47838452458381653, "y": 0.696703314781189}, {"x": 0.08759954571723938, "y": 0.696703314781189}], "text": "Task formats. We provide 5 external documents for each\nquestion. In our experiments on noise robustness, we evalu-\nate scenarios with noise ratios ranging from 0 to 0.8.\nModels We evaluate 6 state-of-the-art LLMs includ-\ning ChatGPT (OpenAI 2022) (gpt-3.5-turbo), ChatGLM-\n6B (THUDM 2023a), ChatGLM2-6B (THUDM 2023b),\nVicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-Chat (Bai\net al. 2023), BELLE-7B-2M (BELLEGroup 2023).\n"}
{"page": 5, "bbox": [{"x": 0.5216154456138611, "y": 0.5191208720207214}, {"x": 0.9129692912101746, "y": 0.5191208720207214}, {"x": 0.9129692912101746, "y": 0.8887912034988403}, {"x": 0.5216154456138611, "y": 0.8887912034988403}], "text": "decreased from 96.33% to 76.00%, while the performance\nof ChatGLM2-6B has decreased from 91.33% to 57.33%.\nError Analysis To better comprehend the negative impact\nof noise on model generation, we examined the incorrect an-\nswers and found that these errors typically originate from\nthree reasons, as shown in Table 2.\n(1) Long-distance information. LLMs often face diffi-\nculty in identifying the correct answer from external docu-\nments when the information related to the question is distant\nfrom the information related to the answer. This scenario\nis quite common as longer texts are frequently encountered\non the internet. In such cases, it is typical for the question's\ninformation to be initially presented at the start of the doc-\nument and subsequently referred to using pronouns. In Ta-\nble 2, the question information (“Qatar Open 2022\") is only\nmentioned once at the beginning and is far from where the\nanswer text “Anett Kontaveit” appears. This situation may\ncause LLMs to depend on information from other docu-\nments and create false impressions, i.e., hallucination.\n(2) Evidence uncertainty. Before highly anticipated\nevents, like the release of new Apple products or the an-\nnouncement of the Oscars, there is often a significant\namount of speculative information circulating on the inter-\nnet. Although the relevant documents explicitly state that\nit is uncertain or speculative content, they can still impact\non the retrieval-augmented generation of LLMs. In Table 2,\n"}
{"page": 5, "bbox": [{"x": 0.08816837519407272, "y": 0.7147252559661865}, {"x": 0.30318543314933777, "y": 0.7147252559661865}, {"x": 0.30318543314933777, "y": 0.7243956327438354}, {"x": 0.08816837519407272, "y": 0.7243956327438354}], "text": "Results on Noise Robustness\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.7362637519836426}, {"x": 0.47838452458381653, "y": 0.7362637519836426}, {"x": 0.47838452458381653, "y": 0.8892307877540588}, {"x": 0.08703071624040604, "y": 0.8892307877540588}], "text": "We evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table 1.\nWe can see that:\n(1) RAG can effect improve the responses of LLMs.\nLLMs have shown strong performance even in the presence\nof noise, indicating that RAG is a promising way for LLMs\nto generate accurate and reliable responses.\n(2) The increasing noise rate poses a challenge for\nRAG in LLMs. Specifically, when the noise ratio exceeds\n80%, the accuracy decreases significantly at a significance\nlevel of 0.05. For example, the performance of ChatGPT has\n"}
{"page": 5, "bbox": [{"x": 0.48009100556373596, "y": 0.9432967305183411}, {"x": 0.5170648694038391, "y": 0.9432967305183411}, {"x": 0.5170648694038391, "y": 0.9503296613693237}, {"x": 0.48009100556373596, "y": 0.9503296613693237}], "text": "17758\n"}
{"page": 6, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 6, "bbox": [{"x": 0.372013658285141, "y": 0.07340659201145172}, {"x": 0.4260523319244385, "y": 0.07384615391492844}, {"x": 0.4260523319244385, "y": 0.08307692408561707}, {"x": 0.372013658285141, "y": 0.08263736218214035}], "text": "Chinese\n"}
{"page": 6, "bbox": [{"x": 0.668373167514801, "y": 0.072967030107975}, {"x": 0.7320818901062012, "y": 0.07252747565507889}, {"x": 0.7320818901062012, "y": 0.0839560404419899}, {"x": 0.668373167514801, "y": 0.08439560234546661}], "text": "| Answer\n"}
{"page": 6, "bbox": [{"x": 0.57337886095047, "y": 0.07384615391492844}, {"x": 0.6319681406021118, "y": 0.07384615391492844}, {"x": 0.6319681406021118, "y": 0.08351648598909378}, {"x": 0.57337886095047, "y": 0.08351648598909378}], "text": "Question\n"}
{"page": 6, "bbox": [{"x": 0.7895335555076599, "y": 0.07428571581840515}, {"x": 0.852104663848877, "y": 0.07428571581840515}, {"x": 0.852104663848877, "y": 0.08439560234546661}, {"x": 0.7895335555076599, "y": 0.08439560234546661}], "text": "Response\n"}
{"page": 6, "bbox": [{"x": 0.13481228053569794, "y": 0.07428571581840515}, {"x": 0.2081911265850067, "y": 0.07428571581840515}, {"x": 0.2081911265850067, "y": 0.08483516424894333}, {"x": 0.13481228053569794, "y": 0.08483516424894333}], "text": "Languages\n"}
{"page": 6, "bbox": [{"x": 0.2673492729663849, "y": 0.07428571581840515}, {"x": 0.31854379177093506, "y": 0.07428571581840515}, {"x": 0.31854379177093506, "y": 0.08483516424894333}, {"x": 0.2673492729663849, "y": 0.08483516424894333}], "text": "English\n"}
{"page": 6, "bbox": [{"x": 0.4135380983352661, "y": 0.09450549632310867}, {"x": 0.4431171715259552, "y": 0.09450549632310867}, {"x": 0.4431171715259552, "y": 0.10505494475364685}, {"x": 0.4135380983352661, "y": 0.10505494475364685}], "text": "Rej*\n"}
{"page": 6, "bbox": [{"x": 0.7548350691795349, "y": 0.1006593406200409}, {"x": 0.7650739550590515, "y": 0.1006593406200409}, {"x": 0.7650739550590515, "y": 0.10241758078336716}, {"x": 0.7548350691795349, "y": 0.10241758078336716}], "text": "...\n"}
{"page": 6, "bbox": [{"x": 0.5352673530578613, "y": 0.10021977871656418}, {"x": 0.6575654149055481, "y": 0.09978021681308746}, {"x": 0.6575654149055481, "y": 0.1230769231915474}, {"x": 0.5352673530578613, "y": 0.12351648509502411}], "text": "who will direct\nIrredeemable film?\n"}
{"page": 6, "bbox": [{"x": 0.6797497272491455, "y": 0.1006593406200409}, {"x": 0.7309442758560181, "y": 0.1006593406200409}, {"x": 0.7309442758560181, "y": 0.1230769231915474}, {"x": 0.6797497272491455, "y": 0.1230769231915474}], "text": "Jeymes\nSamuel\n"}
{"page": 6, "bbox": [{"x": 0.4095562994480133, "y": 0.11560439318418503}, {"x": 0.44766780734062195, "y": 0.11560439318418503}, {"x": 0.44766780734062195, "y": 0.12351648509502411}, {"x": 0.4095562994480133, "y": 0.12351648509502411}], "text": "43.33\n"}
{"page": 6, "bbox": [{"x": 0.12059158086776733, "y": 0.09362637251615524}, {"x": 0.38964733481407166, "y": 0.09318681061267853}, {"x": 0.38964733481407166, "y": 0.15252746641635895}, {"x": 0.12059158086776733, "y": 0.15296703577041626}], "text": "Rej Rej* Rej\nChatGPT 24.67 45.00 5.33\nChatGLM-6B 9.00 25.00 6.33\nChatGLM2-6B 10.33 41.33 6.33\n"}
{"page": 6, "bbox": [{"x": 0.4112628102302551, "y": 0.12967033684253693}, {"x": 0.44653013348579407, "y": 0.12967033684253693}, {"x": 0.44653013348579407, "y": 0.13802197575569153}, {"x": 0.4112628102302551, "y": 0.13802197575569153}], "text": "17.00\n"}
{"page": 6, "bbox": [{"x": 0.7542662024497986, "y": 0.09274725615978241}, {"x": 0.8981797695159912, "y": 0.09318681061267853}, {"x": 0.8976109027862549, "y": 0.19164834916591644}, {"x": 0.753697395324707, "y": 0.19120879471302032}], "text": "Adam McKay to\nmovie adaptation of\n\"Irredeemable” from\nthat won the most\nmedals... is German-\ny. It has won a total\nof 31 medals...\n"}
{"page": 6, "bbox": [{"x": 0.5346985459327698, "y": 0.1402197778224945}, {"x": 0.6535836458206177, "y": 0.1428571492433548}, {"x": 0.6530147790908813, "y": 0.15296703577041626}, {"x": 0.5341296792030334, "y": 0.15032966434955597}], "text": "Which country w-\n"}
{"page": 6, "bbox": [{"x": 0.4106939733028412, "y": 0.14329670369625092}, {"x": 0.44653013348579407, "y": 0.14373625814914703}, {"x": 0.44653013348579407, "y": 0.15252746641635895}, {"x": 0.4106939733028412, "y": 0.15208791196346283}], "text": "36.33\n"}
{"page": 6, "bbox": [{"x": 0.5358361601829529, "y": 0.15516483783721924}, {"x": 0.6604095697402954, "y": 0.1542857140302658}, {"x": 0.6604095697402954, "y": 0.16307692229747772}, {"x": 0.5358361601829529, "y": 0.16395604610443115}], "text": "on the most medals\n"}
{"page": 6, "bbox": [{"x": 0.4095562994480133, "y": 0.15780219435691833}, {"x": 0.44653013348579407, "y": 0.15736263990402222}, {"x": 0.44653013348579407, "y": 0.16615384817123413}, {"x": 0.4095562994480133, "y": 0.16659340262413025}], "text": "24.67\n"}
{"page": 6, "bbox": [{"x": 0.6797497272491455, "y": 0.1621977984905243}, {"x": 0.73549485206604, "y": 0.1621977984905243}, {"x": 0.73549485206604, "y": 0.17230768501758575}, {"x": 0.6797497272491455, "y": 0.17230768501758575}], "text": "Norway\n"}
{"page": 6, "bbox": [{"x": 0.11831627041101456, "y": 0.15560439229011536}, {"x": 0.3890784978866577, "y": 0.15560439229011536}, {"x": 0.3890784978866577, "y": 0.19516482949256897}, {"x": 0.11831627041101456, "y": 0.19516482949256897}], "text": "Vicuna-7B-v1.3 17.00 33.33 3.37\nQwen-7B-Chat 31.00 35.67 8.67\nBELLE-7B-2M 5.67 32.33 5.33\n"}
{"page": 6, "bbox": [{"x": 0.4095562994480133, "y": 0.17230768501758575}, {"x": 0.447098970413208, "y": 0.17230768501758575}, {"x": 0.447098970413208, "y": 0.18109890818595886}, {"x": 0.4095562994480133, "y": 0.18109890818595886}], "text": "25.33\n"}
{"page": 6, "bbox": [{"x": 0.5352673530578613, "y": 0.1679120808839798}, {"x": 0.6558589339256287, "y": 0.16747252643108368}, {"x": 0.6558589339256287, "y": 0.19252747297286987}, {"x": 0.5352673530578613, "y": 0.192967027425766}], "text": "at the 2022 Winter\nOlympics?\n"}
{"page": 6, "bbox": [{"x": 0.4112628102302551, "y": 0.18681319057941437}, {"x": 0.44653013348579407, "y": 0.18681319057941437}, {"x": 0.44653013348579407, "y": 0.19516482949256897}, {"x": 0.4112628102302551, "y": 0.19516482949256897}], "text": "13.67\n"}
{"page": 6, "bbox": [{"x": 0.5216154456138611, "y": 0.21054944396018982}, {"x": 0.9118316173553467, "y": 0.21230769157409668}, {"x": 0.9118316173553467, "y": 0.23736263811588287}, {"x": 0.5216154456138611, "y": 0.235604390501976}], "text": "Table 4: Error cases of negative rejection generated by\nChatGLM2-6B. The bold text highlights the error answers.\n"}
{"page": 6, "bbox": [{"x": 0.08703071624040604, "y": 0.2153846174478531}, {"x": 0.4778156876564026, "y": 0.21494504809379578}, {"x": 0.4778156876564026, "y": 0.26769229769706726}, {"x": 0.08703071624040604, "y": 0.26813188195228577}], "text": "Table 3: The result of negative rejection. Rej means the re-\njection rate (%) and Rej* means the rejection rate evaluated\nby ChatGPT. We can see that negative rejection poses a chal-\nlenge for RAG in LLMs.\n"}
{"page": 6, "bbox": [{"x": 0.08759954571723938, "y": 0.3002197742462158}, {"x": 0.47838452458381653, "y": 0.3002197742462158}, {"x": 0.47838452458381653, "y": 0.5353845953941345}, {"x": 0.08759954571723938, "y": 0.5353845953941345}], "text": "when the noise ratio increases, the content of erroneous\ndocuments is all about some people's predictions about the\nname of the headset (\"Apple Reality Pro\"). Even if there is\na correct answer (\"Vision Pro\") in the relevant documents,\nLLMs can still be misled by uncertain evidences.\n(3) Concept confusion. The concepts in external docu-\nments may be similar to, but different from, the concepts in\nthe question. This can cause confusion for LLMs and make\nLLMs generate incorrect answers. In Table 2, the model an-\nswer focuses on the concept \"automotive revenue” in the\ndocument rather than \"revenue\" in the question.\nBased on the analysis above, we have identified certain\nlimitations in LLMs regarding retrieval-augmented genera-\ntion. To effectively handle the vast amount of noise present\non the internet, further detailed enhancements are required\nfor the model such as long documents modeling and precise\nconcept comprehension.\n"}
{"page": 6, "bbox": [{"x": 0.5210466384887695, "y": 0.26769229769706726}, {"x": 0.9129692912101746, "y": 0.26813188195228577}, {"x": 0.912400484085083, "y": 0.6839560270309448}, {"x": 0.520477831363678, "y": 0.6835165023803711}], "text": "negative rejection compared to answer directly as it presents\nrelevant documents that could potentially mislead the LLMs\nand result in incorrect responses. In future developments, it\nwill be crucial for LLMs to enhance their ability to accu-\nrately match questions with the appropriate documents.\nResults on Information Integration Testbed\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table 5.\nWhen comparing the model to Table 1, we observed that\nit has a weak information integration ability, which in turn\naffects its noise robustness. We can see that:\n(1) Information integration poses a challenge for RAG\nin LLMs. Even without noise, the highest accuracy of LLMs\ncan only reach 60% and 67% for English and Chinese,\nrespectively. After adding noise, the highest accuracy de-\ncreases to 43% and 55%. These results suggest that LLMs\nstruggle with integrating information effectively and are not\nwell-suited for directly answering complex questions.\n(2) Complex questions are more challenging for RAG\nwith noisy documents. Performance decline becomes sig-\nnificant when the noise ratio is 0.4, but for simple problems,\na significant decline occurs only at a noise ratio of 0.8 at a\nsignificance level of 0.05. This indicates that complex prob-\nlems are more vulnerable to interference from noise. We\nspeculate that this is because solving complex problems re-\nquires integrating information from multiple documents, and\nthis information can be considered as noise to each other,\nmaking it harder for the model to extract relevant informa-\ntion from the documents.\n"}
{"page": 6, "bbox": [{"x": 0.08703071624040604, "y": 0.5512087941169739}, {"x": 0.4795221984386444, "y": 0.5512087941169739}, {"x": 0.4795221984386444, "y": 0.8892307877540588}, {"x": 0.08703071624040604, "y": 0.8892307877540588}], "text": "Results on Negative Rejection Testbed\nWe evaluated the rejection rate when only noise documents\nwere provided. The results are shown in Table 3. In addi-\ntion to evaluating the rejection rate through exact matching\n(Rej in Table 3), we also utilize ChatGPT to determine if\nthe responses from the LLMs contain any rejection informa-\ntion (Rej* in Table 3). We can see that: Negative Rejection\nposes a challenge for RAG in LLMs. The highest rejection\nrates for LLMs in English and Chinese were only 45% and\n43.33%, respectively. This suggests that LLMs can be easily\nmisled by noisy documents, leading to incorrect answers.\nIn addition, through comparing Rej and Rej*, we found\nthat LLMs fail to strictly follow instructions, and they often\ngenerate unpredictable responses, which make it hard to use\nthem as state triggers (such as for recognizing rejection).\nWe conduct case studies in Table 4. The first error is\nbecause of Evidence uncertainty. Although the document\nonly mentions contact with \"Adam McKay\" and does not\nexplicitly state that he is the director of the movie, the\nmodel still concludes that he holds this role. The first er-\nror is because of Concept confusion. The information pro-\nvided in the answer pertains to \"the 2018 Winter Olympics\"\ninstead of \"the 2022 Olympics\" mentioned in the question.\nRetrieval-augmented generation poses a greater challenge of\n"}
{"page": 6, "bbox": [{"x": 0.5216154456138611, "y": 0.696703314781189}, {"x": 0.912400484085083, "y": 0.696703314781189}, {"x": 0.912400484085083, "y": 0.8879120945930481}, {"x": 0.5216154456138611, "y": 0.8879120945930481}], "text": "Error Analysis We conducted an error analysis on\nChatGLM2-6B (noise ratio is 0). Apart from the similar er-\nrors founded in the noise robustness experiment (38% of the\ntotal), there are also three types of unique errors. We have\npresented these cases in Table 6.\n(1) Merging Error (28% of the total). The model some-\ntimes merges the answers of the two sub-questions, resulting\nin an error. It mistakenly uses the answer from one question\nto address both two questions. At this point, the model will\ndisregard any documents related to one sub-question. For\nexample, in Table 6, it incorrectly states that Group D is the\nWorld Cup group for both France and Germany, while in fact\nGermany is actually assigned to Group E.\n(2) Ignoring Error (28% of the total). Sometimes, the\n"}
{"page": 6, "bbox": [{"x": 0.48009100556373596, "y": 0.9432967305183411}, {"x": 0.5170648694038391, "y": 0.9432967305183411}, {"x": 0.5170648694038391, "y": 0.9507692456245422}, {"x": 0.48009100556373596, "y": 0.9507692456245422}], "text": "17759\n"}
{"page": 7, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 7, "bbox": [{"x": 0.3737201392650604, "y": 0.0698901116847992}, {"x": 0.424914687871933, "y": 0.0698901116847992}, {"x": 0.424914687871933, "y": 0.0782417580485344}, {"x": 0.3737201392650604, "y": 0.0782417580485344}], "text": "Chinese\n"}
{"page": 7, "bbox": [{"x": 0.26279863715171814, "y": 0.0698901116847992}, {"x": 0.31058019399642944, "y": 0.0698901116847992}, {"x": 0.31058019399642944, "y": 0.07999999821186066}, {"x": 0.26279863715171814, "y": 0.07999999821186066}], "text": "English\n"}
{"page": 7, "bbox": [{"x": 0.8731513023376465, "y": 0.07384615391492844}, {"x": 0.894197940826416, "y": 0.07384615391492844}, {"x": 0.894197940826416, "y": 0.08175823837518692}, {"x": 0.8731513023376465, "y": 0.08175823837518692}], "text": "CR\n"}
{"page": 7, "bbox": [{"x": 0.6695107817649841, "y": 0.07428571581840515}, {"x": 0.6951080560684204, "y": 0.07428571581840515}, {"x": 0.6951080560684204, "y": 0.08175823837518692}, {"x": 0.6695107817649841, "y": 0.08175823837518692}], "text": "Acc\n"}
{"page": 7, "bbox": [{"x": 0.7150170803070068, "y": 0.07164835184812546}, {"x": 0.8458475470542908, "y": 0.07384615391492844}, {"x": 0.8452787399291992, "y": 0.08571428805589676}, {"x": 0.7144482135772705, "y": 0.08351648598909378}], "text": "Accdoc ED ED*\n"}
{"page": 7, "bbox": [{"x": 0.2440273016691208, "y": 0.0839560404419899}, {"x": 0.2531285583972931, "y": 0.0839560404419899}, {"x": 0.2531285583972931, "y": 0.09142857044935226}, {"x": 0.2440273016691208, "y": 0.09142857044935226}], "text": "0\n"}
{"page": 7, "bbox": [{"x": 0.13253697752952576, "y": 0.08307692408561707}, {"x": 0.2081911265850067, "y": 0.0839560404419899}, {"x": 0.2081911265850067, "y": 0.0923076942563057}, {"x": 0.13253697752952576, "y": 0.09142857044935226}], "text": "Noise Ratio\n"}
{"page": 7, "bbox": [{"x": 0.3566552996635437, "y": 0.0839560404419899}, {"x": 0.4453924894332886, "y": 0.08439560234546661}, {"x": 0.4453924894332886, "y": 0.0923076942563057}, {"x": 0.3566552996635437, "y": 0.09186813235282898}], "text": "0 0.2 0.4\n"}
{"page": 7, "bbox": [{"x": 0.27474403381347656, "y": 0.0839560404419899}, {"x": 0.3333333432674408, "y": 0.08483516424894333}, {"x": 0.3333333432674408, "y": 0.09274725615978241}, {"x": 0.27474403381347656, "y": 0.09186813235282898}], "text": "0.2 0.4\n"}
{"page": 7, "bbox": [{"x": 0.31569966673851013, "y": 0.0980219766497612}, {"x": 0.33162686228752136, "y": 0.0980219766497612}, {"x": 0.33162686228752136, "y": 0.10505494475364685}, {"x": 0.31569966673851013, "y": 0.10505494475364685}], "text": "34\n"}
{"page": 7, "bbox": [{"x": 0.3526735007762909, "y": 0.0980219766497612}, {"x": 0.3686006963253021, "y": 0.0980219766497612}, {"x": 0.3686006963253021, "y": 0.10549450665712357}, {"x": 0.3526735007762909, "y": 0.10549450665712357}], "text": "63\n"}
{"page": 7, "bbox": [{"x": 0.3890784978866577, "y": 0.09758241474628448}, {"x": 0.4431171715259552, "y": 0.0980219766497612}, {"x": 0.4431171715259552, "y": 0.10593406856060028}, {"x": 0.3890784978866577, "y": 0.10549450665712357}], "text": "58 47\n"}
{"page": 7, "bbox": [{"x": 0.2389078438282013, "y": 0.09846153855323792}, {"x": 0.2929465174674988, "y": 0.09626373648643494}, {"x": 0.2935153543949127, "y": 0.10593406856060028}, {"x": 0.23947668075561523, "y": 0.10813187062740326}], "text": "55 51\n"}
{"page": 7, "bbox": [{"x": 0.12571103870868683, "y": 0.09714286029338837}, {"x": 0.21387940645217896, "y": 0.09714286029338837}, {"x": 0.21387940645217896, "y": 0.11956044286489487}, {"x": 0.12571103870868683, "y": 0.11956044286489487}], "text": "ChatGPT\nChatGLM-6B\n"}
{"page": 7, "bbox": [{"x": 0.6615471839904785, "y": 0.0980219766497612}, {"x": 0.9135380983352661, "y": 0.0980219766497612}, {"x": 0.9135380983352661, "y": 0.12263736128807068}, {"x": 0.6615471839904785, "y": 0.12263736128807068}], "text": "171225.000\n"}
{"page": 7, "bbox": [{"x": 0.5318543910980225, "y": 0.09362637251615524}, {"x": 0.650170624256134, "y": 0.09318681061267853}, {"x": 0.650170624256134, "y": 0.12923076748847961}, {"x": 0.5318543910980225, "y": 0.12967033684253693}], "text": "ChatGPT-zh\nQwen-7B-Chat-zh\nChatGPT-en\n"}
{"page": 7, "bbox": [{"x": 0.2389078438282013, "y": 0.11120878905057907}, {"x": 0.2935153543949127, "y": 0.11120878905057907}, {"x": 0.2935153543949127, "y": 0.11912088096141815}, {"x": 0.2389078438282013, "y": 0.11912088096141815}], "text": "45 36\n"}
{"page": 7, "bbox": [{"x": 0.35210466384887695, "y": 0.11164835095405579}, {"x": 0.3680318593978882, "y": 0.11164835095405579}, {"x": 0.3680318593978882, "y": 0.11868131905794144}, {"x": 0.35210466384887695, "y": 0.11868131905794144}], "text": "60\n"}
{"page": 7, "bbox": [{"x": 0.3890784978866577, "y": 0.11120878905057907}, {"x": 0.40500569343566895, "y": 0.11120878905057907}, {"x": 0.40500569343566895, "y": 0.11912088096141815}, {"x": 0.3890784978866577, "y": 0.11912088096141815}], "text": "53\n"}
{"page": 7, "bbox": [{"x": 0.31569966673851013, "y": 0.11164835095405579}, {"x": 0.33162686228752136, "y": 0.11164835095405579}, {"x": 0.33162686228752136, "y": 0.11912088096141815}, {"x": 0.31569966673851013, "y": 0.11912088096141815}], "text": "35\n"}
{"page": 7, "bbox": [{"x": 0.4277588129043579, "y": 0.11164835095405579}, {"x": 0.44368600845336914, "y": 0.11164835095405579}, {"x": 0.44368600845336914, "y": 0.11912088096141815}, {"x": 0.4277588129043579, "y": 0.11912088096141815}], "text": "52\n"}
{"page": 7, "bbox": [{"x": 0.42832764983177185, "y": 0.12483516335487366}, {"x": 0.4431171715259552, "y": 0.12483516335487366}, {"x": 0.4431171715259552, "y": 0.13274724781513214}, {"x": 0.42832764983177185, "y": 0.13274724781513214}], "text": "32\n"}
{"page": 7, "bbox": [{"x": 0.35210466384887695, "y": 0.12527473270893097}, {"x": 0.3680318593978882, "y": 0.12527473270893097}, {"x": 0.3680318593978882, "y": 0.13230769336223602}, {"x": 0.35210466384887695, "y": 0.13230769336223602}], "text": "44\n"}
{"page": 7, "bbox": [{"x": 0.12172923982143402, "y": 0.12483516335487366}, {"x": 0.2923777103424072, "y": 0.12483516335487366}, {"x": 0.2923777103424072, "y": 0.13318681716918945}, {"x": 0.12172923982143402, "y": 0.13318681716918945}], "text": "ChatGLM2-6B 34 32\n"}
{"page": 7, "bbox": [{"x": 0.3151308298110962, "y": 0.12527473270893097}, {"x": 0.3304891884326935, "y": 0.12527473270893097}, {"x": 0.3304891884326935, "y": 0.13274724781513214}, {"x": 0.3151308298110962, "y": 0.13274724781513214}], "text": "21\n"}
{"page": 7, "bbox": [{"x": 0.3885096609592438, "y": 0.12527473270893097}, {"x": 0.40500569343566895, "y": 0.12527473270893097}, {"x": 0.40500569343566895, "y": 0.13274724781513214}, {"x": 0.3885096609592438, "y": 0.13274724781513214}], "text": "43\n"}
{"page": 7, "bbox": [{"x": 0.351535826921463, "y": 0.13802197575569153}, {"x": 0.3680318593978882, "y": 0.13802197575569153}, {"x": 0.3680318593978882, "y": 0.1454945057630539}, {"x": 0.351535826921463, "y": 0.1454945057630539}], "text": "43\n"}
{"page": 7, "bbox": [{"x": 0.3890784978866577, "y": 0.13802197575569153}, {"x": 0.4055745303630829, "y": 0.13802197575569153}, {"x": 0.4055745303630829, "y": 0.1454945057630539}, {"x": 0.3890784978866577, "y": 0.1454945057630539}], "text": "36\n"}
{"page": 7, "bbox": [{"x": 0.31456199288368225, "y": 0.13802197575569153}, {"x": 0.33162686228752136, "y": 0.13802197575569153}, {"x": 0.33162686228752136, "y": 0.14593406021595}, {"x": 0.31456199288368225, "y": 0.14593406021595}], "text": "43\n"}
{"page": 7, "bbox": [{"x": 0.11945392191410065, "y": 0.13802197575569153}, {"x": 0.2935153543949127, "y": 0.13802197575569153}, {"x": 0.2935153543949127, "y": 0.14637362957000732}, {"x": 0.11945392191410065, "y": 0.14637362957000732}], "text": "Vicuna-7B-v1.3 60 53\n"}
{"page": 7, "bbox": [{"x": 0.4277588129043579, "y": 0.13846154510974884}, {"x": 0.4442548453807831, "y": 0.13846154510974884}, {"x": 0.4442548453807831, "y": 0.14637362957000732}, {"x": 0.4277588129043579, "y": 0.14637362957000732}], "text": "25\n"}
{"page": 7, "bbox": [{"x": 0.31569966673851013, "y": 0.1516483575105667}, {"x": 0.33162686228752136, "y": 0.1516483575105667}, {"x": 0.33162686228752136, "y": 0.1595604419708252}, {"x": 0.31569966673851013, "y": 0.1595604419708252}], "text": "37\n"}
{"page": 7, "bbox": [{"x": 0.3526735007762909, "y": 0.1516483575105667}, {"x": 0.3691695034503937, "y": 0.1516483575105667}, {"x": 0.3691695034503937, "y": 0.1595604419708252}, {"x": 0.3526735007762909, "y": 0.1595604419708252}], "text": "67\n"}
{"page": 7, "bbox": [{"x": 0.12172923982143402, "y": 0.1516483575105667}, {"x": 0.2929465174674988, "y": 0.14989010989665985}, {"x": 0.2929465174674988, "y": 0.1599999964237213}, {"x": 0.12172923982143402, "y": 0.16175824403762817}], "text": "Qwen-7B-Chat 55 50\n"}
{"page": 7, "bbox": [{"x": 0.3890784978866577, "y": 0.15208791196346283}, {"x": 0.40614333748817444, "y": 0.15208791196346283}, {"x": 0.40614333748817444, "y": 0.1595604419708252}, {"x": 0.3890784978866577, "y": 0.1595604419708252}], "text": "56\n"}
{"page": 7, "bbox": [{"x": 0.4277588129043579, "y": 0.15208791196346283}, {"x": 0.4442548453807831, "y": 0.15208791196346283}, {"x": 0.4442548453807831, "y": 0.1595604419708252}, {"x": 0.4277588129043579, "y": 0.1595604419708252}], "text": "55\n"}
{"page": 7, "bbox": [{"x": 0.351535826921463, "y": 0.1652747243642807}, {"x": 0.44368600845336914, "y": 0.1652747243642807}, {"x": 0.44368600845336914, "y": 0.17318680882453918}, {"x": 0.351535826921463, "y": 0.17318680882453918}], "text": "49 41 38\n"}
{"page": 7, "bbox": [{"x": 0.12059158086776733, "y": 0.1652747243642807}, {"x": 0.3321956694126129, "y": 0.16571427881717682}, {"x": 0.3321956694126129, "y": 0.1736263781785965}, {"x": 0.12059158086776733, "y": 0.17318680882453918}], "text": "BELLE-7B-2M 40 34 24\n"}
{"page": 7, "bbox": [{"x": 0.5216154456138611, "y": 0.14989010989665985}, {"x": 0.912400484085083, "y": 0.15032966434955597}, {"x": 0.912400484085083, "y": 0.2294505536556244}, {"x": 0.5216154456138611, "y": 0.22901098430156708}], "text": "Table 7: The result of counterfactual robustness. ACC is the\naccuracy (%) of LLMs without external documents. ACC doc\nis the accuracy (%) of LLMs with counterfactual documents.\nED and ED* are error detection rates evaluated by exact\nmatching and ChatGPT, respectively. CR is the error cor-\nrection rate.\n"}
{"page": 7, "bbox": [{"x": 0.08759954571723938, "y": 0.19164834916591644}, {"x": 0.47724688053131104, "y": 0.190769225358963}, {"x": 0.47724688053131104, "y": 0.2158241719007492}, {"x": 0.08759954571723938, "y": 0.21670329570770264}], "text": "Table 5: The experimental result of information integration\nmeasured by accuracy (%) under different noise ratios.\n"}
{"page": 7, "bbox": [{"x": 0.2286689430475235, "y": 0.24175824224948883}, {"x": 0.27815699577331543, "y": 0.24175824224948883}, {"x": 0.27815699577331543, "y": 0.2492307722568512}, {"x": 0.2286689430475235, "y": 0.2492307722568512}], "text": "Answer\n"}
{"page": 7, "bbox": [{"x": 0.42150169610977173, "y": 0.24219779670238495}, {"x": 0.4653014838695526, "y": 0.24219779670238495}, {"x": 0.4653014838695526, "y": 0.24967032670974731}, {"x": 0.42150169610977173, "y": 0.24967032670974731}], "text": "Errors\n"}
{"page": 7, "bbox": [{"x": 0.12343572080135345, "y": 0.24175824224948883}, {"x": 0.1820250302553177, "y": 0.24175824224948883}, {"x": 0.1820250302553177, "y": 0.2514285743236542}, {"x": 0.12343572080135345, "y": 0.2514285743236542}], "text": "Question\n"}
{"page": 7, "bbox": [{"x": 0.3219567835330963, "y": 0.24175824224948883}, {"x": 0.3839590549468994, "y": 0.24175824224948883}, {"x": 0.3839590549468994, "y": 0.2518681287765503}, {"x": 0.3219567835330963, "y": 0.2518681287765503}], "text": "Response\n"}
{"page": 7, "bbox": [{"x": 0.09670079499483109, "y": 0.25890108942985535}, {"x": 0.19567690789699554, "y": 0.2610988914966583}, {"x": 0.1951080709695816, "y": 0.271208792924881}, {"x": 0.09613196551799774, "y": 0.269010990858078}], "text": "What groupings\n"}
{"page": 7, "bbox": [{"x": 0.3077360689640045, "y": 0.26725274324417114}, {"x": 0.3748577833175659, "y": 0.26725274324417114}, {"x": 0.3748577833175659, "y": 0.2751648426055908}, {"x": 0.3077360689640045, "y": 0.2751648426055908}], "text": "France and\n"}
{"page": 7, "bbox": [{"x": 0.5216154456138611, "y": 0.2602197825908661}, {"x": 0.9118316173553467, "y": 0.2628571391105652}, {"x": 0.9112628102302551, "y": 0.2918681204319}, {"x": 0.5210466384887695, "y": 0.2892307639122009}], "text": "ever, these methods slow down the inference speed and can-\nnot provide timely responses.\n"}
{"page": 7, "bbox": [{"x": 0.09726962447166443, "y": 0.27340659499168396}, {"x": 0.18657565116882324, "y": 0.2720879018306732}, {"x": 0.18657565116882324, "y": 0.28131869435310364}, {"x": 0.09726962447166443, "y": 0.282637357711792}], "text": "are France and\n"}
{"page": 7, "bbox": [{"x": 0.21729238331317902, "y": 0.27340659499168396}, {"x": 0.27474403381347656, "y": 0.2725274860858917}, {"x": 0.27474403381347656, "y": 0.2830769121646881}, {"x": 0.21729238331317902, "y": 0.28395605087280273}], "text": "Group D\n"}
{"page": 7, "bbox": [{"x": 0.4158134162425995, "y": 0.27296704053878784}, {"x": 0.47212740778923035, "y": 0.27340659499168396}, {"x": 0.47212740778923035, "y": 0.2949450612068176}, {"x": 0.4158134162425995, "y": 0.2945055067539215}], "text": "Merging\nError\n"}
{"page": 7, "bbox": [{"x": 0.09726962447166443, "y": 0.28703296184539795}, {"x": 0.19852104783058167, "y": 0.28483515977859497}, {"x": 0.1990898698568344, "y": 0.2949450612068176}, {"x": 0.09783845394849777, "y": 0.2971428632736206}], "text": "Germany in Wo-\n"}
{"page": 7, "bbox": [{"x": 0.3077360689640045, "y": 0.280439555644989}, {"x": 0.38680317997932434, "y": 0.2795604467391968}, {"x": 0.3873720169067383, "y": 0.3019780218601227}, {"x": 0.30830490589141846, "y": 0.3028571307659149}], "text": "Germany are\nGroup D\n"}
{"page": 7, "bbox": [{"x": 0.21729238331317902, "y": 0.28659340739250183}, {"x": 0.2690557539463043, "y": 0.2852747142314911}, {"x": 0.2696245610713959, "y": 0.296263724565506}, {"x": 0.21786120533943176, "y": 0.2975824177265167}], "text": "Group E\n"}
{"page": 7, "bbox": [{"x": 0.09670079499483109, "y": 0.29890111088752747}, {"x": 0.18430034816265106, "y": 0.29846152663230896}, {"x": 0.18430034816265106, "y": 0.30901098251342773}, {"x": 0.09670079499483109, "y": 0.30945053696632385}], "text": "rld Cup 2022?\n"}
{"page": 7, "bbox": [{"x": 0.09726962447166443, "y": 0.31736263632774353}, {"x": 0.18373151123523712, "y": 0.31824174523353577}, {"x": 0.18373151123523712, "y": 0.32659339904785156}, {"x": 0.09726962447166443, "y": 0.3257142901420593}], "text": "Who were the\n"}
{"page": 7, "bbox": [{"x": 0.21786120533943176, "y": 0.31824174523353577}, {"x": 0.266211599111557, "y": 0.31824174523353577}, {"x": 0.266211599111557, "y": 0.3283516466617584}, {"x": 0.21786120533943176, "y": 0.3283516466617584}], "text": "Cooper\n"}
{"page": 7, "bbox": [{"x": 0.21729238331317902, "y": 0.33142855763435364}, {"x": 0.26166099309921265, "y": 0.33142855763435364}, {"x": 0.26166099309921265, "y": 0.35208791494369507}, {"x": 0.21729238331317902, "y": 0.35208791494369507}], "text": "Kupp\nPatrick\n"}
{"page": 7, "bbox": [{"x": 0.41524460911750793, "y": 0.33142855763435364}, {"x": 0.47212740778923035, "y": 0.33186814188957214}, {"x": 0.47212740778923035, "y": 0.3525274693965912}, {"x": 0.41524460911750793, "y": 0.35208791494369507}], "text": "Ignoring\nError\n"}
{"page": 7, "bbox": [{"x": 0.3077360689640045, "y": 0.32351648807525635}, {"x": 0.3981797397136688, "y": 0.3243955969810486}, {"x": 0.39761093258857727, "y": 0.3617582321166992}, {"x": 0.3071672320365906, "y": 0.360879123210907}], "text": "MVP of Super\nBowl LVI was\nCooper Kupp\n"}
{"page": 7, "bbox": [{"x": 0.21672354638576508, "y": 0.35692307353019714}, {"x": 0.27701935172080994, "y": 0.35736262798309326}, {"x": 0.27701935172080994, "y": 0.36571428179740906}, {"x": 0.21672354638576508, "y": 0.36527472734451294}], "text": "Mahomes\n"}
{"page": 7, "bbox": [{"x": 0.09726962447166443, "y": 0.3305494487285614}, {"x": 0.19681456685066223, "y": 0.33142855763435364}, {"x": 0.19567690789699554, "y": 0.436923086643219}, {"x": 0.09613196551799774, "y": 0.43604394793510437}], "text": "MVP of Super\nBowl 2022 and\n2023?\nWhat films won\nthe 2022 and\n2023 Academy\nAwards for Best\nPicture?\n"}
{"page": 7, "bbox": [{"x": 0.41922640800476074, "y": 0.38813185691833496}, {"x": 0.4687144458293915, "y": 0.38813185691833496}, {"x": 0.4687144458293915, "y": 0.4228571355342865}, {"x": 0.41922640800476074, "y": 0.4228571355342865}], "text": "Misali\ngnment\nError\n"}
{"page": 7, "bbox": [{"x": 0.30830490589141846, "y": 0.38197803497314453}, {"x": 0.3879408538341522, "y": 0.38241758942604065}, {"x": 0.3873720169067383, "y": 0.42989009618759155}, {"x": 0.3077360689640045, "y": 0.42945054173469543}], "text": "CODA won\naward for\nBest Picture\nat the 95th ...\n"}
{"page": 7, "bbox": [{"x": 0.21672354638576508, "y": 0.38241758942604065}, {"x": 0.2906712293624878, "y": 0.38285714387893677}, {"x": 0.29010239243507385, "y": 0.42989009618759155}, {"x": 0.21615472435951233, "y": 0.42945054173469543}], "text": "CODA\nEverything\nEverywhere\nAll at Once\n"}
{"page": 7, "bbox": [{"x": 0.08759954571723938, "y": 0.4562637507915497}, {"x": 0.47724688053131104, "y": 0.45538461208343506}, {"x": 0.47724688053131104, "y": 0.49494504928588867}, {"x": 0.08759954571723938, "y": 0.4958241879940033}], "text": "Table 6: Error cases of information integration, the re-\nsponses are generated by ChatGLM2-6B. The bold and ital-\nicized texts represent the answers to two sub-questions.\n"}
{"page": 7, "bbox": [{"x": 0.5210466384887695, "y": 0.3059340715408325}, {"x": 0.912400484085083, "y": 0.3054945170879364}, {"x": 0.9129692912101746, "y": 0.6826373338699341}, {"x": 0.5216154456138611, "y": 0.6830769181251526}], "text": "Results on Counterfactual Robustness Testbed\nIn order to ensure that LLMs possess relevant knowledge,\nwe assess their performance by directly asking them ques-\ntions. However, we found that most LLMs struggle to an-\nswer them correctly. To ensure a more reasonable evalua-\ntion, we only consider LLMs that have an accuracy rate of\nover 70% as this threshold is relatively high and encom-\npasses more LLMs. The results are shown in Table 7. We\npresent the following metrics: accuracy without any docu-\nments, accuracy with counterfactual documents, error de-\ntection rates, and error correction rates. We can see that It\nis hard for LLMs to identify and correct factual errors in the\ndocuments. This suggests that the model can be easily mis-\nled by documents containing incorrect facts.\nIt is important to note that retrieval-augmented generation\nis not designed to automatically address factual errors within\na given context, as this contradicts the underlying assump-\ntion that the model lacks knowledge and relies on retrieved\ndocuments for additional information. However, this issue is\ncrucial in practical applications due to the abundance of fake\nnews on the internet. Existing LLMs do not have a safeguard\nto handle inaccurate responses caused by misinformation. In\nfact, they heavily depend on the information they retrieve.\nEven when LLMs contain the internal knowledge about the\nquestions, they often trust false information that is retrieved.\nThis presents significant a challenge for the future develop-\nment of RAG in LLMs.\n"}
{"page": 7, "bbox": [{"x": 0.08759954571723938, "y": 0.5287911891937256}, {"x": 0.47895336151123047, "y": 0.5287911891937256}, {"x": 0.47895336151123047, "y": 0.8883516192436218}, {"x": 0.08759954571723938, "y": 0.8883516192436218}], "text": "model may ignore one of the sub-questions and only answer\nthe other. This error occurs when the model lacks a complete\nunderstanding of the problem and fails to recognize that it\nconsists of multiple sub-problems. As a result, the model\nonly considers relevant documents for one sub-problem in\norder to generate an answer, disregarding the question posed\nby another sub-problem. For example, in Table 6, the model\nonly provides the answer for the MVP of Super Bowl 2022\nand does not consider 2023.\n(3) Misalignment Error (6% of the total). Sometimes,\nthe model incorrectly identifies the documents for one sub-\nquestion as the documents for another sub-question, leading\nto misaligned answers. For example, in Table 6, the third an-\nswer has two errors: an ignoring error and a misalignment er-\nror. Firstly, the model only mentioned the Best Picture of the\n2023 (95th) Academy Awards, completely disregarding the\n2022 awards. Additionally, it incorrectly stated that \"CODA\"\nis the Best Picture of 2023 when it was actually awarded as\nthe Best Picture in 2022.\nThe errors mentioned above are primarily caused by the\nlimited understanding of complex questions, which hinders\nthe ability to effectively utilize information from different\nsub-problems. The key lies in improving the model's rea-\nsoning capability. One possible solution is to use a chain-of-\nthought approach to break down complex problems (Zhou\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-\n"}
{"page": 7, "bbox": [{"x": 0.6700796484947205, "y": 0.7037362456321716}, {"x": 0.7616609930992126, "y": 0.7037362456321716}, {"x": 0.7616609930992126, "y": 0.7138461470603943}, {"x": 0.6700796484947205, "y": 0.7138461470603943}], "text": "Conclusion\n"}
{"page": 7, "bbox": [{"x": 0.5210466384887695, "y": 0.7239560484886169}, {"x": 0.9135380983352661, "y": 0.7239560484886169}, {"x": 0.9135380983352661, "y": 0.8887912034988403}, {"x": 0.5210466384887695, "y": 0.8887912034988403}], "text": "In this paper, we evaluated four abilities of retrieval-\naugmented generation in LLMs: noise robustness, nega-\ntive rejection, information integration, and counterfactual\nrobustness. To conduct the evaluation, we built Retrieval-\nAugmented Generation Benchmark (RGB). The instances of\nRGB are generated from latest news articles and the external\ndocuments obtained from search engines. The experimental\nresults suggest that current LLMs have limitations in the 4\nabilities. This indicates that there is still a significant amount\nof work needed to effectively apply RAG to LLMs. To en-\nsure accurate and reliable responses from LLMs, it is crucial\nto exercise caution and carefully design for RAG.\n"}
{"page": 7, "bbox": [{"x": 0.48009100556373596, "y": 0.942417562007904}, {"x": 0.5164960026741028, "y": 0.9432967305183411}, {"x": 0.5164960026741028, "y": 0.9507692456245422}, {"x": 0.48009100556373596, "y": 0.9503296613693237}], "text": "17760\n"}
{"page": 8, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 8, "bbox": [{"x": 0.20136518776416779, "y": 0.07076922804117203}, {"x": 0.36348122358322144, "y": 0.07032967358827591}, {"x": 0.36348122358322144, "y": 0.08307692408561707}, {"x": 0.20136518776416779, "y": 0.08351648598909378}], "text": "Acknowledgements\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.089670330286026}, {"x": 0.4778156876564026, "y": 0.089670330286026}, {"x": 0.4778156876564026, "y": 0.1819780170917511}, {"x": 0.08759954571723938, "y": 0.1819780170917511}], "text": "This research work is supported by the National Natural\nScience Foundation of China under Grants no. 62122077,\n62106251, 62306303, the CAS Project for Young Scien-\ntists in Basic Research under Grant No. YSBR-040 and\nBeijing Municipal Science & Technology Comission No.\nZ231100010323002. Xianpei Han is sponsored by CCF-\nBaiChuan-Ebtech Foundation Model Fund.\n"}
{"page": 8, "bbox": [{"x": 0.2377701997756958, "y": 0.20043955743312836}, {"x": 0.3282138705253601, "y": 0.20043955743312836}, {"x": 0.3282138705253601, "y": 0.2101098895072937}, {"x": 0.2377701997756958, "y": 0.2101098895072937}], "text": "References\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.21934065222740173}, {"x": 0.4778156876564026, "y": 0.21934065222740173}, {"x": 0.4778156876564026, "y": 0.47164836525917053}, {"x": 0.08759954571723938, "y": 0.47164836525917053}], "text": "Adlakha, V.; BehnamGhader, P.; Lu, X. H.; Meade, N.; and\nReddy, S. 2023. Evaluating Correctness and Faithfulness\nof Instruction-Following Models for Question Answering.\narXiv:2307.16877.\nBai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan,\nY.; Ge, W.; Han, Y.; Huang, F.; Hui, B.; Ji, L.; Li, M.; Lin,\nJ.; Lin, R.; Liu, D.; Liu, G.; Lu, C.; Lu, K.; Ma, J.; Men,\nR.; Ren, X.; Ren, X.; Tan, C.; Tan, S.; Tu, J.; Wang, P.;\nWang, S.; Wang, W.; Wu, S.; Xu, B.; Xu, J.; Yang, A.; Yang,\nH.; Yang, J.; Yang, S.; Yao, Y.; Yu, B.; Yuan, H.; Yuan, Z.;\nZhang, J.; Zhang, X.; Zhang, Y.; Zhang, Z.; Zhou, C.; Zhou,\nJ.; Zhou, X.; and Zhu, T. 2023. Qwen Technical Report.\narXiv preprint arXiv:2309.16609.\nBang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V.; Xu,\nY.; and Fung, P. 2023. A Multitask, Multilingual, Multi-\nmodal Evaluation of ChatGPT on Reasoning, Hallucination,\nand Interactivity. arXiv:2302.04023.\n"}
{"page": 8, "bbox": [{"x": 0.520477831363678, "y": 0.06901098787784576}, {"x": 0.9152445793151855, "y": 0.06945054978132248}, {"x": 0.914675772190094, "y": 0.8887912034988403}, {"x": 0.5199089646339417, "y": 0.8883516192436218}], "text": "1866-1875. Hong Kong, China: Association for Computa-\ntional Linguistics.\nCao, M.; Dong, Y.; Wu, J.; and Cheung, J. C. K. 2020. Fac-\ntual Error Correction for Abstractive Summarization Mod-\nels. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 6251–—\n6258. Online: Association for Computational Linguistics.\nChang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu,\nK.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; Ye, W.;\nZhang, Y.; Chang, Y.; Yu, P. S.; Yang, Q.; and Xie, X.\n2023. A Survey on Evaluation of Large Language Models.\narXiv:2307.03109.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nCui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023. Chat-\nLaw: Open-Source Legal Large Language Model with Inte-\ngrated External Knowledge Bases. arXiv:2306.16092.\nDrozdov, A.; Schärli, N.; Akyürek, E.; Scales, N.; Song,\nX.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compo-\nsitional Semantic Parsing with Large Language Models. In\nThe Eleventh International Conference on Learning Repre-\nsentations.\nEdward Beeching, N. H. S. H. N. L. N. R. O. S. L. T.\nT. W., Clémentine Fourrier. 2023. Open LLM Leader-\nboard. https://huggingface.co/spaces/HuggingFaceH4/\nopen_llm_leaderboard. Accessed: 2024-01-10.\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y.;\nYue, J.; and Wu, Y. 2023. How Close is ChatGPT to Hu-\nman Experts? Comparison Corpus, Evaluation, and Detec-\ntion. arXiv:2301.07597.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.\n2020. REALM: Retrieval-Augmented Language Model Pre-\nTraining. In Proceedings of the 37th International Confer-\nence on Machine Learning, ICML'20. JMLR.org.\nHe, H.; Zhang, H.; and Roth, D. 2022. Rethinking\nwith Retrieval: Faithful Large Language Model Inference.\narXiv:2301.00303.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\ntitask Language Understanding. In International Conference\non Learning Representations.\nHuang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;\nLiu, J.; Lv, C.; Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He,\nJ. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese\nEvaluation Suite for Foundation Models. arXiv preprint\narXiv:2305.08322.\nIzacard, G.; and Grave, E. 2021. Leveraging Passage Re-\ntrieval with Generative Models for Open Domain Ques-\ntion Answering. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational\nLinguistics: Main Volume, 874-880. Online: Association for\nComputational Linguistics.\nIzacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni,\nF.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.47780218720436096}, {"x": 0.23094426095485687, "y": 0.4764835238456726}, {"x": 0.23094426095485687, "y": 0.48835164308547974}, {"x": 0.08759954571723938, "y": 0.4896703362464905}], "text": "BELLEGroup. 2023.\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.475604385137558}, {"x": 0.4817974865436554, "y": 0.4760439693927765}, {"x": 0.48122867941856384, "y": 0.8892307877540588}, {"x": 0.08703071624040604, "y": 0.8887912034988403}], "text": "BELLE: Be Everyone's Large\nLanguage model Engine. https://github.com/LianjiaTech/\nBELLE. Accessed: 2024-01-10.\nBian, N.; Liu, P.; Han, X.; Lin, H.; Lu, Y.; He, B.; and\nSun, L. 2023. A Drop of Ink Makes a Million Think: The\nSpread of False Information in Large Language Models.\narXiv:2305.04812.\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\nford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;\nDamoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\nRing, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\nCassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\nO.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\nSifre, L. 2022. Improving language models by retrieving\nfrom trillions of tokens. arXiv:2112.04426.\nCai, D.; Wang, Y.; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and\nShi, S. 2019a. Skeleton-to-Response: Dialogue Genera-\ntion Guided by Retrieval Memory. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 1219–\n1228. Minneapolis, Minnesota: Association for Computa-\ntional Linguistics.\nCai, D.; Wang, Y.; Bi, W.; Tu, Z.; Liu, X.; and Shi, S.\n2019b. Retrieval-guided Dialogue Response Generation via\na Matching-to-Generation Framework. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP),\n"}
{"page": 8, "bbox": [{"x": 0.4806598424911499, "y": 0.9432967305183411}, {"x": 0.5164960026741028, "y": 0.9432967305183411}, {"x": 0.5164960026741028, "y": 0.9507692456245422}, {"x": 0.4806598424911499, "y": 0.9507692456245422}], "text": "17761\n"}
{"page": 9, "bbox": [{"x": 0.31854379177093506, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.046593405306339264}, {"x": 0.6786120533943176, "y": 0.05494505539536476}, {"x": 0.31854379177093506, "y": 0.05494505539536476}], "text": "The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n"}
{"page": 9, "bbox": [{"x": 0.5216154456138611, "y": 0.07208791375160217}, {"x": 0.9118316173553467, "y": 0.07208791375160217}, {"x": 0.9118316173553467, "y": 0.18593406677246094}, {"x": 0.5216154456138611, "y": 0.18593406677246094}], "text": "Factual Knowledge Boundary of Large Language Models\nwith Retrieval Augmentation. arXiv:2307.11019.\nShen, X.; Chen, Z.; Backes, M.; and Zhang, Y. 2023. In\nChatGPT We Trust? Measuring and Characterizing the Re-\nliability of ChatGPT. arXiv:2304.08979.\nShi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.;\nLewis, M.; Zettlemoyer, L.; and tau Yih, W. 2023. RE-\nPLUG: Retrieval-Augmented Black-Box Language Models.\n"}
{"page": 9, "bbox": [{"x": 0.5221843123435974, "y": 0.18945054709911346}, {"x": 0.6439135670661926, "y": 0.18945054709911346}, {"x": 0.6439135670661926, "y": 0.19780220091342926}, {"x": 0.5221843123435974, "y": 0.19780220091342926}], "text": "arXiv:2301.12652.\n"}
{"page": 9, "bbox": [{"x": 0.7889647483825684, "y": 0.20571428537368774}, {"x": 0.9118316173553467, "y": 0.20527473092079163}, {"x": 0.9118316173553467, "y": 0.21670329570770264}, {"x": 0.7889647483825684, "y": 0.21714285016059875}], "text": "https://github.com/\n"}
{"page": 9, "bbox": [{"x": 0.08703071624040604, "y": 0.07120878994464874}, {"x": 0.48122867941856384, "y": 0.07120878994464874}, {"x": 0.48122867941856384, "y": 0.8887912034988403}, {"x": 0.08703071624040604, "y": 0.8887912034988403}], "text": "Grave, E. 2022. Atlas: Few-shot Learning with Retrieval\nAugmented Language Models. arXiv:2208.03299.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii, E.;\nBang, Y. J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\nlucination in Natural Language Generation. ACM Comput.\nSurv., 55(12).\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel,\nT.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In Pro-\nceedings of the 34th International Conference on Neural\nInformation Processing Systems, NIPS'20. Red Hook, NY,\nUSA: Curran Associates Inc. ISBN 9781713829546.\nLi, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\nVeit, A.; Yu, F.; and Kumar, S. 2023a. Large Language\nModels with Controllable Working Memory. In Findings of\nthe Association for Computational Linguistics: ACL 2023,\n1774-1793. Toronto, Canada: Association for Computa-\ntional Linguistics.\nLi, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. Al-\npacaEval: An Automatic Evaluator of Instruction-following\nModels. https://github.com/tatsu-lab/alpaca_eval. Accessed:\n2024-01-10.\nLi, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c. Are\nChatGPT and GPT-4 General-Purpose Solvers for Financial\nText Analytics? An Examination on Several Typical Tasks.\narXiv:2305.05862.\nLiu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifi-\nability in Generative Search Engines. arXiv:2304.09848.\nMaynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.\n2020. On Faithfulness and Factuality in Abstractive Sum-\nmarization. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 1906–1919.\nOnline: Association for Computational Linguistics.\nOpenAI. 2022. Chatgpt: Optimizing language models for\ndialogue. https://openai.com/blog/chatgpt. Accessed: 2024-\n01-10.\nPeng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y.; Hu, Y.;\nHuang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.\nCheck Your Facts and Try Again: Improving Large Lan-\nguage Models with External Knowledge and Automated\nFeedback. arXiv:2302.12813.\nQin, Y.; Liang, S.; Ye, Y.; Zhu, K.; Yan, L.; Lu, Y.; Lin, Y.;\nCong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.;\nZhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023.\nToolLLM: Facilitating Large Language Models to Master\n16000+ Real-world APIs. arXiv:2307.16789.\nRaunak, V.; Menezes, A.; and Junczys-Dowmunt, M. 2021.\nThe Curious Case of Hallucinations in Neural Machine\nTranslation. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 1172-\n1183. Online: Association for Computational Linguistics.\nRen, R.; Wang, Y.; Qu, Y.; Zhao, W. X.; Liu, J.; Tian, H.;\nWu, H.; Wen, J.-R.; and Wang, H. 2023. Investigating the\n"}
{"page": 9, "bbox": [{"x": 0.520477831363678, "y": 0.20615383982658386}, {"x": 0.9129692912101746, "y": 0.20615383982658386}, {"x": 0.9129692912101746, "y": 0.8571428656578064}, {"x": 0.520477831363678, "y": 0.8571428656578064}], "text": "THUDM. 2023a. ChatGLM-6B.\nTHUDM/ChatGLM-6B. Accessed: 2024-01-10.\nTHUDM. 2023b. ChatGLM2-6B. https://github.com/\nTHUDM/ChatGLM2-6B. Accessed: 2024-01-10.\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\nA. 2023. Interleaving Retrieval with Chain-of-Thought Rea-\nsoning for Knowledge-Intensive Multi-Step Questions. In\nProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n10014–10037. Toronto, Canada: Association for Computa-\ntional Linguistics.\nWang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.;\nMichael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. Su-\nperGLUE: A Stickier Benchmark for General-Purpose Lan-\nguage Understanding Systems. Red Hook, NY, USA: Curran\nAssociates Inc.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019b. GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understanding.\nIn International Conference on Learning Representations.\nXu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;\nGao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;\nand Zhou, J. 2023a. CValues: Measuring the Values of Chi-\nnese Large Language Models from Safety to Responsibility.\narXiv:2307.09705.\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-\nS. 2023b. Search-in-the-Chain: Towards Accurate, Credi-\nble and Traceable Large Language Models for Knowledge-\nintensive Tasks. arXiv:2304.14732.\nZhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y. K.; and Bing,\nL. 2023. M3Exam: A Multilingual, Multimodal, Multilevel\nBenchmark for Examining Large Language Models.\nZhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang,\nY.; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:\nA Human-Centric Benchmark for Evaluating Foundation\nModels. arXiv:2304.06364.\nZhou, D.; Schärli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V.; and\nChi, E. H. 2023a. Least-to-Most Prompting Enables Com-\nplex Reasoning in Large Language Models. In The Eleventh\nInternational Conference on Learning Representations.\nZhou, S.; Alon, U.; Xu, F. F.; Jiang, Z.; and Neubig, G.\n2023b. DocPrompting: Generating Code by Retrieving the\nDocs. In The Eleventh International Conference on Learn-\ning Representations.\n"}
{"page": 9, "bbox": [{"x": 0.48009100556373596, "y": 0.9432967305183411}, {"x": 0.5164960026741028, "y": 0.9432967305183411}, {"x": 0.5164960026741028, "y": 0.9503296613693237}, {"x": 0.48009100556373596, "y": 0.9503296613693237}], "text": "17762\n"}
{"page": 1, "bbox": [{"x": 0.006257110275328159, "y": 0.11164835095405579}, {"x": 0.03583617880940437, "y": 0.11164835095405579}, {"x": 0.03583617880940437, "y": 0.11560439318418503}, {"x": 0.006257110275328159, "y": 0.11560439318418503}], "text": "Check for\n"}
{"page": 1, "bbox": [{"x": 0.009101251140236855, "y": 0.11648351699113846}, {"x": 0.0335608646273613, "y": 0.11648351699113846}, {"x": 0.0335608646273613, "y": 0.11999999731779099}, {"x": 0.009101251140236855, "y": 0.11999999731779099}], "text": "updates\n"}
{"page": 1, "bbox": [{"x": 0.0904436856508255, "y": 0.1063736230134964}, {"x": 0.9055745005607605, "y": 0.10197801887989044}, {"x": 0.9061433672904968, "y": 0.14901098608970642}, {"x": 0.09101251512765884, "y": 0.15340659022331238}], "text": "A Survey on RAG Meeting LLMS: Towards Retrieval-Augmented\nLarge Language Models\n"}
{"page": 1, "bbox": [{"x": 0.13765642046928406, "y": 0.1683516502380371}, {"x": 0.3265073895454407, "y": 0.16703297197818756}, {"x": 0.3270762264728546, "y": 0.2268131822347641}, {"x": 0.138225257396698, "y": 0.22813187539577484}], "text": "Wenqi Fan\nwenqifan03@gmail.com\nThe Hong Kong Polytechnic\nUniversity, HK SAR\n"}
{"page": 1, "bbox": [{"x": 0.40614333748817444, "y": 0.16967032849788666}, {"x": 0.5955631136894226, "y": 0.16923077404499054}, {"x": 0.5955631136894226, "y": 0.2268131822347641}, {"x": 0.40614333748817444, "y": 0.2272527515888214}], "text": "Yujuan Ding*\ndingyujuan385@gmail.com\nThe Hong Kong Polytechnic\nUniversity, HK SAR\n"}
{"page": 1, "bbox": [{"x": 0.6746302843093872, "y": 0.16967032849788666}, {"x": 0.8634812235832214, "y": 0.16923077404499054}, {"x": 0.8634812235832214, "y": 0.2268131822347641}, {"x": 0.6746302843093872, "y": 0.2272527515888214}], "text": "Liangbo Ning\nBigLemon1123@gmail.com\nThe Hong Kong Polytechnic\nUniversity, HK SAR\n"}
{"page": 1, "bbox": [{"x": 0.7025028467178345, "y": 0.2435164898633957}, {"x": 0.8339021801948547, "y": 0.2435164898633957}, {"x": 0.8339021801948547, "y": 0.2835164964199066}, {"x": 0.7025028467178345, "y": 0.2835164964199066}], "text": "Dawei Yin\nyindawei@acm.org\nBaidu Inc, China\n"}
{"page": 1, "bbox": [{"x": 0.12969283759593964, "y": 0.2435164898633957}, {"x": 0.33162686228752136, "y": 0.24175824224948883}, {"x": 0.3321956694126129, "y": 0.2997802197933197}, {"x": 0.13026165962219238, "y": 0.30153846740722656}], "text": "Shijie Wang\nshijie.wang1999@outlook.com\nThe Hong Kong Polytechnic\nUniversity, HK SAR\n"}
{"page": 1, "bbox": [{"x": 0.4015927314758301, "y": 0.2435164898633957}, {"x": 0.5995449423789978, "y": 0.24263736605644226}, {"x": 0.6001137495040894, "y": 0.30065932869911194}, {"x": 0.40216153860092163, "y": 0.30153846740722656}], "text": "Hengyun Li\nneilhengyun.li@polyu.edu.hk\nThe Hong Kong Polytechnic\nUniversity, HK SAR\n"}
{"page": 1, "bbox": [{"x": 0.30659839510917664, "y": 0.3169230818748474}, {"x": 0.42320817708969116, "y": 0.31472527980804443}, {"x": 0.4237770140171051, "y": 0.3274725377559662}, {"x": 0.3071672320365906, "y": 0.32967033982276917}], "text": "Tat-Seng Chua\n"}
{"page": 1, "bbox": [{"x": 0.6035267114639282, "y": 0.3164835274219513}, {"x": 0.6626848578453064, "y": 0.3142857253551483}, {"x": 0.6632537245750427, "y": 0.32879120111465454}, {"x": 0.6046643853187561, "y": 0.3309890031814575}], "text": "Qing Li\n"}
{"page": 1, "bbox": [{"x": 0.2548350393772125, "y": 0.33186814188957214}, {"x": 0.47838452458381653, "y": 0.3331868052482605}, {"x": 0.4778156876564026, "y": 0.3753846287727356}, {"x": 0.2542662024497986, "y": 0.37406593561172485}], "text": "dcscts@nus.edu.sg\nNational University of Singapore,\nSingapore\n"}
{"page": 1, "bbox": [{"x": 0.5187713503837585, "y": 0.3336263597011566}, {"x": 0.7286689281463623, "y": 0.3327472507953644}, {"x": 0.7292377948760986, "y": 0.39648351073265076}, {"x": 0.5193401575088501, "y": 0.3973626494407654}], "text": "csqli@comp.polyu.edu.hk\nThe Hong Kong Polytechnic\nUniversity, HK SAR\nCCS CONCEPTS\n"}
{"page": 1, "bbox": [{"x": 0.08759954571723938, "y": 0.38813185691833496}, {"x": 0.1820250302553177, "y": 0.38769230246543884}, {"x": 0.1820250302553177, "y": 0.3969230651855469}, {"x": 0.08759954571723938, "y": 0.3973626494407654}], "text": "ABSTRACT\n"}
{"page": 1, "bbox": [{"x": 0.5199089646339417, "y": 0.41054946184158325}, {"x": 0.5238907933235168, "y": 0.41054946184158325}, {"x": 0.5238907933235168, "y": 0.41318681836128235}, {"x": 0.5199089646339417, "y": 0.41318681836128235}], "text": "•\n"}
{"page": 1, "bbox": [{"x": 0.5199089646339417, "y": 0.40527471899986267}, {"x": 0.9141069650650024, "y": 0.4065934121608734}, {"x": 0.9141069650650024, "y": 0.4479120969772339}, {"x": 0.5199089646339417, "y": 0.44659340381622314}], "text": "General and reference →>> Surveys and overviews; • Comput-\ning methodologies - → Natural language generation; • Infor-\nmation systems → Retrieval models and ranking.\n"}
{"page": 1, "bbox": [{"x": 0.5199089646339417, "y": 0.46329671144485474}, {"x": 0.6205915808677673, "y": 0.46329671144485474}, {"x": 0.6205915808677673, "y": 0.47208791971206665}, {"x": 0.5199089646339417, "y": 0.47208791971206665}], "text": "KEYWORDS\n"}
{"page": 1, "bbox": [{"x": 0.5193401575088501, "y": 0.4817582368850708}, {"x": 0.912400484085083, "y": 0.4817582368850708}, {"x": 0.912400484085083, "y": 0.5059340596199036}, {"x": 0.5193401575088501, "y": 0.5059340596199036}], "text": "Retrieval Augmented Generation (RAG), Large Language Model\n(LLM), Pre-training, Fine-tuning, In-context Learning, Prompting\n"}
{"page": 1, "bbox": [{"x": 0.5187713503837585, "y": 0.5199999809265137}, {"x": 0.660978376865387, "y": 0.5204395651817322}, {"x": 0.660978376865387, "y": 0.5274725556373596}, {"x": 0.5187713503837585, "y": 0.5270329713821411}], "text": "ACM Reference Format:\n"}
{"page": 1, "bbox": [{"x": 0.08589305728673935, "y": 0.4065934121608734}, {"x": 0.48350396752357483, "y": 0.4065934121608734}, {"x": 0.48350396752357483, "y": 0.6940659284591675}, {"x": 0.08589305728673935, "y": 0.6940659284591675}], "text": "As one of the most advanced techniques in AI, Retrieval-Augmented\nGeneration (RAG) can offer reliable and up-to-date external knowl-\nedge, providing huge convenience for numerous tasks. Particularly\nin the era of AI-Generated Content (AIGC), the powerful capacity of\nretrieval in providing additional knowledge enables RAG to assist\nexisting generative AI in producing high-quality outputs. Recently,\nLarge Language Models (LLMs) have demonstrated revolutionary\nabilities in language understanding and generation, while still fac-\ning inherent limitations such as hallucinations and out-of-date in-\nternal knowledge. Given the powerful abilities of RAG in providing\nthe latest and helpful auxiliary information, Retrieval-Augmented\nLarge Language Models (RA-LLMs) have emerged to harness exter-\nnal and authoritative knowledge bases, rather than solely relying\non the model's internal knowledge, to augment the quality of the\ngenerated content of LLMs. In this survey, we comprehensively\nreview existing research studies in RA-LLMs, covering three pri-\nmary technical perspectives: architectures, training strategies, and\napplications. Furthermore, to deliver deeper insights, we discuss\ncurrent limitations and several promising directions for future re-\nsearch. Updated information about this survey can be found at https:\n//advanced-recommender-systems.github.io/RAG-Meets-LLMs/¹.\n"}
{"page": 1, "bbox": [{"x": 0.5182024836540222, "y": 0.5323076844215393}, {"x": 0.9158134460449219, "y": 0.5318681597709656}, {"x": 0.9158134460449219, "y": 0.6048351526260376}, {"x": 0.5182024836540222, "y": 0.6052747368812561}], "text": "Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei\nYin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs:\nTowards Retrieval-Augmented Large Language Models. In Proceedings of\nthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA,\n11 pages. https://doi.org/10.1145/3637528.3671470\n"}
{"page": 1, "bbox": [{"x": 0.5466439127922058, "y": 0.6312087774276733}, {"x": 0.6860068440437317, "y": 0.6312087774276733}, {"x": 0.6860068440437317, "y": 0.6404395699501038}, {"x": 0.5466439127922058, "y": 0.6404395699501038}], "text": "INTRODUCTION\n"}
{"page": 1, "bbox": [{"x": 0.5199089646339417, "y": 0.6325274705886841}, {"x": 0.5284414291381836, "y": 0.6325274705886841}, {"x": 0.5284414291381836, "y": 0.6399999856948853}, {"x": 0.5199089646339417, "y": 0.6399999856948853}], "text": "1\n"}
{"page": 1, "bbox": [{"x": 0.08703071624040604, "y": 0.7292307615280151}, {"x": 0.2599544823169708, "y": 0.7292307615280151}, {"x": 0.2599544823169708, "y": 0.7371428608894348}, {"x": 0.08703071624040604, "y": 0.7371428608894348}], "text": "*Corresponding author: Yujuan Ding\n"}
{"page": 1, "bbox": [{"x": 0.08703071624040604, "y": 0.7393406629562378}, {"x": 0.4738338887691498, "y": 0.7393406629562378}, {"x": 0.4738338887691498, "y": 0.7481318712234497}, {"x": 0.08703071624040604, "y": 0.7481318712234497}], "text": "¹For the long version of this survey, please refer to https://arxiv.org/abs/2405.06211\n"}
{"page": 1, "bbox": [{"x": 0.5176336765289307, "y": 0.6496703028678894}, {"x": 0.9152445793151855, "y": 0.6496703028678894}, {"x": 0.9152445793151855, "y": 0.89670330286026}, {"x": 0.5176336765289307, "y": 0.89670330286026}], "text": "As one of the most fundamental data mining techniques, retrieval\naims to understand the input query and extract relevant information\nfrom external data sources [23, 29, 60, 118]. It has found extensive\napplication in various fields [8, 27, 90, 144], such as search, ques-\ntion answering, and recommender systems. For instance, search\nengines (e.g., Google, Bing, and Baidu) are the most successful ap-\nplications of retrieval in the industry; they can filter and retrieve\nthe most relevant web pages or documents that can match a user's\nquery [19, 144], enabling users to find the desired information effec-\ntively. Meanwhile, retrieval models, through effective data mainte-\nnance in external databases, can provide faithful and timely external\nknowledge, thereby serving vital functions in various knowledge-\nintensive tasks. Due to their powerful capacities, retrieval tech-\nniques have been successfully incorporated into advanced genera-\ntive models in the era of AI-Generated Content (AIGC) [68, 112, 134].\nNotably, the integration of retrieval models with language mod-\nels has given rise to Retrieval-Augmented Generation (RAG) [66],\nwhich has emerged as one of the most representative techniques\n"}
{"page": 1, "bbox": [{"x": 0.08646188676357269, "y": 0.7837362885475159}, {"x": 0.48122867941856384, "y": 0.7837362885475159}, {"x": 0.48122867941856384, "y": 0.8940659165382385}, {"x": 0.08646188676357269, "y": 0.8940659165382385}], "text": "Permission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD '24, August 25-29, 2024, Barcelona, Spain\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08.\nhttps://doi.org/10.1145/3637528.3671470\n"}
{"page": 1, "bbox": [{"x": 0.4857792854309082, "y": 0.954285740852356}, {"x": 0.511945366859436, "y": 0.954285740852356}, {"x": 0.511945366859436, "y": 0.9613186717033386}, {"x": 0.4857792854309082, "y": 0.9613186717033386}], "text": "6491\n"}
{"page": 2, "bbox": [{"x": 0.8350397944450378, "y": 0.07868131995201111}, {"x": 0.9106939435005188, "y": 0.07780219614505768}, {"x": 0.9106939435005188, "y": 0.08571428805589676}, {"x": 0.8350397944450378, "y": 0.08659340441226959}], "text": "Wenqi Fan et al.\n"}
{"page": 2, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 2, "bbox": [{"x": 0.30887371301651, "y": 0.11296703666448593}, {"x": 0.34015926718711853, "y": 0.11340659111738205}, {"x": 0.34015926718711853, "y": 0.11956044286489487}, {"x": 0.30887371301651, "y": 0.11912088096141815}], "text": "Output\n"}
{"page": 2, "bbox": [{"x": 0.40216153860092163, "y": 0.11868131905794144}, {"x": 0.46814560890197754, "y": 0.11824175715446472}, {"x": 0.46814560890197754, "y": 0.13670329749584198}, {"x": 0.40216153860092163, "y": 0.1371428519487381}], "text": "Pre-trained\nLLMs\n"}
{"page": 2, "bbox": [{"x": 0.13936291635036469, "y": 0.117802195250988}, {"x": 0.2275312840938568, "y": 0.11736264079809189}, {"x": 0.2275312840938568, "y": 0.14505495131015778}, {"x": 0.13936291635036469, "y": 0.1454945057630539}], "text": "Which country won\nthe Women's World\nCup 2023?\n"}
{"page": 2, "bbox": [{"x": 0.3077360689640045, "y": 0.13890109956264496}, {"x": 0.33902162313461304, "y": 0.13934065401554108}, {"x": 0.33902162313461304, "y": 0.14593406021595}, {"x": 0.3077360689640045, "y": 0.1454945057630539}], "text": "Prompt\n"}
{"page": 2, "bbox": [{"x": 0.2377701997756958, "y": 0.14725275337696075}, {"x": 0.28896471858024597, "y": 0.14725275337696075}, {"x": 0.28896471858024597, "y": 0.1538461595773697}, {"x": 0.2377701997756958, "y": 0.1538461595773697}], "text": "w/o RAG\n"}
{"page": 2, "bbox": [{"x": 0.095563143491745, "y": 0.1512087881565094}, {"x": 0.12229806929826736, "y": 0.1516483575105667}, {"x": 0.12229806929826736, "y": 0.15824176371097565}, {"x": 0.095563143491745, "y": 0.15780219435691833}], "text": "User\n"}
{"page": 2, "bbox": [{"x": 0.19340158998966217, "y": 0.16131867468357086}, {"x": 0.20705346763134003, "y": 0.16131867468357086}, {"x": 0.20705346763134003, "y": 0.16747252643108368}, {"x": 0.19340158998966217, "y": 0.16747252643108368}], "text": "ටලි\n"}
{"page": 2, "bbox": [{"x": 0.2918088734149933, "y": 0.17846153676509857}, {"x": 0.31342434883117676, "y": 0.16043956577777863}, {"x": 0.319112628698349, "y": 0.16439560055732727}, {"x": 0.2974971532821655, "y": 0.18285714089870453}], "text": "Output\n"}
{"page": 2, "bbox": [{"x": 0.3913538157939911, "y": 0.16923077404499054}, {"x": 0.4266211688518524, "y": 0.16967032849788666}, {"x": 0.4266211688518524, "y": 0.17494505643844604}, {"x": 0.3913538157939911, "y": 0.17450548708438873}], "text": "Context\n"}
{"page": 2, "bbox": [{"x": 0.3122866749763489, "y": 0.190769225358963}, {"x": 0.3344709873199463, "y": 0.17318680882453918}, {"x": 0.34072810411453247, "y": 0.17802198231220245}, {"x": 0.31854379177093506, "y": 0.19560439884662628}], "text": "Prompt\n"}
{"page": 2, "bbox": [{"x": 0.13651877641677856, "y": 0.1736263781785965}, {"x": 0.22923776507377625, "y": 0.1736263781785965}, {"x": 0.22923776507377625, "y": 0.20615383982658386}, {"x": 0.13651877641677856, "y": 0.20615383982658386}], "text": "As of my last update\nin January 2022, I\ncan't provide which\ncountry won... 2023.\n"}
{"page": 2, "bbox": [{"x": 0.40898749232292175, "y": 0.20219780504703522}, {"x": 0.4613196849822998, "y": 0.20219780504703522}, {"x": 0.4613196849822998, "y": 0.21978022158145905}, {"x": 0.40898749232292175, "y": 0.21978022158145905}], "text": "External\nDatabase\n"}
{"page": 2, "bbox": [{"x": 0.3060295879840851, "y": 0.2158241719007492}, {"x": 0.33503982424736023, "y": 0.2158241719007492}, {"x": 0.33503982424736023, "y": 0.22197802364826202}, {"x": 0.3060295879840851, "y": 0.22197802364826202}], "text": "Query\n"}
{"page": 2, "bbox": [{"x": 0.1370875984430313, "y": 0.22769230604171753}, {"x": 0.19738338887691498, "y": 0.22637362778186798}, {"x": 0.19738338887691498, "y": 0.23384615778923035}, {"x": 0.1370875984430313, "y": 0.2351648360490799}], "text": "Spain won the\n"}
{"page": 2, "bbox": [{"x": 0.09385665506124496, "y": 0.2272527515888214}, {"x": 0.12286689132452011, "y": 0.22769230604171753}, {"x": 0.12229806929826736, "y": 0.25010988116264343}, {"x": 0.09328782558441162, "y": 0.24967032670974731}], "text": "B\n"}
{"page": 2, "bbox": [{"x": 0.13765642046928406, "y": 0.23472528159618378}, {"x": 0.22696246206760406, "y": 0.2351648360490799}, {"x": 0.22696246206760406, "y": 0.25010988116264343}, {"x": 0.13765642046928406, "y": 0.24967032670974731}], "text": "Women's World Cup\n2023.\n"}
{"page": 2, "bbox": [{"x": 0.31569966673851013, "y": 0.23472528159618378}, {"x": 0.46188852190971375, "y": 0.23472528159618378}, {"x": 0.46188852190971375, "y": 0.25670328736305237}, {"x": 0.31569966673851013, "y": 0.25670328736305237}], "text": "Additional information:\nNew, Domain-specific, etc.\n"}
{"page": 2, "bbox": [{"x": 0.2349260449409485, "y": 0.2549450695514679}, {"x": 0.2883959114551544, "y": 0.2549450695514679}, {"x": 0.2883959114551544, "y": 0.26153847575187683}, {"x": 0.2349260449409485, "y": 0.26153847575187683}], "text": "with RAG\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.2887912094593048}, {"x": 0.48407280445098877, "y": 0.2892307639122009}, {"x": 0.48407280445098877, "y": 0.3832966983318329}, {"x": 0.08703071624040604, "y": 0.38285714387893677}], "text": "Figure 1: Retrieval-Augmented Generation (RAG) meets\nLarge Language Models (LLMs). When the user's query is out-\nof-scope, e.g., unseen content in training data or requiring\nthe latest information for the answer, LLMs might show in-\nferior generation performance. With the help of RAG, LLMS\ncan leverage additional relevant information from external\ndatabase to enhance their text generation capability.\n"}
{"page": 2, "bbox": [{"x": 0.5182024836540222, "y": 0.10901098698377609}, {"x": 0.914675772190094, "y": 0.10901098698377609}, {"x": 0.9152445793151855, "y": 0.7292307615280151}, {"x": 0.5187713503837585, "y": 0.7292307615280151}], "text": "instance, a recent study has demonstrated that legal hallucinations\nare pervasive and disturbing, with hallucination rates ranging from\n69% to 88% in responses to specific legal queries for state-of-the-art\nLLMS [20]. Moreover, the challenges of tackling the hallucination\nproblem become even harder due to the substantial computational\nresources required for fine-tuning LLMs with domain-specific or\nthe latest data. This, in turn, significantly hinders the widespread\nadoption of LLMs in various real-world applications.\nTo address these limitations, recent efforts have been made to\ntake advantage of RAG to enhance the capabilities of LLMs in var-\nious tasks [6, 49, 56, 114], especially those demanding high for\nthe latest and reliable knowledge such as Question Answer (QA),\nAI4Science, and software engineering. For example, Lozano et al.\n[80] introduces a scientific QA system based on retrieving scientific\nliterature dynamically. MolReGPT leverages RAG to enhance the\nin-context learning ability of ChatGPT for molecular discovery [68].\nIt is also been demonstrated that RAG can effectively reduce halluci-\nnations in conversational tasks [116, 139]. As illustrated in Figure 1,\nan LLM-based dialog system will not be able to answer well for\nout-of-scope queries. With the help of RAG to retrieve relevant\nknowledge from external database and integrate it into the process\nof generation, the dialog system succeeds in giving correct answers.\nGiven the remarkable progress in advancing LLMs with RAG, there\nis an imperative need for a systematic review of recent advances in\nRetrieval-Augmented Large Language Models (RA-LLMs).\nThis survey provides a comprehensive overview of RA-LLMS\nby summarizing representative methods from the aspects of the\narchitecture, training strategy, and application area respectively.\nIt first review the architecture of existing RA-LLMs from three\nprimary perspectives: retrieval, generation, and augmentation in\nSection 2. Training techniques are further summarized in Section 3.\nSubsequently, various RA-LLMs applications are presented in Sec-\ntion 4. In Section 5, key challenges and potential directions for\nfuture exploration are further discussed. Due to the page limit, this\npublished version omits a part content including background knowl-\nedge of LLMs, details of RA-LLM architectures, visual illustrations,\netc. Please refer to the long version for more information [32].\nConcurrent to our survey, several related surveys have diverse fo-\ncuses for RAG and LLMs. For example, Zhao et al. [156] specifically\nreview multi-modal information-based RAG techniques and Zhao\net al. [155] discuss the RAG for AIGC. Gao et al. [39] conduct a rela-\ntively comprehensive overview of RAG for LLMs. Our survey differs\nfrom these surveys in concentrating on technical perspectives and\nsystematically reviewing models according to the architecture and\ntraining paradigm in RA-LLMs, as well as application tasks.\n"}
{"page": 2, "bbox": [{"x": 0.08703071624040604, "y": 0.4008791148662567}, {"x": 0.48350396752357483, "y": 0.4008791148662567}, {"x": 0.48350396752357483, "y": 0.8958241939544678}, {"x": 0.08703071624040604, "y": 0.8958241939544678}], "text": "in the field of generative AI, aiming to enhance the quality of the\ngenerated text content with retrieved information [6, 66, 68].\nMore specifically, to facilitate the generation task in the NLP\narea, RAG incorporates information or knowledge from external\ndata sources, which serves as supplementary reference/instruction\nfor the input query or the generated output [56, 87]. In general, RAG\nfirst invokes the retriever to search and extract relevant documents\nin the external database. These documents are then combined with\nthe original query as the context to enhance the answer generation\nprocess [50]. In practice, RAG techniques are feasible and efficient\nto apply in various generation tasks, by simply adapting the re-\ntrieval component and requiring minimal or even no additional\ntraining[98]. Recent studies have demonstrated the great potential\nof RAG not only for knowledge-intensive tasks such as open do-\nmain question answering (OpenQA) [6, 44, 92], but also for general\nlanguage tasks and downstream applications [56, 78, 134, 138].\nRecent years have witnessed the rapid development of pre-trained\nfoundation models, particularly Large Language Models (LLMs).\nThese models have demonstrated impressive performance across\nvarious tasks [1, 18], such as recommender systems [37], molecule\ndiscovery [68], and report generation [26]. Technically, the great\nsuccess of LLMs can be attributed to the advanced architecture with\nbillion-level parameters pre-training on a huge amount of training\ncorpus from various sources. These technical improvements lead\nto the emergence of remarkable capabilities of LLMs [37, 157], par-\nticularly in language understanding and generation, in-context\nlearning, and other aspects. For instance, GPT-FAR introduces\ndetailed prompts to teach GPT-4 to perform image tagging, sta-\ntistical analysis and text analysis for multi-modal fashion report\ngeneration [26]. LLMs also achieve promising performance in rec-\nommender systems by understanding users' preferences towards\nitems [37, 127]. Despite these success, LLMs still suffer from intrin-\nsic limitations [37, 157], such as the lack of domain-specific knowl-\nedge, the issue of “hallucination”, and the substantial computational\nresources required for updating the LLMs. These problems are par-\nticularly notable in domain-specific fields like medicine and law. For\n"}
{"page": 2, "bbox": [{"x": 0.5199089646339417, "y": 0.7485714554786682}, {"x": 0.5273037552833557, "y": 0.7485714554786682}, {"x": 0.5273037552833557, "y": 0.7560439705848694}, {"x": 0.5199089646339417, "y": 0.7560439705848694}], "text": "2\n"}
{"page": 2, "bbox": [{"x": 0.5182024836540222, "y": 0.746813178062439}, {"x": 0.9129692912101746, "y": 0.7472527623176575}, {"x": 0.9129692912101746, "y": 0.8219780325889587}, {"x": 0.5182024836540222, "y": 0.8215384483337402}], "text": "RETRIEVAL-AUGMENTED LARGE\nLANGUAGE MODELS (RA-LLMS)\nThe RAG framework in the era of LLMs consists of several major\nprocesses: retrieval, generation, and augmentation. In this section,\nwe will introduce important techniques involved in each process.\n"}
{"page": 2, "bbox": [{"x": 0.5193401575088501, "y": 0.8391208648681641}, {"x": 0.6336746215820312, "y": 0.8382417559623718}, {"x": 0.6336746215820312, "y": 0.847912073135376}, {"x": 0.5193401575088501, "y": 0.8487911820411682}], "text": "2.1 Retrieval\n"}
{"page": 2, "bbox": [{"x": 0.5187713503837585, "y": 0.8575823903083801}, {"x": 0.914675772190094, "y": 0.8575823903083801}, {"x": 0.914675772190094, "y": 0.8958241939544678}, {"x": 0.5187713503837585, "y": 0.8958241939544678}], "text": "Given the query from the input of LLMs, the retriever is an infor-\nmation provider in RAG, aiming to return relevant knowledge by\nmeasuring the distance between the query and documents from\n"}
{"page": 2, "bbox": [{"x": 0.48634812235832214, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9613186717033386}, {"x": 0.48634812235832214, "y": 0.9613186717033386}], "text": "6492\n"}
{"page": 3, "bbox": [{"x": 0.08646188676357269, "y": 0.07868131995201111}, {"x": 0.502275288105011, "y": 0.07868131995201111}, {"x": 0.502275288105011, "y": 0.08615384250879288}, {"x": 0.08646188676357269, "y": 0.08615384250879288}], "text": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\n"}
{"page": 3, "bbox": [{"x": 0.693401575088501, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.693401575088501, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 3, "bbox": [{"x": 0.5978384613990784, "y": 0.10901098698377609}, {"x": 0.640500545501709, "y": 0.10901098698377609}, {"x": 0.640500545501709, "y": 0.1147252768278122}, {"x": 0.5978384613990784, "y": 0.1147252768278122}], "text": "Retrieval\n"}
{"page": 3, "bbox": [{"x": 0.7730375528335571, "y": 0.12703296542167664}, {"x": 0.8139931559562683, "y": 0.12835164368152618}, {"x": 0.8134243488311768, "y": 0.13494504988193512}, {"x": 0.7724686861038208, "y": 0.13362637162208557}], "text": "Chunking\n"}
{"page": 3, "bbox": [{"x": 0.647895336151123, "y": 0.12747253477573395}, {"x": 0.6894198060035706, "y": 0.12791208922863007}, {"x": 0.6894198060035706, "y": 0.134505495429039}, {"x": 0.647895336151123, "y": 0.13406594097614288}], "text": "Indexing\n"}
{"page": 3, "bbox": [{"x": 0.08703071624040604, "y": 0.10945054888725281}, {"x": 0.4829351603984833, "y": 0.10989011079072952}, {"x": 0.4829351603984833, "y": 0.20395603775978088}, {"x": 0.08703071624040604, "y": 0.20351648330688477}], "text": "the external knowledge sources. As shown in Figure 2, the retrieval\ncomponent consists of several compulsory or optional procedures\nto function as a whole for effective information retrieval. The spe-\ncific pipeline of the retrieval part is jointly determined by several\nperspectives of design, such as retriever type and retrieval granu-\nlarity. In this subsection, we will introduce the existing retrieval\nmethods in RA-LLMS based on these key aspects.\n"}
{"page": 3, "bbox": [{"x": 0.8037542700767517, "y": 0.1595604419708252}, {"x": 0.8509669899940491, "y": 0.1595604419708252}, {"x": 0.8509669899940491, "y": 0.16571427881717682}, {"x": 0.8037542700767517, "y": 0.16571427881717682}], "text": "Datastore\n"}
{"page": 3, "bbox": [{"x": 0.5978384613990784, "y": 0.1599999964237213}, {"x": 0.6598407030105591, "y": 0.1569230705499649}, {"x": 0.6604095697402954, "y": 0.16615384817123413}, {"x": 0.5984072685241699, "y": 0.16923077404499054}], "text": "B Embedding\n"}
{"page": 3, "bbox": [{"x": 0.7104664444923401, "y": 0.15824176371097565}, {"x": 0.7593856453895569, "y": 0.15912087261676788}, {"x": 0.7593856453895569, "y": 0.1679120808839798}, {"x": 0.7104664444923401, "y": 0.16703297197818756}], "text": "Searching\n"}
{"page": 3, "bbox": [{"x": 0.587599515914917, "y": 0.18373626470565796}, {"x": 0.6513082981109619, "y": 0.18329671025276184}, {"x": 0.6513082981109619, "y": 0.19120879471302032}, {"x": 0.587599515914917, "y": 0.19164834916591644}], "text": "Augmentation\n"}
{"page": 3, "bbox": [{"x": 0.7263936400413513, "y": 0.1934065967798233}, {"x": 0.776450514793396, "y": 0.19384615123271942}, {"x": 0.776450514793396, "y": 0.20967033505439758}, {"x": 0.7263936400413513, "y": 0.20923076570034027}], "text": "Retrieved\nDocuments\n"}
{"page": 3, "bbox": [{"x": 0.5233219861984253, "y": 0.20527473092079163}, {"x": 0.5523322224617004, "y": 0.20615383982658386}, {"x": 0.5517633557319641, "y": 0.21406593918800354}, {"x": 0.522753119468689, "y": 0.2131868153810501}], "text": "Input\n"}
{"page": 3, "bbox": [{"x": 0.8742889761924744, "y": 0.20615383982658386}, {"x": 0.9118316173553467, "y": 0.2070329636335373}, {"x": 0.9112628102302551, "y": 0.21494504809379578}, {"x": 0.873720109462738, "y": 0.21406593918800354}], "text": "Output\n"}
{"page": 3, "bbox": [{"x": 0.08816837519407272, "y": 0.22241757810115814}, {"x": 0.11433447152376175, "y": 0.22241757810115814}, {"x": 0.11433447152376175, "y": 0.22857142984867096}, {"x": 0.08816837519407272, "y": 0.22857142984867096}], "text": "2.1.1\n"}
{"page": 3, "bbox": [{"x": 0.6831626892089844, "y": 0.2210988998413086}, {"x": 0.7565415501594543, "y": 0.2210988998413086}, {"x": 0.7565415501594543, "y": 0.235604390501976}, {"x": 0.6831626892089844, "y": 0.235604390501976}], "text": "Intermediate Layer\nIntegration\n"}
{"page": 3, "bbox": [{"x": 0.5921501517295837, "y": 0.22241757810115814}, {"x": 0.6365187764167786, "y": 0.22241757810115814}, {"x": 0.6365187764167786, "y": 0.235604390501976}, {"x": 0.5921501517295837, "y": 0.235604390501976}], "text": "Input Layer\nIntegration\n"}
{"page": 3, "bbox": [{"x": 0.7974971532821655, "y": 0.22241757810115814}, {"x": 0.8469852209091187, "y": 0.22241757810115814}, {"x": 0.8469852209091187, "y": 0.235604390501976}, {"x": 0.7974971532821655, "y": 0.235604390501976}], "text": "Output Layer\nIntegration\n"}
{"page": 3, "bbox": [{"x": 0.6968145370483398, "y": 0.25010988116264343}, {"x": 0.7707622051239014, "y": 0.25054946541786194}, {"x": 0.7707622051239014, "y": 0.2659340798854828}, {"x": 0.6968145370483398, "y": 0.2654944956302643}], "text": "Large Language\nModels\n"}
{"page": 3, "bbox": [{"x": 0.5938566327095032, "y": 0.26813188195228577}, {"x": 0.6450511813163757, "y": 0.26769229769706726}, {"x": 0.6450511813163757, "y": 0.2742857038974762}, {"x": 0.5938566327095032, "y": 0.2747252881526947}], "text": "Generation\n"}
{"page": 3, "bbox": [{"x": 0.5193401575088501, "y": 0.294065922498703}, {"x": 0.914675772190094, "y": 0.2949450612068176}, {"x": 0.9141069650650024, "y": 0.39076924324035645}, {"x": 0.5187713503837585, "y": 0.3898901045322418}], "text": "Figure 2: Illustration of the basic Retrieval-Augmented Large\nLanguage Models (RA-LLMs) framework, which consists of\nthree major components: retrieval, generation, and augmen-\ntation. Retrieval includes different procedures depending\non the specific designs. The retrieved documents are fur-\nther leveraged in generation with the augmentation module,\nwhich may be at different integration stages.\n"}
{"page": 3, "bbox": [{"x": 0.5187713503837585, "y": 0.42945054173469543}, {"x": 0.9152445793151855, "y": 0.42945054173469543}, {"x": 0.9152445793151855, "y": 0.6052747368812561}, {"x": 0.5187713503837585, "y": 0.6052747368812561}], "text": "similarity-based retrieval, as well as for some special requirement\nin ICL, such as diverse example retrieval [141]. Another stream of\ndense retrievers having been widely applied in RA-LLMs uses one\nencoder only, which may be based on Transformer, BERT or other\noff-the-shelf sequence modeling backbones. These one-encoder\nretrievers are generally pre-trained on large-scale unaligned doc-\numents by contrastive learning [103], which may therefore excel\nfor their versatility, meaning that they can transfer and generalize\nbetter to new domains or tasks. Such general-purpose pre-trained\nretrievers, e.g., Contriever [40] and Spider [99], would be more flexi-\nble to use in LLMs targeting on various tasks and have demonstrated\ntheir effectiveness in many RA-LLM methods, such as In-Context\nRALM [98], Atlas [51] and Self-RAG [5].\n"}
{"page": 3, "bbox": [{"x": 0.08646188676357269, "y": 0.21934065222740173}, {"x": 0.48350396752357483, "y": 0.21934065222740173}, {"x": 0.48350396752357483, "y": 0.8949450254440308}, {"x": 0.08646188676357269, "y": 0.8949450254440308}], "text": "Retriever Type. Retrieval methods can be generally catego-\nrized into two types: sparse and dense, based on the information\nencoding methods. Sparse retrieval is word-based and applied in\ntext retrieval mostly, while dense retrieval embeds queries and ex-\nternal knowledge into vector spaces and can be applied to various\ndata formats.\nAs a straightforward approach, sparse retrieval, e.g., TF-IDF and\nBM25 [106, 121], usually relies on inverted index matching along\nwith the raw data input. For example, many studies directly ap-\nply BM25 for passage-level retrieval to facilitate their RAG [10, 52,\n98, 137, 159, 160], where passages are specifically represented as\na bag of words and ranked based on term and inverse document\nfrequencies [50]. On top of offering supplementary to enhance\nthe input of the generator, sparse retrieval has also been used to\nfind demonstrations to function in In-Context Learning (ICL) for\nRA-LLMS [2, 83, 107, 117, 141]. The main limitation of applying\nsparse retrieval in RAG is its no-training nature, which makes the\nretrieval performance heavily rely on the quality of the database\nand the query. Moreover, such fixed term-based methods only sup-\nport similarity-based retrieval, while cannot be adapted for other\nretrieval criteria possibly existing in LLM applications, such as the\ndiversity [30].\nDense retrieval, on the contrary, embeds the query and docu-\nments into continuous vector space with certain criteria, for exam-\nple, semantic similarity [55]. Dense retrieval methods are usually\ntrainable, therefore holding more flexibility and potential in adap-\ntation. As the key component of dense retriever, the embedding\nmodels have delicately different designs in existing RAG models.\nA simple design [56, 64, 136] is to directly use a part of the gener-\nation model as the embedding layer of the retriever, which might\nbe able to enhance the alignment between the retrieval and gen-\neration processes. BERT-based backbone [24] is widely applied\nin retrieval models. One common retriever design in RAG is to\nconstruct two-stream encoders with the BERT structure (one en-\ncoder for the query and the other for the documents), which is\nalso called bi-encoder [114, 135]. Early-stage RAG methods tend to\nfreeze [6, 98] or partially freeze [66] the parameters of the retriever\nto perform general-level relevant knowledge extraction and pay\nmore attention to the knowledge leveraging and generator fine-\ntuning. Large-scale specialized pre-training further enhances RAG\nmodels to excel in more knowledge-intensive tasks. One typical\nsuccess is Dense Passage Retriever (DPR) [55], which uses a BERT-\nbased backbone and is pre-trained specifically for the OpenQA task\nwith question-answer pair data. A recent study [103] has also dis-\ncovered that DPR training decentralizes how knowledge is stored\nin the network, creating multiple access pathways to the same in-\nformation. With effective fine-tuning, bi-encoder retrievers are also\napplied widely in ICL-based RAG [72, 81, 86, 93, 107, 141]. Specif-\nically, they have been more often used for sentence embedding\n"}
{"page": 3, "bbox": [{"x": 0.5187713503837585, "y": 0.6224175691604614}, {"x": 0.914675772190094, "y": 0.6224175691604614}, {"x": 0.914675772190094, "y": 0.8958241939544678}, {"x": 0.5187713503837585, "y": 0.8958241939544678}], "text": "2.1.2 Retrieval Granularity. Retrieval granularity denotes the re-\ntrieval unit in which the corpus is indexed, e.g., document, passage,\ntoken, or other levels like entity. For RAG, the choice of retrieval\ngranularity can significantly impact the overall performance of the\nmodel in terms of effectiveness and efficiency as they determine\nthe saving space for the database as well as the computational cost\nfor searching [4]. Early stage retrieval-augmented language mod-\nels [10] propose to retrieve whole pieces of documents, and then\napply a machine comprehension model trained to detect answer\nspans in the returned documents, which focuses more on language\nreading and key information locating in the document. In gener-\native language models, Chunk retrieval (also called passages in\nsome references [44, 52, 55]) is common, which has been used in\nboth traditional and LLM-based RAG models such as REALM [44],\nRAG [66] and Atlas [51]. A more fine-grained retrieval, i.e., token\nretrieval, instead can be done with faster searching but will bring\nmore burden for the database saving. Token retrieval is more suit-\nable in cases requiring rare patterns or out-of-domain data [56],\nmeanwhile cooperates well with the every-token retrieval strat-\negy as applied in kNN-LM and other similar work [45, 88, 145]. In\n"}
{"page": 3, "bbox": [{"x": 0.48634812235832214, "y": 0.954285740852356}, {"x": 0.5136518478393555, "y": 0.954285740852356}, {"x": 0.5136518478393555, "y": 0.9613186717033386}, {"x": 0.48634812235832214, "y": 0.9613186717033386}], "text": "6493\n"}
{"page": 4, "bbox": [{"x": 0.8350397944450378, "y": 0.07868131995201111}, {"x": 0.9106939435005188, "y": 0.07780219614505768}, {"x": 0.9106939435005188, "y": 0.08571428805589676}, {"x": 0.8350397944450378, "y": 0.08659340441226959}], "text": "Wenqi Fan et al.\n"}
{"page": 4, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.11032967269420624}, {"x": 0.9141069650650024, "y": 0.11032967269420624}, {"x": 0.9141069650650024, "y": 0.25846153497695923}, {"x": 0.5187713503837585, "y": 0.25846153497695923}], "text": "on the retrieval and augmentation processes, trying to enhance\nthe generator by augmenting the input (also called prompt in the\ncontext of LLMs) with better knowledge, guidance, or examples for\nthe generation. For example, Rubin et al. [107] proposes to train a\nprompt retriever with the data labeled by language models them-\nselves, which can be used to provide better examples for in-context\nlearning, therefore enhancing the final generation performance. Xu\net al. [137] propose to compress the retrieved documents before\nin-context integration, which can reduce the computational costs\nand also relieve the burden of LMs to identify relevant information\nin long retrieved documents.\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.10989011079072952}, {"x": 0.48236632347106934, "y": 0.10989011079072952}, {"x": 0.48236632347106934, "y": 0.3279120922088623}, {"x": 0.08703071624040604, "y": 0.3279120922088623}], "text": "comparison, a text chunk may contain compact and complete infor-\nmation with less redundancy and irrelevancy, therefore becoming\nthe mainstream retrieval text granularity in RAG.\nAnother major retrieval granularity proposed in RAG is entity\nretrieval. Unlike the above types of granularity, entity retrieval is\ndesigned from the perspective of knowledge rather than language.\nFévry et al. [38] introduce the Entities as Experts (EAE) model,\nwhich divides the parameter space of language models according\nto the entity identity. The proposed EAE model aims to learn entity\nrepresentations from the text along with other model parameters\nwith the Wikipedia database and represent knowledge with entity\nmemory. At a more fine-grained level, de Jong et al. [21] propose to\nbuild the knowledge base by learning and retrieving mention rather\nthan entity. Overall, applying entity or mention-level retrieval in\nRAG would be more effective for entity-centric tasks, and more\nefficient in space compared to token-wise retrieval.\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.282637357711792}, {"x": 0.5426621437072754, "y": 0.282637357711792}, {"x": 0.5426621437072754, "y": 0.29054945707321167}, {"x": 0.5187713503837585, "y": 0.29054945707321167}], "text": "2.3\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.28131869435310364}, {"x": 0.914675772190094, "y": 0.28087911009788513}, {"x": 0.914675772190094, "y": 0.38241758942604065}, {"x": 0.5187713503837585, "y": 0.38285714387893677}], "text": "Retrieval Integration for Generation\nAugmentation\nAugmentation describes the technical process that integrates re-\ntrieval and generation parts, which is the essential part of RA-LLMs.\nIn this subsection, we introduce three main designs of augmenta-\ntion, which are conducted at the input, output, and intermediate\nlayers of generator respectively, as illustrated in Figure 2.\n"}
{"page": 4, "bbox": [{"x": 0.127986341714859, "y": 0.3468131721019745}, {"x": 0.21786120533943176, "y": 0.34637361764907837}, {"x": 0.21786120533943176, "y": 0.3560439646244049}, {"x": 0.127986341714859, "y": 0.356483519077301}], "text": "Generation\n"}
{"page": 4, "bbox": [{"x": 0.08759954571723938, "y": 0.3468131721019745}, {"x": 0.10921501368284225, "y": 0.3468131721019745}, {"x": 0.10921501368284225, "y": 0.356483519077301}, {"x": 0.08759954571723938, "y": 0.356483519077301}], "text": "2.2\n"}
{"page": 4, "bbox": [{"x": 0.08703071624040604, "y": 0.36527472734451294}, {"x": 0.48350396752357483, "y": 0.36571428179740906}, {"x": 0.48350396752357483, "y": 0.47252747416496277}, {"x": 0.08703071624040604, "y": 0.47208791971206665}], "text": "The design of the generator heavily depends on the downstream\ntasks. For most text generation tasks, Decoder-only and Encoder-\nDecoder are two dominant structures [157]. The recent develop-\nment of commercial closed-sourced large foundation models makes\nblack-box generation models mainstream in RA-LLMs. In this part,\nwe will briefly review studies with these two types of genera-\ntors: parameter-accessible (white-box) and parameter-inaccessible\n(black-box).\n"}
{"page": 4, "bbox": [{"x": 0.08816837519407272, "y": 0.4874725341796875}, {"x": 0.11433447152376175, "y": 0.4879120886325836}, {"x": 0.11433447152376175, "y": 0.49494504928588867}, {"x": 0.08816837519407272, "y": 0.49450549483299255}], "text": "2.2.1\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.4008791148662567}, {"x": 0.914675772190094, "y": 0.4008791148662567}, {"x": 0.914675772190094, "y": 0.8232967257499695}, {"x": 0.5187713503837585, "y": 0.8232967257499695}], "text": "2.3.1 Input-Layer Integration. A common way to integrate retrieved\ninformation/documents is to combine them with the original in-\nput/query and jointly pass them to the generator, which is called\ninput-layer integration. For example, In-Context RALM [98] applies\ninput-layer integration by specifically concatenating the original\ninput and all retrieved documents into a single sequence as the\nnew input for the generation model. Despite the effectiveness, such\nintegration is limited to the number of retrieved documents, since\nthe concatenated new input may be too long to be processed by\nthe generation model. In-context RALM specifically alleviates this\nlimitation by removing tokens from the beginning of the new in-\nput. To avoid information loss with such a token removing strategy,\nFID [50] employs a different integration method that\neach\nprocesses\nretrieved document independently in the encoder. This strategy\nis scalable to a large number of contexts as it only performs self-\nattention over one context at a time in the follow-up processing.\nAtlas [51] and REPLUG [114] apply a similar parallel integration\nby concatenating the query and one retrieved document at a time.\nIn general, most black-box generation-based RAG methods apply\ninput-layer integration since neither the intermediate layer of the\ngeneration model or the output distribution is accessible.\nMore specially for LLMs, input-layer integration may use the\nretrieved content as (additional) prompts or demonstrations on top\nof using it as supplementary to the original input as in traditional\nRAGS [107]. Prompt retrieval aims to find suitable natural language\nprompts automatically through retrieval to teach the LLM to learn\nin context [7] or to induce the LLM to reason[133]. It may boost\nthe zero-shot ability of LLMs without delicate prompt engineering.\nFor example, Cheng et al. [16] propose to learn a prompt retriever\nbased on the input-prompt pair data with score labels resulting\nfrom a frozen LLM.\n"}
{"page": 4, "bbox": [{"x": 0.08646188676357269, "y": 0.4870329797267914}, {"x": 0.48350396752357483, "y": 0.4870329797267914}, {"x": 0.48350396752357483, "y": 0.894505500793457}, {"x": 0.08646188676357269, "y": 0.894505500793457}], "text": "Parameter-Accessible Generators (White-box). The structure\nof Encoder-Decoder processes the input and the target independently\nwith different sets of parameters, in which a cross-attention compo-\nnent is developed to connect input tokens to target tokens. Repre-\nsentative Encoder-Decoder models include T5 [97] and BART [65].\nIn comparison, Decoder-only models process inputs and targets after\nconcatenation, which makes the representations of the two parts\nconcurrently built layer-by-layer as they propagate up the network.\nThese two types of generators are widely applied in existing RAG\nwork. For example, RAG [66] and Re²G [42] employ BART; FID [50]\nand EMDR² utilize T5. There are other models [6, 73] leveraging\nTransformer-based Encoder-Decoder architecture but with some\ncustomized design. Generators in RAG differ themselves from gen-\neral ones by incorporating retrieved data to enhance the generation\naccuracy and relevance. Furthermore, white-box generators allow\nparameter optimization, which can be trained to adapt to different\nretrieval and augmentation approaches for a better performance of\ngeneration.\n2.2.2 Parameter-Inaccessible Generators (Black-box). A certain pro-\nportion of LLMs are released without the disclosure of internal\nstructures or the accessibility of parameters, especially those par-\nticularly large-scale ones such as GPT series [1], Codex [12] and\nClaude, which are called black-box generation models. These gen-\nerators only allow the operations of feeding queries (input) and re-\nceiving responses (output) while not allowing the internal structure\nto be altered or parameters to be updated. From another perspec-\ntive, LLMs, even those open for fine-tuning, are large in scale and\ndifficult to tune for downstream domain-specific tasks with only a\nlimited amount of data. Black-box RA-LLMs, therefore, focus more\n"}
{"page": 4, "bbox": [{"x": 0.5187713503837585, "y": 0.84351646900177}, {"x": 0.9118316173553467, "y": 0.8430769443511963}, {"x": 0.9118316173553467, "y": 0.8953846096992493}, {"x": 0.5187713503837585, "y": 0.8958241939544678}], "text": "2.3.2 Output-Layer Integration. Another kind of augmentation is\npost-hoc, i.e., output-layer integration, which joints retrieval and\ngeneration results. For example, kNN-LM [56] interpolates two\nnext-token distributions in prediction: one induced by the LM and\n"}
{"page": 4, "bbox": [{"x": 0.48634812235832214, "y": 0.9547252655029297}, {"x": 0.5136518478393555, "y": 0.9547252655029297}, {"x": 0.5136518478393555, "y": 0.9608791470527649}, {"x": 0.48634812235832214, "y": 0.9608791470527649}], "text": "6494\n"}
{"page": 5, "bbox": [{"x": 0.08646188676357269, "y": 0.07868131995201111}, {"x": 0.502275288105011, "y": 0.07868131995201111}, {"x": 0.502275288105011, "y": 0.08615384250879288}, {"x": 0.08646188676357269, "y": 0.08615384250879288}], "text": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\n"}
{"page": 5, "bbox": [{"x": 0.693401575088501, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.693401575088501, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 5, "bbox": [{"x": 0.5182024836540222, "y": 0.11032967269420624}, {"x": 0.9135380983352661, "y": 0.11032967269420624}, {"x": 0.9135380983352661, "y": 0.21802197396755219}, {"x": 0.5182024836540222, "y": 0.21802197396755219}], "text": "these training-based approaches into three classes: 1) Independent\nTraining approaches independently train each component in the\nRAG procedure, 2) Sequential Training methods train one module\nfirst and freeze the well-trained component to guide the tuning\nprocess of the other part, and 3) Joint Training approaches train\nretriever and generator simultaneously. In the following section,\nwe will comprehensively review the training-free, independent\ntraining, sequential training, and joint training methods.\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.10989011079072952}, {"x": 0.48350396752357483, "y": 0.11032967269420624}, {"x": 0.48350396752357483, "y": 0.3415384590625763}, {"x": 0.08703071624040604, "y": 0.3410989046096802}], "text": "the other induced by the nearest neighbors from the retrieval cor-\npus. Output-layer linear integration [43, 159] is flexible to apply\nsince it can be plugged into most generation models without addi-\ntional training. However, the simplicity of output-layer integration\nalso limits the model's ability to reason about the retrieved text.\nTo tackle this limitation, Yogatama et al. [145] propose to add an\nextra gating network to post-process the retrieved data and achieve\ncomparatively better performance. For LLMs, output-layer inte-\ngration is as reasonable and adaptive as input-layer integration.\nREFEED [148] proposes an answer refining mechanism that applies\nan LLM to evaluate the retrieved information and adjust the initial\nanswer accordingly to enhance the accuracy of the response. Sim-\nilarly, Zhang et al. [154] propose the COMBO framework, which\nmatches LLM-generated passages with retrieved counterparts into\ncompatible pairs based on pre-trained discriminators. The passage\npairs are then handled by a Fusion-in-Decoder-based [50] to derive\na final answer.\n"}
{"page": 5, "bbox": [{"x": 0.5193401575088501, "y": 0.24879120290279388}, {"x": 0.6689419746398926, "y": 0.24747252464294434}, {"x": 0.6689419746398926, "y": 0.2602197825908661}, {"x": 0.5193401575088501, "y": 0.26153847575187683}], "text": "3.1 Training-free\n"}
{"page": 5, "bbox": [{"x": 0.5176336765289307, "y": 0.266813188791275}, {"x": 0.9152445793151855, "y": 0.26725274324417114}, {"x": 0.914675772190094, "y": 0.5410988926887512}, {"x": 0.5170648694038391, "y": 0.5406593680381775}], "text": "With the huge number of parameters, large language models have\nexhibited human-level intelligence and achieved promising pre-\ndiction performance on various downstream tasks. However, it is\nextremely challenging to frequently perform fine-tuning and up-\ndate the knowledge stored in the model parameters [66] due to the\nconsiderable time and computational resources required. Recently,\nnumerous studies have suggested enhancing large language models\nwith retrieval mechanisms, enabling them to dynamically acquire\nnew knowledge from external sources without extra training pro-\ncesses (i.e., training-free) [50, 52, 57], instead of relying solely on\nthe implicit knowledge encoded in the model's parameters. These\napproaches have shown significant performance improvement for\nvarious knowledge-intensive tasks, such as open-domain question\nanswering [66] and document summarization [120]. According\nto the different ways in which large language models utilize re-\ntrieved information, we categorize these training-free methods into\ntwo categories: 1) Prompt Engineering-based Methods inte-\ngrate retrieved knowledge into the original prompt directly, and 2)\nRetrieval-Guided Token Generation Methods retrieve infor-\nmation to calibrate the token generation process.\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.3551648259162903}, {"x": 0.48350396752357483, "y": 0.3551648259162903}, {"x": 0.48350396752357483, "y": 0.6835165023803711}, {"x": 0.08703071624040604, "y": 0.6835165023803711}], "text": "2.3.3 Intermediate-Layer Integration. Compared to the above two\nnon-parametric approaches, a more engaging augmentation is to\ndesign a semi-parametric module to integrate the retrieved results\nthrough the internal layers of the generation model, which is called\nintermediate-layer integration. Such integration might add addi-\ntional complexity and is promising to enhance the capability of the\ngeneration model with effective training. Typically, a Transformer\nmodule is introduced to leverage retrieved information (mostly\nencoded into dense representations) into the generation model to\ninteract with the representations in the middle stage of the genera-\ntion. For example, RETRO [6] introduces a Chunked Cross Attention\n(CCA) layer to process the retrieved chunks in the generator blocks,\nand Wu et al. [136] introduces the kNN-Augmented Attention Layer.\nSimilarly, EAE [38] and TOME [21] use Entity Memory and Mem-\noryAttention layer to incorporate the retrieved Entity and Entity\nMentions, respectively. Such intermediate-layer integration can\nuse many blocks frequently and efficiently to enhance the capa-\nbility of the whole RAG model. It offers an efficient alternative to\nincorporate a large number of text chunks frequently retrieved,\nwhich are challenging to process with input-layer integration due\nto the input length limit of LMs [6]. However, it also needs to be\nnoted that intermediate-layer integration requires high access to\nthe generation models, which is not feasible for most LLMs that\nare accessible through inference APIs [85].\n"}
{"page": 5, "bbox": [{"x": 0.08816837519407272, "y": 0.7006593346595764}, {"x": 0.2855517566204071, "y": 0.7006593346595764}, {"x": 0.2855517566204071, "y": 0.7098901271820068}, {"x": 0.08816837519407272, "y": 0.7098901271820068}], "text": "3 RA-LLMS TRAINING\n"}
{"page": 5, "bbox": [{"x": 0.5187713503837585, "y": 0.5670329928398132}, {"x": 0.914675772190094, "y": 0.5670329928398132}, {"x": 0.914675772190094, "y": 0.8953846096992493}, {"x": 0.5187713503837585, "y": 0.8953846096992493}], "text": "3.1.1 Prompt Engineering-based Methods. As the LLMs' genera-\ntion performance highly depends on the input query, numerous\ntraining-free RAG approaches employ external knowledge by re-\nfining the original prompts [52, 57, 71]. Specifically, the retrieved\ntexts are usually used as contextual information and combined with\nthe original prompt to guide the generation of large language mod-\nels [50, 52, 57, 59, 71, 94, 129]. For example, In-Context RALM [98]\nkeeps the large language model parameters unchanged and directly\nincorporates the retrieved document before the original prompt to\naugment the generation process. IRCOT [124] interleaves chain-of-\nthought (COT) generation and knowledge retrieval steps, enabling\nthe retrieval of more relevant information for the subsequent rea-\nsoning compared to standard retrieval methods that rely solely on\nthe question as the query. Instead of retrieving knowledge from a\nlarge corpus, GENREAD [147] first prompts a large language model\nto generate contextual documents for the query, and then based\non them to generates answers. SKR [130] proposes guiding LMs to\ndetermine whether they can answer a given question based on their\ninternal knowledge, enabling flexible utilization of both internal and\nexternal knowledge by selectively calling the retriever. TOC [59]\nfirst retrieves relevant knowledge for ambiguous questions and\nrecursively constructs a tree structure by clarifying ambiguous\nquestions into multiple disambiguate questions, which is further\naggregated to generate long-form answers.\n"}
{"page": 5, "bbox": [{"x": 0.08703071624040604, "y": 0.719560444355011}, {"x": 0.48236632347106934, "y": 0.719560444355011}, {"x": 0.48236632347106934, "y": 0.8962637186050415}, {"x": 0.08703071624040604, "y": 0.8962637186050415}], "text": "Based on whether training is required or not, existing RAG methods\ncan be categorized into two main classes: train-free approaches\nand training-based approaches. Training-free methods usually\ndirectly leverage the retrieved knowledge during inference time\nwithout introducing extra training by inserting the retrieved text\ninto the prompt, which is computationally efficient. However, one\npotential challenge is that the retriever and generator components\nare not specifically optimized for downstream tasks, which could\neasily lead to suboptimal utilization of the retrieved knowledge.\nTo fully exploit the external knowledge, extensive methods are\nproposed to fine-tune the retriever and generator, thereby guiding\nlarge language models to effectively adapt and integrate retrieved\ninformation. According to the training strategies, we categorize\n"}
{"page": 5, "bbox": [{"x": 0.48634812235832214, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9613186717033386}, {"x": 0.48634812235832214, "y": 0.9613186717033386}], "text": "6495\n"}
{"page": 6, "bbox": [{"x": 0.8350397944450378, "y": 0.07868131995201111}, {"x": 0.9112628102302551, "y": 0.07780219614505768}, {"x": 0.9112628102302551, "y": 0.08571428805589676}, {"x": 0.8350397944450378, "y": 0.08659340441226959}], "text": "Wenqi Fan et al.\n"}
{"page": 6, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 6, "bbox": [{"x": 0.08759954571723938, "y": 0.11032967269420624}, {"x": 0.48350396752357483, "y": 0.11032967269420624}, {"x": 0.48350396752357483, "y": 0.25890108942985535}, {"x": 0.08759954571723938, "y": 0.25890108942985535}], "text": "3.1.2 Retrieval-Guided Token Generation Methods. In addition to\ndirectly integrating external knowledge into the original prompt,\nthe auxiliary information can be employed to adjust the token gen-\neration process. For example, KNN-KMs [56] first retrieves k most\nrelevant contexts from the datastore based on the given query, and\ncomputes a neighbor distribution based on the distance. The output\ndistribution is calibrated by interpolating the neighbor distribution\nand the original model's output distribution. Rest [46] is proposed\nto replace the parametric draft model with a non-parametric re-\ntrieval datastore, and retrieves relevant tokens based on the current\ncontext for speculative decoding [9, 63, 122].\n"}
{"page": 6, "bbox": [{"x": 0.5182024836540222, "y": 0.10989011079072952}, {"x": 0.9152445793151855, "y": 0.11032967269420624}, {"x": 0.914675772190094, "y": 0.37054944038391113}, {"x": 0.5176336765289307, "y": 0.370109885931015}], "text": "3.3.1 Retriever First. These methods first train the retrieval model\nand then fix it. Large language models are then trained by utiliz-\ning the retrieved knowledge. For instance, RETRO [6] adopts the\nBERT model that is pretrained independently as the retriever, and\nan encoder-decoder architecture is trained to integrate retrieval\nchunks into the model's predictions. RALMS [146] adopts Google\nSearch and the open-source COLBERTV2 [58] as the pretrained\nretriever and fine-tunes the large language model to effectively\nleverage the retrieved passages. ITER-RTGEN [105] utilizes the pre-\ntrained S-BERT [104] as the retriever and introduces an adaptive hy-\nbrid retrieval strategy for retrieving demonstrations. Additionally,\nit leverages T5 [97] as the generator, which undergoes further fine-\ntuning based on the target label and input combining the original\nprompt with retrieved demonstrations. SMALLCAP [102] proposes\nusing the CLIP [95], which is a powerful pretrianed multimodal\nnetwork, to encode the input image and the textual data of the ex-\nternal datastore and retrieve the most relevant items based on the\ncosine similarity. A cross-attention layer is trained and GPT-2 [96]\nis used as the decoder to produce captions.\n"}
{"page": 6, "bbox": [{"x": 0.5199089646339417, "y": 0.3973626494407654}, {"x": 0.5466439127922058, "y": 0.3973626494407654}, {"x": 0.5466439127922058, "y": 0.4035164713859558}, {"x": 0.5199089646339417, "y": 0.4035164713859558}], "text": "3.3.2\n"}
{"page": 6, "bbox": [{"x": 0.08646188676357269, "y": 0.28747251629829407}, {"x": 0.48521047830581665, "y": 0.2883516550064087}, {"x": 0.48407280445098877, "y": 0.6061538457870483}, {"x": 0.0853242352604866, "y": 0.6052747368812561}], "text": "3.2 Independent Training\nIndependent training refers to training the retriever and large lan-\nguage models (LLMs) as two entirely independent processes, in\nwhich there is no interaction between the retriever and the LLMs\nduring the training process [55, 62, 160]. For the training of large\nlanguage models, the negative loglikelihood loss is the most repre-\nsentative training objective [96, 123], which aims to guide the large\nlanguage models to generate desired output y based on the given\ninput x, formulated as - log PLLM (y|x). Regarding the retriever, it\ncan be categorized into two types: 1) Sparse retriever [101, 106],\nand 2) Dense retriever [55, 62, 160]. The sparse retrievers usually\nexploit sparse features, e.g., word frequencies, to represent the doc-\numents and calculate the relevance scores based on task-specific\nmetrics [68, 101, 106] such as TF-IDF and BM25. As for the dense\nretrievers, deep neural networks are employed to encode the query\nand documents into dense representations, and then the inner prod-\nuct is usually used to calculate relevance scores and retrieve the\nrelevant external knowledge. For example, DPR [55] adopts two\nindependent BERT [24] networks to encode the query and\npas-\nsages respectively, and trains these models by utilizing contrastive\nlearning. CoG [62] proposes to train a prefix encoder and a phrase\nencoder for retrieval and reformulate the text generation as multiple\ncopy-and-paste operations from existing source text collection.\n"}
{"page": 6, "bbox": [{"x": 0.5187713503837585, "y": 0.39604395627975464}, {"x": 0.914675772190094, "y": 0.39604395627975464}, {"x": 0.914675772190094, "y": 0.641318678855896}, {"x": 0.5187713503837585, "y": 0.641318678855896}], "text": "LLMs First. Similarly, we can also pre-train large language\nmodels first, and then tune the retriever under the supervision of the\nwell-trained LLMs. For example, DKRR [49] shows that attention\nscores from a sequence-to-sequence model can indicate the docu-\nment's relevance. Therefore, they propose to leverage the attention\nscores of a reader model to produce synthetic labels to train the\nretriever. AAR [149] proposes using a small language model to gen-\nerate the supervised signal for training retrievers. The well-trained\nretriever can be further leveraged to enhance the performance of\nlarge black-box language models. RA-DIT [75] first fine-tune the\nlarge language models to enhance their ability to leverage retrieved\nknowledge, and then train the retriever to better align its output\nwith large language models. UPRISE [16] proposes a lightweight\nmethod to enhance the zero-shot performance of LLMs in unseen\ntasks by introducing a prompt retriever. A frozen LLM is employed\nto guide the fine-tuning process of the prompt retriever, and this\nretriever then retrieves prompts for different tasks with various\nLLMs during inference.\n"}
{"page": 6, "bbox": [{"x": 0.5193401575088501, "y": 0.6716483235359192}, {"x": 0.6751990914344788, "y": 0.6725274920463562}, {"x": 0.6751990914344788, "y": 0.6848351359367371}, {"x": 0.5193401575088501, "y": 0.6839560270309448}], "text": "3.4 Joint Training\n"}
{"page": 6, "bbox": [{"x": 0.08703071624040604, "y": 0.6334065794944763}, {"x": 0.48407280445098877, "y": 0.6342856884002686}, {"x": 0.4829351603984833, "y": 0.89670330286026}, {"x": 0.08589305728673935, "y": 0.8958241939544678}], "text": "3.3 Sequential Training\nIndependent training is an efficient approach to exploit the external\nknowledge during the generation process, since the retriever and\ngenerator can be trained offline and any off-the-shelf models can be\nutilized, avoiding extra training costs. To better enhance the synergy\nbetween the retriever and generator, several methods have been\nproposed to train the retriever and large language models sequen-\ntially. In these sequential training methods, the process typically\nbegins with the independent pretraining of either the retriever or\nthe generator, after which the pretrained module is fixed while the\nother module undergoes training. Note that various existing models\n(e.g., BERT [24], CLIP [95], T5 [97]) can be directly employed as the\nfixed retriever and generator, thereby bypassing the first pertaining\nprocess. Compared to independent training, sequential training\ninvolves coordinated training of the retriever and generator, where\nthe trainable module benefits from the assistance of the fixed mod-\nule. Based on different training orders, sequential training can be\ncategorized into two classes: 1) Retriever First [5, 108, 109, 126],\nand 2) LLMs First [110, 114, 128].\n"}
{"page": 6, "bbox": [{"x": 0.5176336765289307, "y": 0.6909890174865723}, {"x": 0.9152445793151855, "y": 0.6905494332313538}, {"x": 0.9152445793151855, "y": 0.8949450254440308}, {"x": 0.5176336765289307, "y": 0.8953846096992493}], "text": "Joint training methods [17, 47, 54, 70, 159] employ the end-to-end\nparadigm to optimize the retriever and generator simultaneously.\nInstead of training each module sequentially, joint training methods\neffectively enhance the retriever's ability to locate external knowl-\nedge for generation and the generator's capacity to effectively lever-\nage the retrieved information. For instance, RAG [66] minimizes the\nnegative loglikelihood to jointly train the retriever and generator.\nREALM [44] adopts a similar training paradigm to that of RAG [66],\nand Maximum Inner Product Search (MIPS) [15, 28, 100, 111] tech-\nnique is used to locate the most relevant documents. To employ\nMIPS, all external documents are embedded first and a search index\nis produced for each embedding. An asynchronous index updating\nstrategy [44, 48, 51, 119] is proposed to refresh the index every\nseveral hundred training steps to avoid time consumption of re-\nindexing all documents.\n"}
{"page": 6, "bbox": [{"x": 0.48634812235832214, "y": 0.9547252655029297}, {"x": 0.5136518478393555, "y": 0.9547252655029297}, {"x": 0.5136518478393555, "y": 0.9608791470527649}, {"x": 0.48634812235832214, "y": 0.9608791470527649}], "text": "6496\n"}
{"page": 7, "bbox": [{"x": 0.693401575088501, "y": 0.07868131995201111}, {"x": 0.9112628102302551, "y": 0.07868131995201111}, {"x": 0.9112628102302551, "y": 0.08615384250879288}, {"x": 0.693401575088501, "y": 0.08615384250879288}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 7, "bbox": [{"x": 0.08646188676357269, "y": 0.07868131995201111}, {"x": 0.5028441548347473, "y": 0.07868131995201111}, {"x": 0.5028441548347473, "y": 0.08659340441226959}, {"x": 0.08646188676357269, "y": 0.08659340441226959}], "text": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\n"}
{"page": 7, "bbox": [{"x": 0.11490330100059509, "y": 0.10901098698377609}, {"x": 0.24459613859653473, "y": 0.10901098698377609}, {"x": 0.24459613859653473, "y": 0.11824175715446472}, {"x": 0.11490330100059509, "y": 0.11824175715446472}], "text": "APPLICATIONS\n"}
{"page": 7, "bbox": [{"x": 0.5597269535064697, "y": 0.10945054888725281}, {"x": 0.713879406452179, "y": 0.10945054888725281}, {"x": 0.713879406452179, "y": 0.11824175715446472}, {"x": 0.5597269535064697, "y": 0.11824175715446472}], "text": "Downstream Tasks\n"}
{"page": 7, "bbox": [{"x": 0.08646188676357269, "y": 0.11032967269420624}, {"x": 0.09670079499483109, "y": 0.11032967269420624}, {"x": 0.09670079499483109, "y": 0.117802195250988}, {"x": 0.08646188676357269, "y": 0.117802195250988}], "text": "4\n"}
{"page": 7, "bbox": [{"x": 0.5187713503837585, "y": 0.11032967269420624}, {"x": 0.5409556031227112, "y": 0.11032967269420624}, {"x": 0.5409556031227112, "y": 0.117802195250988}, {"x": 0.5187713503837585, "y": 0.117802195250988}], "text": "4.2\n"}
{"page": 7, "bbox": [{"x": 0.5193401575088501, "y": 0.1261538416147232}, {"x": 0.9112628102302551, "y": 0.12747253477573395}, {"x": 0.9112628102302551, "y": 0.15208791196346283}, {"x": 0.5193401575088501, "y": 0.15076923370361328}], "text": "RA-LLMS can also be applied to various downstream tasks, such as\nrecommendations and software engineering.\n"}
{"page": 7, "bbox": [{"x": 0.08703071624040604, "y": 0.1257142871618271}, {"x": 0.4829351603984833, "y": 0.12659341096878052}, {"x": 0.4829351603984833, "y": 0.19472527503967285}, {"x": 0.08703071624040604, "y": 0.19384615123271942}], "text": "In this section, we will introduce some representative applications\nof retrieval-augmented large language models (RA-LLMs). To pro-\nvide a clear overview of the applications of RA-LLMs, we will review\nthem from three perspectives: NLP applications, downstream tasks,\nand domain-specific applications.\n"}
{"page": 7, "bbox": [{"x": 0.5193401575088501, "y": 0.1679120808839798}, {"x": 0.5460751056671143, "y": 0.1679120808839798}, {"x": 0.5460751056671143, "y": 0.17406593263149261}, {"x": 0.5193401575088501, "y": 0.17406593263149261}], "text": "4.2.1\n"}
{"page": 7, "bbox": [{"x": 0.08703071624040604, "y": 0.2101098895072937}, {"x": 0.11035267263650894, "y": 0.2101098895072937}, {"x": 0.11035267263650894, "y": 0.21802197396755219}, {"x": 0.08703071624040604, "y": 0.21802197396755219}], "text": "4.1\n"}
{"page": 7, "bbox": [{"x": 0.12855517864227295, "y": 0.2070329636335373}, {"x": 0.2701933979988098, "y": 0.20835164189338684}, {"x": 0.2701933979988098, "y": 0.2215384542942047}, {"x": 0.12855517864227295, "y": 0.22021977603435516}], "text": "NLP Applications\n"}
{"page": 7, "bbox": [{"x": 0.5187713503837585, "y": 0.16483516991138458}, {"x": 0.9152445793151855, "y": 0.16483516991138458}, {"x": 0.9152445793151855, "y": 0.3410989046096802}, {"x": 0.5187713503837585, "y": 0.3410989046096802}], "text": "Recommendations. Recommender systems play an impor-\ntant role in modeling users' preferences and providing personalized\nrecommendations [33–35, 127, 153, 158]. Recently, RA-LLMs have\ndemonstrated great potential in providing personalized and con-\ntextually relevant recommendations by integrating retrieval and\ngeneration processes [25, 82, 134]. For example, Di Palma [25] pro-\nposes a simple retrieval-augmented recommendation model, that\nleverages knowledge from movie or book datasets to enhance rec-\nommendations. Additionally, Lu et al. [82] further retrieval from\nthe reviews to enrich item information in recommender systems.\nCORAL [134] utilizes reinforcement learning to retrieve collabo-\nrative information from the dataset and align it with semantic\ninformation for more accurate recommendations.\n"}
{"page": 7, "bbox": [{"x": 0.5182024836540222, "y": 0.35780221223831177}, {"x": 0.9141069650650024, "y": 0.356483519077301}, {"x": 0.914675772190094, "y": 0.4646153748035431}, {"x": 0.5187713503837585, "y": 0.46593406796455383}], "text": "4.2.2 Software Engineering. The rise of RA-LLMs has influenced\nmany aspects of software engineering [89, 142, 160]. For example,\nsome studies\npropose the retrieval-augmented generation paradigm\nfor code generation [160] and program repair [89]. Similarly, Parvez\net al. [91] retrieve top-ranked codes or summaries from the codebase\nand aggregate them with input to enhance code generation and\nsummarization. In addition, RA-LLMs show potential in tabular data\nprocessing [67, 142] and Text-to-SQL semantic parsing [93, 113].\n"}
{"page": 7, "bbox": [{"x": 0.5176336765289307, "y": 0.4839560389518738}, {"x": 0.912400484085083, "y": 0.4843955934047699}, {"x": 0.912400484085083, "y": 0.5257142782211304}, {"x": 0.5176336765289307, "y": 0.5252747535705566}], "text": "4.3 Domain-specific Applications\nRA-LLMs have been widely adopted for various domain-specific\ntasks, such as AI for Science and Finance.\n"}
{"page": 7, "bbox": [{"x": 0.08589305728673935, "y": 0.2268131822347641}, {"x": 0.48407280445098877, "y": 0.22637362778186798}, {"x": 0.48521047830581665, "y": 0.8958241939544678}, {"x": 0.08703071624040604, "y": 0.8962637186050415}], "text": "Due to the intrinsic capability in text generation, RA-LLMs have\nvarious applications in the NLP field, such as Question Answer\n(QA) Systems, ChatBot, and Fact Verification.\n4.1.1 QA Systems. QA Systems aim to provide precise answers\nto user's queries. However, even when trained on extensive data,\nthese systems may lack the latest information or specific domain\nknowledge that is not included in their training data [50, 79]. To\naddress this limitation, the integration of RA-LLMS has played a cru-\ncial role in advancing the capabilities of QA systems by enhancing\ntheir ability to retrieve and synthesize relevant information [6, 50].\nSpecifically, RA-LLMs can provide coherent and contextually rele-\nvant answers by leveraging their retrieval component to access a\nvast knowledge base. For example, REALM [44] integrates a knowl-\nedge retriever that can retrieve information from a large corpus\nduring pre-training, fine-tuning, and inference. This approach al-\nlows REALM to effectively retrieve from a vast knowledge corpus,\nthereby improving the accuracy of its responses. Similarly, Fusion-\nin-Decoder [50] retrieves passages from support documents and\nthen fuses them with questions to generate the answer, achieving\nhigher accuracy. In addition, Borgeaud et al. [6] indicate that the\nquality of the answers may rely more on the result of retrieval.\n4.1.2 ChatBot. ChatBot is designed to interact with users in a\nnatural and conversational manner [76]. Different from the QA\nsystem, ChatBot focuses on maintaining a coherent and contextu-\nally rich conversation with the user. To enhance these capabilities,\nrecent methods focus on integrating RA-LLMs [54, 61, 152] for its\nability to augment the ChatBot with relevant external knowledge,\nfacilitating more engaging and context-rich interactions with users.\nFor example, some studies [14, 41] retrieve relevant knowledge\nfrom static databases (e.g., a Wikipedia dump) to augment conver-\nsation. Komeili et al. [61] propose retrieving information from the\ninternet search to further augment conversation performance. Con-\nsidering the dynamic nature of knowledge in the world, another\nmodel [125] further accesses large amounts of dynamic information\nin search engines to generate responses.\n4.1.3 Fact Verification. Fact Verification is a critical task in veri-\nfying the accuracy and reliability of information. With the need\nfor trusted evidence, RA-LLMs are being utilized to enhance the\ncapabilities of fact verification [51, 66, 66]. Lewis et al. [66] first\npropose retrieval of external knowledge to augment a range of\nknowledge-intensive tasks including fact verification. On the other\nhand, Atlas [51] examines the performance of the RA-LLMs for fact\nverification under few-shot learning. Recently, Self-RAG [5] has\ngreatly made a notable impression by incorporating a self-reflective\nmechanism. Specifically, Self-RAG reflects on whether retrieved\ninformation is helpful and judges the reliability of retrieved infor-\nmation, thereby greatly improving the verification accuracy.\n"}
{"page": 7, "bbox": [{"x": 0.5187713503837585, "y": 0.5419780015945435}, {"x": 0.914675772190094, "y": 0.5419780015945435}, {"x": 0.914675772190094, "y": 0.7736263871192932}, {"x": 0.5187713503837585, "y": 0.7736263871192932}], "text": "4.3.1 Al for Science. RA-LLMs have proven to be beneficial for\nthe realms of science, such as molecular and protein. Molecules\ninclude identifying the molecule's property and predicting new\nmolecules, thereby favoring drug discovery. Currently, some RA-\nLLMs have been applied to molecules by integrating retrieval of\nmolecule structure and biomedical entities like protein, molecule,\nand disease [78, 131, 132, 140], etc. Li et al. [68], Wang et al. [131]\npropose retrieval-based frameworks by retrieving from the database\nto guide molecule generation. Liu et al. [78] introduce a multi-modal\nmolecule structure-text model by retrieving textual knowledge from\na large-scale dataset for molecular property prediction. In addition,\nRA-LLMs also significantly influence Protein representation and\ngeneration. For instance, RSA [84] queries protein sequences as-\nsociated with a collection of structurally or functionally similar\nsequences in the database to enhance protein representations. Fur-\nthermore, Lozano et al. [80] present a clinical QA system based on\nretrieving published review articles.\n"}
{"page": 7, "bbox": [{"x": 0.5193401575088501, "y": 0.7894505262374878}, {"x": 0.5460751056671143, "y": 0.7894505262374878}, {"x": 0.5460751056671143, "y": 0.795604407787323}, {"x": 0.5193401575088501, "y": 0.795604407787323}], "text": "4.3.2\n"}
{"page": 7, "bbox": [{"x": 0.5187713503837585, "y": 0.7881318926811218}, {"x": 0.9135380983352661, "y": 0.7881318926811218}, {"x": 0.9135380983352661, "y": 0.8953846096992493}, {"x": 0.5187713503837585, "y": 0.8953846096992493}], "text": "Finance. In the highly data-driven and information-intensive\nfield of finance, RA-LLMs have proved to be a significant technology\nfor enhancing decision-making [69, 143, 151]. For example, Zhang\net al. [151] retrieve financial information from external sources,\nsuch as news platforms (e.g., Bloomberg and Reuters) and social\nmedia platforms (e.g., Twitter, Reddit), to combine with the original\nquery to enhance the precision of financial sentiment analysis. In\naddition, financial QA is another primary task of financial analysis,\n"}
{"page": 7, "bbox": [{"x": 0.48634812235832214, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9613186717033386}, {"x": 0.48634812235832214, "y": 0.9613186717033386}], "text": "6497\n"}
{"page": 8, "bbox": [{"x": 0.8350397944450378, "y": 0.07868131995201111}, {"x": 0.9106939435005188, "y": 0.07780219614505768}, {"x": 0.9106939435005188, "y": 0.08571428805589676}, {"x": 0.8350397944450378, "y": 0.08659340441226959}], "text": "Wenqi Fan et al.\n"}
{"page": 8, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 8, "bbox": [{"x": 0.08703071624040604, "y": 0.11032967269420624}, {"x": 0.48350396752357483, "y": 0.11032967269420624}, {"x": 0.48350396752357483, "y": 0.2043956071138382}, {"x": 0.08703071624040604, "y": 0.2043956071138382}], "text": "which usually extracts relevant knowledge from financial docu-\nments. As professional documents are usually stored in PDF format,\nLin [74] introduces a PDF parser combined with RA-LLMs to re-\ntrieve knowledge from financial reports. On the other hand, Yepes\net al. [143] propose a document chunking method based on struc-\nture instead of chunking based on paragraphs, further improving\nthe quality of RA-LLMs outputs.\n"}
{"page": 8, "bbox": [{"x": 0.08759954571723938, "y": 0.22857142984867096}, {"x": 0.095563143491745, "y": 0.22857142984867096}, {"x": 0.095563143491745, "y": 0.23648351430892944}, {"x": 0.08759954571723938, "y": 0.23648351430892944}], "text": "5\n"}
{"page": 8, "bbox": [{"x": 0.11433447152376175, "y": 0.22769230604171753}, {"x": 0.35324230790138245, "y": 0.22769230604171753}, {"x": 0.35324230790138245, "y": 0.25318682193756104}, {"x": 0.11433447152376175, "y": 0.25318682193756104}], "text": "FUTURE CHALLENGES AND\nOPPORTUNITIES\n"}
{"page": 8, "bbox": [{"x": 0.5187713503837585, "y": 0.10901098698377609}, {"x": 0.9152445793151855, "y": 0.10901098698377609}, {"x": 0.9152445793151855, "y": 0.46681317687034607}, {"x": 0.5187713503837585, "y": 0.46681317687034607}], "text": "data modalities such as images, videos, and audio. By integrating\nvarious modalities, LLMs can leverage richer contextual informa-\ntion than single-modal RAG and develop a more comprehensive\nunderstanding of users' needs, bringing precise, fine-grained, and\nhigh-quality generation. For instance, an image or video can provide\nvaluable visual cues that complement textual information, leading\nto more precise language generation [47, 161]. By effectively fus-\ning multiple modalities, multimodal RA-LLMS can develop a more\ncomprehensive understanding of the world, leading to more accu-\nrate and insightful outputs, benefiting a wide range of domains,\nincluding healthcare [161], drug discovery [115], molecular analy-\nsis [3, 78, 115], etc.\nQuality of External Knowledge. As a commonly used datastore\nin current RAG systems, Wikipedia [55, 161] serves as a vast reposi-\ntory of external textual knowledge used to augment the generation\nprocess, which contains millions of articles covering various disci-\nplines. However, the reliability and accuracy of individual articles\nwithin Wikipedia vary significantly, and the introduction of some\ntexts that deviate from facts might even mislead the model's gener-\nation process. Therefore, it is crucial to enhance the quality of the\nexternal knowledge corpus and mitigate the negative impact of low-\nquality knowledge on the performance of LLMs. By enhancing the\nquality of the external knowledge and tailing robust mechanisms\nby filtering out low-quality or unreliable information, the RA-LLM\nsystems might produce more accurate, reliable outputs, thereby\nimproving their effectiveness in various real-world applications.\n"}
{"page": 8, "bbox": [{"x": 0.5193401575088501, "y": 0.48615384101867676}, {"x": 0.6615471839904785, "y": 0.48615384101867676}, {"x": 0.6615471839904785, "y": 0.4953846037387848}, {"x": 0.5193401575088501, "y": 0.4953846037387848}], "text": "6 CONCLUSION\n"}
{"page": 8, "bbox": [{"x": 0.08646188676357269, "y": 0.2606593370437622}, {"x": 0.48521047830581665, "y": 0.2610988914966583}, {"x": 0.48407280445098877, "y": 0.8962637186050415}, {"x": 0.0853242352604866, "y": 0.8958241939544678}], "text": "Since the studies of RA-LLMs are still in the early stage, we present\nsome potential research directions that can be explored in the future\nin the field of RA-LLMs.\nTrustworthy RA-LLMs. The essential objective of developing\nRAG-empowered LLMs is to enhance the capability of the language\nmodels, thereby benefiting users and society by alleviating redun-\ndant and meaningless labor, increasing conveniences, and spurring\nsocial\nprogress. However, recent research indicates that RA-LLMS\ncan be maliciously and unintentionally manipulated to make un-\nreliable decisions and harm humans [22, 162], which may have\nserious consequences in safety-critical scenarios [11, 13, 31, 36, 77].\nIn addition, private retrieval database has a risk of leakage, raising\nconcerns regarding the privacy of RA-LLMS [150]. Therefore, de-\nveloping trustworthy RA-LLMS is of paramount importance as it\ncan significantly mitigate the potential negative impacts of LLMs\ntechnology and provide people with powerful AI models that can be\nfully trusted. To be specific, the ideal trustworthiness in RA-LLMS\nsystems should possess the following characteristics: 1) robust-\nness, 2) fairness, 3) explainability, and 4) privacy. For example,\nrobustness means a trustworthy RA-LLMS system should be ro-\nbust against malicious or inadvertent perturbations introduced by\nattackers. Fairness indicates a trustworthy RA-LLMs system ought\nto avoid discrimination during the decision-making process. Ex-\nplainability requires a complete understanding of the intrinsic\nworkings of RA-LLMs systems, i.e., the predictions of RA-LLMS sys-\ntems are explainable and transparent. Privacy entails safeguarding\nthe safety of this private information housed within the datastore\nwhen establishing trustworthy RA-LLMs systems.\nMulti-Lingual RA-LLMs. The ability to leverage knowledge from\nmultiple languages can greatly enhance the capabilities of retrieval-\naugmented large language models. As the world becomes increas-\ningly interconnected, there is a growing need for AI systems that\ncan understand and communicate across different languages. By\nincorporating multilingual knowledge retrieval and generation,\nthese models can access and synthesize information from diverse\nlinguistic sources, enabling more comprehensive and nuanced un-\nderstanding and generation capabilities. Additionally, multilingual\nmodels can facilitate cross-cultural communication and knowledge\nsharing and breaking down language barriers, thereby bringing con-\nvenience to people across different regions of the world, especially\nthose in areas with minority languages [53, 71]. For example, users\nfrom countries with less prevalent languages can utilize abundant\nEnglish and Chinese corpora for knowledge retrieval, enhancing\nthe performance of large language models in downstream tasks.\nMultimodal RA-LLMs. Multimodal retrieval-augmented genera-\ntion extends the knowledge sources beyond text to include various\n"}
{"page": 8, "bbox": [{"x": 0.5187713503837585, "y": 0.5046153664588928}, {"x": 0.9152445793151855, "y": 0.5046153664588928}, {"x": 0.9152445793151855, "y": 0.7498900890350342}, {"x": 0.5187713503837585, "y": 0.7498900890350342}], "text": "Retrieval-augmented generation (RAG), a cutting-edge AI tech-\nnique, has achieved remarkable success across various applications,\nincluding recommendation, molecule generation, protein represen-\ntation, and software engineering, owing to the potent capabilities of\nretrieval in providing supplementary information to enhance gen-\neration performance. Recently, increasing efforts have been made\nto alleviate the limitations of large language models (LLMs), such\nas hallucination and out-of-date internal knowledge, by leveraging\nretrieval to provide the latest auxiliary information and teaching\nLLMs to harness the retrieved external knowledge. With the rapid\nadvancements in retrieval-augmented large language models (RA-\nLLMs), there is a pressing need for a comprehensive and systematic\noverview. To bridge this gap, in this paper, we comprehensively\nreview the RA-LLMs from the perspectives of model architecture,\ntraining strategy, and application area, providing researchers with\nan in-depth understanding. Moreover, since the studies of RA-LLMs\nare still in the early stage, we also discuss the current limitations\nand several potential research directions for future research.\n"}
{"page": 8, "bbox": [{"x": 0.5193401575088501, "y": 0.769670307636261}, {"x": 0.7110352516174316, "y": 0.769670307636261}, {"x": 0.7110352516174316, "y": 0.7789011001586914}, {"x": 0.5193401575088501, "y": 0.7789011001586914}], "text": "ACKNOWLEDGMENTS\n"}
{"page": 8, "bbox": [{"x": 0.5187713503837585, "y": 0.7863736152648926}, {"x": 0.9192264080047607, "y": 0.7868131995201111}, {"x": 0.9192264080047607, "y": 0.8958241939544678}, {"x": 0.5187713503837585, "y": 0.8953846096992493}], "text": "The research described in this paper has been partly supported\nby the National Natural Science Foundation of China (project no.\n62102335), General Research Funds from the Hong Kong Research\nGrants Council (project no. PolyU 15200021, 15207322, and 15200023),\ninternal research funds from The Hong Kong Polytechnic Uni-\nversity (project no. P0036200, P0042693, P0048625, P0048752, and\nP0051361), Research Collaborative Project no. P0041282, and SHTM\nInterdisciplinary Large Grant (project no. P0043302).\n"}
{"page": 8, "bbox": [{"x": 0.4857792854309082, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9608791470527649}, {"x": 0.4857792854309082, "y": 0.9608791470527649}], "text": "6498\n"}
{"page": 9, "bbox": [{"x": 0.08646188676357269, "y": 0.07868131995201111}, {"x": 0.5028441548347473, "y": 0.07868131995201111}, {"x": 0.5028441548347473, "y": 0.08615384250879288}, {"x": 0.08646188676357269, "y": 0.08615384250879288}], "text": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\n"}
{"page": 9, "bbox": [{"x": 0.693401575088501, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.07912088185548782}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.693401575088501, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 9, "bbox": [{"x": 0.08759954571723938, "y": 0.10901098698377609}, {"x": 0.20136518776416779, "y": 0.10901098698377609}, {"x": 0.20136518776416779, "y": 0.11824175715446472}, {"x": 0.08759954571723938, "y": 0.11824175715446472}], "text": "REFERENCES\n"}
{"page": 9, "bbox": [{"x": 0.5506256818771362, "y": 0.11164835095405579}, {"x": 0.5836177468299866, "y": 0.11252747476100922}, {"x": 0.583048939704895, "y": 0.11868131905794144}, {"x": 0.5500568747520447, "y": 0.117802195250988}], "text": "103434.\n"}
{"page": 9, "bbox": [{"x": 0.5238907933235168, "y": 0.12087912112474442}, {"x": 0.9141069650650024, "y": 0.12087912112474442}, {"x": 0.914675772190094, "y": 0.8848351836204529}, {"x": 0.5244596004486084, "y": 0.8848351836204529}], "text": "[28] Yujuan Ding, Wai Keung Wong, Zhihui Lai, and Zheng Zhang. 2020. Bilinear\nSupervised Hashing Based on 2D Image Features. IEEE Trans. Circuits Syst.\nVideo Technol. 30, 2 (2020), 590-602.\n[29] Yujuan Ding, Wai Keung Wong, Zhihui Lai, and Zheng Zhang. 2020. Discrim-\ninative dual-stream deep hashing for large-scale image retrieval. IP&M 57, 6\n(2020), 102288.\n[30] Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying\nSong, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. In ICLR.\n[31] Wenqi Fan, Tyler Derr, Xiangyu Zhao, Yao Ma, Hui Liu, Jianping Wang, Jiliang\nTang, and Qing Li. 2021. Attacking black-box recommendations via copying\ncross-domain user profiles. In ICDE. 1583-1594.\n[32] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin,\nTat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: Towards\nRetrieval-Augmented Large Language Models. arXiv:2405.06211 (2024).\n[33] Wenqi Fan, Xiaorui Liu, Wei Jin, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2022.\nGraph Trend Filtering Networks for Recommendation. In SIGIR. 112-121.\n[34] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.\n2019. Graph neural networks for social recommendation. In WWW. 417-426.\n[35] Wenqi Fan, Yao Ma, Qing Li, Jianping Wang, Guoyong Cai, Jiliang Tang, and\nDawei Yin. 2020. A graph neural network framework for social recommenda-\ntions. TKDE (2020).\n[36] Wenqi Fan, Xiangyu Zhao, Xiao Chen, Jingran Su, Jingtong Gao, Lin Wang,\nQidong Liu, Yiqi Wang, Han Xu, Lei Chen, et al. 2022. A Comprehensive Survey\non Trustworthy Recommender Systems. arXiv:2209.10117 (2022).\n[37] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang,\nJiliang Tang, and Qing Li. 2023. Recommender systems in the era of large\nlanguage models (llms). arXiv:2307.02046 (2023).\n[38] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom\nKwiatkowski. 2020. Entities as Experts: Sparse Memory Access with Entity\nSupervision. In EMNLP. 4937-4951.\n[39] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,\nJiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large\nlanguage models: A survey. arXiv:2312.10997 (2023).\n[40] Izacard Gautier, Caron Mathilde, Hosseini Lucas, Riedel Sebastian, Bojanowski\nPiotr, Joulin Armand, and Grave Edouard. 2022. Unsupervised dense information\nretrieval with contrastive learning. J Mach Learn Res (2022).\n[41] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng\nGao, Wen-tau Yih, and Michel Galley. 2018. A knowledge-grounded neural\nconversation model. In AAAI, Vol. 32.\n[42] Michael R. Glass, Gaetano Rossiello, Md. Faisal Mahbub Chowdhury, Ankita\nNaik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, Rerank, Generate.\nIn NAACL-HLT. 2701-2715.\n[43] Edouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving Neural\nLanguage Models with a Continuous Cache. In ICLR.\n[44] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.\n2020. Retrieval augmented language model pre-training. In ICML. 3929–3938.\n[45] Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021. Efficient Near-\nest Neighbor Language Models. In EMNLP (1). 5703-5714.\n[46] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. 2023. Rest:\nRetrieval-based speculative decoding. arXiv:2311.08252 (2023).\n[47] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun,\nCordelia Schmid, David A Ross, and Alireza Fathi. 2023. Reveal: Retrieval-\naugmented visual-language pre-training with multi-source multimodal knowl-\nedge memory. In CVPR. 23369-23379.\n[48] Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang,\nand Bryan Catanzaro. 2023. Raven: In-context learning with retrieval augmented\nencoder-decoder language models. arXiv:2308.07922 (2023).\n[49] Gautier Izacard and Edouard Grave. 2021. Distilling Knowledge from Reader to\nRetriever for Question Answering. In ICLR.\n[50] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with\nGenerative Models for Open Domain Question Answering. In EACL. 874-880.\n[51] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,\nTimo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language\nModels. J Mach Learn Res 24, 251 (2023), 1-43.\n[52] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-\nYu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval\nAugmented Generation. In EMNLP. 7969-7992.\n[53] Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Winata,\nSamuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, and Graham Neubig.\n2023. Multi-lingual and Multi-cultural Figurative Language Understanding. In\nACL.\n[54] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. Knowl-\nedge graph-augmented language models for knowledge-grounded dialogue\ngeneration. arXiv:2305.18846 (2023).\n"}
{"page": 9, "bbox": [{"x": 0.09328782558441162, "y": 0.1257142871618271}, {"x": 0.4829351603984833, "y": 0.1257142871618271}, {"x": 0.48236632347106934, "y": 0.89670330286026}, {"x": 0.09271899610757828, "y": 0.89670330286026}], "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv:2303.08774 (2023).\n[2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan\nGhazvininejad. 2023. In-context Examples Selection for Machine Translation.\nIn ACL (Findings). 8857-8873.\n[3] Miles C Andrews, Junna Oba, Chang-Jiun Wu, Haifeng Zhu, Tatiana Karpinets,\nCaitlin A Creasy, Marie-Andrée Forget, Xiaoxing Yu, Xingzhi Song, Xizeng\nMao, et al. 2022. Multi-modal molecular programs regulate melanoma cell state.\nNature communications 13, 1 (2022), 4000.\n[4] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023. Retrieval-based\nlanguage models and applications. In ACL (Tutorial). 41-46.\n[5] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.\nSelf-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.\nIn ICLR.\n[6] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\nford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-\ndan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving\nfrom trillions of tokens. In ICML. 2206-2240.\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. In NeurIPS.\n[8] Stefan Buttcher, Charles LA Clarke, and Gordon V Cormack. 2016. Information\nretrieval: Implementing and evaluating search engines. Mit Press.\n[9] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Lau-\nrent Sifre, and John Jumper. 2023. Accelerating large language model decoding\nwith speculative sampling. arXiv:2302.01318 (2023).\n[10] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading\nWikipedia to Answer Open-Domain Questions. In ACL. 1870–1879.\n[11] Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing\nLi, and Yihua Huang. 2022. Knowledge-enhanced Black-box Attacks for Recom-\nmendations. In KDD. 108-117.\n[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large language models trained on code.\narXiv:2107.03374 (2021).\n[13] Xiao Chen, Wenqi Fan, Jingfan Chen, Haochen Liu, Zitao Liu, Zhaoxiang Zhang,\nand Qing Li. 2023. Fairly adaptive negative sampling for recommendations. In\nWWW.3723-3733.\n[14] Xiuyi Chen, Fandong Meng, Peng Li, Feilong Chen, Shuang Xu, Bo Xu, and Jie\nZhou. 2020. Bridging the gap between prior and posterior knowledge selection\nfor knowledge-grounded dialogue generation. In EMNLP. 3426-3437.\n[15] Yudong Chen, Zhihui Lai, Yujuan Ding, Kaiyi Lin, and Wai Keung Wong. 2019.\nDeep supervised hashing with anchor graph. In ICCV. 9796-9804.\n[16] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing\nWang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang. 2023. UPRISE: Universal\nPrompt Retrieval for Improving Zero-Shot Evaluation. In EMNLP. 12318-12337.\n[17] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.\n2024. Lift yourself up: Retrieval-augmented text generation with self-memory.\nIn NeurIPS.\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\nJ Mach Learn Res 24, 240 (2023), 1-113.\n[19] W Bruce Croft, Donald Metzler, and Trevor Strohman. 2010. Search engines:\nInformation retrieval in practice. Vol. 520. Addison-Wesley Reading.\n[20] Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel E Ho. 2024. Large legal\nfictions: Profiling legal hallucinations in large language models. arXiv:2401.01301\n(2024).\n[21] Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William W.\nCohen. 2022. Mention Memory: incorporating textual knowledge into Trans-\nformers through entity mention attention. In ICLR.\n[22] Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, and Yang Liu.\n2024. Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning.\narXiv:2402.08416 (2024).\n[23] Ziqing Deng, Zhihui Lai, Yujuan Ding, Heng Kong, and Xu Wu. 2024. Deep\nScaling Factor Quantization Network for Large-scale Image Retrieval. In ICMR.\n851-859.\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nIn NAACL-HLT (1). 4171-4186.\n[25] Dario Di Palma. 2023. Retrieval-augmented recommender system: Enhancing\nrecommender systems with large language models. In RecSys. 1369–1373.\n[26] Yujuan Ding, Yunshan Ma, Wenqi Fan, Yige Yao, Tat-Seng Chua, and Qing Li.\n2024. FashionReGen: LLM-Empowered Fashion Report Generation. In WWW.\n[27] Yujuan Ding, P. Y. Mok, Yunshan Ma, and Yi Bin. 2023. Personalized fashion\noutfit generation with user coordination preference learning. IP&M 60, 5 (2023),\n"}
{"page": 9, "bbox": [{"x": 0.4857792854309082, "y": 0.9547252655029297}, {"x": 0.5125142335891724, "y": 0.9547252655029297}, {"x": 0.5125142335891724, "y": 0.9608791470527649}, {"x": 0.4857792854309082, "y": 0.9608791470527649}], "text": "6499\n"}
{"page": 10, "bbox": [{"x": 0.8350397944450378, "y": 0.07868131995201111}, {"x": 0.9112628102302551, "y": 0.07780219614505768}, {"x": 0.9112628102302551, "y": 0.08571428805589676}, {"x": 0.8350397944450378, "y": 0.08659340441226959}], "text": "Wenqi Fan et al.\n"}
{"page": 10, "bbox": [{"x": 0.08816837519407272, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.07912088185548782}, {"x": 0.30659839510917664, "y": 0.08659340441226959}, {"x": 0.08816837519407272, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 10, "bbox": [{"x": 0.09271899610757828, "y": 0.1120879128575325}, {"x": 0.4829351603984833, "y": 0.1120879128575325}, {"x": 0.4829351603984833, "y": 0.6232966780662537}, {"x": 0.09271899610757828, "y": 0.6232966780662537}], "text": "[55] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu,\nSergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval\nfor Open-Domain Question Answering. In EMNLP. 6769-6781.\n[56] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike\nLewis. 2020. Generalization through Memorization: Nearest Neighbor Language\nModels. In ICLR.\n[57] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,\nChristopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict:\nComposing retrieval and language models for knowledge-intensive nlp.\narXiv:2212.14024 (2022).\n[58] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage\nsearch via contextualized late interaction over bert. In SIGIR. 39-48.\n[59] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo\nKang. 2023. Tree of Clarifications: Answering Ambiguous Questions with\nRetrieval-Augmented Large Language Models. In EMNLP.\n[60] Mei Kobayashi and Koichi Takeda. 2000. Information retrieval on the web. CSUR\n32, 2 (2000), 144-173.\n[61] Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-Augmented\nDialogue Generation. In ACL. 8460-8478.\n[62] Tian Lan, Deng Cai, Yan Wang, Heyan Huang, and Xian-Ling Mao. 2022. Copy\nis All You Need. In ICLR.\n[63] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from\ntransformers via speculative decoding. In ICML. 19274-19286.\n[64] Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida\nWang, and Luke Zettlemoyer. 2020. Pre-training via paraphrasing. In NeurIPS.\n[65] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In ACL. 7871-7880.\n[66] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. In NeurIPS. 9459-9474.\n[67] Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and ZHAO-XIANG ZHANG.\n2024. SheetCopilot: Bringing Software Productivity to the Next Level through\nLarge Language Models. In NeurIPS.\n[68] Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and\nQing Li. 2023. Empowering Molecule Discovery for Molecule-Caption Transla-\ntion with Large Language Models: A ChatGPT Perspective. arXiv:2306.06615\n(2023).\n[69] Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang,\nand Wei Lin. 2024. AlphaFin: Benchmarking Financial Analysis with Retrieval-\nAugmented Stock-Chain Framework. arXiv:2403.12582 (2024).\n[70] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu.\n2023. Structure-Aware Language Model Pretraining Improves Dense Retrieval\non Structured Data. In ACL.\n[71] Xiaoqian Li, Ercong Nie, and Sheng Liang. 2023. From Classification to Gen-\neration: Insights into Crosslingual Retrieval Augmented ICL. In NeurIPS 2023\nWorkshop on Instruction Tuning and Instruction Following.\n[72] Xiaonan Li and Xipeng Qiu. 2023. MoT: Memory-of-Thought Enables ChatGPT\nto Self-Improve. In EMNLP. 6354–6374.\n"}
{"page": 10, "bbox": [{"x": 0.5193401575088501, "y": 0.1120879128575325}, {"x": 0.914675772190094, "y": 0.1120879128575325}, {"x": 0.914675772190094, "y": 0.9050549268722534}, {"x": 0.5193401575088501, "y": 0.9050549268722534}], "text": "[82] Yu Lu, Junwei Bao, Yan Song, Zichen Ma, Shuguang Cui, Youzheng Wu, and\nXiaodong He. 2021. RevCore: Review-Augmented Conversational Recommen-\ndation. In ACL/IJCNLP (Findings). 1161–1173.\n[83] Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral,\nVaiva Imbrasaite, and Vincent Y Zhao. 2023. Dr. icl: Demonstration-retrieved\nin-context learning. arXiv:2305.14128 (2023).\n[84] Chang Ma, Haiteng Zhao, Lin Zheng, Jiayi Xin, Qintong Li, Lijun Wu, Zhi-\nhong Deng, Yang Lu, Qi Liu, and Lingpeng Kong. 2023. Retrieved Sequence\nAugmentation for Protein Representation Learning. bioRxiv (2023), 2023-02.\n[85] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query\nrewriting for retrieval-augmented large language models. arXiv:2305.14283\n(2023).\n[86] Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. 2023. In-context learning\nfor text classification with many labels. In Proceedings of the 1st GenBench\nWorkshop on (Benchmarking) Generalisation in NLP. 173–184.\n[87] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020.\nAmbigQA: Answering Ambiguous Open-domain Questions. In EMNLP.\n[88] Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Ha-\njishirzi, and Luke Zettlemoyer. 2023. Nonparametric Masked Language Model-\ning. In ACL (Findings). 2097-2118.\n[89] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt\nselection for code-related few-shot learning. In ICSE. 2450-2462.\n[90] Neil O'Hare, Paloma De Juan, Rossano Schifanella, Yunlong He, Dawei Yin, and\nYi Chang. 2016. Leveraging user interaction signals for web image search. In\nSIGIR. 559-568.\n[91] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2021. Retrieval Augmented Code Generation and Summa-\nrization. In EMNLP (Findings). 2719-2734.\n[92] Fabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang\nWu, Alexander H. Miller, and Sebastian Riedel. 2020. How Context Affects\nLanguage Models' Factual Predictions. In AKBC.\n[93] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher\nMeek, and Sumit Gulwani. 2022. Synchromesh: Reliable Code Generation from\nPre-trained Language Models. In ICLR.\n[94] Anupam Purwar and Rahul Sundar. 2023. Keyword Augmented Retrieval:\nNovel framework for Information Retrieval integrated with speech interface.\narXiv:2310.04205 (2023).\n[95] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n2021. Learning transferable visual models from natural language supervision.\nIn ICML. 8748-8763.\n[96] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019. Language models are unsupervised multitask learners.\nOpenAI blog 1, 8 (2019), 9.\n[97] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. J Mach Learn Res\n21, 140 (2020), 1-67.\n[98] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin\nLeyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lan-\nguage models. Trans. Assoc. Comput. Linguist. 11 (2023), 1316–1331.\n[99] Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2022.\nLearning to Retrieve Passages without Supervision. In NAACL-HLT. 2687-2700.\n[100] Parikshit Ram and Alexander G Gray. 2012. Maximum inner-product search\nusing cone trees. In KDD. 931-939.\n[101] Juan Ramos et al. 2003. Using tf-idf to determine word relevance in document\nqueries. In Proceedings of the first instructional conference on machine learning,\nVol. 242. Citeseer, 29-48.\n[102] Rita Ramos, Bruno Martins, Desmond Elliott, and Yova Kementchedjhieva. 2023.\nSmallcap: lightweight image captioning prompted with retrieval augmentation.\nIn CVPR. 2840-2849.\n[103] Benjamin Z. Reichman and Larry Heck. 2024. Retrieval-Augmented Genera-\ntion: Is Dense Passage Retrieval Retrieving? https://arxiv.org/html/2402.11035v1\n(2024).\n[104] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In EMNLP-IJCNLP. 3982-3992.\n[105] Yubing Ren, Yanan Cao, Ping Guo, Fang Fang, Wei Ma, and Zheng Lin. 2023.\nRetrieve-and-sample: Document-level event argument extraction via hybrid\nretrieval augmentation. In ACL. 293–306.\n[106] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and TrendsⓇ in Information Retrieval\n3, 4 (2009), 333-389.\n[107] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve\nPrompts for In-Context Learning. In NAACL-HLT. 2655-2671.\n[108] Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2022. Retrieval-\naugmented transformer for image captioning. In CBMI. 1-7.\n[109] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,\nEric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024.\nToolformer: Language models can teach themselves to use tools. In NeurIPS.\n"}
{"page": 10, "bbox": [{"x": 0.09271899610757828, "y": 0.40175825357437134}, {"x": 0.11035267263650894, "y": 0.40175825357437134}, {"x": 0.1097838431596756, "y": 0.7542856931686401}, {"x": 0.09215017408132553, "y": 0.7542856931686401}], "text": "35 E E E E E E E E E\n"}
{"page": 10, "bbox": [{"x": 0.09328782558441162, "y": 0.6254944801330566}, {"x": 0.48236632347106934, "y": 0.6254944801330566}, {"x": 0.48236632347106934, "y": 0.8848351836204529}, {"x": 0.09328782558441162, "y": 0.8848351836204529}], "text": "[73] Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022. Decoupled context processing\nfor context augmented language modeling. In NeurIPS. 21698-21710.\n[74] Demiao Lin. 2024. Revolutionizing Retrieval-Augmented Generation with En-\nhanced PDF Structure Recognition. arXiv:2401.12599 (2024).\n[75] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard\nJames, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.\nRA-DIT: Retrieval-Augmented Dual Instruction Tuning. In ICLR.\n[76] Haochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zitao Liu, and Jiliang Tang.\n2020. Does Gender Matter? Towards Fairness in Dialogue Systems. In ACL.\n[77] Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Yunhao\nLiu, Anil K Jain, and Jiliang Tang. 2021. Trustworthy ai: A computational\nperspective. arXiv:2107.06641 (2021).\n[78] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu,\nJian Tang, Chaowei Xiao, and Animashree Anandkumar. 2023. Multi-modal\nmolecule structure-text model for text-based retrieval and editing. Nature\nMachine Intelligence 5, 12 (2023), 1447–1457.\n[79] Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, and Yingbo\nZhou. 2022. Uni-Parser: Unified Semantic Parser for Question Answering on\nKnowledge Base and Database. In EMNLP. 8858-8869.\n[80] Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, and Nigam Shah. 2023.\nClinfo. ai: An open-source retrieval-augmented large language model system for\nanswering medical questions using scientific literature. In PACIFIC SYMPOSIUM\nON BIOCOMPUTING 2024. 8-23.\n[81] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay\nRajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic Prompt Learning\nvia Policy Gradient for Semi-structured Mathematical Reasoning. In ICLR.\n"}
{"page": 10, "bbox": [{"x": 0.4857792854309082, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9547252655029297}, {"x": 0.5130830407142639, "y": 0.9608791470527649}, {"x": 0.4857792854309082, "y": 0.9608791470527649}], "text": "6500\n"}
{"page": 11, "bbox": [{"x": 0.08646188676357269, "y": 0.07868131995201111}, {"x": 0.5028441548347473, "y": 0.07868131995201111}, {"x": 0.5028441548347473, "y": 0.08659340441226959}, {"x": 0.08646188676357269, "y": 0.08659340441226959}], "text": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\n"}
{"page": 11, "bbox": [{"x": 0.693401575088501, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.07868131995201111}, {"x": 0.9118316173553467, "y": 0.08659340441226959}, {"x": 0.693401575088501, "y": 0.08659340441226959}], "text": "KDD '24, August 25-29, 2024, Barcelona, Spain\n"}
{"page": 11, "bbox": [{"x": 0.5500568747520447, "y": 0.11252747476100922}, {"x": 0.640500545501709, "y": 0.11252747476100922}, {"x": 0.640500545501709, "y": 0.11824175715446472}, {"x": 0.5500568747520447, "y": 0.11824175715446472}], "text": "EMNLP. 6397-6407.\n"}
{"page": 11, "bbox": [{"x": 0.08759954571723938, "y": 0.11120878905057907}, {"x": 0.48236632347106934, "y": 0.11120878905057907}, {"x": 0.48236632347106934, "y": 0.8848351836204529}, {"x": 0.08759954571723938, "y": 0.8848351836204529}], "text": "[110] Zhihong Shao, Yeyun Gong, Minlie Huang, Nan Duan, Weizhu Chen, et al.\n2023. Enhancing Retrieval-Augmented Large Language Models with Iterative\nRetrieval-Generation Synergy. In EMNLP.\n[111] Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, and Heng Tao Shen. 2015.\nLearning binary codes for maximum inner product search. In ICCV. 4148-4156.\n[112] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya\nNachmani, and Yaniv Taigman. 2023. kNN-Diffusion: Image Generation via\nLarge-Scale Retrieval. In ICLR.\n[113] Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022. XRICL: Cross-lingual\nRetrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Se-\nmantic Parsing. In EMNLP (Findings). 5248-5259.\n[114] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models. arXiv:2301.12652 (2023).\n[115] Guy Shtar. 2021. Multimodal machine learning for drug knowledge discovery.\nIn WSDM. 1115-1116.\n[116] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.\nRetrieval Augmentation Reduces Hallucination in Conversation. In EMNLP\n(Findings). 3784-3803.\n[117] Suzanna Sia and Kevin Duh. 2023. In-context learning as maintaining co-\nherency: A study of on-the-fly machine translation using large language models.\narXiv:2305.03573 (2023).\n[118] Amit Singhal et al. 2001. Modern information retrieval: A brief overview. IEEE\nData Eng. Bull. 24, 4 (2001), 35-43.\n[119] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-\narachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (RAG) models for open\ndomain\nquestion answering. TACL 11 (2023), 1-17.\n[120] Mingyang Song, Yi Feng, and Liping Jing. 2023. Hisum: Hyperbolic interaction\nmodel for extractive multi-document summarization. In WWW. 1427-1436.\n[121] Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its\napplication in retrieval. Journal of documentation 28, 1 (1972), 11-21.\n[122] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu\nJain, and Felix Yu. 2024. Spectr: Fast speculative decoding via optimal transport.\nIn NeurIPS.\n[123] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\narXiv:2307.09288 (2023).\n[124] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-\nIntensive Multi-Step Questions. In ACL.\n[125] Ante Wang, Linfeng Song, Qi Liu, Haitao Mi, Longyue Wang, Zhaopeng Tu,\nJinsong Su, and Dong Yu. 2023. Search-engine-augmented dialogue response\ngeneration with cheaply supervised query production. Artificial Intelligence 319\n(2023), 103874.\n[126] Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad\nShoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. 2023. Shall We\nPretrain Autoregressive Language Models with Retrieval? A Comprehensive\nStudy. In EMNLP. 7763-7786.\n[127] Hanbing Wang, Xiaorui Liu, Wenqi Fan, Xiangyu Zhao, Venkataramana Kini,\nDevendra Yadav, Fei Wang, Zhen Wen, Jiliang Tang, and Hui Liu. 2024. Re-\nthinking Large Language Model Architectures for Sequential Recommendations.\narXiv:2402.09543 (2024).\n[128] Liang Wang, Nan Yang, and Furu Wei. 2024. Learning to Retrieve In-Context\nExamples for Large Language Models. In EACL. 1752-1767.\n[129] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He,\nZhouhong Gu, Yanghua Xiao, and Wei Wang. 2023. Knowledgpt: Enhanc-\ning large language models with retrieval and storage access on knowledge bases.\narXiv:2308.11761 (2023).\n[130] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided\nRetrieval Augmentation for Large Language Models. In EMNLP.\n[131] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard G. Baraniuk, and\nAnima Anandkumar. 2023. Retrieval-based Controllable Molecule Generation.\nIn ICLR.\n[132] Zifeng Wang, Zichen Wang, Balasubramaniam Srinivasan, Vassilis N Ioannidis,\nHuzefa Rangwala, and Rishita Anubhai. 2023. BioBridge: Bridging Biomedical\nFoundation Models via Knowledge Graph. arXiv:2310.03320 (2023).\n[133] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In NeurIPS. 24824-24837.\n[134] Junda Wu, Cheng-Chun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng\nHou, and Julian McAuley. 2024. CORAL: Collaborative Retrieval-Augmented\nLarge Language Models Improve Long-tail Recommendation. arXiv:2403.06447\n(2024).\n[135] Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettle-\nmoyer. 2020. Scalable Zero-shot Entity Linking with Dense Entity Retrieval. In\n"}
{"page": 11, "bbox": [{"x": 0.5187713503837585, "y": 0.12131868302822113}, {"x": 0.9141069650650024, "y": 0.12131868302822113}, {"x": 0.914675772190094, "y": 0.8848351836204529}, {"x": 0.5193401575088501, "y": 0.8848351836204529}], "text": "[136] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy.\n2022. Memorizing Transformers. In ICLR.\n[137] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. RECOMP: Improving retrieval-\naugmented LMs with context compression and selective augmentation. In ICLR.\n[138] Jitao Xu, Josep-Maria Crego, and Jean Senellart. 2020. Boosting neural machine\ntranslation with similar translations. In ACL. 1570-1579.\n[139] Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond Goldfish Memory:\nLong-Term Open-Domain Conversation. In ACL (1). 5180-5197.\n[140] Ling Yang, Zhilin Huang, Xiangxin Zhou, Minkai Xu, Wentao Zhang, Yu Wang,\nXiawu Zheng, Wenming Yang, Ron O Dror, Shenda Hong, et al. 2023. Prompt-\nbased 3d molecular diffusion models for structure-based drug design. (2023).\n[141] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023.\nCompositional exemplars for in-context learning. In ICML. 39818-39833.\n[142] Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023.\nLarge Language Models are Versatile Decomposers: Decomposing Evidence and\nQuestions for Table-based Reasoning. In SIGIR. 174-184.\n[143] Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, and Leah Li.\n2024. Financial Report Chunking for Effective Retrieval Augmented Generation.\narXiv:2402.05131 (2024).\n[144] Dawei Yin, Yuening Hu, Jiliang Tang, Tim Daly, Mianwei Zhou, Hua Ouyang,\nJianhui Chen, Changsung Kang, Hongbo Deng, Chikashi Nobata, et al. 2016.\nRanking relevance in yahoo search. In KDD. 323-332.\n[145] Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. 2021. Adap-\ntive semiparametric language models. TACL 9 (2021), 362-373.\n[146] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making\nRetrieval-Augmented Language Models Robust to Irrelevant Context. In ICLR.\n[147] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya\nSanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather\nthan Retrieve: Large Language Models are Strong Context Generators. In ICLR.\n[148] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabhar-\nwal. 2023. Improving language models via plug-and-play retrieval feedback.\narXiv:2305.14002 (2023).\n[149] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023. Augmentation-\nAdapted Retriever Improves Generalization of Language Models as Generic\nPlug-In. In ACL. 2421-2436.\n[150] Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu,\nJie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, et al. 2024. The Good and\nThe Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG).\narXiv:2402.16893 (2024).\n[151] Boyu Zhang, Hongyang Yang, Tianyu Zhou, Muhammad Ali Babar, and Xiao-\nYang Liu. 2023. Enhancing financial sentiment analysis via retrieval augmented\nlarge language models. In CM International Conference on AI in Finance. 349–356.\n[152] Houyu Zhang, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2020. Grounded\nConversation Generation as Guided Traverses in Commonsense Knowledge\nGraphs. In ACL. 2031-2043.\n[153] Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, and Xiaorui Liu.\n2024. Linear-Time Graph Neural Networks for Scalable Recommendations.\narXiv:2402.13973 (2024).\n[154] Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee,\nHonglak Lee, and Lu Wang. 2023. Merging generated and retrieved knowledge\nfor open-domain QA. arXiv:2310.14393 (2023).\n[155] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\nFangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024. Retrieval-\nAugmented Generation for AI-Generated Content: A Survey. arXiv:2402.19473\n(2024).\n[156] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do,\nChengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al.\n2023. Retrieving multimodal information for augmented generation: A survey.\narXiv:2303.10868 (2023).\n[157] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv:2303.18223 (2023).\n[158] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen\nWen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2024. Recommender systems\nin the era of large language models (llms). TKDE (2024).\n[159] Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training Language Models with\nMemory Augmentation. In EMNLP.\n[160] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. 2022.\nDocprompting: Generating code by retrieving the docs. In ICLR.\n[161] Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang\nWang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, et al. 2024. REALM: RAG-Driven\nEnhancement of Multimodal Electronic Health Records Analysis via Large\nLanguage Models. arXiv:2402.07016 (2024).\n[162] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. PoisonedRAG:\nKnowledge Poisoning Attacks to Retrieval-Augmented Generation of Large\nLanguage Models. arXiv:2402.07867 (2024).\n"}
{"page": 11, "bbox": [{"x": 0.48634812235832214, "y": 0.9547252655029297}, {"x": 0.511945366859436, "y": 0.9547252655029297}, {"x": 0.511945366859436, "y": 0.9613186717033386}, {"x": 0.48634812235832214, "y": 0.9613186717033386}], "text": "6501\n"}
{"page": 1, "bbox": [{"x": 0.2792946398258209, "y": 0.12791208922863007}, {"x": 0.7201365232467651, "y": 0.1261538416147232}, {"x": 0.7207053303718567, "y": 0.16879120469093323}, {"x": 0.27986347675323486, "y": 0.1705494523048401}], "text": "Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\n"}
{"page": 1, "bbox": [{"x": 0.3924914598464966, "y": 0.2268131822347641}, {"x": 0.6075085401535034, "y": 0.22857142984867096}, {"x": 0.6075085401535034, "y": 0.23956044018268585}, {"x": 0.3924914598464966, "y": 0.237802192568779}], "text": "Patrick Lewis**, Ethan Perez*,\n"}
{"page": 1, "bbox": [{"x": 0.1848691701889038, "y": 0.2632966935634613}, {"x": 0.8378839492797852, "y": 0.2632966935634613}, {"x": 0.8378839492797852, "y": 0.2764835059642792}, {"x": 0.1848691701889038, "y": 0.2764835059642792}], "text": "Aleksandra Piktus*, Fabio Petroni†, Vladimir Karpukhin³, Naman Goyal, Heinrich Küttler*,\n"}
{"page": 1, "bbox": [{"x": 0.2144482433795929, "y": 0.29890111088752747}, {"x": 0.7838453054428101, "y": 0.29890111088752747}, {"x": 0.7838453054428101, "y": 0.3112087845802307}, {"x": 0.2144482433795929, "y": 0.3112087845802307}], "text": "Mike Lewis, Wen-tau Yiht, Tim Rocktäschel*‡, Sebastian Riedel**, Douwe Kiela*\n"}
{"page": 1, "bbox": [{"x": 0.2468714416027069, "y": 0.33186814188957214}, {"x": 0.7514220476150513, "y": 0.33186814188957214}, {"x": 0.7514220476150513, "y": 0.34593406319618225}, {"x": 0.2468714416027069, "y": 0.34593406319618225}], "text": "*Facebook AI Research; *University College London; *New York University;\n"}
{"page": 1, "bbox": [{"x": 0.44368600845336914, "y": 0.35164836049079895}, {"x": 0.5557451844215393, "y": 0.35120880603790283}, {"x": 0.5557451844215393, "y": 0.3613186776638031}, {"x": 0.44368600845336914, "y": 0.3617582321166992}], "text": "plewis@fb.com\n"}
{"page": 1, "bbox": [{"x": 0.46302616596221924, "y": 0.40175825357437134}, {"x": 0.5364050269126892, "y": 0.40175825357437134}, {"x": 0.5364050269126892, "y": 0.4114285707473755}, {"x": 0.46302616596221924, "y": 0.4114285707473755}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.233788400888443, "y": 0.43252748250961304}, {"x": 0.7679181098937988, "y": 0.43252748250961304}, {"x": 0.7679181098937988, "y": 0.719560444355011}, {"x": 0.233788400888443, "y": 0.719560444355011}], "text": "Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on down-\nstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their perfor-\nmance lags behind task-specific architectures. Additionally, providing provenance\nfor their decisions and updating their world knowledge remain open research prob-\nlems. Pre-trained models with a differentiable access mechanism to explicit non-\nparametric memory can overcome this issue, but have so far been only investigated\nfor extractive downstream tasks. We explore a general-purpose fine-tuning recipe\nfor retrieval-augmented generation (RAG) — models which combine pre-trained\nparametric and non-parametric memory for language generation. We introduce\nRAG models where the parametric memory is a pre-trained seq2seq model and\nthe non-parametric memory is a dense vector index of Wikipedia, accessed with\na pre-trained neural retriever. We compare two RAG formulations, one which\nconditions on the same retrieved passages across the whole generated sequence,\nand another which can use different passages per token. We fine-tune and evaluate\nour models on a wide range of knowledge-intensive NLP tasks and set the state of\nthe art on three open domain QA tasks, outperforming parametric seq2seq models\nand task-specific retrieve-and-extract architectures. For language generation tasks,\nwe find that RAG models generate more specific, diverse and factual language than\na state-of-the-art parametric-only seq2seq baseline.\n"}
{"page": 1, "bbox": [{"x": 0.1769055724143982, "y": 0.7481318712234497}, {"x": 0.3117178678512573, "y": 0.7485714554786682}, {"x": 0.3117178678512573, "y": 0.7595604658126831}, {"x": 0.1769055724143982, "y": 0.7591208815574646}], "text": "1 Introduction\n"}
{"page": 1, "bbox": [{"x": 0.1757679134607315, "y": 0.7793406844139099}, {"x": 0.8265073895454407, "y": 0.7793406844139099}, {"x": 0.8265073895454407, "y": 0.9015384912490845}, {"x": 0.1757679134607315, "y": 0.9015384912490845}], "text": "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\nsides: They cannot easily expand or revise their memory, can't straightforwardly provide insight into\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [20] and ORQA [31]], two recently introduced models that\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\n"}
{"page": 1, "bbox": [{"x": 0.1757679134607315, "y": 0.926153838634491}, {"x": 0.7491467595100403, "y": 0.926153838634491}, {"x": 0.7491467595100403, "y": 0.9362637400627136}, {"x": 0.1757679134607315, "y": 0.9362637400627136}], "text": "34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n"}
{"page": 2, "bbox": [{"x": 0.1848691701889038, "y": 0.09890110045671463}, {"x": 0.2895335555076599, "y": 0.09890110045671463}, {"x": 0.2895335555076599, "y": 0.10329670459032059}, {"x": 0.1848691701889038, "y": 0.10329670459032059}], "text": "Define \"middle ear\" (x)\n"}
{"page": 2, "bbox": [{"x": 0.38964733481407166, "y": 0.10241758078336716}, {"x": 0.5989761352539062, "y": 0.10329670459032059}, {"x": 0.5989761352539062, "y": 0.11252747476100922}, {"x": 0.38964733481407166, "y": 0.11164835095405579}], "text": "End-to-End Backprop through q and pe\n"}
{"page": 2, "bbox": [{"x": 0.7036405205726624, "y": 0.0953846126794815}, {"x": 0.8185437917709351, "y": 0.09582417458295822}, {"x": 0.8185437917709351, "y": 0.13318681716918945}, {"x": 0.7036405205726624, "y": 0.13274724781513214}], "text": "The middle ear includes\nthe tympanic cavity and\nthe three ossicles.\n(y)\nQuestion Answering:\nAnswer Generation\n"}
{"page": 2, "bbox": [{"x": 0.1962457299232483, "y": 0.11120878905057907}, {"x": 0.27076223492622375, "y": 0.11252747476100922}, {"x": 0.2701933979988098, "y": 0.12483516335487366}, {"x": 0.19567690789699554, "y": 0.12351648509502411}], "text": "Question Answering:\nQuestion Query\n"}
{"page": 2, "bbox": [{"x": 0.6069397330284119, "y": 0.11956044286489487}, {"x": 0.69112628698349, "y": 0.12175824493169785}, {"x": 0.6905574798583984, "y": 0.1314285695552826}, {"x": 0.6063708662986755, "y": 0.12923076748847961}], "text": "Generator pe\n"}
{"page": 2, "bbox": [{"x": 0.3811149001121521, "y": 0.1204395592212677}, {"x": 0.457337886095047, "y": 0.1230769231915474}, {"x": 0.45676904916763306, "y": 0.13274724781513214}, {"x": 0.38054606318473816, "y": 0.13010989129543304}], "text": "Retriever Pn\n"}
{"page": 2, "bbox": [{"x": 0.5034129619598389, "y": 0.1204395592212677}, {"x": 0.5523322224617004, "y": 0.12087912112474442}, {"x": 0.5523322224617004, "y": 0.13670329749584198}, {"x": 0.5034129619598389, "y": 0.13626374304294586}], "text": "Document\nIndex\n"}
{"page": 2, "bbox": [{"x": 0.29522183537483215, "y": 0.12087912112474442}, {"x": 0.3356086313724518, "y": 0.12087912112474442}, {"x": 0.3356086313724518, "y": 0.13670329749584198}, {"x": 0.29522183537483215, "y": 0.13670329749584198}], "text": "Query\nEncoder\n"}
{"page": 2, "bbox": [{"x": 0.38225257396698, "y": 0.13494504988193512}, {"x": 0.45392492413520813, "y": 0.13494504988193512}, {"x": 0.45392492413520813, "y": 0.14109890162944794}, {"x": 0.38225257396698, "y": 0.14109890162944794}], "text": "(Non-Parametric)\n"}
{"page": 2, "bbox": [{"x": 0.6234357357025146, "y": 0.134505495429039}, {"x": 0.6746302843093872, "y": 0.13538461923599243}, {"x": 0.6746302843093872, "y": 0.14241757988929749}, {"x": 0.6234357357025146, "y": 0.14153845608234406}], "text": "(Parametric)\n"}
{"page": 2, "bbox": [{"x": 0.49146756529808044, "y": 0.14065934717655182}, {"x": 0.5136518478393555, "y": 0.14109890162944794}, {"x": 0.5130830407142639, "y": 0.14945055544376373}, {"x": 0.4908987581729889, "y": 0.14901098608970642}], "text": "d(z)\n"}
{"page": 2, "bbox": [{"x": 0.1899886280298233, "y": 0.13934065401554108}, {"x": 0.27588167786598206, "y": 0.1397802233695984}, {"x": 0.27588167786598206, "y": 0.15296703577041626}, {"x": 0.1899886280298233, "y": 0.15252746641635895}], "text": "Barack Obama was\nborn in Hawaii. (x)\n"}
{"page": 2, "bbox": [{"x": 0.7326507568359375, "y": 0.14505495131015778}, {"x": 0.788395881652832, "y": 0.14461538195610046}, {"x": 0.788395881652832, "y": 0.15032966434955597}, {"x": 0.7326507568359375, "y": 0.15076923370361328}], "text": "supports (y)\n"}
{"page": 2, "bbox": [{"x": 0.34926050901412964, "y": 0.14593406021595}, {"x": 0.37258246541023254, "y": 0.14593406021595}, {"x": 0.37258246541023254, "y": 0.15516483783721924}, {"x": 0.34926050901412964, "y": 0.15516483783721924}], "text": "q(x)\n"}
{"page": 2, "bbox": [{"x": 0.7315130829811096, "y": 0.15604396164417267}, {"x": 0.7912400364875793, "y": 0.15604396164417267}, {"x": 0.7912400364875793, "y": 0.16087912023067474}, {"x": 0.7315130829811096, "y": 0.16087912023067474}], "text": "Fact Verification:\n"}
{"page": 2, "bbox": [{"x": 0.17974971234798431, "y": 0.15780219435691833}, {"x": 0.28213879466056824, "y": 0.15912087261676788}, {"x": 0.28213879466056824, "y": 0.1652747243642807}, {"x": 0.17974971234798431, "y": 0.16395604610443115}], "text": "Fact Verification: Fact Query\n"}
{"page": 2, "bbox": [{"x": 0.6632537245750427, "y": 0.1569230705499649}, {"x": 0.6996586918830872, "y": 0.15736263990402222}, {"x": 0.6990898847579956, "y": 0.17318680882453918}, {"x": 0.6626848578453064, "y": 0.17274725437164307}], "text": "Margin-\nalize\n"}
{"page": 2, "bbox": [{"x": 0.7303754091262817, "y": 0.16351647675037384}, {"x": 0.7912400364875793, "y": 0.16307692229747772}, {"x": 0.7912400364875793, "y": 0.1683516502380371}, {"x": 0.7303754091262817, "y": 0.16879120469093323}], "text": "Label Generation\n"}
{"page": 2, "bbox": [{"x": 0.3873720169067383, "y": 0.1767033040523529}, {"x": 0.4158134162425995, "y": 0.1762637346982956}, {"x": 0.4158134162425995, "y": 0.18373626470565796}, {"x": 0.3873720169067383, "y": 0.18417581915855408}], "text": "MIPS\n"}
{"page": 2, "bbox": [{"x": 0.3424345850944519, "y": 0.16439560055732727}, {"x": 0.38680317997932434, "y": 0.16439560055732727}, {"x": 0.38680317997932434, "y": 0.19736263155937195}, {"x": 0.3424345850944519, "y": 0.19736263155937195}], "text": "+\n"}
{"page": 2, "bbox": [{"x": 0.6376564502716064, "y": 0.179340660572052}, {"x": 0.6496018171310425, "y": 0.179340660572052}, {"x": 0.6496018171310425, "y": 0.18549451231956482}, {"x": 0.6376564502716064, "y": 0.18549451231956482}], "text": "Pe\n"}
{"page": 2, "bbox": [{"x": 0.20705346763134003, "y": 0.17494505643844604}, {"x": 0.2741751968860626, "y": 0.17494505643844604}, {"x": 0.2741751968860626, "y": 0.2131868153810501}, {"x": 0.20705346763134003, "y": 0.2131868153810501}], "text": "The Divine\nComedy (x)\nJeopardy Question\nGeneration:\nAnswer Query\n"}
{"page": 2, "bbox": [{"x": 0.7104664444923401, "y": 0.17890110611915588}, {"x": 0.8168373107910156, "y": 0.17846153676509857}, {"x": 0.8168373107910156, "y": 0.2131868153810501}, {"x": 0.7104664444923401, "y": 0.21362636983394623}], "text": "This 14th century work\nis divided into 3\nsections: \"Inferno\",\n\"Purgatorio\" &\n\"Paradiso\"\n"}
{"page": 2, "bbox": [{"x": 0.8037542700767517, "y": 0.2101098895072937}, {"x": 0.8151308298110962, "y": 0.2101098895072937}, {"x": 0.8151308298110962, "y": 0.21450549364089966}, {"x": 0.8037542700767517, "y": 0.21450549364089966}], "text": "(y)\n"}
{"page": 2, "bbox": [{"x": 0.7275313138961792, "y": 0.21934065222740173}, {"x": 0.8009101152420044, "y": 0.21890109777450562}, {"x": 0.8009101152420044, "y": 0.224175825715065}, {"x": 0.7275313138961792, "y": 0.22461538016796112}], "text": "Question Generation\n"}
{"page": 2, "bbox": [{"x": 0.1757679134607315, "y": 0.2439560443162918}, {"x": 0.8253697156906128, "y": 0.2439560443162918}, {"x": 0.8253697156906128, "y": 0.2967033088207245}, {"x": 0.1757679134607315, "y": 0.2967033088207245}], "text": "Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\nIndex) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we\ntreat z as a latent variable and marginalize over seq2seq predictions given different documents.\n"}
{"page": 2, "bbox": [{"x": 0.17463025450706482, "y": 0.3257142901420593}, {"x": 0.8265073895454407, "y": 0.3257142901420593}, {"x": 0.8265073895454407, "y": 0.7578021883964539}, {"x": 0.17463025450706482, "y": 0.7578021883964539}], "text": "but have only explored open-domain extractive question answering. Here, we bring hybrid parametric\nand non-parametric memory to the \"workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\na general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG).\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\ncan be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\nmemory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-\naugmented networks [25] and memory layers [30]]. In contrast, we explore a setting where both\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\npresent without additional training.\nOur results highlight the benefits of combining parametric and non-parametric memory with genera-\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\nextractive tasks, we find that unconstrained generation outperforms previous extractive approaches.\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\ngeneration, and we find that our models generate responses that are more factual, specific, and\ndiverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\nthe non-parametric memory can be replaced to update the models' knowledge as the world changes.\n"}
{"page": 2, "bbox": [{"x": 0.1757679134607315, "y": 0.7802197933197021}, {"x": 0.27872583270072937, "y": 0.7806593179702759}, {"x": 0.27872583270072937, "y": 0.7907692193984985}, {"x": 0.1757679134607315, "y": 0.7903296947479248}], "text": "2 Methods\n"}
{"page": 2, "bbox": [{"x": 0.17463025450706482, "y": 0.8101099133491516}, {"x": 0.8242321014404297, "y": 0.8101099133491516}, {"x": 0.8242321014404297, "y": 0.8632966876029968}, {"x": 0.17463025450706482, "y": 0.8632966876029968}], "text": "We explore RAG models, which use the input sequence x to retrieve text documents z and use them\nas additional context when generating the target sequence y. As shown in Figure I, our models\nleverage two components: (i) a retriever pη(z|x) with parameters 7 that returns (top-K truncated)\ndistributions over text passages given a query x and (ii) a generator po(yi|x, Z, Y1:i−1) parametrized\n"}
{"page": 2, "bbox": [{"x": 0.615472137928009, "y": 0.8408791422843933}, {"x": 0.6240045428276062, "y": 0.8408791422843933}, {"x": 0.6240045428276062, "y": 0.8487911820411682}, {"x": 0.615472137928009, "y": 0.8487911820411682}], "text": "η\n"}
{"page": 2, "bbox": [{"x": 0.17633675038814545, "y": 0.8742856979370117}, {"x": 0.8248009085655212, "y": 0.8742856979370117}, {"x": 0.8248009085655212, "y": 0.9116483330726624}, {"x": 0.17633675038814545, "y": 0.9116483330726624}], "text": "¹Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\n"}
{"page": 2, "bbox": [{"x": 0.49431172013282776, "y": 0.9384615421295166}, {"x": 0.5034129619598389, "y": 0.9384615421295166}, {"x": 0.5034129619598389, "y": 0.9459340572357178}, {"x": 0.49431172013282776, "y": 0.9459340572357178}], "text": "2\n"}
{"page": 3, "bbox": [{"x": 0.17463025450706482, "y": 0.09054945409297943}, {"x": 0.8259385824203491, "y": 0.0923076942563057}, {"x": 0.8253697156906128, "y": 0.21230769157409668}, {"x": 0.17406143248081207, "y": 0.21054944396018982}], "text": "by that generates a current token based on a context of the previous i - 1 tokens y1:i−1, the original\ninput x and a retrieved passage z.\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\nWe propose two models that marginalize over the latent documents in different ways to produce a\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document\nto predict each target token. The second approach, RAG-Token, can predict each target token based\non a different document. In the following, we formally introduce both models and then describe the\nand I pe components, as well as the training and decoding procedure.\n"}
{"page": 3, "bbox": [{"x": 0.17519909143447876, "y": 0.2013186812400818}, {"x": 0.19055745005607605, "y": 0.2013186812400818}, {"x": 0.19055745005607605, "y": 0.21098901331424713}, {"x": 0.17519909143447876, "y": 0.21098901331424713}], "text": "Ρη\n"}
{"page": 3, "bbox": [{"x": 0.1757679134607315, "y": 0.22857142984867096}, {"x": 0.2633674740791321, "y": 0.22857142984867096}, {"x": 0.2633674740791321, "y": 0.23648351430892944}, {"x": 0.1757679134607315, "y": 0.23648351430892944}], "text": "2.1 Models\n"}
{"page": 3, "bbox": [{"x": 0.1757679134607315, "y": 0.25362637639045715}, {"x": 0.8242321014404297, "y": 0.25362637639045715}, {"x": 0.8242321014404297, "y": 0.3199999928474426}, {"x": 0.1757679134607315, "y": 0.3199999928474426}], "text": "RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\nprobability for each document, which are then marginalized,\n"}
{"page": 3, "bbox": [{"x": 0.6291239857673645, "y": 0.3283516466617584}, {"x": 0.6393629312515259, "y": 0.3283516466617584}, {"x": 0.6393629312515259, "y": 0.3340659439563751}, {"x": 0.6291239857673645, "y": 0.3340659439563751}], "text": "N\n"}
{"page": 3, "bbox": [{"x": 0.5221843123435974, "y": 0.3446153700351715}, {"x": 0.5329920649528503, "y": 0.3446153700351715}, {"x": 0.5329920649528503, "y": 0.34901097416877747}, {"x": 0.5221843123435974, "y": 0.34901097416877747}], "text": "=\n"}
{"page": 3, "bbox": [{"x": 0.5676905512809753, "y": 0.3393406569957733}, {"x": 0.7639362812042236, "y": 0.33978021144866943}, {"x": 0.7639362812042236, "y": 0.35428571701049805}, {"x": 0.5676905512809753, "y": 0.35384616255760193}], "text": "Pn(x) Po(Yi X, Z, Y1:i−1)\n"}
{"page": 3, "bbox": [{"x": 0.3697383403778076, "y": 0.3393406569957733}, {"x": 0.5130830407142639, "y": 0.3393406569957733}, {"x": 0.5130830407142639, "y": 0.3551648259162903}, {"x": 0.3697383403778076, "y": 0.3551648259162903}], "text": "ΣPn(z|x)po(y|x, z)\n"}
{"page": 3, "bbox": [{"x": 0.23208190500736237, "y": 0.34329670667648315}, {"x": 0.3589306175708771, "y": 0.3393406569957733}, {"x": 0.35949942469596863, "y": 0.35120880603790283}, {"x": 0.2326507419347763, "y": 0.3551648259162903}], "text": "PRAG-Sequence (y|x) ≈\n"}
{"page": 3, "bbox": [{"x": 0.6313993334770203, "y": 0.35956043004989624}, {"x": 0.6353811025619507, "y": 0.35956043004989624}, {"x": 0.6353811025619507, "y": 0.3648351728916168}, {"x": 0.6313993334770203, "y": 0.3648351728916168}], "text": "i\n"}
{"page": 3, "bbox": [{"x": 0.34015926718711853, "y": 0.358681321144104}, {"x": 0.42207053303718567, "y": 0.3582417666912079}, {"x": 0.42207053303718567, "y": 0.3674725294113159}, {"x": 0.34015926718711853, "y": 0.36791208386421204}], "text": "zЄtop-k(p(x))\n"}
{"page": 3, "bbox": [{"x": 0.5142207145690918, "y": 0.3591208755970001}, {"x": 0.5961319804191589, "y": 0.358681321144104}, {"x": 0.5961319804191589, "y": 0.3674725294113159}, {"x": 0.5142207145690918, "y": 0.36791208386421204}], "text": "zЄtop-k(p(x))\n"}
{"page": 3, "bbox": [{"x": 0.1757679134607315, "y": 0.38285714387893677}, {"x": 0.8248009085655212, "y": 0.38285714387893677}, {"x": 0.8248009085655212, "y": 0.44967031478881836}, {"x": 0.1757679134607315, "y": 0.44967031478881836}], "text": "RAG-Token Model In the RAG-Token model we can draw a different latent document for each\ntarget token and marginalize accordingly. This allows the generator to choose content from several\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\nretriever, and then the generator produces a distribution for the next output token for each document,\nbefore marginalizing, and repeating the process with the following output token, Formally, we define:\n"}
{"page": 3, "bbox": [{"x": 0.42207053303718567, "y": 0.45758241415023804}, {"x": 0.4328782856464386, "y": 0.45758241415023804}, {"x": 0.4328782856464386, "y": 0.46373626589775085}, {"x": 0.42207053303718567, "y": 0.46373626589775085}], "text": "N\n"}
{"page": 3, "bbox": [{"x": 0.4726962447166443, "y": 0.46681317687034607}, {"x": 0.5, "y": 0.46681317687034607}, {"x": 0.5, "y": 0.4848351776599884}, {"x": 0.4726962447166443, "y": 0.4848351776599884}], "text": "Σ\n"}
{"page": 3, "bbox": [{"x": 0.5295790433883667, "y": 0.46945056319236755}, {"x": 0.7053470015525818, "y": 0.4707692265510559}, {"x": 0.7053470015525818, "y": 0.48307693004608154}, {"x": 0.5295790433883667, "y": 0.4817582368850708}], "text": "Pn(x)po(Yix, Zi, Y1:i−1)\n"}
{"page": 3, "bbox": [{"x": 0.4163822531700134, "y": 0.4676923155784607}, {"x": 0.43742889165878296, "y": 0.4676923155784607}, {"x": 0.43742889165878296, "y": 0.4848351776599884}, {"x": 0.4163822531700134, "y": 0.4848351776599884}], "text": "II\n"}
{"page": 3, "bbox": [{"x": 0.2923777103424072, "y": 0.47252747416496277}, {"x": 0.40500569343566895, "y": 0.46945056319236755}, {"x": 0.4055745303630829, "y": 0.48087912797927856}, {"x": 0.2929465174674988, "y": 0.4839560389518738}], "text": "PRAG-Token (y|x) ≈\n"}
{"page": 3, "bbox": [{"x": 0.42548349499702454, "y": 0.48879119753837585}, {"x": 0.4288964867591858, "y": 0.48879119753837585}, {"x": 0.4288964867591858, "y": 0.4936263859272003}, {"x": 0.42548349499702454, "y": 0.4936263859272003}], "text": "i\n"}
{"page": 3, "bbox": [{"x": 0.4453924894332886, "y": 0.4879120886325836}, {"x": 0.5273037552833557, "y": 0.4874725341796875}, {"x": 0.5273037552833557, "y": 0.49670329689979553}, {"x": 0.4453924894332886, "y": 0.49714285135269165}], "text": "zЄtop-k(p(x))\n"}
{"page": 3, "bbox": [{"x": 0.1757679134607315, "y": 0.5120879411697388}, {"x": 0.8230944275856018, "y": 0.5120879411697388}, {"x": 0.8230944275856018, "y": 0.5367032885551453}, {"x": 0.1757679134607315, "y": 0.5367032885551453}], "text": "Finally, we note that RAG can be used for sequence classification tasks by considering the target class\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\n"}
{"page": 3, "bbox": [{"x": 0.17519909143447876, "y": 0.5556043982505798}, {"x": 0.3213879466056824, "y": 0.5564835071563721}, {"x": 0.3213879466056824, "y": 0.5657142996788025}, {"x": 0.17519909143447876, "y": 0.5648351907730103}], "text": "2.2 Retriever: DPR\n"}
{"page": 3, "bbox": [{"x": 0.17633675038814545, "y": 0.5806593298912048}, {"x": 0.7957906723022461, "y": 0.5806593298912048}, {"x": 0.7957906723022461, "y": 0.616263747215271}, {"x": 0.17633675038814545, "y": 0.616263747215271}], "text": "The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\nd(z) = BERTɖ(z), q(x) = BERTq(x)\n"}
{"page": 3, "bbox": [{"x": 0.2508532404899597, "y": 0.6017582416534424}, {"x": 0.4402730464935303, "y": 0.6004395484924316}, {"x": 0.4402730464935303, "y": 0.6153846383094788}, {"x": 0.2508532404899597, "y": 0.6167032718658447}], "text": "Pn(z|x) ∞ exp (d(z)¯q(x))\n"}
{"page": 3, "bbox": [{"x": 0.17519909143447876, "y": 0.6232966780662537}, {"x": 0.8242321014404297, "y": 0.6237362623214722}, {"x": 0.8242321014404297, "y": 0.7191208600997925}, {"x": 0.17519909143447876, "y": 0.7186813354492188}], "text": "where d(z) is a dense representation of a document produced by a BERT BASE document encoder [8],\nand q(x) a query representation produced by a query encoder, also based on BERT BASE. Calculating\ntop-k(pn(x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\nNatural Questions [29]]. We refer to the document index as the non-parametric memory.\n"}
{"page": 3, "bbox": [{"x": 0.17519909143447876, "y": 0.7371428608894348}, {"x": 0.3395904302597046, "y": 0.7371428608894348}, {"x": 0.3395904302597046, "y": 0.7454944849014282}, {"x": 0.17519909143447876, "y": 0.7454944849014282}], "text": "2.3 Generator: BART\n"}
{"page": 3, "bbox": [{"x": 0.17519909143447876, "y": 0.7613186836242676}, {"x": 0.8242321014404297, "y": 0.7613186836242676}, {"x": 0.8242321014404297, "y": 0.8430769443511963}, {"x": 0.17519909143447876, "y": 0.8430769443511963}], "text": "The generator component po(yi|x, Z, Y1:i-1) could be modelled using any encoder-decoder. We use\nBART-large [32]], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\nx with the retrieved content z when generating from BART, we simply concatenate them. BART was\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\nmodels [32]]. We refer to the BART generator parameters 0 as the parametric memory henceforth.\n"}
{"page": 3, "bbox": [{"x": 0.1757679134607315, "y": 0.8597801923751831}, {"x": 0.27303755283355713, "y": 0.8610988855361938}, {"x": 0.27303755283355713, "y": 0.8720878958702087}, {"x": 0.1757679134607315, "y": 0.870769202709198}], "text": "2.4 Training\n"}
{"page": 3, "bbox": [{"x": 0.17519909143447876, "y": 0.8857142925262451}, {"x": 0.8236632347106934, "y": 0.8865934014320374}, {"x": 0.8236632347106934, "y": 0.9129670262336731}, {"x": 0.17519909143447876, "y": 0.9120879173278809}], "text": "We jointly train the retriever and generator components without any direct supervision on what\ndocument should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\n"}
{"page": 3, "bbox": [{"x": 0.4948805570602417, "y": 0.9384615421295166}, {"x": 0.5039817690849304, "y": 0.9384615421295166}, {"x": 0.5039817690849304, "y": 0.9459340572357178}, {"x": 0.4948805570602417, "y": 0.9459340572357178}], "text": "3\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.09362637251615524}, {"x": 0.8248009085655212, "y": 0.09362637251615524}, {"x": 0.8248009085655212, "y": 0.16395604610443115}, {"x": 0.17519909143447876, "y": 0.16395604610443115}], "text": "minimize the negative marginal log-likelihood of each target, Σ; – log p(y;|x;) using stochastic\ngradient descent with Adam [28]. Updating the document encoder BERTɖ during training is costly as\nit requires the document index to be periodically updated as REALM does during pre-training [20].\nWe do not find this step necessary for strong performance, and keep the document encoder (and\nindex) fixed, only fine-tuning the query encoder BERT, and the BART generator.\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.18109890818595886}, {"x": 0.2792946398258209, "y": 0.18285714089870453}, {"x": 0.27872583270072937, "y": 0.1934065967798233}, {"x": 0.17463025450706482, "y": 0.19164834916591644}], "text": "2.5 Decoding\n"}
{"page": 4, "bbox": [{"x": 0.17463025450706482, "y": 0.20571428537368774}, {"x": 0.8259385824203491, "y": 0.20571428537368774}, {"x": 0.8259385824203491, "y": 0.2800000011920929}, {"x": 0.17463025450706482, "y": 0.2800000011920929}], "text": "At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy. p(y|x).\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\ntor with transition probability: p'q(Yi|x, Y1:i−1) = ΣzЄtop-k(p(-|x)) Pn(Zi|x)Po(Yi|x, Zi, Y1:i−1) To\ndecode, we can plug po (yi|x, Y1:i−1) into a standard beam decoder.\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.29582417011260986}, {"x": 0.8248009085655212, "y": 0.29582417011260986}, {"x": 0.8248009085655212, "y": 0.4457142949104309}, {"x": 0.17519909143447876, "y": 0.4457142949104309}], "text": "RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\neach document z, scoring each hypothesis using pe(Yi|x, Z, Y1:i−1). This yields a set of hypotheses\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\nof an hypothesis y we run an additional forward pass for each document z for which y does not\nappear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across\nbeams for the marginals. We refer to this decoding procedure as \"Thorough Decoding.” For longer\noutput sequences, Y| can become large, requiring many forward passes. For more efficient decoding,\nwe can make a further approximation that po(y|x, zi) ≈ 0 where y was not generated during beam\nsearch from x, zi. This avoids the need to run additional forward passes once the candidate set Y has\nbeen generated. We refer to this decoding procedure as “Fast Decoding.\"\n"}
{"page": 4, "bbox": [{"x": 0.17406143248081207, "y": 0.46901097893714905}, {"x": 0.18941979110240936, "y": 0.46901097893714905}, {"x": 0.18941979110240936, "y": 0.47780218720436096}, {"x": 0.17406143248081207, "y": 0.47780218720436096}], "text": "3\n"}
{"page": 4, "bbox": [{"x": 0.2053469866514206, "y": 0.4681318700313568}, {"x": 0.3122866749763489, "y": 0.4681318700313568}, {"x": 0.3122866749763489, "y": 0.48087912797927856}, {"x": 0.2053469866514206, "y": 0.48087912797927856}], "text": "Experiments\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.4989010989665985}, {"x": 0.8248009085655212, "y": 0.4989010989665985}, {"x": 0.8248009085655212, "y": 0.6065934300422668}, {"x": 0.17519909143447876, "y": 0.6065934300422668}], "text": "We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\nk documents for each query. We consider k = {5, 10} for training and set k for test time using dev\ndata. We now discuss experimental details for each task.\n"}
{"page": 4, "bbox": [{"x": 0.1757679134607315, "y": 0.6263736486434937}, {"x": 0.4550625681877136, "y": 0.6263736486434937}, {"x": 0.4550625681877136, "y": 0.6373626589775085}, {"x": 0.1757679134607315, "y": 0.6373626589775085}], "text": "3.1 Open-domain Question Answering\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.6514285802841187}, {"x": 0.8253697156906128, "y": 0.6514285802841187}, {"x": 0.8253697156906128, "y": 0.8004395365715027}, {"x": 0.17519909143447876, "y": 0.8004395365715027}], "text": "Open-domain question answering (QA) is an important real-world application and common testbed\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to \"Closed-Book\nQA\" approaches [52]], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3]] and CuratedTrec (CT) [2]. As\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n"}
{"page": 4, "bbox": [{"x": 0.17633675038814545, "y": 0.8175824284553528}, {"x": 0.43970420956611633, "y": 0.8193406462669373}, {"x": 0.43970420956611633, "y": 0.8307692408561707}, {"x": 0.17633675038814545, "y": 0.8290109634399414}], "text": "3.2 Abstractive Question Answering\n"}
{"page": 4, "bbox": [{"x": 0.17519909143447876, "y": 0.8448351621627808}, {"x": 0.8248009085655212, "y": 0.8452747464179993}, {"x": 0.8248009085655212, "y": 0.9125275015830994}, {"x": 0.17519909143447876, "y": 0.9120879173278809}], "text": "RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\ntext generation. To test RAG's natural language generation (NLG) in a knowledge-intensive setting,\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages\nretrieved from a search engine for each question, and a full sentence answer annotated from the\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n"}
{"page": 4, "bbox": [{"x": 0.4948805570602417, "y": 0.9389011263847351}, {"x": 0.5039817690849304, "y": 0.9389011263847351}, {"x": 0.5039817690849304, "y": 0.9463736414909363}, {"x": 0.4948805570602417, "y": 0.9463736414909363}], "text": "4\n"}
{"page": 5, "bbox": [{"x": 0.17292377352714539, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.16175824403762817}, {"x": 0.17292377352714539, "y": 0.16175824403762817}], "text": "MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\nanswered in a way that matches the reference answer without access to the gold passages, such as\n\"What is the weather in Volcano, CA?” so performance will be lower without using gold passages.\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\nRAG can rely on parametric knowledge to generate reasonable responses.\n"}
{"page": 5, "bbox": [{"x": 0.1757679134607315, "y": 0.17978021502494812}, {"x": 0.4260523319244385, "y": 0.17890110611915588}, {"x": 0.4260523319244385, "y": 0.1903296709060669}, {"x": 0.1757679134607315, "y": 0.19120879471302032}], "text": "3.3 Jeopardy Question Generation\n"}
{"page": 5, "bbox": [{"x": 0.17519909143447876, "y": 0.20527473092079163}, {"x": 0.8265073895454407, "y": 0.20527473092079163}, {"x": 0.8265073895454407, "y": 0.45758241415023804}, {"x": 0.17519909143447876, "y": 0.45758241415023804}], "text": "To evaluate RAG's generation abilities in a non-QA setting, we study open-domain question gen-\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the first\ncountry to host this international sports competition twice.\" As Jeopardy questions are precise,\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\nchallenging knowledge-intensive generation task.\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\nSQUAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\nmatching entities and has higher correlation with human judgment for question generation than\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\none for specificity. We define factuality as whether a statement can be corroborated by trusted external\nsources, and specificity as high mutual dependence between the input and output [33]]. We follow\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\noptions-quuestion A is better, question B is better, both are good, or neither is good.\n"}
{"page": 5, "bbox": [{"x": 0.1757679134607315, "y": 0.4769230782985687}, {"x": 0.3270762264728546, "y": 0.4769230782985687}, {"x": 0.3270762264728546, "y": 0.4852747321128845}, {"x": 0.1757679134607315, "y": 0.4852747321128845}], "text": "3.4 Fact Verification\n"}
{"page": 5, "bbox": [{"x": 0.17463025450706482, "y": 0.5015384554862976}, {"x": 0.8242321014404297, "y": 0.5015384554862976}, {"x": 0.8242321014404297, "y": 0.6646153926849365}, {"x": 0.17463025450706482, "y": 0.6646153926849365}], "text": "FEVER [56] requires classifying whether a natural language claim is supported or refuted by\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\nwhether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\nexploring the RAG models' ability to handle classification rather than generation. We map FEVER\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\nretrieved evidence. In many real-world applications, retrieval supervision signals aren't available, and\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\ntwo variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n"}
{"page": 5, "bbox": [{"x": 0.17519909143447876, "y": 0.68703293800354}, {"x": 0.266211599111557, "y": 0.68703293800354}, {"x": 0.266211599111557, "y": 0.696703314781189}, {"x": 0.17519909143447876, "y": 0.696703314781189}], "text": "4 Results\n"}
{"page": 5, "bbox": [{"x": 0.17463025450706482, "y": 0.7164835333824158}, {"x": 0.4550625681877136, "y": 0.7164835333824158}, {"x": 0.4550625681877136, "y": 0.7274725437164307}, {"x": 0.17463025450706482, "y": 0.7274725437164307}], "text": "4.1 Open-domain Question Answering\n"}
{"page": 5, "bbox": [{"x": 0.1757679134607315, "y": 0.7419780492782593}, {"x": 0.8265073895454407, "y": 0.7419780492782593}, {"x": 0.8265073895454407, "y": 0.9120879173278809}, {"x": 0.1757679134607315, "y": 0.9120879173278809}], "text": "Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\nthe generation flexibility of the \"closed-book\" (parametric only) approaches and the performance of\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\nwithout expensive, specialized \"salient span masking” pre-training [20]. It is worth noting that RAG's\nretriever is initialized using DPR's retriever, which uses retrieval supervision on Natural Questions\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \"cross-\nencoder\" to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\na correct answer being generated, which is not possible with standard extractive approaches, leading\n"}
{"page": 5, "bbox": [{"x": 0.49544936418533325, "y": 0.9384615421295166}, {"x": 0.5034129619598389, "y": 0.9384615421295166}, {"x": 0.5034129619598389, "y": 0.9463736414909363}, {"x": 0.49544936418533325, "y": 0.9463736414909363}], "text": "5\n"}
{"page": 6, "bbox": [{"x": 0.1757679134607315, "y": 0.1006593406200409}, {"x": 0.4960182011127472, "y": 0.10021977871656418}, {"x": 0.4960182011127472, "y": 0.15296703577041626}, {"x": 0.1757679134607315, "y": 0.15340659022331238}], "text": "Table 1: Open-Domain QA Test Scores. For TQA,\nleft column uses the standard test set for Open-\nDomain QA, right column uses the TQA-Wiki\ntest set. See Appendix D for further details.\n"}
{"page": 6, "bbox": [{"x": 0.5056883096694946, "y": 0.1006593406200409}, {"x": 0.8265073895454407, "y": 0.1006593406200409}, {"x": 0.8265073895454407, "y": 0.15340659022331238}, {"x": 0.5056883096694946, "y": 0.15340659022331238}], "text": "Table 2: Generation and classification Test Scores.\nMS-MARCO SotA is [4], FEVER-3 is [68]] and\nFEVER-2 is [57] *Uses gold context/evidence.\nBest model without gold access underlined.\n"}
{"page": 6, "bbox": [{"x": 0.372013658285141, "y": 0.16659340262413025}, {"x": 0.48009100556373596, "y": 0.16571427881717682}, {"x": 0.48009100556373596, "y": 0.17450548708438873}, {"x": 0.372013658285141, "y": 0.17538461089134216}], "text": "TQA WQ CT\n"}
{"page": 6, "bbox": [{"x": 0.23094426095485687, "y": 0.16703297197818756}, {"x": 0.2673492729663849, "y": 0.16703297197818756}, {"x": 0.2673492729663849, "y": 0.17406593263149261}, {"x": 0.23094426095485687, "y": 0.17406593263149261}], "text": "Model\n"}
{"page": 6, "bbox": [{"x": 0.3253697454929352, "y": 0.16703297197818756}, {"x": 0.34584754705429077, "y": 0.16703297197818756}, {"x": 0.34584754705429077, "y": 0.17538461089134216}, {"x": 0.3253697454929352, "y": 0.17538461089134216}], "text": "NQ\n"}
{"page": 6, "bbox": [{"x": 0.5250284671783447, "y": 0.17714285850524902}, {"x": 0.562571108341217, "y": 0.17714285850524902}, {"x": 0.562571108341217, "y": 0.18417581915855408}, {"x": 0.5250284671783447, "y": 0.18417581915855408}], "text": "Model\n"}
{"page": 6, "bbox": [{"x": 0.18543799221515656, "y": 0.18373626470565796}, {"x": 0.30147895216941833, "y": 0.18417581915855408}, {"x": 0.30147895216941833, "y": 0.19208791851997375}, {"x": 0.18543799221515656, "y": 0.19164834916591644}], "text": "Closed T5-11B [52]\n"}
{"page": 6, "bbox": [{"x": 0.38282138109207153, "y": 0.18417581915855408}, {"x": 0.4482366442680359, "y": 0.18373626470565796}, {"x": 0.4482366442680359, "y": 0.19164834916591644}, {"x": 0.38282138109207153, "y": 0.19208791851997375}], "text": "/50.1 37.4\n"}
{"page": 6, "bbox": [{"x": 0.3242320716381073, "y": 0.1850549429655075}, {"x": 0.34698522090911865, "y": 0.1850549429655075}, {"x": 0.34698522090911865, "y": 0.19164834916591644}, {"x": 0.3242320716381073, "y": 0.19164834916591644}], "text": "34.5\n"}
{"page": 6, "bbox": [{"x": 0.3697383403778076, "y": 0.18813186883926392}, {"x": 0.3737201392650604, "y": 0.18813186883926392}, {"x": 0.3737201392650604, "y": 0.18945054709911346}, {"x": 0.3697383403778076, "y": 0.18945054709911346}], "text": "-\n"}
{"page": 6, "bbox": [{"x": 0.3230944275856018, "y": 0.1960439532995224}, {"x": 0.3475540280342102, "y": 0.1960439532995224}, {"x": 0.3475540280342102, "y": 0.20351648330688477}, {"x": 0.3230944275856018, "y": 0.20351648330688477}], "text": "36.6\n"}
{"page": 6, "bbox": [{"x": 0.1860068291425705, "y": 0.1964835226535797}, {"x": 0.2155858874320984, "y": 0.1964835226535797}, {"x": 0.2155858874320984, "y": 0.20351648330688477}, {"x": 0.1860068291425705, "y": 0.20351648330688477}], "text": "Book\n"}
{"page": 6, "bbox": [{"x": 0.38282138109207153, "y": 0.1964835226535797}, {"x": 0.44766780734062195, "y": 0.1964835226535797}, {"x": 0.44766780734062195, "y": 0.20351648330688477}, {"x": 0.38282138109207153, "y": 0.20351648330688477}], "text": "/60.5 44.7\n"}
{"page": 6, "bbox": [{"x": 0.233788400888443, "y": 0.1964835226535797}, {"x": 0.31058019399642944, "y": 0.1960439532995224}, {"x": 0.31058019399642944, "y": 0.20395603775978088}, {"x": 0.233788400888443, "y": 0.2043956071138382}], "text": "T5-11B+SSM[52]\n"}
{"page": 6, "bbox": [{"x": 0.5147895216941833, "y": 0.20615383982658386}, {"x": 0.5432309508323669, "y": 0.20659340918064117}, {"x": 0.5432309508323669, "y": 0.21406593918800354}, {"x": 0.5147895216941833, "y": 0.21362636983394623}], "text": "SotA\n"}
{"page": 6, "bbox": [{"x": 0.6291239857673645, "y": 0.21098901331424713}, {"x": 0.6331058144569397, "y": 0.21098901331424713}, {"x": 0.6331058144569397, "y": 0.21230769157409668}, {"x": 0.6291239857673645, "y": 0.21230769157409668}], "text": "-\n"}
{"page": 6, "bbox": [{"x": 0.5915813446044922, "y": 0.21098901331424713}, {"x": 0.5955631136894226, "y": 0.21098901331424713}, {"x": 0.5955631136894226, "y": 0.2127472460269928}, {"x": 0.5915813446044922, "y": 0.2127472460269928}], "text": "-\n"}
{"page": 6, "bbox": [{"x": 0.3230944275856018, "y": 0.21450549364089966}, {"x": 0.3464163839817047, "y": 0.21450549364089966}, {"x": 0.3464163839817047, "y": 0.2215384542942047}, {"x": 0.3230944275856018, "y": 0.2215384542942047}], "text": "40.4\n"}
{"page": 6, "bbox": [{"x": 0.233788400888443, "y": 0.21450549364089966}, {"x": 0.30659839510917664, "y": 0.21450549364089966}, {"x": 0.30659839510917664, "y": 0.22285714745521545}, {"x": 0.233788400888443, "y": 0.22285714745521545}], "text": "REALM [20]\n"}
{"page": 6, "bbox": [{"x": 0.18543799221515656, "y": 0.21406593918800354}, {"x": 0.21501706540584564, "y": 0.21494504809379578}, {"x": 0.2144482433795929, "y": 0.224175825715065}, {"x": 0.1848691701889038, "y": 0.22329670190811157}], "text": "Open\n"}
{"page": 6, "bbox": [{"x": 0.5142207145690918, "y": 0.1767033040523529}, {"x": 0.8156996369361877, "y": 0.1762637346982956}, {"x": 0.8156996369361877, "y": 0.26461538672447205}, {"x": 0.5142207145690918, "y": 0.26505494117736816}], "text": "Jeopardy MSMARCO FVR3 FVR2\nB-1 QB-1 R-L B-1 Label Acc.\n49.8* 49.9* 76.8 92.2*\nBART 15.1 19.7 38.2 41.6 64.0 81.1\nRAG-Tok. 17.3 22.2 40.1 41.5\nRAG-Seq. 14.7 21.4 40.8 44.2\n"}
{"page": 6, "bbox": [{"x": 0.35949942469596863, "y": 0.21406593918800354}, {"x": 0.48350396752357483, "y": 0.21450549364089966}, {"x": 0.48350396752357483, "y": 0.23384615778923035}, {"x": 0.35949942469596863, "y": 0.23340658843517303}], "text": "- / - 40.7 46.8\n57.9/ - 41.1 50.6\n"}
{"page": 6, "bbox": [{"x": 0.32252559065818787, "y": 0.22637362778186798}, {"x": 0.3464163839817047, "y": 0.22637362778186798}, {"x": 0.3464163839817047, "y": 0.23340658843517303}, {"x": 0.32252559065818787, "y": 0.23340658843517303}], "text": "41.5\n"}
{"page": 6, "bbox": [{"x": 0.1860068291425705, "y": 0.22593407332897186}, {"x": 0.2855517566204071, "y": 0.22593407332897186}, {"x": 0.2855517566204071, "y": 0.23472528159618378}, {"x": 0.1860068291425705, "y": 0.23472528159618378}], "text": "Book DPR [26]\n"}
{"page": 6, "bbox": [{"x": 0.7423208355903625, "y": 0.2492307722568512}, {"x": 0.80887371301651, "y": 0.24835164844989777}, {"x": 0.80887371301651, "y": 0.25670328736305237}, {"x": 0.7423208355903625, "y": 0.257582426071167}], "text": "72.5 89.5\n"}
{"page": 6, "bbox": [{"x": 0.3230944275856018, "y": 0.2439560443162918}, {"x": 0.48350396752357483, "y": 0.2439560443162918}, {"x": 0.48350396752357483, "y": 0.2628571391105652}, {"x": 0.3230944275856018, "y": 0.2628571391105652}], "text": "44.1 55.2/66.1 45.5 50.0\n44.5 56.8/68.0 45.2 52.2\n"}
{"page": 6, "bbox": [{"x": 0.23321956396102905, "y": 0.2439560443162918}, {"x": 0.2997724711894989, "y": 0.2439560443162918}, {"x": 0.2997724711894989, "y": 0.26505494117736816}, {"x": 0.23321956396102905, "y": 0.26505494117736816}], "text": "RAG-Token\nRAG-Seq.\n"}
{"page": 6, "bbox": [{"x": 0.1757679134607315, "y": 0.29890111088752747}, {"x": 0.8236632347106934, "y": 0.29890111088752747}, {"x": 0.8236632347106934, "y": 0.3362637460231781}, {"x": 0.1757679134607315, "y": 0.3362637460231781}], "text": "to more effective marginalization over documents. Furthermore, RAG can generate correct answers\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\ncases for NQ, where an extractive model would score 0%.\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.3551648259162903}, {"x": 0.43970420956611633, "y": 0.35692307353019714}, {"x": 0.43970420956611633, "y": 0.36835163831710815}, {"x": 0.17519909143447876, "y": 0.3665934205055237}], "text": "4.2 Abstractive Question Answering\n"}
{"page": 6, "bbox": [{"x": 0.17463025450706482, "y": 0.3810988962650299}, {"x": 0.8236632347106934, "y": 0.381538450717926}, {"x": 0.8236632347106934, "y": 0.49098899960517883}, {"x": 0.17463025450706482, "y": 0.4905494451522827}], "text": "As shown in Table 2 RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\nimpressive given that (i) those models access gold passages with specific information required to\ngenerate the reference answer, (ii) many questions are unanswerable without the gold passages, and\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\nfrom our models. Qualitatively, we find that RAG models hallucinate less and generate factually\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\nBART generations (see §4.5)\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.5094505548477173}, {"x": 0.4260523319244385, "y": 0.5081318616867065}, {"x": 0.4260523319244385, "y": 0.5191208720207214}, {"x": 0.17519909143447876, "y": 0.5204395651817322}], "text": "4.3 Jeopardy Question Generation\n"}
{"page": 6, "bbox": [{"x": 0.17519909143447876, "y": 0.5345054864883423}, {"x": 0.8265073895454407, "y": 0.5345054864883423}, {"x": 0.8265073895454407, "y": 0.8285714387893677}, {"x": 0.17519909143447876, "y": 0.8285714387893677}], "text": "Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\nwith both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over\n452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more\nfactual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both\nRAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of\nRAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to\nbe more specific by a large margin. Table 3 shows typical generations from each model.\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\nbest because it can generate responses that combine content from several documents. Figure 2 shows\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms\" is\ngenerated. Intriguingly, after the first token of each book is generated, the document posterior flattens.\nThis observation suggests that the generator can complete the titles without depending on specific\ndocuments. In other words, the model's parametric knowledge is sufficient to complete the titles. We\nfind evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART's parameters. Similarly,\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows\nhow parametric and non-parametric memories work together—the non-parametric component helps\nto guide the generation, drawing out specific knowledge stored in the parametric memory.\n"}
{"page": 6, "bbox": [{"x": 0.17463025450706482, "y": 0.847912073135376}, {"x": 0.32764506340026855, "y": 0.847912073135376}, {"x": 0.32764506340026855, "y": 0.8562637567520142}, {"x": 0.17463025450706482, "y": 0.8562637567520142}], "text": "4.4 Fact Verification\n"}
{"page": 6, "bbox": [{"x": 0.1757679134607315, "y": 0.872967004776001}, {"x": 0.8259385824203491, "y": 0.872967004776001}, {"x": 0.8259385824203491, "y": 0.9116483330726624}, {"x": 0.1757679134607315, "y": 0.9116483330726624}], "text": "Table 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of\nstate-of-the-art models, which are complex pipeline systems with domain-specific architectures and\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\n"}
{"page": 6, "bbox": [{"x": 0.49544936418533325, "y": 0.9384615421295166}, {"x": 0.5045506358146667, "y": 0.9384615421295166}, {"x": 0.5045506358146667, "y": 0.9463736414909363}, {"x": 0.49544936418533325, "y": 0.9463736414909363}], "text": "6\n"}
{"page": 7, "bbox": [{"x": 0.4755403995513916, "y": 0.09714286029338837}, {"x": 0.49943116307258606, "y": 0.09758241474628448}, {"x": 0.49943116307258606, "y": 0.10285714268684387}, {"x": 0.4755403995513916, "y": 0.10241758078336716}], "text": "Doc 1\n"}
{"page": 7, "bbox": [{"x": 0.4755403995513916, "y": 0.10857142508029938}, {"x": 0.5045506358146667, "y": 0.10857142508029938}, {"x": 0.5045506358146667, "y": 0.11340659111738205}, {"x": 0.4755403995513916, "y": 0.11340659111738205}], "text": "Doc 2-\n"}
{"page": 7, "bbox": [{"x": 0.4755403995513916, "y": 0.11912088096141815}, {"x": 0.49943116307258606, "y": 0.11912088096141815}, {"x": 0.49943116307258606, "y": 0.12439560145139694}, {"x": 0.4755403995513916, "y": 0.12439560145139694}], "text": "Doc 3\n"}
{"page": 7, "bbox": [{"x": 0.17974971234798431, "y": 0.09670329838991165}, {"x": 0.46416381001472473, "y": 0.09626373648643494}, {"x": 0.46416381001472473, "y": 0.148131862282753}, {"x": 0.17974971234798431, "y": 0.1485714316368103}], "text": "Document 1: his works are considered classics of American\nliterature... His wartime experiences formed the basis for his novel\n\"A Farewell to Arms\" (1929)...\nDocument 2: ... artists of the 1920s \"Lost Generation\" expatriate\ncommunity. His debut novel, \"The Sun Also Rises\", was published\nin 1926.\n"}
{"page": 7, "bbox": [{"x": 0.4755403995513916, "y": 0.13054944574832916}, {"x": 0.49943116307258606, "y": 0.13098901510238647}, {"x": 0.49943116307258606, "y": 0.13582417368888855}, {"x": 0.4755403995513916, "y": 0.13538461923599243}], "text": "Doc 4\n"}
{"page": 7, "bbox": [{"x": 0.4755403995513916, "y": 0.14109890162944794}, {"x": 0.4988623559474945, "y": 0.14109890162944794}, {"x": 0.4988623559474945, "y": 0.14637362957000732}, {"x": 0.4755403995513916, "y": 0.14637362957000732}], "text": "Doc 5\n"}
{"page": 7, "bbox": [{"x": 0.6234357357025146, "y": 0.14901098608970642}, {"x": 0.6416382193565369, "y": 0.14901098608970642}, {"x": 0.6416382193565369, "y": 0.15208791196346283}, {"x": 0.6234357357025146, "y": 0.15208791196346283}], "text": "T T\n"}
{"page": 7, "bbox": [{"x": 0.6205915808677673, "y": 0.15516483783721924}, {"x": 0.6291239857673645, "y": 0.15472526848316193}, {"x": 0.6296928524971008, "y": 0.16087912023067474}, {"x": 0.6211603879928589, "y": 0.16131867468357086}], "text": "is\n"}
{"page": 7, "bbox": [{"x": 0.7343572378158569, "y": 0.15824176371097565}, {"x": 0.7394766807556152, "y": 0.15560439229011536}, {"x": 0.7423208355903625, "y": 0.15912087261676788}, {"x": 0.7372013926506042, "y": 0.16175824403762817}], "text": "A\n"}
{"page": 7, "bbox": [{"x": 0.776450514793396, "y": 0.1595604419708252}, {"x": 0.7815699577331543, "y": 0.15560439229011536}, {"x": 0.7849829196929932, "y": 0.15824176371097565}, {"x": 0.7798634767532349, "y": 0.1621977984905243}], "text": "to\n"}
{"page": 7, "bbox": [{"x": 0.5318543910980225, "y": 0.1626373678445816}, {"x": 0.5432309508323669, "y": 0.1538461595773697}, {"x": 0.5483503937721252, "y": 0.15780219435691833}, {"x": 0.5369738340377808, "y": 0.16659340262413025}], "text": "The\n"}
{"page": 7, "bbox": [{"x": 0.6734926104545593, "y": 0.1626373678445816}, {"x": 0.6837314963340759, "y": 0.1538461595773697}, {"x": 0.6894198060035706, "y": 0.15780219435691833}, {"x": 0.6791808605194092, "y": 0.16659340262413025}], "text": "this\n"}
{"page": 7, "bbox": [{"x": 0.5893060564994812, "y": 0.16307692229747772}, {"x": 0.6001137495040894, "y": 0.1542857140302658}, {"x": 0.6052331924438477, "y": 0.15780219435691833}, {"x": 0.5944254994392395, "y": 0.16659340262413025}], "text": "ises\n"}
{"page": 7, "bbox": [{"x": 0.5597269535064697, "y": 0.1626373678445816}, {"x": 0.5824800729751587, "y": 0.15296703577041626}, {"x": 0.5870307087898254, "y": 0.15868131816387177}, {"x": 0.5637087821960449, "y": 0.1683516502380371}], "text": "Also R\n"}
{"page": 7, "bbox": [{"x": 0.7576791644096375, "y": 0.16439560055732727}, {"x": 0.7690557241439819, "y": 0.15340659022331238}, {"x": 0.7747440338134766, "y": 0.15736263990402222}, {"x": 0.7633674740791321, "y": 0.1679120808839798}], "text": "well\n"}
{"page": 7, "bbox": [{"x": 0.7434584498405457, "y": 0.16439560055732727}, {"x": 0.7548350691795349, "y": 0.1538461595773697}, {"x": 0.7605233192443848, "y": 0.15780219435691833}, {"x": 0.7491467595100403, "y": 0.1679120808839798}], "text": "Fare\n"}
{"page": 7, "bbox": [{"x": 0.7838453054428101, "y": 0.16747252643108368}, {"x": 0.8037542700767517, "y": 0.15076923370361328}, {"x": 0.8094425201416016, "y": 0.15472526848316193}, {"x": 0.7895335555076599, "y": 0.17142857611179352}], "text": "Arms,,\n"}
{"page": 7, "bbox": [{"x": 0.5466439127922058, "y": 0.16439560055732727}, {"x": 0.5574516654014587, "y": 0.15472526848316193}, {"x": 0.5620023012161255, "y": 0.15824176371097565}, {"x": 0.5511945486068726, "y": 0.16747252643108368}], "text": "Sun\n"}
{"page": 7, "bbox": [{"x": 0.502275288105011, "y": 0.16439560055732727}, {"x": 0.5153583884239197, "y": 0.1538461595773697}, {"x": 0.5210466384887695, "y": 0.15824176371097565}, {"x": 0.5079635977745056, "y": 0.16879120469093323}], "text": "BOS\n"}
{"page": 7, "bbox": [{"x": 0.6422070264816284, "y": 0.16703297197818756}, {"x": 0.6581342220306396, "y": 0.15296703577041626}, {"x": 0.6643913388252258, "y": 0.15736263990402222}, {"x": 0.6484641432762146, "y": 0.17142857611179352}], "text": "novel\n"}
{"page": 7, "bbox": [{"x": 0.6825938820838928, "y": 0.16879120469093323}, {"x": 0.7081910967826843, "y": 0.15032966434955597}, {"x": 0.7167235612869263, "y": 0.15780219435691833}, {"x": 0.69112628698349, "y": 0.17582418024539948}], "text": "author of\n"}
{"page": 7, "bbox": [{"x": 0.1757679134607315, "y": 0.17758241295814514}, {"x": 0.8265073895454407, "y": 0.17758241295814514}, {"x": 0.8265073895454407, "y": 0.21670329570770264}, {"x": 0.1757679134607315, "y": 0.21670329570770264}], "text": "Figure 2: RAG-Token document posterior p(zi|x, Yi, Y-i) for each generated token for input “Hem-\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n"}
{"page": 7, "bbox": [{"x": 0.1757679134607315, "y": 0.24263736605644226}, {"x": 0.8236632347106934, "y": 0.24263736605644226}, {"x": 0.8236632347106934, "y": 0.26813188195228577}, {"x": 0.1757679134607315, "y": 0.26813188195228577}], "text": "Table 3: Examples from generation tasks. RAG models generate more specific and factually accurate\nresponses. “?” indicates factually incorrect responses, * indicates partially correct responses.\n"}
{"page": 7, "bbox": [{"x": 0.1848691701889038, "y": 0.28087911009788513}, {"x": 0.20705346763134003, "y": 0.28087911009788513}, {"x": 0.20705346763134003, "y": 0.28659340739250183}, {"x": 0.1848691701889038, "y": 0.28659340739250183}], "text": "Task\n"}
{"page": 7, "bbox": [{"x": 0.3139931857585907, "y": 0.28087911009788513}, {"x": 0.3452787399291992, "y": 0.28087911009788513}, {"x": 0.3452787399291992, "y": 0.28703296184539795}, {"x": 0.3139931857585907, "y": 0.28703296184539795}], "text": "Model\n"}
{"page": 7, "bbox": [{"x": 0.3577929437160492, "y": 0.28087911009788513}, {"x": 0.4112628102302551, "y": 0.28087911009788513}, {"x": 0.4112628102302551, "y": 0.28703296184539795}, {"x": 0.3577929437160492, "y": 0.28703296184539795}], "text": "Generation\n"}
{"page": 7, "bbox": [{"x": 0.2377701997756958, "y": 0.28131869435310364}, {"x": 0.26393628120422363, "y": 0.28175824880599976}, {"x": 0.26393628120422363, "y": 0.2892307639122009}, {"x": 0.2377701997756958, "y": 0.2887912094593048}], "text": "Input\n"}
{"page": 7, "bbox": [{"x": 0.2377701997756958, "y": 0.3019780218601227}, {"x": 0.30318543314933777, "y": 0.30153846740722656}, {"x": 0.30318543314933777, "y": 0.3085714280605316}, {"x": 0.2377701997756958, "y": 0.30901098251342773}], "text": "define middle\n"}
{"page": 7, "bbox": [{"x": 0.2377701997756958, "y": 0.3142857253551483}, {"x": 0.25142207741737366, "y": 0.3142857253551483}, {"x": 0.25142207741737366, "y": 0.3186813294887543}, {"x": 0.2377701997756958, "y": 0.3186813294887543}], "text": "ear\n"}
{"page": 7, "bbox": [{"x": 0.18430034816265106, "y": 0.31824174523353577}, {"x": 0.20591580867767334, "y": 0.31824174523353577}, {"x": 0.20591580867767334, "y": 0.3243955969810486}, {"x": 0.18430034816265106, "y": 0.3243955969810486}], "text": "MS-\n"}
{"page": 7, "bbox": [{"x": 0.18430034816265106, "y": 0.3283516466617584}, {"x": 0.2275312840938568, "y": 0.3279120922088623}, {"x": 0.2275312840938568, "y": 0.33450549840927124}, {"x": 0.18430034816265106, "y": 0.33494505286216736}], "text": "MARCO\n"}
{"page": 7, "bbox": [{"x": 0.2377701997756958, "y": 0.3336263597011566}, {"x": 0.3060295879840851, "y": 0.3331868052482605}, {"x": 0.3060295879840851, "y": 0.36043956875801086}, {"x": 0.2377701997756958, "y": 0.360879123210907}], "text": "what\nI currency\nneeded in\nscotland\n"}
{"page": 7, "bbox": [{"x": 0.3128555119037628, "y": 0.2949450612068176}, {"x": 0.7172923684120178, "y": 0.2945055067539215}, {"x": 0.7172923684120178, "y": 0.4000000059604645}, {"x": 0.3128555119037628, "y": 0.4004395604133606}], "text": "The middle ear includes the tympanic cavity and the three ossicles.\nBART \"The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S\nBART\nThe currency needed in Scotland is Pound sterling.\nRAG-T Pound is the currency needed in Scotland.\nRAG-S The currency needed in Scotland is the pound sterling.\nBART \"This state has the largest number of counties in the U.S.\nRAG-T It's the only U.S. state named for a U.S. president\nRAG-S It's the state where you'll find Mount Rainier National Park\n"}
{"page": 7, "bbox": [{"x": 0.18259385228157043, "y": 0.38197803497314453}, {"x": 0.29522183537483215, "y": 0.38197803497314453}, {"x": 0.29522183537483215, "y": 0.41890108585357666}, {"x": 0.18259385228157043, "y": 0.41890108585357666}], "text": "Jeopardy Washington\nQuestion\nGener\n-ation\n"}
{"page": 7, "bbox": [{"x": 0.3139931857585907, "y": 0.4087912142276764}, {"x": 0.3447099030017853, "y": 0.4087912142276764}, {"x": 0.3447099030017853, "y": 0.4145054817199707}, {"x": 0.3139931857585907, "y": 0.4145054817199707}], "text": "BART\n"}
{"page": 7, "bbox": [{"x": 0.3139931857585907, "y": 0.40747252106666565}, {"x": 0.8122866749763489, "y": 0.4083516597747803}, {"x": 0.8122866749763489, "y": 0.43824175000190735}, {"x": 0.3139931857585907, "y": 0.4373626410961151}], "text": "*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\nRAG-T Dante's \"Inferno\" is the first part of this epic poem\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\n"}
{"page": 7, "bbox": [{"x": 0.23720136284828186, "y": 0.41362637281417847}, {"x": 0.2918088734149933, "y": 0.4140659272670746}, {"x": 0.2918088734149933, "y": 0.43252748250961304}, {"x": 0.23720136284828186, "y": 0.43208789825439453}], "text": "The Divine\nComedy\n"}
{"page": 7, "bbox": [{"x": 0.17463025450706482, "y": 0.46945056319236755}, {"x": 0.8259385824203491, "y": 0.46945056319236755}, {"x": 0.8259385824203491, "y": 0.5635164976119995}, {"x": 0.17463025450706482, "y": 0.5635164976119995}], "text": "For 2-way classification, we compare against Thorne and Vlachos [57]], who train RoBERTa [35]\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\nby RAG and gold evidence annotations. We find that the top retrieved document is from a gold article\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n"}
{"page": 7, "bbox": [{"x": 0.17519909143447876, "y": 0.5828571319580078}, {"x": 0.34186574816703796, "y": 0.5828571319580078}, {"x": 0.34186574816703796, "y": 0.5916483402252197}, {"x": 0.17519909143447876, "y": 0.5916483402252197}], "text": "4.5 Additional Results\n"}
{"page": 7, "bbox": [{"x": 0.1757679134607315, "y": 0.6079120635986328}, {"x": 0.8242321014404297, "y": 0.6074725389480591}, {"x": 0.8242321014404297, "y": 0.6874725222587585}, {"x": 0.1757679134607315, "y": 0.687912106513977}], "text": "Generation Diversity Section 4.3 shows that RAG models are more factual and specific than\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence's generations are\nmore diverse than RAG-Token's, and both are significantly more diverse than BART without needing\nany diversity-promoting decoding.\n"}
{"page": 7, "bbox": [{"x": 0.17519909143447876, "y": 0.7059340476989746}, {"x": 0.8265073895454407, "y": 0.7059340476989746}, {"x": 0.8265073895454407, "y": 0.8140659332275391}, {"x": 0.17519909143447876, "y": 0.8140659332275391}], "text": "Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\nduring training. As shown in Table 6, learned retrieval improves results for all tasks. We compare\nRAG's dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG's retriever\nwith a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table\n6 show the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily\nentity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves\nresults on all other tasks, especially for Open-Domain QA, where it is crucial.\n"}
{"page": 7, "bbox": [{"x": 0.17463025450706482, "y": 0.8316483497619629}, {"x": 0.8248009085655212, "y": 0.8316483497619629}, {"x": 0.8248009085655212, "y": 0.9120879173278809}, {"x": 0.17463025450706482, "y": 0.9120879173278809}], "text": "Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\nbetween these dates and use a template \"Who is {position}?\" (e.g. \"Who is the President of Peru?\")\n"}
{"page": 7, "bbox": [{"x": 0.49544936418533325, "y": 0.9384615421295166}, {"x": 0.5039817690849304, "y": 0.9384615421295166}, {"x": 0.5039817690849304, "y": 0.94681316614151}, {"x": 0.49544936418533325, "y": 0.94681316614151}], "text": "7\n"}
{"page": 8, "bbox": [{"x": 0.1757679134607315, "y": 0.1006593406200409}, {"x": 0.8236632347106934, "y": 0.0980219766497612}, {"x": 0.8236632347106934, "y": 0.12219779938459396}, {"x": 0.1757679134607315, "y": 0.12483516335487366}], "text": "Table 4: Human assessments for the Jeopardy Table 5: Ratio of distinct to total tri-grams for\nQuestion Generation Task.\n"}
{"page": 8, "bbox": [{"x": 0.5056883096694946, "y": 0.11516483873128891}, {"x": 0.6149032711982727, "y": 0.11384615302085876}, {"x": 0.6149032711982727, "y": 0.12395604699850082}, {"x": 0.5056883096694946, "y": 0.12527473270893097}], "text": "generation tasks.\n"}
{"page": 8, "bbox": [{"x": 0.3111490309238434, "y": 0.13934065401554108}, {"x": 0.4533560872077942, "y": 0.13934065401554108}, {"x": 0.4533560872077942, "y": 0.14989010989665985}, {"x": 0.3111490309238434, "y": 0.14989010989665985}], "text": "Factuality Specificity\n"}
{"page": 8, "bbox": [{"x": 0.6166098117828369, "y": 0.14593406021595}, {"x": 0.69112628698349, "y": 0.14593406021595}, {"x": 0.69112628698349, "y": 0.15340659022331238}, {"x": 0.6166098117828369, "y": 0.15340659022331238}], "text": "MSMARCO\n"}
{"page": 8, "bbox": [{"x": 0.7195677161216736, "y": 0.14593406021595}, {"x": 0.8111490607261658, "y": 0.14505495131015778}, {"x": 0.8111490607261658, "y": 0.15472526848316193}, {"x": 0.7195677161216736, "y": 0.15560439229011536}], "text": "Jeopardy QGen\n"}
{"page": 8, "bbox": [{"x": 0.32593855261802673, "y": 0.15824176371097565}, {"x": 0.3543799817562103, "y": 0.15868131816387177}, {"x": 0.3543799817562103, "y": 0.16615384817123413}, {"x": 0.32593855261802673, "y": 0.16571427881717682}], "text": "7.1%\n"}
{"page": 8, "bbox": [{"x": 0.40273037552833557, "y": 0.15868131816387177}, {"x": 0.4379977285861969, "y": 0.15868131816387177}, {"x": 0.4379977285861969, "y": 0.16615384817123413}, {"x": 0.40273037552833557, "y": 0.16615384817123413}], "text": "16.8%\n"}
{"page": 8, "bbox": [{"x": 0.6348122954368591, "y": 0.16439560055732727}, {"x": 0.6717861294746399, "y": 0.16439560055732727}, {"x": 0.6717861294746399, "y": 0.17186813056468964}, {"x": 0.6348122954368591, "y": 0.17186813056468964}], "text": "89.6%\n"}
{"page": 8, "bbox": [{"x": 0.5170648694038391, "y": 0.16483516991138458}, {"x": 0.5460751056671143, "y": 0.16483516991138458}, {"x": 0.5460751056671143, "y": 0.17230768501758575}, {"x": 0.5170648694038391, "y": 0.17230768501758575}], "text": "Gold\n"}
{"page": 8, "bbox": [{"x": 0.21729238331317902, "y": 0.15868131816387177}, {"x": 0.2929465174674988, "y": 0.15868131816387177}, {"x": 0.2929465174674988, "y": 0.17890110611915588}, {"x": 0.21729238331317902, "y": 0.17890110611915588}], "text": "BART better\nRAG better\n"}
{"page": 8, "bbox": [{"x": 0.7468714714050293, "y": 0.16483516991138458}, {"x": 0.7855517864227295, "y": 0.16483516991138458}, {"x": 0.7855517864227295, "y": 0.17274725437164307}, {"x": 0.7468714714050293, "y": 0.17274725437164307}], "text": "90.0%\n"}
{"page": 8, "bbox": [{"x": 0.3202502727508545, "y": 0.1709890067577362}, {"x": 0.36006826162338257, "y": 0.1709890067577362}, {"x": 0.36006826162338257, "y": 0.17890110611915588}, {"x": 0.3202502727508545, "y": 0.17890110611915588}], "text": "42.7%\n"}
{"page": 8, "bbox": [{"x": 0.4004550576210022, "y": 0.1709890067577362}, {"x": 0.43970420956611633, "y": 0.1709890067577362}, {"x": 0.43970420956611633, "y": 0.17890110611915588}, {"x": 0.4004550576210022, "y": 0.17890110611915588}], "text": "37.4%\n"}
{"page": 8, "bbox": [{"x": 0.7474402785301208, "y": 0.1767033040523529}, {"x": 0.7844141125679016, "y": 0.17714285850524902}, {"x": 0.7844141125679016, "y": 0.1850549429655075}, {"x": 0.7474402785301208, "y": 0.1846153885126114}], "text": "32.4%\n"}
{"page": 8, "bbox": [{"x": 0.5170648694038391, "y": 0.17758241295814514}, {"x": 0.5534698367118835, "y": 0.17758241295814514}, {"x": 0.5534698367118835, "y": 0.1846153885126114}, {"x": 0.5170648694038391, "y": 0.1846153885126114}], "text": "BART\n"}
{"page": 8, "bbox": [{"x": 0.6353811025619507, "y": 0.1767033040523529}, {"x": 0.6729238033294678, "y": 0.17714285850524902}, {"x": 0.6729238033294678, "y": 0.18549451231956482}, {"x": 0.6353811025619507, "y": 0.1850549429655075}], "text": "70.7%\n"}
{"page": 8, "bbox": [{"x": 0.40273037552833557, "y": 0.18329671025276184}, {"x": 0.43856656551361084, "y": 0.18285714089870453}, {"x": 0.43856656551361084, "y": 0.190769225358963}, {"x": 0.40273037552833557, "y": 0.19120879471302032}], "text": "11.8%\n"}
{"page": 8, "bbox": [{"x": 0.3230944275856018, "y": 0.18285714089870453}, {"x": 0.3589306175708771, "y": 0.18329671025276184}, {"x": 0.3589306175708771, "y": 0.19164834916591644}, {"x": 0.3230944275856018, "y": 0.19120879471302032}], "text": "11.7%\n"}
{"page": 8, "bbox": [{"x": 0.21729238331317902, "y": 0.18153846263885498}, {"x": 0.27986347675323486, "y": 0.18373626470565796}, {"x": 0.2792946398258209, "y": 0.19428572058677673}, {"x": 0.21672354638576508, "y": 0.19208791851997375}], "text": "Both good\n"}
{"page": 8, "bbox": [{"x": 0.6353811025619507, "y": 0.18989011645317078}, {"x": 0.6717861294746399, "y": 0.18945054709911346}, {"x": 0.6717861294746399, "y": 0.19736263155937195}, {"x": 0.6353811025619507, "y": 0.19780220091342926}], "text": "77.8%\n"}
{"page": 8, "bbox": [{"x": 0.746302604675293, "y": 0.1903296709060669}, {"x": 0.7838453054428101, "y": 0.18945054709911346}, {"x": 0.7838453054428101, "y": 0.19692307710647583}, {"x": 0.746302604675293, "y": 0.19780220091342926}], "text": "46.8%\n"}
{"page": 8, "bbox": [{"x": 0.4055745303630829, "y": 0.19560439884662628}, {"x": 0.4351535737514496, "y": 0.1960439532995224}, {"x": 0.4351535737514496, "y": 0.20395603775978088}, {"x": 0.4055745303630829, "y": 0.20351648330688477}], "text": "6.9%\n"}
{"page": 8, "bbox": [{"x": 0.3230944275856018, "y": 0.19560439884662628}, {"x": 0.3589306175708771, "y": 0.1964835226535797}, {"x": 0.35836178064346313, "y": 0.20483516156673431}, {"x": 0.32252559065818787, "y": 0.20395603775978088}], "text": "17.7%\n"}
{"page": 8, "bbox": [{"x": 0.5164960026741028, "y": 0.18989011645317078}, {"x": 0.587599515914917, "y": 0.18945054709911346}, {"x": 0.587599515914917, "y": 0.21230769157409668}, {"x": 0.5164960026741028, "y": 0.2127472460269928}], "text": "RAG-Token\nRAG-Seq.\n"}
{"page": 8, "bbox": [{"x": 0.21786120533943176, "y": 0.19428572058677673}, {"x": 0.27701935172080994, "y": 0.1986813247203827}, {"x": 0.27588167786598206, "y": 0.20791208744049072}, {"x": 0.21672354638576508, "y": 0.20351648330688477}], "text": "Both poor\n"}
{"page": 8, "bbox": [{"x": 0.7468714714050293, "y": 0.20219780504703522}, {"x": 0.7849829196929932, "y": 0.2017582356929779}, {"x": 0.7849829196929932, "y": 0.20923076570034027}, {"x": 0.7468714714050293, "y": 0.20967033505439758}], "text": "53.8%\n"}
{"page": 8, "bbox": [{"x": 0.6353811025619507, "y": 0.2017582356929779}, {"x": 0.6717861294746399, "y": 0.20219780504703522}, {"x": 0.6717861294746399, "y": 0.2101098895072937}, {"x": 0.6353811025619507, "y": 0.20967033505439758}], "text": "83.5%\n"}
{"page": 8, "bbox": [{"x": 0.4015927314758301, "y": 0.20879121124744415}, {"x": 0.4391353726387024, "y": 0.20879121124744415}, {"x": 0.4391353726387024, "y": 0.21670329570770264}, {"x": 0.4015927314758301, "y": 0.21670329570770264}], "text": "20.1%\n"}
{"page": 8, "bbox": [{"x": 0.21672354638576508, "y": 0.2074725329875946}, {"x": 0.35836178064346313, "y": 0.20923076570034027}, {"x": 0.35836178064346313, "y": 0.21890109777450562}, {"x": 0.21672354638576508, "y": 0.21714285016059875}], "text": "No majority 20.8%\n"}
{"page": 8, "bbox": [{"x": 0.17633675038814545, "y": 0.2527472674846649}, {"x": 0.8230944275856018, "y": 0.2527472674846649}, {"x": 0.8230944275856018, "y": 0.2632966935634613}, {"x": 0.17633675038814545, "y": 0.2632966935634613}], "text": "Table 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.\n"}
{"page": 8, "bbox": [{"x": 0.2349260449409485, "y": 0.2778021991252899}, {"x": 0.2741751968860626, "y": 0.2773626446723938}, {"x": 0.2741751968860626, "y": 0.2852747142314911}, {"x": 0.2349260449409485, "y": 0.2857142984867096}], "text": "Model\n"}
{"page": 8, "bbox": [{"x": 0.48634812235832214, "y": 0.2778021991252899}, {"x": 0.5039817690849304, "y": 0.2778021991252899}, {"x": 0.5039817690849304, "y": 0.2857142984867096}, {"x": 0.48634812235832214, "y": 0.2857142984867096}], "text": "CT\n"}
{"page": 8, "bbox": [{"x": 0.5278725624084473, "y": 0.27824175357818604}, {"x": 0.6205915808677673, "y": 0.2773626446723938}, {"x": 0.6205915808677673, "y": 0.28703296184539795}, {"x": 0.5278725624084473, "y": 0.2879121005535126}], "text": "Jeopardy-QGen\n"}
{"page": 8, "bbox": [{"x": 0.34414106607437134, "y": 0.27824175357818604}, {"x": 0.36632537841796875, "y": 0.27824175357818604}, {"x": 0.36632537841796875, "y": 0.28747251629829407}, {"x": 0.34414106607437134, "y": 0.28747251629829407}], "text": "NQ\n"}
{"page": 8, "bbox": [{"x": 0.3879408538341522, "y": 0.27824175357818604}, {"x": 0.46302616596221924, "y": 0.27868130803108215}, {"x": 0.46302616596221924, "y": 0.29846152663230896}, {"x": 0.3879408538341522, "y": 0.29802197217941284}], "text": "TQA WQ\nExact Match\n"}
{"page": 8, "bbox": [{"x": 0.7315130829811096, "y": 0.2769230902194977}, {"x": 0.8321956992149353, "y": 0.2778021991252899}, {"x": 0.831626832485199, "y": 0.2997802197933197}, {"x": 0.7309442758560181, "y": 0.29890111088752747}], "text": "FVR-3 FVR-2\nLabel Accuracy\n"}
{"page": 8, "bbox": [{"x": 0.18657565116882324, "y": 0.30989012122154236}, {"x": 0.29920363426208496, "y": 0.30989012122154236}, {"x": 0.29920363426208496, "y": 0.31736263632774353}, {"x": 0.18657565116882324, "y": 0.31736263632774353}], "text": "RAG-Token-BM25\n"}
{"page": 8, "bbox": [{"x": 0.3424345850944519, "y": 0.30989012122154236}, {"x": 0.36746302247047424, "y": 0.30989012122154236}, {"x": 0.36746302247047424, "y": 0.31736263632774353}, {"x": 0.3424345850944519, "y": 0.31736263632774353}], "text": "29.7\n"}
{"page": 8, "bbox": [{"x": 0.38680317997932434, "y": 0.3103296756744385}, {"x": 0.41296929121017456, "y": 0.3103296756744385}, {"x": 0.41296929121017456, "y": 0.31736263632774353}, {"x": 0.38680317997932434, "y": 0.31736263632774353}], "text": "41.5\n"}
{"page": 8, "bbox": [{"x": 0.6399317383766174, "y": 0.30945053696632385}, {"x": 0.7110352516174316, "y": 0.30901098251342773}, {"x": 0.7110352516174316, "y": 0.32923075556755066}, {"x": 0.6399317383766174, "y": 0.32967033982276917}], "text": "55.5 48.4\n56.5 46.9\n"}
{"page": 8, "bbox": [{"x": 0.7389078736305237, "y": 0.3160439431667328}, {"x": 0.7622298002243042, "y": 0.3160439431667328}, {"x": 0.7622298002243042, "y": 0.32307693362236023}, {"x": 0.7389078736305237, "y": 0.32307693362236023}], "text": "75.1\n"}
{"page": 8, "bbox": [{"x": 0.7986348271369934, "y": 0.3160439431667328}, {"x": 0.8259385824203491, "y": 0.3160439431667328}, {"x": 0.8259385824203491, "y": 0.32351648807525635}, {"x": 0.7986348271369934, "y": 0.32351648807525635}], "text": "91.6\n"}
{"page": 8, "bbox": [{"x": 0.1860068291425705, "y": 0.2769230902194977}, {"x": 0.7081910967826843, "y": 0.27824175357818604}, {"x": 0.7076222896575928, "y": 0.3947252631187439}, {"x": 0.18543799221515656, "y": 0.39340659976005554}], "text": "MSMarco\nB-1 QB-1 R-L B-1\n32.1 33.1 17.5 22.3\nRAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5\n37.1 51.1 16.7 21.7\n41.8 52.6 11.8 19.6\n46.5 51.9 17.9 22.6\n44.9 53.4 15.3 21.5\n"}
{"page": 8, "bbox": [{"x": 0.7394766807556152, "y": 0.3476923108100891}, {"x": 0.76450514793396, "y": 0.3476923108100891}, {"x": 0.76450514793396, "y": 0.3551648259162903}, {"x": 0.7394766807556152, "y": 0.3551648259162903}], "text": "72.9\n"}
{"page": 8, "bbox": [{"x": 0.799203634262085, "y": 0.34813186526298523}, {"x": 0.8248009085655212, "y": 0.34813186526298523}, {"x": 0.8248009085655212, "y": 0.35472527146339417}, {"x": 0.799203634262085, "y": 0.35472527146339417}], "text": "89.4\n"}
{"page": 8, "bbox": [{"x": 0.1860068291425705, "y": 0.3389011025428772}, {"x": 0.4124004542827606, "y": 0.34021976590156555}, {"x": 0.4124004542827606, "y": 0.3648351728916168}, {"x": 0.1860068291425705, "y": 0.3635164797306061}], "text": "RAG-Token-Frozen\n37.8 50.1\nRAG-Sequence-Frozen 41.2 52.1\n"}
{"page": 8, "bbox": [{"x": 0.6399317383766174, "y": 0.34065935015678406}, {"x": 0.711604118347168, "y": 0.34065935015678406}, {"x": 0.711604118347168, "y": 0.39384615421295166}, {"x": 0.6399317383766174, "y": 0.39384615421295166}], "text": "55.9 49.4\n56.7 47.3\n56.2 49.4\n57.2 47.5\n"}
{"page": 8, "bbox": [{"x": 0.7986348271369934, "y": 0.37934064865112305}, {"x": 0.8248009085655212, "y": 0.37890109419822693}, {"x": 0.8248009085655212, "y": 0.3868131935596466}, {"x": 0.7986348271369934, "y": 0.3872527480125427}], "text": "90.6\n"}
{"page": 8, "bbox": [{"x": 0.34186574816703796, "y": 0.372307687997818}, {"x": 0.41296929121017456, "y": 0.3731868267059326}, {"x": 0.4124004542827606, "y": 0.39384615421295166}, {"x": 0.3412969410419464, "y": 0.3929670453071594}], "text": "43.5 54.8\n44.0 55.8\n"}
{"page": 8, "bbox": [{"x": 0.7394766807556152, "y": 0.38021978735923767}, {"x": 0.7639362812042236, "y": 0.38021978735923767}, {"x": 0.7639362812042236, "y": 0.3872527480125427}, {"x": 0.7394766807556152, "y": 0.3872527480125427}], "text": "74.5\n"}
{"page": 8, "bbox": [{"x": 0.1860068291425705, "y": 0.372307687997818}, {"x": 0.2775881588459015, "y": 0.3727472424507141}, {"x": 0.2775881588459015, "y": 0.3956044018268585}, {"x": 0.1860068291425705, "y": 0.3951648473739624}], "text": "RAG-Token\nRAG-Sequence\n"}
{"page": 8, "bbox": [{"x": 0.1757679134607315, "y": 0.43516483902931213}, {"x": 0.8253697156906128, "y": 0.43516483902931213}, {"x": 0.8253697156906128, "y": 0.48835164308547974}, {"x": 0.1757679134607315, "y": 0.48835164308547974}], "text": "to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\nThis shows we can update RAG's world knowledge by simply replacing its non-parametric memory.\n"}
{"page": 8, "bbox": [{"x": 0.1757679134607315, "y": 0.510769248008728}, {"x": 0.8248009085655212, "y": 0.510769248008728}, {"x": 0.8248009085655212, "y": 0.6052747368812561}, {"x": 0.1757679134607315, "y": 0.6052747368812561}], "text": "Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe significant differences in performance between them. We have the\nflexibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\n"}
{"page": 8, "bbox": [{"x": 0.18941979110240936, "y": 0.6303296685218811}, {"x": 0.19738338887691498, "y": 0.6303296685218811}, {"x": 0.19738338887691498, "y": 0.6338461637496948}, {"x": 0.18941979110240936, "y": 0.6338461637496948}], "text": "44\n"}
{"page": 8, "bbox": [{"x": 0.39362913370132446, "y": 0.6316483616828918}, {"x": 0.4135380983352661, "y": 0.6312087774276733}, {"x": 0.4135380983352661, "y": 0.6373626589775085}, {"x": 0.39362913370132446, "y": 0.6378021836280823}], "text": "✓ 80\n"}
{"page": 8, "bbox": [{"x": 0.7684869170188904, "y": 0.6523076891899109}, {"x": 0.8128555417060852, "y": 0.6523076891899109}, {"x": 0.8128555417060852, "y": 0.6562637090682983}, {"x": 0.7684869170188904, "y": 0.6562637090682983}], "text": "RAG-Tok R-L\n"}
{"page": 8, "bbox": [{"x": 0.7684869170188904, "y": 0.6606593132019043}, {"x": 0.8105801939964294, "y": 0.6602197885513306}, {"x": 0.8105801939964294, "y": 0.6650549173355103}, {"x": 0.7684869170188904, "y": 0.6654945015907288}], "text": "RAG-Tok B-1\n"}
{"page": 8, "bbox": [{"x": 0.18828213214874268, "y": 0.6303296685218811}, {"x": 0.19738338887691498, "y": 0.6303296685218811}, {"x": 0.19795222580432892, "y": 0.7046154141426086}, {"x": 0.18885096907615662, "y": 0.7046154141426086}], "text": "ཝཱ གླ བ རྒ རྒ ལྷ\n"}
{"page": 8, "bbox": [{"x": 0.6086462140083313, "y": 0.7024176120758057}, {"x": 0.6103526949882507, "y": 0.6325274705886841}, {"x": 0.6188850998878479, "y": 0.6325274705886841}, {"x": 0.6171786189079285, "y": 0.7024176120758057}], "text": "Bleu-1 Rouge-L score\n"}
{"page": 8, "bbox": [{"x": 0.1769055724143982, "y": 0.6927472352981567}, {"x": 0.17633675038814545, "y": 0.6426373720169067}, {"x": 0.18543799221515656, "y": 0.6426373720169067}, {"x": 0.1860068291425705, "y": 0.6927472352981567}], "text": "NQ Exact Match\n"}
{"page": 8, "bbox": [{"x": 0.3930602967739105, "y": 0.7028571367263794}, {"x": 0.39362913370132446, "y": 0.6325274705886841}, {"x": 0.4015927314758301, "y": 0.6325274705886841}, {"x": 0.40102389454841614, "y": 0.7028571367263794}], "text": "NQ Answer Recall @ K\n"}
{"page": 8, "bbox": [{"x": 0.6217292547225952, "y": 0.6694505214691162}, {"x": 0.6296928524971008, "y": 0.6694505214691162}, {"x": 0.6296928524971008, "y": 0.6729670166969299}, {"x": 0.6217292547225952, "y": 0.6729670166969299}], "text": "52\n"}
{"page": 8, "bbox": [{"x": 0.7684869170188904, "y": 0.6694505214691162}, {"x": 0.8128555417060852, "y": 0.6694505214691162}, {"x": 0.8128555417060852, "y": 0.6747252941131592}, {"x": 0.7684869170188904, "y": 0.6747252941131592}], "text": "RAG-Seq R-L\n"}
{"page": 8, "bbox": [{"x": 0.5620023012161255, "y": 0.6725274920463562}, {"x": 0.5927190184593201, "y": 0.6725274920463562}, {"x": 0.5927190184593201, "y": 0.68703293800354}, {"x": 0.5620023012161255, "y": 0.68703293800354}], "text": "RAG-Tok\nRAG-Seq\n"}
{"page": 8, "bbox": [{"x": 0.7684869170188904, "y": 0.6773626208305359}, {"x": 0.8105801939964294, "y": 0.6769230961799622}, {"x": 0.8105801939964294, "y": 0.6821978092193604}, {"x": 0.7684869170188904, "y": 0.6826373338699341}], "text": "RAG-Seq B-1\n"}
{"page": 8, "bbox": [{"x": 0.4055745303630829, "y": 0.6813187003135681}, {"x": 0.41296929121017456, "y": 0.6813187003135681}, {"x": 0.41296929121017456, "y": 0.6852747201919556}, {"x": 0.4055745303630829, "y": 0.6852747201919556}], "text": "50\n"}
{"page": 8, "bbox": [{"x": 0.3498293459415436, "y": 0.69010990858078}, {"x": 0.3799772560596466, "y": 0.689230740070343}, {"x": 0.3799772560596466, "y": 0.693626344203949}, {"x": 0.3498293459415436, "y": 0.694505512714386}], "text": "RAG-Tok\n"}
{"page": 8, "bbox": [{"x": 0.5620023012161255, "y": 0.69010990858078}, {"x": 0.5961319804191589, "y": 0.69010990858078}, {"x": 0.5961319804191589, "y": 0.7028571367263794}, {"x": 0.5620023012161255, "y": 0.7028571367263794}], "text": "Fixed DPR\nBM25\n"}
{"page": 8, "bbox": [{"x": 0.40614333748817444, "y": 0.6980219483375549}, {"x": 0.4124004542827606, "y": 0.6980219483375549}, {"x": 0.4124004542827606, "y": 0.7019780278205872}, {"x": 0.40614333748817444, "y": 0.7019780278205872}], "text": "40\n"}
{"page": 8, "bbox": [{"x": 0.3498293459415436, "y": 0.6989011168479919}, {"x": 0.38054606318473816, "y": 0.6989011168479919}, {"x": 0.38054606318473816, "y": 0.7041758298873901}, {"x": 0.3498293459415436, "y": 0.7041758298873901}], "text": "RAG-Seq\n"}
{"page": 8, "bbox": [{"x": 0.6217292547225952, "y": 0.7002198100090027}, {"x": 0.628555178642273, "y": 0.7002198100090027}, {"x": 0.628555178642273, "y": 0.7032967209815979}, {"x": 0.6217292547225952, "y": 0.7032967209815979}], "text": "48\n"}
{"page": 8, "bbox": [{"x": 0.1899886280298233, "y": 0.7010989189147949}, {"x": 0.19738338887691498, "y": 0.7010989189147949}, {"x": 0.19738338887691498, "y": 0.7041758298873901}, {"x": 0.1899886280298233, "y": 0.7041758298873901}], "text": "39\n"}
{"page": 8, "bbox": [{"x": 0.7411831617355347, "y": 0.7138461470603943}, {"x": 0.7485779523849487, "y": 0.7138461470603943}, {"x": 0.7485779523849487, "y": 0.7169230580329895}, {"x": 0.7411831617355347, "y": 0.7169230580329895}], "text": "30\n"}
{"page": 8, "bbox": [{"x": 0.3833902180194855, "y": 0.7134066224098206}, {"x": 0.39078497886657715, "y": 0.7134066224098206}, {"x": 0.39078497886657715, "y": 0.717362642288208}, {"x": 0.3833902180194855, "y": 0.717362642288208}], "text": "50\n"}
{"page": 8, "bbox": [{"x": 0.4880546033382416, "y": 0.7134066224098206}, {"x": 0.4948805570602417, "y": 0.7134066224098206}, {"x": 0.4948805570602417, "y": 0.717362642288208}, {"x": 0.4880546033382416, "y": 0.717362642288208}], "text": "20\n"}
{"page": 8, "bbox": [{"x": 0.7042093276977539, "y": 0.7134066224098206}, {"x": 0.711604118347168, "y": 0.7134066224098206}, {"x": 0.711604118347168, "y": 0.717362642288208}, {"x": 0.7042093276977539, "y": 0.717362642288208}], "text": "20\n"}
{"page": 8, "bbox": [{"x": 0.30887371301651, "y": 0.7138461470603943}, {"x": 0.3168373107910156, "y": 0.7138461470603943}, {"x": 0.3168373107910156, "y": 0.717362642288208}, {"x": 0.30887371301651, "y": 0.717362642288208}], "text": "30\n"}
{"page": 8, "bbox": [{"x": 0.34584754705429077, "y": 0.7138461470603943}, {"x": 0.35324230790138245, "y": 0.7138461470603943}, {"x": 0.35324230790138245, "y": 0.717362642288208}, {"x": 0.34584754705429077, "y": 0.717362642288208}], "text": "40\n"}
{"page": 8, "bbox": [{"x": 0.7781569957733154, "y": 0.7138461470603943}, {"x": 0.786120593547821, "y": 0.7138461470603943}, {"x": 0.786120593547821, "y": 0.717362642288208}, {"x": 0.7781569957733154, "y": 0.717362642288208}], "text": "40\n"}
{"page": 8, "bbox": [{"x": 0.8151308298110962, "y": 0.7138461470603943}, {"x": 0.8230944275856018, "y": 0.7138461470603943}, {"x": 0.8230944275856018, "y": 0.717362642288208}, {"x": 0.8151308298110962, "y": 0.717362642288208}], "text": "50\n"}
{"page": 8, "bbox": [{"x": 0.4510807693004608, "y": 0.7134066224098206}, {"x": 0.4584755301475525, "y": 0.7134066224098206}, {"x": 0.4584755301475525, "y": 0.7178022265434265}, {"x": 0.4510807693004608, "y": 0.7178022265434265}], "text": "10\n"}
{"page": 8, "bbox": [{"x": 0.27189987897872925, "y": 0.7138461470603943}, {"x": 0.27986347675323486, "y": 0.7138461470603943}, {"x": 0.27986347675323486, "y": 0.7178022265434265}, {"x": 0.27189987897872925, "y": 0.7178022265434265}], "text": "20\n"}
{"page": 8, "bbox": [{"x": 0.5250284671783447, "y": 0.7138461470603943}, {"x": 0.5329920649528503, "y": 0.7138461470603943}, {"x": 0.5329920649528503, "y": 0.7178022265434265}, {"x": 0.5250284671783447, "y": 0.7178022265434265}], "text": "30\n"}
{"page": 8, "bbox": [{"x": 0.23549488186836243, "y": 0.7142857313156128}, {"x": 0.2428896427154541, "y": 0.7142857313156128}, {"x": 0.2428896427154541, "y": 0.7178022265434265}, {"x": 0.23549488186836243, "y": 0.7178022265434265}], "text": "10\n"}
{"page": 8, "bbox": [{"x": 0.562571108341217, "y": 0.7142857313156128}, {"x": 0.5688282251358032, "y": 0.7142857313156128}, {"x": 0.5688282251358032, "y": 0.7178022265434265}, {"x": 0.562571108341217, "y": 0.7178022265434265}], "text": "40\n"}
{"page": 8, "bbox": [{"x": 0.5995449423789978, "y": 0.7142857313156128}, {"x": 0.6069397330284119, "y": 0.7142857313156128}, {"x": 0.6069397330284119, "y": 0.7178022265434265}, {"x": 0.5995449423789978, "y": 0.7178022265434265}], "text": "50\n"}
{"page": 8, "bbox": [{"x": 0.6672354936599731, "y": 0.7142857313156128}, {"x": 0.6746302843093872, "y": 0.7142857313156128}, {"x": 0.6746302843093872, "y": 0.7178022265434265}, {"x": 0.6672354936599731, "y": 0.7178022265434265}], "text": "10\n"}
{"page": 8, "bbox": [{"x": 0.693401575088501, "y": 0.7208791375160217}, {"x": 0.7610921263694763, "y": 0.7213186621665955}, {"x": 0.7610921263694763, "y": 0.7270329594612122}, {"x": 0.693401575088501, "y": 0.7265934348106384}], "text": "K Retrieved Docs\n"}
{"page": 8, "bbox": [{"x": 0.2610921561717987, "y": 0.7213186621665955}, {"x": 0.32878270745277405, "y": 0.721758246421814}, {"x": 0.32878270745277405, "y": 0.7270329594612122}, {"x": 0.2610921561717987, "y": 0.7265934348106384}], "text": "K Retrieved Docs\n"}
{"page": 8, "bbox": [{"x": 0.47724688053131104, "y": 0.7208791375160217}, {"x": 0.5449374318122864, "y": 0.7213186621665955}, {"x": 0.5449374318122864, "y": 0.7274725437164307}, {"x": 0.47724688053131104, "y": 0.7270329594612122}], "text": "K Retrieved Docs\n"}
{"page": 8, "bbox": [{"x": 0.17633675038814545, "y": 0.7301098704338074}, {"x": 0.8259385824203491, "y": 0.7301098704338074}, {"x": 0.8259385824203491, "y": 0.7551648616790771}, {"x": 0.17633675038814545, "y": 0.7551648616790771}], "text": "Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\n"}
{"page": 8, "bbox": [{"x": 0.1757679134607315, "y": 0.797802209854126}, {"x": 0.32252559065818787, "y": 0.797802209854126}, {"x": 0.32252559065818787, "y": 0.8079121112823486}, {"x": 0.1757679134607315, "y": 0.8079121112823486}], "text": "5 Related Work\n"}
{"page": 8, "bbox": [{"x": 0.1757679134607315, "y": 0.8312087655067444}, {"x": 0.8248009085655212, "y": 0.8316483497619629}, {"x": 0.8248009085655212, "y": 0.9125275015830994}, {"x": 0.1757679134607315, "y": 0.9120879173278809}], "text": "Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\nfact checking [56], fact completion [48], long-form question answering [12], Wikipedia article\ngeneration [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\nwork unifies previous successes in incorporating retrieval into individual tasks, showing that a single\nretrieval-based architecture is capable of achieving strong performance across several tasks.\n"}
{"page": 8, "bbox": [{"x": 0.49544936418533325, "y": 0.9389011263847351}, {"x": 0.5034129619598389, "y": 0.9389011263847351}, {"x": 0.5034129619598389, "y": 0.9459340572357178}, {"x": 0.49544936418533325, "y": 0.9459340572357178}], "text": "8\n"}
{"page": 9, "bbox": [{"x": 0.1757679134607315, "y": 0.09274725615978241}, {"x": 0.8259385824203491, "y": 0.09274725615978241}, {"x": 0.8259385824203491, "y": 0.21626374125480652}, {"x": 0.1757679134607315, "y": 0.21626374125480652}], "text": "General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\nhas been shown to achieve strong performance on various classification tasks in the GLUE bench-\nmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\nlanguage model could achieve strong performance across both discriminative and generative tasks.\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, unified\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\n"}
{"page": 9, "bbox": [{"x": 0.1757679134607315, "y": 0.23428571224212646}, {"x": 0.8248009085655212, "y": 0.23428571224212646}, {"x": 0.8248009085655212, "y": 0.3283516466617584}, {"x": 0.1757679134607315, "y": 0.3283516466617584}], "text": "Learned Retrieval There is significant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\nwork optimizes the retrieval module to aid in a specific, downstream task such as question answering,\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be fine-tuned for strong performance on a variety of tasks.\n"}
{"page": 9, "bbox": [{"x": 0.1757679134607315, "y": 0.34593406319618225}, {"x": 0.8253697156906128, "y": 0.34593406319618225}, {"x": 0.8253697156906128, "y": 0.4536263644695282}, {"x": 0.1757679134607315, "y": 0.4536263644695282}], "text": "Memory-based Architectures Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [9, 13] or, closer to our work, over retrieved text directly [15]. A key feature of our\nmemory is that it is comprised of raw text rather distributed representations, which makes the memory\nboth (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable,\nenabling us to dynamically update the model's memory by editing the document index.\n"}
{"page": 9, "bbox": [{"x": 0.1757679134607315, "y": 0.47164836525917053}, {"x": 0.8253697156906128, "y": 0.471208781003952}, {"x": 0.8253697156906128, "y": 0.5797802209854126}, {"x": 0.1757679134607315, "y": 0.5802198052406311}], "text": "Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a final output. These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\ncould represent promising future work.\n"}
{"page": 9, "bbox": [{"x": 0.17633675038814545, "y": 0.6026373505592346}, {"x": 0.2935153543949127, "y": 0.6026373505592346}, {"x": 0.2935153543949127, "y": 0.6127472519874573}, {"x": 0.17633675038814545, "y": 0.6127472519874573}], "text": "6 Discussion\n"}
{"page": 9, "bbox": [{"x": 0.17519909143447876, "y": 0.6325274705886841}, {"x": 0.8242321014404297, "y": 0.6325274705886841}, {"x": 0.8242321014404297, "y": 0.767472505569458}, {"x": 0.17519909143447876, "y": 0.767472505569458}], "text": "In this work, we presented hybrid generation models with access to parametric and non-parametric\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\nfound that people prefer RAG's generation over purely parametric BART, finding RAG more factual\nand specific. We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\nanother objective. Our work opens up new research directions on how parametric and non-parametric\nmemories interact and how to most effectively combine them, showing promise in being applied to a\nwide variety of NLP tasks.\n"}
{"page": 10, "bbox": [{"x": 0.17633675038814545, "y": 0.09186813235282898}, {"x": 0.31058019399642944, "y": 0.09362637251615524}, {"x": 0.3100113868713379, "y": 0.1063736230134964}, {"x": 0.1757679134607315, "y": 0.10461538285017014}], "text": "Broader Impact\n"}
{"page": 10, "bbox": [{"x": 0.17519909143447876, "y": 0.12219779938459396}, {"x": 0.8248009085655212, "y": 0.12263736128807068}, {"x": 0.8248009085655212, "y": 0.32087913155555725}, {"x": 0.17519909143447876, "y": 0.32043954730033875}], "text": "This work offers several positive societal benefits over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate\" less\nwith generations that are more factual, and offers more control and interpretability. RAG could be\nemployed in a wide variety of scenarios with direct benefit to society, for example by endowing it\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\neffective at their jobs.\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\nemployed as a language model, similar concerns as for GPT-2 [50]] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against\nmisleading content and automated spam/phishing.\n"}
{"page": 10, "bbox": [{"x": 0.17519909143447876, "y": 0.34285715222358704}, {"x": 0.32992035150527954, "y": 0.34329670667648315}, {"x": 0.32992035150527954, "y": 0.3556044101715088}, {"x": 0.17519909143447876, "y": 0.3551648259162903}], "text": "Acknowledgments\n"}
{"page": 10, "bbox": [{"x": 0.1757679134607315, "y": 0.37362638115882874}, {"x": 0.8236632347106934, "y": 0.37362638115882874}, {"x": 0.8236632347106934, "y": 0.41274726390838623}, {"x": 0.1757679134607315, "y": 0.41274726390838623}], "text": "The authors would like to thank the reviewers for their thoughtful and constructive feedback on this\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice.\n"}
{"page": 10, "bbox": [{"x": 0.1757679134607315, "y": 0.43516483902931213}, {"x": 0.3373151421546936, "y": 0.4334065914154053}, {"x": 0.3373151421546936, "y": 0.44659340381622314}, {"x": 0.1757679134607315, "y": 0.44835165143013}], "text": "Funding Disclosure\n"}
{"page": 10, "bbox": [{"x": 0.1757679134607315, "y": 0.4654945135116577}, {"x": 0.8230944275856018, "y": 0.46329671144485474}, {"x": 0.8230944275856018, "y": 0.48923078179359436}, {"x": 0.1757679134607315, "y": 0.49142858386039734}], "text": "EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\nprogram. This work was funded by Facebook.\n"}
{"page": 10, "bbox": [{"x": 0.17633675038814545, "y": 0.5134065747261047}, {"x": 0.2673492729663849, "y": 0.5134065747261047}, {"x": 0.2673492729663849, "y": 0.5235164761543274}, {"x": 0.17633675038814545, "y": 0.5235164761543274}], "text": "References\n"}
{"page": 10, "bbox": [{"x": 0.18430034816265106, "y": 0.535824179649353}, {"x": 0.8265073895454407, "y": 0.535824179649353}, {"x": 0.8265073895454407, "y": 0.7441758513450623}, {"x": 0.18430034816265106, "y": 0.7441758513450623}], "text": "[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\nReading Comprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\n//arxiv.org/abs/1611.09268 arXiv: 1611.09268.\n[2] Petr Baudiš and Jan Šedivý. Modeling of the question answering task in the yodaqa system. In\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\n2F978-3-319-24027-5_20.\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\nD13-1160.\n"}
{"page": 10, "bbox": [{"x": 0.18430034816265106, "y": 0.7582417726516724}, {"x": 0.8265073895454407, "y": 0.7582417726516724}, {"x": 0.8265073895454407, "y": 0.9116483330726624}, {"x": 0.18430034816265106, "y": 0.9116483330726624}], "text": "[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\n2020. URL https://arxiv.org/abs/2004.07159.\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1870-1879, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\nhttps://www.aclweb.org/anthology/P17-1171.\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\nJonathan Berant. Coarse-to-fine question answering for long documents. In Proceedings of the\n"}
{"page": 10, "bbox": [{"x": 0.4926052391529083, "y": 0.9389011263847351}, {"x": 0.5079635977745056, "y": 0.9389011263847351}, {"x": 0.5079635977745056, "y": 0.9463736414909363}, {"x": 0.4926052391529083, "y": 0.9463736414909363}], "text": "10\n"}
{"page": 11, "bbox": [{"x": 0.21103526651859283, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.13362637162208557}, {"x": 0.21103526651859283, "y": 0.13362637162208557}], "text": "55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\n"}
{"page": 11, "bbox": [{"x": 0.18430034816265106, "y": 0.14681318402290344}, {"x": 0.8265073895454407, "y": 0.14593406021595}, {"x": 0.8265073895454407, "y": 0.18285714089870453}, {"x": 0.18430034816265106, "y": 0.18373626470565796}], "text": "[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\narXiv: 1710.10723.\n"}
{"page": 11, "bbox": [{"x": 0.17633675038814545, "y": 0.1991208791732788}, {"x": 0.8265073895454407, "y": 0.1991208791732788}, {"x": 0.8265073895454407, "y": 0.3956044018268585}, {"x": 0.17633675038814545, "y": 0.3956044018268585}], "text": "[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis,\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nURL https://www.aclweb.org/anthology/N19–1423.\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1173iRqKm.\n[10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\n1704.05179.\n"}
{"page": 11, "bbox": [{"x": 0.17633675038814545, "y": 0.41098901629447937}, {"x": 0.8259385824203491, "y": 0.41098901629447937}, {"x": 0.8259385824203491, "y": 0.46373626589775085}, {"x": 0.17633675038814545, "y": 0.46373626589775085}], "text": "[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\n"}
{"page": 11, "bbox": [{"x": 0.21160408854484558, "y": 0.4672527611255646}, {"x": 0.27986347675323486, "y": 0.4672527611255646}, {"x": 0.27986347675323486, "y": 0.47472527623176575}, {"x": 0.21160408854484558, "y": 0.47472527623176575}], "text": "P18-1082\n"}
{"page": 11, "bbox": [{"x": 0.1769055724143982, "y": 0.4905494451522827}, {"x": 0.8253697156906128, "y": 0.48923078179359436}, {"x": 0.8259385824203491, "y": 0.6074725389480591}, {"x": 0.17747440934181213, "y": 0.6087912321090698}], "text": "[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3558-3567, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\nanthology/P19-1346.\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\nH1gx1CNKPH\n"}
{"page": 11, "bbox": [{"x": 0.17633675038814545, "y": 0.6224175691604614}, {"x": 0.8253697156906128, "y": 0.6224175691604614}, {"x": 0.8253697156906128, "y": 0.7793406844139099}, {"x": 0.17633675038814545, "y": 0.7793406844139099}], "text": "[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,\n2020. URL https://arxiv.org/abs/2004.07202.\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\nConference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\nAAAI/AAAI18/paper/view/16710.\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\nhttp://arxiv.org/abs/1705.08807.\n"}
{"page": 11, "bbox": [{"x": 0.17633675038814545, "y": 0.7929670214653015}, {"x": 0.8236632347106934, "y": 0.7929670214653015}, {"x": 0.8236632347106934, "y": 0.8320879340171814}, {"x": 0.17633675038814545, "y": 0.8320879340171814}], "text": "[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https:\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\n"}
{"page": 11, "bbox": [{"x": 0.1769055724143982, "y": 0.8452747464179993}, {"x": 0.8253697156906128, "y": 0.8443955779075623}, {"x": 0.8253697156906128, "y": 0.9103296995162964}, {"x": 0.1769055724143982, "y": 0.9112088084220886}], "text": "[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd\nAAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.\n32nd AAAI Conference on Artificial Intelligence, AAAI 2018; Conference date: 02-02-2018\nThrough 07-02-2018.\n"}
{"page": 11, "bbox": [{"x": 0.49317407608032227, "y": 0.9384615421295166}, {"x": 0.5068259239196777, "y": 0.9384615421295166}, {"x": 0.5068259239196777, "y": 0.9459340572357178}, {"x": 0.49317407608032227, "y": 0.9459340572357178}], "text": "11\n"}
{"page": 12, "bbox": [{"x": 0.17633675038814545, "y": 0.09450549632310867}, {"x": 0.8253697156906128, "y": 0.09450549632310867}, {"x": 0.8253697156906128, "y": 0.1705494523048401}, {"x": 0.17633675038814545, "y": 0.1705494523048401}], "text": "[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\n"}
{"page": 12, "bbox": [{"x": 0.21103526651859283, "y": 0.17274725437164307}, {"x": 0.43230944871902466, "y": 0.17274725437164307}, {"x": 0.43230944871902466, "y": 0.18373626470565796}, {"x": 0.21103526651859283, "y": 0.18373626470565796}], "text": "//arxiv.org/abs/2002.08909\n"}
{"page": 12, "bbox": [{"x": 0.1769055724143982, "y": 0.19516482949256897}, {"x": 0.8259385824203491, "y": 0.1960439532995224}, {"x": 0.8259385824203491, "y": 0.2773626446723938}, {"x": 0.1769055724143982, "y": 0.2764835059642792}], "text": "[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems 31, pages 10052-\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\n"}
{"page": 12, "bbox": [{"x": 0.21160408854484558, "y": 0.27912089228630066}, {"x": 0.23549488186836243, "y": 0.27868130803108215}, {"x": 0.23606370389461517, "y": 0.28967031836509705}, {"x": 0.21217292547225952, "y": 0.29010990262031555}], "text": "pdf\n"}
{"page": 12, "bbox": [{"x": 0.1757679134607315, "y": 0.30153846740722656}, {"x": 0.8259385824203491, "y": 0.30109891295433044}, {"x": 0.8259385824203491, "y": 0.611868143081665}, {"x": 0.1757679134607315, "y": 0.6123076677322388}], "text": "[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2532-2538, Online, July 2020. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\nanthology/2020. acl-main. 228.\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\naugmented recurrent nets. In Proceedings of the 28th International Conference on\nNeural Information Processing Systems Volume 1, NIPS'15, page 190–198, Cam-\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\n"}
{"page": 12, "bbox": [{"x": 0.502275288105011, "y": 0.5279120802879333}, {"x": 0.5062571167945862, "y": 0.5279120802879333}, {"x": 0.5062571167945862, "y": 0.5292307734489441}, {"x": 0.502275288105011, "y": 0.5292307734489441}], "text": "-\n"}
{"page": 12, "bbox": [{"x": 0.17633675038814545, "y": 0.6241758465766907}, {"x": 0.8259385824203491, "y": 0.6237362623214722}, {"x": 0.8259385824203491, "y": 0.9116483330726624}, {"x": 0.17633675038814545, "y": 0.9120879173278809}], "text": "[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\ntion through memorization: Nearest neighbor language models. In International Conference on\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\nhttp://arxiv.org/abs/1412.6980.\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\ntion Answering Research. Transactions of the Association of Computational Lin-\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\nnatural-questions/main-1455-kwiatkowski.pdf.\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc' Aurelio Ranzato, Ludovic Denoyer, and\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d' Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\n"}
{"page": 12, "bbox": [{"x": 0.4926052391529083, "y": 0.9389011263847351}, {"x": 0.5068259239196777, "y": 0.9389011263847351}, {"x": 0.5068259239196777, "y": 0.9463736414909363}, {"x": 0.4926052391529083, "y": 0.9463736414909363}], "text": "12\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.09406593441963196}, {"x": 0.8253697156906128, "y": 0.09362637251615524}, {"x": 0.8253697156906128, "y": 0.16087912023067474}, {"x": 0.17633675038814545, "y": 0.16131867468357086}], "text": "[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 6086-6096, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\nanthology/P19-1612.\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.1762637346982956}, {"x": 0.8253697156906128, "y": 0.1767033040523529}, {"x": 0.8253697156906128, "y": 0.2298901081085205}, {"x": 0.17633675038814545, "y": 0.2294505536556244}], "text": "[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461\n"}
{"page": 13, "bbox": [{"x": 0.1769055724143982, "y": 0.2439560443162918}, {"x": 0.8253697156906128, "y": 0.24483516812324524}, {"x": 0.8253697156906128, "y": 0.32395604252815247}, {"x": 0.1769055724143982, "y": 0.32307693362236023}], "text": "[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 110–119, San Diego, California, June 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\nN16-1014\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.34021976590156555}, {"x": 0.8236632347106934, "y": 0.3393406569957733}, {"x": 0.8236632347106934, "y": 0.37934064865112305}, {"x": 0.17633675038814545, "y": 0.38021978735923767}], "text": "[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\nhttps://arxiv.org/abs/1909.03087.\n"}
{"page": 13, "bbox": [{"x": 0.1769055724143982, "y": 0.3951648473739624}, {"x": 0.8253697156906128, "y": 0.3951648473739624}, {"x": 0.8253697156906128, "y": 0.4615384638309479}, {"x": 0.1769055724143982, "y": 0.4615384638309479}], "text": "[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 3044-3049, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\nhttps://www.aclweb.org/anthology/P19-1291.\n"}
{"page": 13, "bbox": [{"x": 0.1769055724143982, "y": 0.47736263275146484}, {"x": 0.8253697156906128, "y": 0.47736263275146484}, {"x": 0.8253697156906128, "y": 0.5296703577041626}, {"x": 0.1769055724143982, "y": 0.5296703577041626}], "text": "[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\nid=Hyg0vbWC-.\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.5454944968223572}, {"x": 0.8242321014404297, "y": 0.5454944968223572}, {"x": 0.8242321014404297, "y": 0.5841758251190186}, {"x": 0.17633675038814545, "y": 0.5841758251190186}], "text": "[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 42:824-836, 2016. URL https://arxiv.org/abs/1603.09320.\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.6000000238418579}, {"x": 0.8236632347106934, "y": 0.6000000238418579}, {"x": 0.8236632347106934, "y": 0.6250549554824829}, {"x": 0.17633675038814545, "y": 0.6250549554824829}], "text": "[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.6404395699501038}, {"x": 0.8259385824203491, "y": 0.6404395699501038}, {"x": 0.8259385824203491, "y": 0.6927472352981567}, {"x": 0.17633675038814545, "y": 0.6927472352981567}], "text": "[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\nverifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https:\n//arxiv.org/abs/1911.03587\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.7081318497657776}, {"x": 0.8270761966705322, "y": 0.7081318497657776}, {"x": 0.8270761966705322, "y": 0.8298901319503784}, {"x": 0.17633675038814545, "y": 0.8298901319503784}], "text": "[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\ning background knowledge for building conversation systems. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18–1255.\n"}
{"page": 13, "bbox": [{"x": 0.17633675038814545, "y": 0.8452747464179993}, {"x": 0.8253697156906128, "y": 0.8443955779075623}, {"x": 0.8253697156906128, "y": 0.9107692241668701}, {"x": 0.17633675038814545, "y": 0.9116483330726624}], "text": "[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pages 3950-3959, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\nanthology/D18-1429.\n"}
{"page": 13, "bbox": [{"x": 0.4926052391529083, "y": 0.9384615421295166}, {"x": 0.5068259239196777, "y": 0.9384615421295166}, {"x": 0.5068259239196777, "y": 0.9459340572357178}, {"x": 0.4926052391529083, "y": 0.9459340572357178}], "text": "13\n"}
{"page": 14, "bbox": [{"x": 0.17633675038814545, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.5771428346633911}, {"x": 0.17633675038814545, "y": 0.5771428346633911}], "text": "[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d'Avila Garcez, and Greg Wayne, editors,\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\n2016_paper9.pdf\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb\norg/anthology/N19-4009.\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n2402-2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\nMiller, and Sebastian Riedel. How context affects language models' factual predictions. In\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\nid=025X0zPfn\n"}
{"page": 14, "bbox": [{"x": 0.7912400364875793, "y": 0.6061538457870483}, {"x": 0.8236632347106934, "y": 0.6061538457870483}, {"x": 0.8236632347106934, "y": 0.6136263608932495}, {"x": 0.7912400364875793, "y": 0.6136263608932495}], "text": "URL\n"}
{"page": 14, "bbox": [{"x": 0.17633675038814545, "y": 0.5916483402252197}, {"x": 0.8253697156906128, "y": 0.5916483402252197}, {"x": 0.8253697156906128, "y": 0.6443955898284912}, {"x": 0.17633675038814545, "y": 0.6443955898284912}], "text": "[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\nproving Language Understanding by Generative Pre-Training, 2018.\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\nlanguage-unsupervised/language_understanding_paper.pdf.\n"}
{"page": 14, "bbox": [{"x": 0.7912400364875793, "y": 0.670769214630127}, {"x": 0.8242321014404297, "y": 0.670769214630127}, {"x": 0.8242321014404297, "y": 0.6782417297363281}, {"x": 0.7912400364875793, "y": 0.6782417297363281}], "text": "URL\n"}
{"page": 14, "bbox": [{"x": 0.17633675038814545, "y": 0.6562637090682983}, {"x": 0.8253697156906128, "y": 0.6562637090682983}, {"x": 0.8253697156906128, "y": 0.7964835166931152}, {"x": 0.17633675038814545, "y": 0.7964835166931152}], "text": "[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners, 2019.\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\n"}
{"page": 14, "bbox": [{"x": 0.21160408854484558, "y": 0.800000011920929}, {"x": 0.2997724711894989, "y": 0.800000011920929}, {"x": 0.2997724711894989, "y": 0.8074725270271301}, {"x": 0.21160408854484558, "y": 0.8074725270271301}], "text": "2002.08910.\n"}
{"page": 14, "bbox": [{"x": 0.1757679134607315, "y": 0.8219780325889587}, {"x": 0.8259385824203491, "y": 0.8219780325889587}, {"x": 0.8259385824203491, "y": 0.9098901152610779}, {"x": 0.1757679134607315, "y": 0.9098901152610779}], "text": "[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL https://doi.org/10.1561/1500000019.\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\nArXiv, abs/1908.09203, 2019.\n"}
{"page": 14, "bbox": [{"x": 0.4926052391529083, "y": 0.9384615421295166}, {"x": 0.5073947906494141, "y": 0.9384615421295166}, {"x": 0.5073947906494141, "y": 0.9459340572357178}, {"x": 0.4926052391529083, "y": 0.9459340572357178}], "text": "14\n"}
{"page": 15, "bbox": [{"x": 0.1757679134607315, "y": 0.09450549632310867}, {"x": 0.8265073895454407, "y": 0.09450549632310867}, {"x": 0.8265073895454407, "y": 0.9112088084220886}, {"x": 0.1757679134607315, "y": 0.9112088084220886}], "text": "[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\nlarge-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\nhttps://www.aclweb.org/anthology/N18-1074.\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\nbiases in sentence-pair classification with elastic weight consolidation. ArXiv, abs/2004.14366,\n2020. URL https://arxiv.org/abs/2004.14366.\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\nAAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\nphp/AAAI/AAAI18/paper/view/17329.\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\nanthology/W18-5446.\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems 32, pages 3261-3275. Curran Associates, Inc., 2019. URL https://\narxiv.org/abs/1905.00537.\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R³: Reinforced ranker-reader for open-domain\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\nthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index\nphp/AAAI/AAAI18/paper/view/16712.\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\nranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\nnet/forum?id=rJ13yM-Ab.\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\nhttp://arxiv.org/abs/1410.3916.\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\n"}
{"page": 15, "bbox": [{"x": 0.4926052391529083, "y": 0.9384615421295166}, {"x": 0.5068259239196777, "y": 0.9384615421295166}, {"x": 0.5068259239196777, "y": 0.9463736414909363}, {"x": 0.4926052391529083, "y": 0.9463736414909363}], "text": "15\n"}
{"page": 16, "bbox": [{"x": 0.17633675038814545, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.09450549632310867}, {"x": 0.8259385824203491, "y": 0.3560439646244049}, {"x": 0.17633675038814545, "y": 0.3560439646244049}], "text": "International Workshop on Search-Oriented Conversational AI, pages 87-92, Brussels, Belgium,\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\nhttps://www.aclweb.org/anthology/W18-5713.\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface's transformers:\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pages 2495-2509, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\nhttps://www.aclweb.org/anthology/D19-1253.\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\nURL https://arxiv.org/abs/1909.03745\n"}
{"page": 16, "bbox": [{"x": 0.4926052391529083, "y": 0.9384615421295166}, {"x": 0.5073947906494141, "y": 0.9384615421295166}, {"x": 0.5073947906494141, "y": 0.9463736414909363}, {"x": 0.4926052391529083, "y": 0.9463736414909363}], "text": "16\n"}
{"page": 16, "bbox": [{"x": 0.4948805570602417, "y": 0.9358241558074951}, {"x": 0.5068259239196777, "y": 0.9358241558074951}, {"x": 0.5068259239196777, "y": 0.9516483545303345}, {"x": 0.4948805570602417, "y": 0.9516483545303345}], "text": "16\n"}
