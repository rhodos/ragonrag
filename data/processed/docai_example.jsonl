{"page": 1, "bbox": [{"x": 0.17549078166484833, "y": 0.08494532853364944}, {"x": 0.8268887400627136, "y": 0.08494532853364944}, {"x": 0.8268887400627136, "y": 0.10050462931394577}, {"x": 0.17549078166484833, "y": 0.10050462931394577}], "text": "Dense Passage Retrieval for Open-Domain Question Answering\n"}
{"page": 1, "bbox": [{"x": 0.16240333020687103, "y": 0.12531539797782898}, {"x": 0.846519947052002, "y": 0.12615643441677094}, {"x": 0.846519947052002, "y": 0.22371740639209747}, {"x": 0.16240333020687103, "y": 0.22287636995315552}], "text": "Vladimir Karpukhin, Barlas Oğuz, Sewon Min†, Patrick Lewis,\nLedell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih\nFacebook AI *University of Washington #Princeton University\n{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n"}
{"page": 1, "bbox": [{"x": 0.2665080428123474, "y": 0.26829269528388977}, {"x": 0.34205830097198486, "y": 0.26829269528388977}, {"x": 0.34205830097198486, "y": 0.27754414081573486}, {"x": 0.2665080428123474, "y": 0.27754414081573486}], "text": "Abstract\n"}
{"page": 1, "bbox": [{"x": 0.14872099459171295, "y": 0.29226240515708923}, {"x": 0.4622248709201813, "y": 0.29226240515708923}, {"x": 0.4622248709201813, "y": 0.5285954475402832}, {"x": 0.14872099459171295, "y": 0.5285954475402832}], "text": "Open-domain question answering relies on ef-\nficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented us-\ning dense representations alone, where em-\nbeddings are learned from a small number\nof questions and passages by a simple dual-\nencoder framework. When evaluated on a\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong Lucene-\nBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks.¹\n"}
{"page": 1, "bbox": [{"x": 0.5098155736923218, "y": 0.2674516439437866}, {"x": 0.8863771557807922, "y": 0.2674516439437866}, {"x": 0.8863771557807922, "y": 0.6820858120918274}, {"x": 0.5098155736923218, "y": 0.6820858120918274}], "text": "Retrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords effi-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense, latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n\"Who is the bad guy in lord of the rings?\", which can\nbe answered from the context \"Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy.\" A term-based system would\nhave difficulty retrieving such a context, while\na dense retrieval system would be able to better\nmatch \"bad guy” with “villain\" and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional flexibility to have a task-specific\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefficiently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)).\n"}
{"page": 1, "bbox": [{"x": 0.12135633826255798, "y": 0.5445752739906311}, {"x": 0.25996431708335876, "y": 0.5445752739906311}, {"x": 0.25996431708335876, "y": 0.554247260093689}, {"x": 0.12135633826255798, "y": 0.554247260093689}], "text": "1 Introduction\n"}
{"page": 1, "bbox": [{"x": 0.12016656994819641, "y": 0.5698065757751465}, {"x": 0.490779310464859, "y": 0.5698065757751465}, {"x": 0.490779310464859, "y": 0.8389402627944946}, {"x": 0.12016656994819641, "y": 0.8389402627944946}], "text": "Open-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions us-\ning a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), inter alia), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context retriever first\nselects a small subset of passages where some\nof them contain the answer to the question, and\nthen (2) a machine reader can thoroughly exam-\nine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\nopen-domain QA to machine reading is a very rea-\nsonable strategy, a huge performance degradation\nis often observed in practice², indicating the needs\nof improving retrieval.\n"}
{"page": 1, "bbox": [{"x": 0.5145746469497681, "y": 0.6879730820655823}, {"x": 0.8863771557807922, "y": 0.6879730820655823}, {"x": 0.8863771557807922, "y": 0.910008430480957}, {"x": 0.5145746469497681, "y": 0.910008430480957}], "text": "However, it is generally believed that learn-\ning a good dense vector representation needs a\nlarge number of labeled pairs of question and con-\ntexts. Dense retrieval methods have thus never\nbe shown to outperform TF-IDF/BM25 for open-\ndomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\nobjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then fine-\ntuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\n"}
{"page": 1, "bbox": [{"x": 0.14812611043453217, "y": 0.8490328192710876}, {"x": 0.2671029269695282, "y": 0.8477712273597717}, {"x": 0.2671029269695282, "y": 0.8566021919250488}, {"x": 0.14812611043453217, "y": 0.85786372423172}], "text": "*Equal contribution\n"}
{"page": 1, "bbox": [{"x": 0.12016656994819641, "y": 0.861648440361023}, {"x": 0.4878048896789551, "y": 0.8612279295921326}, {"x": 0.4878048896789551, "y": 0.9083263278007507}, {"x": 0.12016656994819641, "y": 0.9087468385696411}], "text": "The code and trained models have been released at\nhttps://github.com/facebookresearch/DPR.\n²For instance, the exact match score on SQUAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).\n"}
{"page": 1, "bbox": [{"x": 0.4830458164215088, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.9331371188163757}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6769\n"}
{"page": 1, "bbox": [{"x": 0.16299821436405182, "y": 0.9449117183685303}, {"x": 0.8370018005371094, "y": 0.9449117183685303}, {"x": 0.8370018005371094, "y": 0.9676198363304138}, {"x": 0.16299821436405182, "y": 0.9676198363304138}], "text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6769-6781,\nNovember 16-20, 2020. 2020 Association for Computational Linguistics\n"}
{"page": 2, "bbox": [{"x": 0.5151695609092712, "y": 0.07695542275905609}, {"x": 0.8863771557807922, "y": 0.0786375105381012}, {"x": 0.8851873874664307, "y": 0.22413793206214905}, {"x": 0.5139797925949097, "y": 0.22245584428310394}], "text": "the extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more pas-\nsages in the corpus. Assume that our collection\ncontains D documents, d1, d2,...,dD. We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units³ and get M\ntotal passages in our corpus C {P1, P2, ..., PM},\nwhere each passage p; can be viewed as a sequence\n(2). Given a question q,\n"}
{"page": 2, "bbox": [{"x": 0.7477691769599915, "y": 0.18250630795955658}, {"x": 0.7364664077758789, "y": 0.18250630795955658}, {"x": 0.7364664077758789, "y": 0.178721621632576}, {"x": 0.7477691769599915, "y": 0.178721621632576}], "text": "=\n"}
{"page": 2, "bbox": [{"x": 0.5157644152641296, "y": 0.2052144706249237}, {"x": 0.6603212356567383, "y": 0.20437341928482056}, {"x": 0.6603212356567383, "y": 0.2195121943950653}, {"x": 0.5157644152641296, "y": 0.22035323083400726}], "text": "of tokens w(i), w(i),\n"}
{"page": 2, "bbox": [{"x": 0.7781082391738892, "y": 0.23633305728435516}, {"x": 0.7977394461631775, "y": 0.2359125316143036}, {"x": 0.7977394461631775, "y": 0.23801514506340027}, {"x": 0.7781082391738892, "y": 0.23843565583229065}], "text": "...\n"}
{"page": 2, "bbox": [{"x": 0.8132064342498779, "y": 0.23465096950531006}, {"x": 0.8328375816345215, "y": 0.23507149517536163}, {"x": 0.8328375816345215, "y": 0.24179983139038086}, {"x": 0.8132064342498779, "y": 0.24137930572032928}], "text": "We\n"}
{"page": 2, "bbox": [{"x": 0.8054729104042053, "y": 0.2392767071723938}, {"x": 0.8084473609924316, "y": 0.2392767071723938}, {"x": 0.8084473609924316, "y": 0.24264086782932281}, {"x": 0.8054729104042053, "y": 0.24264086782932281}], "text": "\"\n"}
{"page": 2, "bbox": [{"x": 0.7233789563179016, "y": 0.23969721794128418}, {"x": 0.7263533473014832, "y": 0.23969721794128418}, {"x": 0.7263533473014832, "y": 0.24264086782932281}, {"x": 0.7233789563179016, "y": 0.24264086782932281}], "text": ",\n"}
{"page": 2, "bbox": [{"x": 0.5145746469497681, "y": 0.21488645672798157}, {"x": 0.8875669240951538, "y": 0.2161480188369751}, {"x": 0.8857823014259338, "y": 0.5016821026802063}, {"x": 0.5127900242805481, "y": 0.5004205107688904}], "text": "|pi|\nthe task is to find a span ws\n(\ni) (i)\nWS+1\"\"\nfrom\none of the passages p¿ that can answer the question.\nNotice that to cover a wide variety of domains, the\ncorpus size can easily range from millions of docu-\nments (e.g., Wikipedia) to billions (e.g., the Web).\nAs a result, any open-domain QA system needs to\ninclude an efficient retriever component that can se-\nlect a small set of relevant texts, before applying the\nreader to extract the answer (Chen et al., 2017).4\nFormally speaking, a retriever R : (q,C) → CF\nis a function that takes as input a question q and a\ncorpus C and returns a much smaller filter set of\ntexts CFCC, where |Cƒ| = k < |C|. For a fixed\nk, a retriever can be evaluated in isolation on top-k\nretrieval accuracy, which is the fraction of ques-\ntions for which CF contains a span that answers the\nquestion.\n"}
{"page": 2, "bbox": [{"x": 0.11957168579101562, "y": 0.07737594842910767}, {"x": 0.4913741946220398, "y": 0.07737594842910767}, {"x": 0.4913741946220398, "y": 0.7695542573928833}, {"x": 0.11957168579101562, "y": 0.7695542573928833}], "text": "QA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot fine-tuned using pairs of questions and answers,\nthe corresponding representations could be subop-\ntimal.\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), with-\nout additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\nof question and passage pairs. Through a series\nof careful ablation studies, our final solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective compar-\ning all pairs of questions and passages in a batch.\nOur Dense Passage Retriever (DPR) is exception-\nally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\nOur contributions are twofold. First, we demon-\nstrate that with the proper training setup, sim-\nply fine-tuning the question and passage encoders\non existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\nopen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve compara-\nble or better results on multiple QA datasets in the\nopen-retrieval setting, compared to several, much\ncomplicated systems.\n"}
{"page": 2, "bbox": [{"x": 0.5145746469497681, "y": 0.517241358757019}, {"x": 0.8863771557807922, "y": 0.5168208479881287}, {"x": 0.8869720697402954, "y": 0.713624894618988}, {"x": 0.5151695609092712, "y": 0.7140454053878784}], "text": "3 Dense Passage Retriever (DPR)\nWe focus our research in this work on improv-\ning the retrieval component in open-domain QA.\nGiven a collection of M text passages, the goal of\nour dense passage retriever (DPR) is to index all\nthe passages in a low-dimensional and continuous\nspace, such that it can retrieve efficiently the top\nk passages relevant to the input question for the\nreader at run-time. Note that M can be very large\n(e.g., 21 million passages in our experiments, de-\nscribed in Section 4.1) and k is usually small, such\nas 20-100.\n"}
{"page": 2, "bbox": [{"x": 0.5157644152641296, "y": 0.732127845287323}, {"x": 0.5371802449226379, "y": 0.732127845287323}, {"x": 0.5371802449226379, "y": 0.7405382394790649}, {"x": 0.5157644152641296, "y": 0.7405382394790649}], "text": "3.1\n"}
{"page": 2, "bbox": [{"x": 0.5574063062667847, "y": 0.732127845287323}, {"x": 0.630577027797699, "y": 0.7317073345184326}, {"x": 0.630577027797699, "y": 0.7409588098526001}, {"x": 0.5574063062667847, "y": 0.7413793206214905}], "text": "Overview\n"}
{"page": 2, "bbox": [{"x": 0.11897680163383484, "y": 0.7884777188301086}, {"x": 0.25580012798309326, "y": 0.7884777188301086}, {"x": 0.25580012798309326, "y": 0.7998318076133728}, {"x": 0.11897680163383484, "y": 0.7998318076133728}], "text": "2 Background\n"}
{"page": 2, "bbox": [{"x": 0.5157644152641296, "y": 0.7510513067245483}, {"x": 0.8863771557807922, "y": 0.7514718174934387}, {"x": 0.8863771557807922, "y": 0.9087468385696411}, {"x": 0.5157644152641296, "y": 0.9083263278007507}], "text": "Our dense passage retriever (DPR) uses a dense\nencoder Ep() which maps any text passage to a d-\ndimensional real-valued vectors and builds an index\nfor all the M passages that we will use for retrieval.\n3 The ideal size and boundary of a text passage are func-\ntions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing fixed-length passages performs better in both retrieval\nand final QA accuracy, as observed by Wang et al. (2019).\n*Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which retrieves and generates the answers, respectively.\n"}
{"page": 2, "bbox": [{"x": 0.11957168579101562, "y": 0.8158116340637207}, {"x": 0.48958954215049744, "y": 0.8153910636901855}, {"x": 0.48958954215049744, "y": 0.910008430480957}, {"x": 0.11957168579101562, "y": 0.9104289412498474}], "text": "The problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \"Who first voiced Meg on Family\nGuy?\" or \"Where was the 8th Dalai Lama born?\", a\nsystem is required to answer it using a large corpus\nof diversified topics. More specifically, we assume\n"}
{"page": 2, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5205234885215759, "y": 0.9243061542510986}, {"x": 0.5205234885215759, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6770\n"}
{"page": 3, "bbox": [{"x": 0.5157644152641296, "y": 0.07821699231863022}, {"x": 0.8839976191520691, "y": 0.07527334243059158}, {"x": 0.8845925331115723, "y": 0.10386879742145538}, {"x": 0.5163593292236328, "y": 0.10681244730949402}], "text": "larity) than the irrelevant ones, by learning a better\nembedding function.\n"}
{"page": 3, "bbox": [{"x": 0.6680546998977661, "y": 0.1089150533080101}, {"x": 0.6787626147270203, "y": 0.1089150533080101}, {"x": 0.6787626147270203, "y": 0.11564339697360992}, {"x": 0.6680546998977661, "y": 0.11564339697360992}], "text": "+\n"}
{"page": 3, "bbox": [{"x": 0.8001189827919006, "y": 0.11101765930652618}, {"x": 0.8245092034339905, "y": 0.11101765930652618}, {"x": 0.8245092034339905, "y": 0.1232127845287323}, {"x": 0.8001189827919006, "y": 0.1232127845287323}], "text": "m\ni=1\n"}
{"page": 3, "bbox": [{"x": 0.11897680163383484, "y": 0.07779646664857864}, {"x": 0.48839977383613586, "y": 0.07779646664857864}, {"x": 0.48839977383613586, "y": 0.17031118273735046}, {"x": 0.11897680163383484, "y": 0.17031118273735046}], "text": "At run-time, DPR applies a different encoder EQ (•)\nthat maps the input question to a d-dimensional\nvector, and retrieves k passages of which vectors\nare the closest to the question vector. We define\nthe similarity between the question and the passage\nusing the dot product of their vectors:\n"}
{"page": 3, "bbox": [{"x": 0.5157644152641296, "y": 0.10849453508853912}, {"x": 0.8863771557807922, "y": 0.10807400941848755}, {"x": 0.8863771557807922, "y": 0.20395290851593018}, {"x": 0.5157644152641296, "y": 0.20437341928482056}], "text": "Let D = {{qi, Pi¯‚ Pi¸₁‚· · · ‚ Pin)} be the\ntraining data that consists of m instances. Each\ninstance contains one question q; and one relevant\n(positive) passage pit, along with n irrelevant (neg-\native) passages P₁j. We optimize the loss function\nas the negative log likelihood of the positive pas-\n"}
{"page": 3, "bbox": [{"x": 0.20166566967964172, "y": 0.18671151995658875}, {"x": 0.4057108759880066, "y": 0.1875525712966919}, {"x": 0.4057108759880066, "y": 0.20100925862789154}, {"x": 0.20166566967964172, "y": 0.2001682072877884}], "text": "sim(q, p) Eq(q)'Ep(p).\n"}
{"page": 3, "bbox": [{"x": 0.4663890600204468, "y": 0.18881413340568542}, {"x": 0.4866151213645935, "y": 0.18881413340568542}, {"x": 0.4866151213645935, "y": 0.19974768161773682}, {"x": 0.4663890600204468, "y": 0.19974768161773682}], "text": "(1)\n"}
{"page": 3, "bbox": [{"x": 0.2891136109828949, "y": 0.19259881973266602}, {"x": 0.2891136109828949, "y": 0.1963835209608078}, {"x": 0.27781081199645996, "y": 0.1963835209608078}, {"x": 0.27781081199645996, "y": 0.19259881973266602}], "text": "=\n"}
{"page": 3, "bbox": [{"x": 0.5157644152641296, "y": 0.2106812447309494}, {"x": 0.5538370013237, "y": 0.2106812447309494}, {"x": 0.5538370013237, "y": 0.21825063228607178}, {"x": 0.5157644152641296, "y": 0.21825063228607178}], "text": "sage:\n"}
{"page": 3, "bbox": [{"x": 0.8607971668243408, "y": 0.23549200594425201}, {"x": 0.8828078508377075, "y": 0.23549200594425201}, {"x": 0.8828078508377075, "y": 0.24600504338741302}, {"x": 0.8607971668243408, "y": 0.24600504338741302}], "text": "(2)\n"}
{"page": 3, "bbox": [{"x": 0.5847709774971008, "y": 0.23296888172626495}, {"x": 0.7608566284179688, "y": 0.23549200594425201}, {"x": 0.7602617740631104, "y": 0.2497897446155548}, {"x": 0.5841760635375977, "y": 0.24726660549640656}], "text": "L(qi, P†‚P₁¸¹¨··‚ Pi¸n)\n"}
{"page": 3, "bbox": [{"x": 0.7043426632881165, "y": 0.25651809573173523}, {"x": 0.7787031531333923, "y": 0.2552565038204193}, {"x": 0.7792980074882507, "y": 0.26703113317489624}, {"x": 0.7049375176429749, "y": 0.26829269528388977}], "text": "esim (qi,P)\n"}
{"page": 3, "bbox": [{"x": 0.5853658318519592, "y": 0.26703113317489624}, {"x": 0.6270077228546143, "y": 0.26871320605278015}, {"x": 0.6258179545402527, "y": 0.27964675426483154}, {"x": 0.5841760635375977, "y": 0.27796468138694763}], "text": "― log\n"}
{"page": 3, "bbox": [{"x": 0.6299821734428406, "y": 0.278385192155838}, {"x": 0.8530636429786682, "y": 0.2767031192779541}, {"x": 0.8530636429786682, "y": 0.2931034564971924}, {"x": 0.6299821734428406, "y": 0.2947855293750763}], "text": "esim(qi,p) +1 esim(qi‚Pi,j)\n"}
{"page": 3, "bbox": [{"x": 0.11957168579101562, "y": 0.21867115795612335}, {"x": 0.4913741946220398, "y": 0.21867115795612335}, {"x": 0.490779310464859, "y": 0.7817493677139282}, {"x": 0.11897680163383484, "y": 0.7817493677139282}], "text": "Although more expressive model forms for measur-\ning the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the represen-\ntations of the collection of passages can be pre-\ncomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis dis-\ntance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nfinds other similarity functions perform compara-\nbly (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders.\nEncoders Although in principle the question and\npassage encoders can be implemented by any neu-\nral networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, un-\ncased) and take the representation at the [CLS]\ntoken as the output, so d: == 768.\nInference During inference time, we apply the\npassage encoder Ep to all the passages and index\nthem using FAISS (Johnson et al., 2017) offline.\nFAISS is an extremely efficient, open-source li-\nbrary for similarity search and clustering of dense\nvectors, which can easily be applied to billions of\nvectors. Given a question q at run-time, we derive\nits embedding vq EQ(q) and retrieve the top k\npassages with embeddings closest to vq.\n"}
{"page": 3, "bbox": [{"x": 0.5145746469497681, "y": 0.3090832531452179}, {"x": 0.8857823014259338, "y": 0.3086627423763275}, {"x": 0.8869720697402954, "y": 0.7039529085159302}, {"x": 0.5157644152641296, "y": 0.7043734192848206}], "text": "Positive and negative passages For retrieval\nproblems, it is often the case that positive examples\nare available explicitly, while negative examples\nneed to be selected from an extremely large pool.\nFor instance, passages relevant to a question may\nbe given in a QA dataset, or can be found using the\nanswer. All other passages in the collection, while\nnot specified explicitly, can be viewed as irrelevant\nby default. In practice, how to select negative ex-\namples is often overlooked but could be decisive\nfor learning a high-quality encoder. We consider\nthree different types of negatives: (1) Random: any\nrandom passage from the corpus; (2) BM25: top\npassages returned by BM25 which don't contain\nthe answer but match most question tokens; (3)\nGold: positive passages paired with other questions\nwhich appear in the training set. We will discuss the\nimpact of different types of negative passages and\ntraining schemes in Section 5.2. Our best model\nuses gold passages from the same mini-batch and\none BM25 negative passage. In particular, re-using\ngold passages from the same batch as negatives\ncan make the computation efficient while achiev-\ning great performance. We discuss this approach\nbelow.\n"}
{"page": 3, "bbox": [{"x": 0.2540154755115509, "y": 0.7560975551605225}, {"x": 0.26412850618362427, "y": 0.7560975551605225}, {"x": 0.26412850618362427, "y": 0.7598822712898254}, {"x": 0.2540154755115509, "y": 0.7598822712898254}], "text": "=\n"}
{"page": 3, "bbox": [{"x": 0.1207614541053772, "y": 0.7943649888038635}, {"x": 0.22962522506713867, "y": 0.7952060699462891}, {"x": 0.22962522506713867, "y": 0.8057190775871277}, {"x": 0.1207614541053772, "y": 0.8048780560493469}], "text": "3.2 Training\n"}
{"page": 3, "bbox": [{"x": 0.5145746469497681, "y": 0.7199327349662781}, {"x": 0.8863771557807922, "y": 0.7203532457351685}, {"x": 0.8863771557807922, "y": 0.9087468385696411}, {"x": 0.5145746469497681, "y": 0.9083263278007507}], "text": "In-batch negatives Assume that we have B\nquestions in a mini-batch and each one is asso-\nciated with a relevant passage. Let Q and P be the\n(Bxd) matrix of question and passage embeddings\nin a batch of size B. S = QPT is a (B × B) ma-\ntrix of similarity scores, where each row of which\ncorresponds to a question, paired with B passages.\nIn this way, we reuse computation and effectively\ntrain on B² (qi, pj) question/passage pairs in each\nbatch. Any (qi, pj) pair is a positive example when\ni = j, and negative otherwise. This creates B train-\ning instances in each batch, where there are B 1\n"}
{"page": 3, "bbox": [{"x": 0.12016656994819641, "y": 0.8170731663703918}, {"x": 0.490779310464859, "y": 0.8170731663703918}, {"x": 0.490779310464859, "y": 0.9083263278007507}, {"x": 0.12016656994819641, "y": 0.9083263278007507}], "text": "Training the encoders so that the dot-product sim-\nilarity (Eq. (1)) becomes a good ranking function\nfor retrieval is essentially a metric learning prob-\nlem (Kulis, 2013). The goal is to create a vector\nspace such that relevant pairs of questions and pas-\nsages will have smaller distance (i.e., higher simi-\n"}
{"page": 3, "bbox": [{"x": 0.4836407005786896, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.9331371188163757}, {"x": 0.4836407005786896, "y": 0.9331371188163757}], "text": "6771\n"}
{"page": 4, "bbox": [{"x": 0.5258774757385254, "y": 0.0803195983171463}, {"x": 0.5752528309822083, "y": 0.0803195983171463}, {"x": 0.5752528309822083, "y": 0.0874684602022171}, {"x": 0.5258774757385254, "y": 0.0874684602022171}], "text": "Dataset\n"}
{"page": 4, "bbox": [{"x": 0.6894705295562744, "y": 0.0803195983171463}, {"x": 0.7245687246322632, "y": 0.0803195983171463}, {"x": 0.7245687246322632, "y": 0.0874684602022171}, {"x": 0.6894705295562744, "y": 0.0874684602022171}], "text": "Train\n"}
{"page": 4, "bbox": [{"x": 0.7876263856887817, "y": 0.08074011653661728}, {"x": 0.8114217519760132, "y": 0.08074011653661728}, {"x": 0.8114217519760132, "y": 0.08704794198274612}, {"x": 0.7876263856887817, "y": 0.08704794198274612}], "text": "Dev\n"}
{"page": 4, "bbox": [{"x": 0.8477097153663635, "y": 0.0803195983171463}, {"x": 0.8732897043228149, "y": 0.0803195983171463}, {"x": 0.8732897043228149, "y": 0.08788898587226868}, {"x": 0.8477097153663635, "y": 0.08788898587226868}], "text": "Test\n"}
{"page": 4, "bbox": [{"x": 0.6549673080444336, "y": 0.09882254153490067}, {"x": 0.757287323474884, "y": 0.09798149764537811}, {"x": 0.757287323474884, "y": 0.10555088520050049}, {"x": 0.6549673080444336, "y": 0.10639192909002304}], "text": "79,168 58,880\n"}
{"page": 4, "bbox": [{"x": 0.8393813371658325, "y": 0.09882254153490067}, {"x": 0.8732897043228149, "y": 0.09840201586484909}, {"x": 0.8732897043228149, "y": 0.10597140341997147}, {"x": 0.8393813371658325, "y": 0.10639192909002304}], "text": "3,610\n"}
{"page": 4, "bbox": [{"x": 0.7781082391738892, "y": 0.09924305975437164}, {"x": 0.8120166659355164, "y": 0.09924305975437164}, {"x": 0.8120166659355164, "y": 0.10639192909002304}, {"x": 0.7781082391738892, "y": 0.10639192909002304}], "text": "8,757\n"}
{"page": 4, "bbox": [{"x": 0.655562162399292, "y": 0.1105971410870552}, {"x": 0.8120166659355164, "y": 0.11101765930652618}, {"x": 0.8120166659355164, "y": 0.11858704686164856}, {"x": 0.655562162399292, "y": 0.11816652864217758}], "text": "78,785 60,413 8,837\n"}
{"page": 4, "bbox": [{"x": 0.8334324955940247, "y": 0.11101765930652618}, {"x": 0.8732897043228149, "y": 0.11101765930652618}, {"x": 0.8732897043228149, "y": 0.11816652864217758}, {"x": 0.8334324955940247, "y": 0.11816652864217758}], "text": "11,313\n"}
{"page": 4, "bbox": [{"x": 0.7900059223175049, "y": 0.12237174063920975}, {"x": 0.8108268976211548, "y": 0.12237174063920975}, {"x": 0.8108268976211548, "y": 0.12910008430480957}, {"x": 0.7900059223175049, "y": 0.12910008430480957}], "text": "361\n"}
{"page": 4, "bbox": [{"x": 0.8387864232063293, "y": 0.12237174063920975}, {"x": 0.8726948499679565, "y": 0.12195122241973877}, {"x": 0.8726948499679565, "y": 0.12910008430480957}, {"x": 0.8387864232063293, "y": 0.12952060997486115}], "text": "2,032\n"}
{"page": 4, "bbox": [{"x": 0.6627007722854614, "y": 0.12237174063920975}, {"x": 0.7578822374343872, "y": 0.12195122241973877}, {"x": 0.7578822374343872, "y": 0.12952060997486115}, {"x": 0.6627007722854614, "y": 0.12994112074375153}], "text": "3,417 2,474\n"}
{"page": 4, "bbox": [{"x": 0.5252825617790222, "y": 0.09798149764537811}, {"x": 0.6353361010551453, "y": 0.09798149764537811}, {"x": 0.6353361010551453, "y": 0.15433137118816376}, {"x": 0.5252825617790222, "y": 0.15433137118816376}], "text": "Natural Questions\nTriviaQA\nWebQuestions\nCuratedTREC\nSQUAD\n"}
{"page": 4, "bbox": [{"x": 0.12016656994819641, "y": 0.07821699231863022}, {"x": 0.48839977383613586, "y": 0.0786375105381012}, {"x": 0.48839977383613586, "y": 0.18713204562664032}, {"x": 0.12016656994819641, "y": 0.18671151995658875}], "text": "negative passages for each question.\nThe trick of in-batch negatives has been used in\nthe full batch setting (Yih et al., 2011) and more\nrecently for mini-batch (Henderson et al., 2017;\nGillick et al., 2019). It has been shown to be an\neffective strategy for learning a dual-encoder model\nthat boosts the number of training examples.\n"}
{"page": 4, "bbox": [{"x": 0.7906008362770081, "y": 0.13414634764194489}, {"x": 0.8108268976211548, "y": 0.13414634764194489}, {"x": 0.8108268976211548, "y": 0.1408746838569641}, {"x": 0.7906008362770081, "y": 0.1408746838569641}], "text": "133\n"}
{"page": 4, "bbox": [{"x": 0.8506841063499451, "y": 0.13456685841083527}, {"x": 0.8732897043228149, "y": 0.13456685841083527}, {"x": 0.8732897043228149, "y": 0.14129520952701569}, {"x": 0.8506841063499451, "y": 0.14129520952701569}], "text": "694\n"}
{"page": 4, "bbox": [{"x": 0.655562162399292, "y": 0.1337258219718933}, {"x": 0.7560975551605225, "y": 0.13330529630184174}, {"x": 0.7560975551605225, "y": 0.1534903347492218}, {"x": 0.655562162399292, "y": 0.15391084551811218}], "text": "1,353 1,125\n78,713\n"}
{"page": 4, "bbox": [{"x": 0.7168352007865906, "y": 0.14592094719409943}, {"x": 0.8114217519760132, "y": 0.1463414579629898}, {"x": 0.8114217519760132, "y": 0.1534903347492218}, {"x": 0.7168352007865906, "y": 0.15306980907917023}], "text": "70.096 8,886\n"}
{"page": 4, "bbox": [{"x": 0.8334324955940247, "y": 0.14592094719409943}, {"x": 0.8726948499679565, "y": 0.1463414579629898}, {"x": 0.8726948499679565, "y": 0.1534903347492218}, {"x": 0.8334324955940247, "y": 0.15306980907917023}], "text": "10,570\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.17283432185649872}, {"x": 0.8851873874664307, "y": 0.1732548326253891}, {"x": 0.8851873874664307, "y": 0.22708158195018768}, {"x": 0.5151695609092712, "y": 0.2266610562801361}], "text": "Table 1: Number of questions in each QA dataset. The\ntwo columns of Train denote the original training ex-\namples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n"}
{"page": 4, "bbox": [{"x": 0.11897680163383484, "y": 0.20395290851593018}, {"x": 0.3224271237850189, "y": 0.20437341928482056}, {"x": 0.3224271237850189, "y": 0.21698907017707825}, {"x": 0.11897680163383484, "y": 0.21656854450702667}], "text": "4 Experimental Setup\n"}
{"page": 4, "bbox": [{"x": 0.1207614541053772, "y": 0.23170731961727142}, {"x": 0.48899465799331665, "y": 0.23170731961727142}, {"x": 0.48899465799331665, "y": 0.25988224148750305}, {"x": 0.1207614541053772, "y": 0.25988224148750305}], "text": "In this section, we describe the data we used for\nexperiments and the basic setup.\n"}
{"page": 4, "bbox": [{"x": 0.11957168579101562, "y": 0.2767031192779541}, {"x": 0.1427721530199051, "y": 0.2767031192779541}, {"x": 0.1427721530199051, "y": 0.28511354327201843}, {"x": 0.11957168579101562, "y": 0.28511354327201843}], "text": "4.1\n"}
{"page": 4, "bbox": [{"x": 0.5145746469497681, "y": 0.25651809573173523}, {"x": 0.8869720697402954, "y": 0.257359117269516}, {"x": 0.8857823014259338, "y": 0.46131202578544617}, {"x": 0.5133848786354065, "y": 0.460470974445343}], "text": "as well as various Web sources and is intended for\nopen-domain QA from unstructured corpora.\nSQUAD v1.1 (Rajpurkar et al., 2016) is a popu-\nlar benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia para-\ngraph, and asked to write questions that could be\nanswered from the given text. Although SQUAD\nhas been used previously for open-domain QA re-\nsearch, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n"}
{"page": 4, "bbox": [{"x": 0.11897680163383484, "y": 0.27628257870674133}, {"x": 0.4901844263076782, "y": 0.27628257870674133}, {"x": 0.4901844263076782, "y": 0.5336416959762573}, {"x": 0.11897680163383484, "y": 0.5336416959762573}], "text": "Wikipedia Data Pre-processing\nFollowing (Lee et al., 2019), we use the English\nWikipedia dump from Dec. 20, 2018 as the source\ndocuments for answering questions. We first apply\nthe pre-processing code released in DrQA (Chen\net al., 2017) to extract the clean, text-portion of\narticles from the Wikipedia dump. This step re-\nmoves semi-structured data, such as tables, info-\nboxes, lists, as well as the disambiguation pages.\nWe then split each article into multiple, disjoint text\nblocks of 100 words as passages, serving as our\nbasic retrieval units, following (Wang et al., 2019),\nwhich results in 21,015,324 passages in the end.5\nEach passage is also prepended with the title of the\nWikipedia article where the passage is from, along\nwith an [SEP] token.\n"}
{"page": 4, "bbox": [{"x": 0.12016656994819641, "y": 0.5517241358757019}, {"x": 0.14336703717708588, "y": 0.5517241358757019}, {"x": 0.14336703717708588, "y": 0.5601345896720886}, {"x": 0.12016656994819641, "y": 0.5601345896720886}], "text": "4.2\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.4768713116645813}, {"x": 0.8869720697402954, "y": 0.47729185223579407}, {"x": 0.8863771557807922, "y": 0.7451639771461487}, {"x": 0.5145746469497681, "y": 0.7447434663772583}], "text": "Selection of positive passages Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA6, we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the ques-\ntion will be discarded. For SQUAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the num-\nber of questions in training/dev/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n"}
{"page": 4, "bbox": [{"x": 0.6311719417572021, "y": 0.653069794178009}, {"x": 0.6383105516433716, "y": 0.653069794178009}, {"x": 0.6383105516433716, "y": 0.6589571237564087}, {"x": 0.6311719417572021, "y": 0.6589571237564087}], "text": "7\n"}
{"page": 4, "bbox": [{"x": 0.11838191747665405, "y": 0.550462543964386}, {"x": 0.4901844263076782, "y": 0.5500420331954956}, {"x": 0.490779310464859, "y": 0.9091673493385315}, {"x": 0.11897680163383484, "y": 0.9095878601074219}], "text": "Question Answering Datasets\nWe use the same five QA datasets and train-\ning/dev/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\nNatural Questions (NQ) (Kwiatkowski et al.,\n2019) was designed for end-to-end question an-\nswering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\nTriviaQA (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\nWebQuestions (WQ) (Berant et al., 2013) consists\nof questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\nCurated TREC (TREC) (Baudiš and Šedivỳ,\n2015) sources questions from TREC QA tracks\n\"However, Wang et al. (2019) also propose splitting docu-\nments into overlapping passages, which we do not find advan-\ntageous compared to the non-overlapping version.\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.7645080089569092}, {"x": 0.8185603618621826, "y": 0.7632464170455933}, {"x": 0.8185603618621826, "y": 0.7771236300468445}, {"x": 0.5151695609092712, "y": 0.7783852219581604}], "text": "5 Experiments: Passage Retrieval\n"}
{"page": 4, "bbox": [{"x": 0.5151695609092712, "y": 0.7918418645858765}, {"x": 0.8851873874664307, "y": 0.7914213538169861}, {"x": 0.8851873874664307, "y": 0.9079058170318604}, {"x": 0.5151695609092712, "y": 0.9083263278007507}], "text": "In this section, we evaluate the retrieval perfor-\nmance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Ap-\npendix A.\n"}
{"page": 4, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5193337202072144, "y": 0.9243061542510986}, {"x": 0.5193337202072144, "y": 0.9331371188163757}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6772\n"}
{"page": 5, "bbox": [{"x": 0.1421772688627243, "y": 0.07947855442762375}, {"x": 0.27424153685569763, "y": 0.0786375105381012}, {"x": 0.27424153685569763, "y": 0.08830950409173965}, {"x": 0.1421772688627243, "y": 0.08915054798126221}], "text": "Training Retriever\n"}
{"page": 5, "bbox": [{"x": 0.7037477493286133, "y": 0.0803195983171463}, {"x": 0.7555027008056641, "y": 0.07905802875757217}, {"x": 0.7560975551605225, "y": 0.08873002231121063}, {"x": 0.7043426632881165, "y": 0.08999159187078476}], "text": "Top-100\n"}
{"page": 5, "bbox": [{"x": 0.4259369373321533, "y": 0.0803195983171463}, {"x": 0.4705532491207123, "y": 0.0803195983171463}, {"x": 0.4705532491207123, "y": 0.08957106620073318}, {"x": 0.4259369373321533, "y": 0.08957106620073318}], "text": "Top-20\n"}
{"page": 5, "bbox": [{"x": 0.31766805052757263, "y": 0.09209419786930084}, {"x": 0.3396787643432617, "y": 0.09209419786930084}, {"x": 0.3396787643432617, "y": 0.1000841036438942}, {"x": 0.31766805052757263, "y": 0.1000841036438942}], "text": "NQ\n"}
{"page": 5, "bbox": [{"x": 0.3587150573730469, "y": 0.09167367219924927}, {"x": 0.8607971668243408, "y": 0.09167367219924927}, {"x": 0.8607971668243408, "y": 0.10092514753341675}, {"x": 0.3587150573730469, "y": 0.10092514753341675}], "text": "TriviaQA WQ TREC SQUAD NQ TriviaQA WQ TREC SQUAD\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.11101765930652618}, {"x": 0.5675193071365356, "y": 0.11101765930652618}, {"x": 0.5675193071365356, "y": 0.11690495908260345}, {"x": 0.5425342321395874, "y": 0.11690495908260345}], "text": "68.8\n"}
{"page": 5, "bbox": [{"x": 0.14158238470554352, "y": 0.11017661541700363}, {"x": 0.17370612919330597, "y": 0.1105971410870552}, {"x": 0.17370612919330597, "y": 0.11774600297212601}, {"x": 0.14158238470554352, "y": 0.11732548475265503}], "text": "None\n"}
{"page": 5, "bbox": [{"x": 0.43248066306114197, "y": 0.10975609719753265}, {"x": 0.5080309510231018, "y": 0.1105971410870552}, {"x": 0.5080309510231018, "y": 0.11816652864217758}, {"x": 0.43248066306114197, "y": 0.11732548475265503}], "text": "55.0 70.9\n"}
{"page": 5, "bbox": [{"x": 0.5978584289550781, "y": 0.1105971410870552}, {"x": 0.6228435635566711, "y": 0.1105971410870552}, {"x": 0.6228435635566711, "y": 0.11732548475265503}, {"x": 0.5978584289550781, "y": 0.11732548475265503}], "text": "73.7\n"}
{"page": 5, "bbox": [{"x": 0.8245092034339905, "y": 0.1105971410870552}, {"x": 0.8494943380355835, "y": 0.1105971410870552}, {"x": 0.8494943380355835, "y": 0.11732548475265503}, {"x": 0.8245092034339905, "y": 0.11732548475265503}], "text": "80.0\n"}
{"page": 5, "bbox": [{"x": 0.3152885138988495, "y": 0.11101765930652618}, {"x": 0.3408685326576233, "y": 0.11101765930652618}, {"x": 0.3408685326576233, "y": 0.11732548475265503}, {"x": 0.3152885138988495, "y": 0.11732548475265503}], "text": "59.1\n"}
{"page": 5, "bbox": [{"x": 0.3741820454597473, "y": 0.11101765930652618}, {"x": 0.4003569185733795, "y": 0.11101765930652618}, {"x": 0.4003569185733795, "y": 0.11732548475265503}, {"x": 0.3741820454597473, "y": 0.11732548475265503}], "text": "66.9\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.1105971410870552}, {"x": 0.682331919670105, "y": 0.1105971410870552}, {"x": 0.682331919670105, "y": 0.11774600297212601}, {"x": 0.6561570763587952, "y": 0.11774600297212601}], "text": "76.7\n"}
{"page": 5, "bbox": [{"x": 0.7150505781173706, "y": 0.1105971410870552}, {"x": 0.7894110679626465, "y": 0.11017661541700363}, {"x": 0.7894110679626465, "y": 0.11774600297212601}, {"x": 0.7150505781173706, "y": 0.11816652864217758}], "text": "71.1 84.1\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.11101765930652618}, {"x": 0.25163593888282776, "y": 0.11101765930652618}, {"x": 0.25163593888282776, "y": 0.11774600297212601}, {"x": 0.21475312113761902, "y": 0.11774600297212601}], "text": "BM25\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.128679558634758}, {"x": 0.24390244483947754, "y": 0.12910008430480957}, {"x": 0.24390244483947754, "y": 0.13624894618988037}, {"x": 0.21475312113761902, "y": 0.1358284205198288}], "text": "DPR\n"}
{"page": 5, "bbox": [{"x": 0.655562162399292, "y": 0.12910008430480957}, {"x": 0.6829268336296082, "y": 0.12910008430480957}, {"x": 0.6829268336296082, "y": 0.1358284205198288}, {"x": 0.655562162399292, "y": 0.1358284205198288}], "text": "85.0\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.12825904786586761}, {"x": 0.6234384179115295, "y": 0.12952060997486115}, {"x": 0.6234384179115295, "y": 0.13666947185993195}, {"x": 0.5425342321395874, "y": 0.13540790975093842}], "text": "63.2 85.4\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.12952060997486115}, {"x": 0.34205830097198486, "y": 0.12952060997486115}, {"x": 0.34205830097198486, "y": 0.1358284205198288}, {"x": 0.3158833980560303, "y": 0.1358284205198288}], "text": "78.4\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.12910008430480957}, {"x": 0.5074360370635986, "y": 0.12910008430480957}, {"x": 0.5074360370635986, "y": 0.13624894618988037}, {"x": 0.43307554721832275, "y": 0.13624894618988037}], "text": "73.2 79.8\n"}
{"page": 5, "bbox": [{"x": 0.3741820454597473, "y": 0.12952060997486115}, {"x": 0.4003569185733795, "y": 0.12952060997486115}, {"x": 0.4003569185733795, "y": 0.13624894618988037}, {"x": 0.3741820454597473, "y": 0.13624894618988037}], "text": "79.4\n"}
{"page": 5, "bbox": [{"x": 0.8239143490791321, "y": 0.12952060997486115}, {"x": 0.8506841063499451, "y": 0.12952060997486115}, {"x": 0.8506841063499451, "y": 0.13624894618988037}, {"x": 0.8239143490791321, "y": 0.13624894618988037}], "text": "77.2\n"}
{"page": 5, "bbox": [{"x": 0.7144556641578674, "y": 0.12952060997486115}, {"x": 0.7894110679626465, "y": 0.12994112074375153}, {"x": 0.7894110679626465, "y": 0.13666947185993195}, {"x": 0.7144556641578674, "y": 0.13624894618988037}], "text": "81.4 89.1\n"}
{"page": 5, "bbox": [{"x": 0.14158238470554352, "y": 0.13498738408088684}, {"x": 0.18024985492229462, "y": 0.13498738408088684}, {"x": 0.18024985492229462, "y": 0.14381833374500275}, {"x": 0.14158238470554352, "y": 0.14381833374500275}], "text": "Single\n"}
{"page": 5, "bbox": [{"x": 0.7150505781173706, "y": 0.14045415818691254}, {"x": 0.7894110679626465, "y": 0.14045415818691254}, {"x": 0.7894110679626465, "y": 0.1480235457420349}, {"x": 0.7150505781173706, "y": 0.1480235457420349}], "text": "80.5 92.7\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.14045415818691254}, {"x": 0.6817370653152466, "y": 0.1408746838569641}, {"x": 0.6817370653152466, "y": 0.1480235457420349}, {"x": 0.6561570763587952, "y": 0.14760303497314453}], "text": "84.5\n"}
{"page": 5, "bbox": [{"x": 0.8239143490791321, "y": 0.14045415818691254}, {"x": 0.8500892519950867, "y": 0.1408746838569641}, {"x": 0.8500892519950867, "y": 0.1480235457420349}, {"x": 0.8239143490791321, "y": 0.14760303497314453}], "text": "81.3\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.14003364741802216}, {"x": 0.2980368733406067, "y": 0.1408746838569641}, {"x": 0.2980368733406067, "y": 0.1484440714120865}, {"x": 0.21475312113761902, "y": 0.14760303497314453}], "text": "BM25 + DPR\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.14003364741802216}, {"x": 0.5086258053779602, "y": 0.1408746838569641}, {"x": 0.5086258053779602, "y": 0.1484440714120865}, {"x": 0.43307554721832275, "y": 0.14760303497314453}], "text": "71.0 85.2\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.1408746838569641}, {"x": 0.34205830097198486, "y": 0.14129520952701569}, {"x": 0.34205830097198486, "y": 0.1480235457420349}, {"x": 0.3158833980560303, "y": 0.14760303497314453}], "text": "76.6\n"}
{"page": 5, "bbox": [{"x": 0.3747769296169281, "y": 0.14129520952701569}, {"x": 0.39916715025901794, "y": 0.14129520952701569}, {"x": 0.39916715025901794, "y": 0.14760303497314453}, {"x": 0.3747769296169281, "y": 0.14760303497314453}], "text": "79.8\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.14129520952701569}, {"x": 0.5687090754508972, "y": 0.14129520952701569}, {"x": 0.5687090754508972, "y": 0.14760303497314453}, {"x": 0.5425342321395874, "y": 0.14760303497314453}], "text": "71.5\n"}
{"page": 5, "bbox": [{"x": 0.597263514995575, "y": 0.14129520952701569}, {"x": 0.6234384179115295, "y": 0.14129520952701569}, {"x": 0.6234384179115295, "y": 0.14760303497314453}, {"x": 0.597263514995575, "y": 0.14760303497314453}], "text": "83.8\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.1589571088552475}, {"x": 0.3408685326576233, "y": 0.1589571088552475}, {"x": 0.3408685326576233, "y": 0.16568544507026672}, {"x": 0.3158833980560303, "y": 0.16568544507026672}], "text": "79.4\n"}
{"page": 5, "bbox": [{"x": 0.3747769296169281, "y": 0.15853658318519592}, {"x": 0.39976203441619873, "y": 0.1589571088552475}, {"x": 0.39976203441619873, "y": 0.1661059707403183}, {"x": 0.3747769296169281, "y": 0.16568544507026672}], "text": "78.8\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.15811605751514435}, {"x": 0.5074360370635986, "y": 0.1589571088552475}, {"x": 0.5074360370635986, "y": 0.16652649641036987}, {"x": 0.43307554721832275, "y": 0.16568544507026672}], "text": "75.0 89.1\n"}
{"page": 5, "bbox": [{"x": 0.5966686606407166, "y": 0.1589571088552475}, {"x": 0.6228435635566711, "y": 0.1589571088552475}, {"x": 0.6228435635566711, "y": 0.16568544507026672}, {"x": 0.5966686606407166, "y": 0.16568544507026672}], "text": "86.0\n"}
{"page": 5, "bbox": [{"x": 0.8245092034339905, "y": 0.15811605751514435}, {"x": 0.8512790203094482, "y": 0.1589571088552475}, {"x": 0.8506841063499451, "y": 0.16652649641036987}, {"x": 0.8239143490791321, "y": 0.16568544507026672}], "text": "67.6\n"}
{"page": 5, "bbox": [{"x": 0.21415823698043823, "y": 0.15937763452529907}, {"x": 0.24330756068229675, "y": 0.15937763452529907}, {"x": 0.24330756068229675, "y": 0.16568544507026672}, {"x": 0.21415823698043823, "y": 0.16568544507026672}], "text": "DPR\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.15937763452529907}, {"x": 0.5687090754508972, "y": 0.15937763452529907}, {"x": 0.5687090754508972, "y": 0.16568544507026672}, {"x": 0.5425342321395874, "y": 0.16568544507026672}], "text": "51.6\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.15937763452529907}, {"x": 0.6817370653152466, "y": 0.15937763452529907}, {"x": 0.6817370653152466, "y": 0.16568544507026672}, {"x": 0.6561570763587952, "y": 0.16568544507026672}], "text": "84.7\n"}
{"page": 5, "bbox": [{"x": 0.7144556641578674, "y": 0.1589571088552475}, {"x": 0.7882212996482849, "y": 0.15937763452529907}, {"x": 0.7882212996482849, "y": 0.16652649641036987}, {"x": 0.7144556641578674, "y": 0.1661059707403183}], "text": "82.9 93.9\n"}
{"page": 5, "bbox": [{"x": 0.14158238470554352, "y": 0.16484440863132477}, {"x": 0.17430101335048676, "y": 0.1644238829612732}, {"x": 0.17430101335048676, "y": 0.171572744846344}, {"x": 0.14158238470554352, "y": 0.17199327051639557}], "text": "Multi\n"}
{"page": 5, "bbox": [{"x": 0.7150505781173706, "y": 0.17073170840740204}, {"x": 0.7894110679626465, "y": 0.17073170840740204}, {"x": 0.7894110679626465, "y": 0.17746004462242126}, {"x": 0.7150505781173706, "y": 0.17746004462242126}], "text": "82.3 94.1\n"}
{"page": 5, "bbox": [{"x": 0.3741820454597473, "y": 0.17073170840740204}, {"x": 0.39916715025901794, "y": 0.1711522340774536}, {"x": 0.39916715025901794, "y": 0.17746004462242126}, {"x": 0.3741820454597473, "y": 0.17703953385353088}], "text": "79.9\n"}
{"page": 5, "bbox": [{"x": 0.3158833980560303, "y": 0.1711522340774536}, {"x": 0.3408685326576233, "y": 0.1711522340774536}, {"x": 0.3408685326576233, "y": 0.17746004462242126}, {"x": 0.3158833980560303, "y": 0.17746004462242126}], "text": "78.0\n"}
{"page": 5, "bbox": [{"x": 0.5425342321395874, "y": 0.1711522340774536}, {"x": 0.5687090754508972, "y": 0.1711522340774536}, {"x": 0.5687090754508972, "y": 0.17746004462242126}, {"x": 0.5425342321395874, "y": 0.17746004462242126}], "text": "66.2\n"}
{"page": 5, "bbox": [{"x": 0.5966686606407166, "y": 0.1711522340774536}, {"x": 0.6228435635566711, "y": 0.1711522340774536}, {"x": 0.6228435635566711, "y": 0.17746004462242126}, {"x": 0.5966686606407166, "y": 0.17746004462242126}], "text": "83.9\n"}
{"page": 5, "bbox": [{"x": 0.6561570763587952, "y": 0.1711522340774536}, {"x": 0.682331919670105, "y": 0.1711522340774536}, {"x": 0.682331919670105, "y": 0.17746004462242126}, {"x": 0.6561570763587952, "y": 0.17746004462242126}], "text": "84.4\n"}
{"page": 5, "bbox": [{"x": 0.8245092034339905, "y": 0.1711522340774536}, {"x": 0.8494943380355835, "y": 0.1711522340774536}, {"x": 0.8494943380355835, "y": 0.17746004462242126}, {"x": 0.8245092034339905, "y": 0.17746004462242126}], "text": "78.6\n"}
{"page": 5, "bbox": [{"x": 0.21475312113761902, "y": 0.1711522340774536}, {"x": 0.2980368733406067, "y": 0.1711522340774536}, {"x": 0.2980368733406067, "y": 0.17788057029247284}, {"x": 0.21475312113761902, "y": 0.17788057029247284}], "text": "BM25 + DPR\n"}
{"page": 5, "bbox": [{"x": 0.43307554721832275, "y": 0.1711522340774536}, {"x": 0.5068411827087402, "y": 0.1711522340774536}, {"x": 0.5068411827087402, "y": 0.17788057029247284}, {"x": 0.43307554721832275, "y": 0.17788057029247284}], "text": "74.7 88.5\n"}
{"page": 5, "bbox": [{"x": 0.12016656994819641, "y": 0.1984861195087433}, {"x": 0.8828078508377075, "y": 0.1984861195087433}, {"x": 0.8828078508377075, "y": 0.2375946193933487}, {"x": 0.12016656994819641, "y": 0.2375946193933487}], "text": "Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved\npassages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQUAD). See text for more details.\n"}
{"page": 5, "bbox": [{"x": 0.5466983914375305, "y": 0.2674516439437866}, {"x": 0.5585960745811462, "y": 0.2674516439437866}, {"x": 0.5585960745811462, "y": 0.27207738161087036}, {"x": 0.5466983914375305, "y": 0.27207738161087036}], "text": "90\n"}
{"page": 5, "bbox": [{"x": 0.5472933053970337, "y": 0.29899075627326965}, {"x": 0.5580011606216431, "y": 0.29899075627326965}, {"x": 0.5580011606216431, "y": 0.3036164939403534}, {"x": 0.5472933053970337, "y": 0.3036164939403534}], "text": "80\n"}
{"page": 5, "bbox": [{"x": 0.5300416350364685, "y": 0.3944491147994995}, {"x": 0.5306365489959717, "y": 0.3019343912601471}, {"x": 0.543724000453949, "y": 0.3019343912601471}, {"x": 0.5431290864944458, "y": 0.3944491147994995}], "text": "Top-k accuracy (%)\n"}
{"page": 5, "bbox": [{"x": 0.5615704655647278, "y": 0.3603868782520294}, {"x": 0.5615704655647278, "y": 0.3696383535861969}, {"x": 0.5466983914375305, "y": 0.3696383535861969}, {"x": 0.5466983914375305, "y": 0.3603868782520294}], "text": "8\n"}
{"page": 5, "bbox": [{"x": 0.7638310790061951, "y": 0.357443243265152}, {"x": 0.8239143490791321, "y": 0.3570227026939392}, {"x": 0.8239143490791321, "y": 0.3751051425933838}, {"x": 0.7638310790061951, "y": 0.37552565336227417}], "text": "BM25\n# Train: 1k\n"}
{"page": 5, "bbox": [{"x": 0.7644259333610535, "y": 0.3801513910293579}, {"x": 0.8310529589653015, "y": 0.3805719017982483}, {"x": 0.8310529589653015, "y": 0.38645920157432556}, {"x": 0.7644259333610535, "y": 0.3860386908054352}], "text": "# Train: 10k\n"}
{"page": 5, "bbox": [{"x": 0.7644259333610535, "y": 0.3910849392414093}, {"x": 0.8304580450057983, "y": 0.3910849392414093}, {"x": 0.8304580450057983, "y": 0.39739277958869934}, {"x": 0.7644259333610535, "y": 0.39739277958869934}], "text": "# Train: 20k\n"}
{"page": 5, "bbox": [{"x": 0.5472933053970337, "y": 0.39402860403060913}, {"x": 0.5585960745811462, "y": 0.39402860403060913}, {"x": 0.5585960745811462, "y": 0.39865434169769287}, {"x": 0.5472933053970337, "y": 0.39865434169769287}], "text": "50\n"}
{"page": 5, "bbox": [{"x": 0.7644259333610535, "y": 0.40285953879356384}, {"x": 0.8304580450057983, "y": 0.40285953879356384}, {"x": 0.8304580450057983, "y": 0.40790581703186035}, {"x": 0.7644259333610535, "y": 0.40790581703186035}], "text": "# Train: 40k\n"}
{"page": 5, "bbox": [{"x": 0.7650208473205566, "y": 0.4129520654678345}, {"x": 0.8542534112930298, "y": 0.41337257623672485}, {"x": 0.8542534112930298, "y": 0.4201009273529053}, {"x": 0.7650208473205566, "y": 0.4196804165840149}], "text": "# Train: all (59k)\n"}
{"page": 5, "bbox": [{"x": 0.5466983914375305, "y": 0.42556771636009216}, {"x": 0.5585960745811462, "y": 0.42556771636009216}, {"x": 0.5585960745811462, "y": 0.4301934540271759}, {"x": 0.5466983914375305, "y": 0.4301934540271759}], "text": "40\n"}
{"page": 5, "bbox": [{"x": 0.6145151853561401, "y": 0.43145501613616943}, {"x": 0.6270077228546143, "y": 0.43145501613616943}, {"x": 0.6270077228546143, "y": 0.4360807538032532}, {"x": 0.6145151853561401, "y": 0.4360807538032532}], "text": "20\n"}
{"page": 5, "bbox": [{"x": 0.672218918800354, "y": 0.43145501613616943}, {"x": 0.6847114562988281, "y": 0.43145501613616943}, {"x": 0.6847114562988281, "y": 0.4360807538032532}, {"x": 0.672218918800354, "y": 0.4360807538032532}], "text": "40\n"}
{"page": 5, "bbox": [{"x": 0.7293277978897095, "y": 0.43145501613616943}, {"x": 0.7418203353881836, "y": 0.43145501613616943}, {"x": 0.7418203353881836, "y": 0.4360807538032532}, {"x": 0.7293277978897095, "y": 0.4360807538032532}], "text": "60\n"}
{"page": 5, "bbox": [{"x": 0.7870315313339233, "y": 0.43145501613616943}, {"x": 0.7995240688323975, "y": 0.43145501613616943}, {"x": 0.7995240688323975, "y": 0.4360807538032532}, {"x": 0.7870315313339233, "y": 0.4360807538032532}], "text": "80\n"}
{"page": 5, "bbox": [{"x": 0.8417608737945557, "y": 0.43145501613616943}, {"x": 0.8590124845504761, "y": 0.43145501613616943}, {"x": 0.8590124845504761, "y": 0.4360807538032532}, {"x": 0.8417608737945557, "y": 0.4360807538032532}], "text": "100\n"}
{"page": 5, "bbox": [{"x": 0.6258179545402527, "y": 0.43818333745002747}, {"x": 0.8036882877349854, "y": 0.4407064616680145}, {"x": 0.8030933737754822, "y": 0.449957937002182}, {"x": 0.6258179545402527, "y": 0.44743481278419495}], "text": "k: # of retrieved passages\n"}
{"page": 5, "bbox": [{"x": 0.11957168579101562, "y": 0.26703113317489624}, {"x": 0.4913741946220398, "y": 0.26703113317489624}, {"x": 0.4913741946220398, "y": 0.6816652417182922}, {"x": 0.11957168579101562, "y": 0.6816652417182922}], "text": "traditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Sec-\ntion 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQUAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10-5 using Adam, linear\nscheduling with warm-up and dropout rate 0.1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be de-\nsirable to obtain a single retriever that works well\nacross the board. To this end, we train a multi-\ndataset encoder by combining training data from\nall datasets excluding SQUAD. In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\nof top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25(q,p) + sim(q, p) as the ranking function.\nWe used \\ = 1.1 based on the retrieval accuracy in\nthe development set.\n"}
{"page": 5, "bbox": [{"x": 0.5157644152641296, "y": 0.4638351500034332}, {"x": 0.8863771557807922, "y": 0.4638351500034332}, {"x": 0.8863771557807922, "y": 0.5319596529006958}, {"x": 0.5157644152641296, "y": 0.5319596529006958}], "text": "Figure 1: Retriever top-k accuracy with different num-\nbers of training examples used in our dense passage re-\ntriever vs BM25. The results are measured on the de-\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n"}
{"page": 5, "bbox": [{"x": 0.2373587191104889, "y": 0.6417157053947449}, {"x": 0.24033313989639282, "y": 0.6417157053947449}, {"x": 0.24033313989639282, "y": 0.6438183188438416}, {"x": 0.2373587191104889, "y": 0.6438183188438416}], "text": ".\n"}
{"page": 5, "bbox": [{"x": 0.5145746469497681, "y": 0.5630782246589661}, {"x": 0.8851873874664307, "y": 0.5630782246589661}, {"x": 0.8851873874664307, "y": 0.8195962905883789}, {"x": 0.5145746469497681, "y": 0.8195962905883789}], "text": "tiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\nWe conjecture that the lower performance on\nSQUAD is due to two reasons. First, the annota-\ntors wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\nonly 500+ Wikipedia articles and thus the distribu-\ntion of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n"}
{"page": 5, "bbox": [{"x": 0.12016656994819641, "y": 0.6968040466308594}, {"x": 0.1427721530199051, "y": 0.6968040466308594}, {"x": 0.1427721530199051, "y": 0.7047939300537109}, {"x": 0.12016656994819641, "y": 0.7047939300537109}], "text": "5.1\n"}
{"page": 5, "bbox": [{"x": 0.16180844604969025, "y": 0.696383535861969}, {"x": 0.26531827449798584, "y": 0.6968040466308594}, {"x": 0.26531827449798584, "y": 0.7052144408226013}, {"x": 0.16180844604969025, "y": 0.7047939300537109}], "text": "Main Results\n"}
{"page": 5, "bbox": [{"x": 0.11897680163383484, "y": 0.7165685296058655}, {"x": 0.490779310464859, "y": 0.7165685296058655}, {"x": 0.490779310464859, "y": 0.9087468385696411}, {"x": 0.11897680163383484, "y": 0.9087468385696411}], "text": "Table 2 compares different passage retrieval sys-\ntems on five QA datasets, using the top-k accuracy\n(k = {20, 100}). With the exception of SQUAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when k is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\non Natural Questions). When training with mul-\n8SQUAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\nmore in Section 5.1.\n'Lucene implementation. BM25 parameters b = 0.4 (doc-\nument length normalization) and k₁ = 0.9 (term frequency\nscaling) are tuned using development sets.\n"}
{"page": 5, "bbox": [{"x": 0.5157644152641296, "y": 0.8397813439369202}, {"x": 0.8845925331115723, "y": 0.8406223654747009}, {"x": 0.8839976191520691, "y": 0.9095878601074219}, {"x": 0.5151695609092712, "y": 0.9087468385696411}], "text": "5.2 Ablation Study on Model Training\nTo understand further how different model training\noptions affect the results, we conduct several addi-\ntional experiments and discuss our findings below.\n"}
{"page": 5, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5199286341667175, "y": 0.9238856434822083}, {"x": 0.5199286341667175, "y": 0.9322960376739502}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6773\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.0803195983171463}, {"x": 0.6299821734428406, "y": 0.0803195983171463}, {"x": 0.6299821734428406, "y": 0.0874684602022171}, {"x": 0.612135648727417, "y": 0.0874684602022171}], "text": "#N\n"}
{"page": 6, "bbox": [{"x": 0.6799523830413818, "y": 0.07947855442762375}, {"x": 0.871505081653595, "y": 0.0803195983171463}, {"x": 0.871505081653595, "y": 0.08999159187078476}, {"x": 0.6799523830413818, "y": 0.08915054798126221}], "text": "IB Top-5 Top-20 Top-100\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.07947855442762375}, {"x": 0.5514574646949768, "y": 0.08074011653661728}, {"x": 0.5508626103401184, "y": 0.09041211009025574}, {"x": 0.5193337202072144, "y": 0.08915054798126221}], "text": "Туре\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.09840201586484909}, {"x": 0.7965496778488159, "y": 0.09882254153490067}, {"x": 0.7965496778488159, "y": 0.10555088520050049}, {"x": 0.7703747749328613, "y": 0.10513035953044891}], "text": "64.3\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.09882254153490067}, {"x": 0.6192742586135864, "y": 0.09882254153490067}, {"x": 0.6192742586135864, "y": 0.10555088520050049}, {"x": 0.612135648727417, "y": 0.10555088520050049}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.09924305975437164}, {"x": 0.5693039894104004, "y": 0.09924305975437164}, {"x": 0.5693039894104004, "y": 0.10555088520050049}, {"x": 0.5199286341667175, "y": 0.10555088520050049}], "text": "Random\n"}
{"page": 6, "bbox": [{"x": 0.682331919670105, "y": 0.09924305975437164}, {"x": 0.7418203353881836, "y": 0.09924305975437164}, {"x": 0.7418203353881836, "y": 0.10555088520050049}, {"x": 0.682331919670105, "y": 0.10555088520050049}], "text": "X 47.0\n"}
{"page": 6, "bbox": [{"x": 0.8322427272796631, "y": 0.09924305975437164}, {"x": 0.8572278618812561, "y": 0.09924305975437164}, {"x": 0.8572278618812561, "y": 0.10555088520050049}, {"x": 0.8322427272796631, "y": 0.10555088520050049}], "text": "77.8\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.11017661541700363}, {"x": 0.8584176301956177, "y": 0.1105971410870552}, {"x": 0.8584176301956177, "y": 0.11774600297212601}, {"x": 0.8328375816345215, "y": 0.11732548475265503}], "text": "74.8\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.1105971410870552}, {"x": 0.6198691129684448, "y": 0.1105971410870552}, {"x": 0.6198691129684448, "y": 0.11732548475265503}, {"x": 0.612135648727417, "y": 0.11732548475265503}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.10933557897806168}, {"x": 0.7953599095344543, "y": 0.1105971410870552}, {"x": 0.7947649955749512, "y": 0.11858704686164856}, {"x": 0.7150505781173706, "y": 0.11732548475265503}], "text": "50.0 63.3\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.11101765930652618}, {"x": 0.5580011606216431, "y": 0.11101765930652618}, {"x": 0.5580011606216431, "y": 0.11774600297212601}, {"x": 0.5199286341667175, "y": 0.11774600297212601}], "text": "BM25\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.12237174063920975}, {"x": 0.6198691129684448, "y": 0.12237174063920975}, {"x": 0.6198691129684448, "y": 0.128679558634758}, {"x": 0.612135648727417, "y": 0.128679558634758}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.12237174063920975}, {"x": 0.8590124845504761, "y": 0.12237174063920975}, {"x": 0.8590124845504761, "y": 0.12910008430480957}, {"x": 0.8328375816345215, "y": 0.12910008430480957}], "text": "78.3\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.12237174063920975}, {"x": 0.5496728420257568, "y": 0.12237174063920975}, {"x": 0.5496728420257568, "y": 0.12952060997486115}, {"x": 0.5199286341667175, "y": 0.12952060997486115}], "text": "Gold\n"}
{"page": 6, "bbox": [{"x": 0.6817370653152466, "y": 0.12237174063920975}, {"x": 0.7947649955749512, "y": 0.12237174063920975}, {"x": 0.7947649955749512, "y": 0.12952060997486115}, {"x": 0.6817370653152466, "y": 0.12952060997486115}], "text": "✓ 42.6 63.1\n"}
{"page": 6, "bbox": [{"x": 0.7697799205780029, "y": 0.14045415818691254}, {"x": 0.7935752272605896, "y": 0.14003364741802216}, {"x": 0.7941701412200928, "y": 0.14760303497314453}, {"x": 0.7703747749328613, "y": 0.1480235457420349}], "text": "69.1\n"}
{"page": 6, "bbox": [{"x": 0.7144556641578674, "y": 0.1408746838569641}, {"x": 0.7394407987594604, "y": 0.14045415818691254}, {"x": 0.7394407987594604, "y": 0.14760303497314453}, {"x": 0.7144556641578674, "y": 0.1480235457420349}], "text": "51.1\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.1408746838569641}, {"x": 0.6192742586135864, "y": 0.1408746838569641}, {"x": 0.6192742586135864, "y": 0.14760303497314453}, {"x": 0.612135648727417, "y": 0.14760303497314453}], "text": "7\n"}
{"page": 6, "bbox": [{"x": 0.6811421513557434, "y": 0.14129520952701569}, {"x": 0.6924449801445007, "y": 0.14129520952701569}, {"x": 0.6924449801445007, "y": 0.14718250930309296}, {"x": 0.6811421513557434, "y": 0.14718250930309296}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.14129520952701569}, {"x": 0.8584176301956177, "y": 0.14129520952701569}, {"x": 0.8584176301956177, "y": 0.14760303497314453}, {"x": 0.8328375816345215, "y": 0.14760303497314453}], "text": "80.8\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.15264928340911865}, {"x": 0.7953599095344543, "y": 0.15222875773906708}, {"x": 0.7953599095344543, "y": 0.1589571088552475}, {"x": 0.7703747749328613, "y": 0.15937763452529907}], "text": "70.8\n"}
{"page": 6, "bbox": [{"x": 0.5199286341667175, "y": 0.14045415818691254}, {"x": 0.5502676963806152, "y": 0.14045415818691254}, {"x": 0.5502676963806152, "y": 0.171572744846344}, {"x": 0.5199286341667175, "y": 0.171572744846344}], "text": "Gold\nGold\nGold\n"}
{"page": 6, "bbox": [{"x": 0.6811421513557434, "y": 0.15264928340911865}, {"x": 0.6912552118301392, "y": 0.15264928340911865}, {"x": 0.6912552118301392, "y": 0.15937763452529907}, {"x": 0.6811421513557434, "y": 0.15937763452529907}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.15264928340911865}, {"x": 0.8566329479217529, "y": 0.15264928340911865}, {"x": 0.8566329479217529, "y": 0.15937763452529907}, {"x": 0.8328375816345215, "y": 0.15937763452529907}], "text": "82.1\n"}
{"page": 6, "bbox": [{"x": 0.612135648727417, "y": 0.15306980907917023}, {"x": 0.6252231001853943, "y": 0.15306980907917023}, {"x": 0.6252231001853943, "y": 0.15937763452529907}, {"x": 0.612135648727417, "y": 0.15937763452529907}], "text": "31\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.15306980907917023}, {"x": 0.7412254810333252, "y": 0.15306980907917023}, {"x": 0.7412254810333252, "y": 0.15937763452529907}, {"x": 0.7150505781173706, "y": 0.15937763452529907}], "text": "52.1\n"}
{"page": 6, "bbox": [{"x": 0.8322427272796631, "y": 0.16400335729122162}, {"x": 0.8566329479217529, "y": 0.16358284652233124}, {"x": 0.8566329479217529, "y": 0.1711522340774536}, {"x": 0.8322427272796631, "y": 0.171572744846344}], "text": "83.1\n"}
{"page": 6, "bbox": [{"x": 0.6127305030822754, "y": 0.1644238829612732}, {"x": 0.6341463327407837, "y": 0.1644238829612732}, {"x": 0.6341463327407837, "y": 0.171572744846344}, {"x": 0.6127305030822754, "y": 0.171572744846344}], "text": "127\n"}
{"page": 6, "bbox": [{"x": 0.6817370653152466, "y": 0.16484440863132477}, {"x": 0.6930398344993591, "y": 0.16484440863132477}, {"x": 0.6930398344993591, "y": 0.1711522340774536}, {"x": 0.6817370653152466, "y": 0.1711522340774536}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.7144556641578674, "y": 0.16484440863132477}, {"x": 0.740630567073822, "y": 0.16484440863132477}, {"x": 0.740630567073822, "y": 0.1711522340774536}, {"x": 0.7144556641578674, "y": 0.1711522340774536}], "text": "55.8\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.16484440863132477}, {"x": 0.7965496778488159, "y": 0.16484440863132477}, {"x": 0.7965496778488159, "y": 0.171572744846344}, {"x": 0.7703747749328613, "y": 0.171572744846344}], "text": "73.0\n"}
{"page": 6, "bbox": [{"x": 0.11957168579101562, "y": 0.07821699231863022}, {"x": 0.490779310464859, "y": 0.07821699231863022}, {"x": 0.490779310464859, "y": 0.28343144059181213}, {"x": 0.11957168579101562, "y": 0.28343144059181213}], "text": "Sample efficiency We explore how many train-\ning examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top-k\nretrieval accuracy with respect to different num-\nbers of training examples, measured on the devel-\nopment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 ex-\namples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question-passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n"}
{"page": 6, "bbox": [{"x": 0.7697799205780029, "y": 0.18334734439849854}, {"x": 0.7971445322036743, "y": 0.1837678700685501}, {"x": 0.7965496778488159, "y": 0.19133725762367249}, {"x": 0.7697799205780029, "y": 0.1909167319536209}], "text": "77.3\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.18460892140865326}, {"x": 0.8578227162361145, "y": 0.18460892140865326}, {"x": 0.8578227162361145, "y": 0.19049622118473053}, {"x": 0.8328375816345215, "y": 0.19049622118473053}], "text": "84.4\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.18460892140865326}, {"x": 0.7418203353881836, "y": 0.18460892140865326}, {"x": 0.7418203353881836, "y": 0.1909167319536209}, {"x": 0.7150505781173706, "y": 0.1909167319536209}], "text": "65.0\n"}
{"page": 6, "bbox": [{"x": 0.518738865852356, "y": 0.18166527152061462}, {"x": 0.6924449801445007, "y": 0.1804036945104599}, {"x": 0.6930398344993591, "y": 0.2195121943950653}, {"x": 0.5193337202072144, "y": 0.22077375650405884}], "text": "G.+BM25(1) 31+32 ✓\nG.+BM25\nG.+BM25(1) 127+128 ✓\n"}
{"page": 6, "bbox": [{"x": 0.6817370653152466, "y": 0.19764508306980133}, {"x": 0.6930398344993591, "y": 0.19764508306980133}, {"x": 0.6930398344993591, "y": 0.20437341928482056}, {"x": 0.6817370653152466, "y": 0.20437341928482056}], "text": "✓\n"}
{"page": 6, "bbox": [{"x": 0.8328375816345215, "y": 0.1980656087398529}, {"x": 0.8590124845504761, "y": 0.1980656087398529}, {"x": 0.8590124845504761, "y": 0.20437341928482056}, {"x": 0.8328375816345215, "y": 0.20437341928482056}], "text": "84.0\n"}
{"page": 6, "bbox": [{"x": 0.6127305030822754, "y": 0.1980656087398529}, {"x": 0.6508030891418457, "y": 0.1980656087398529}, {"x": 0.6508030891418457, "y": 0.20479394495487213}, {"x": 0.6127305030822754, "y": 0.20479394495487213}], "text": "31+64\n"}
{"page": 6, "bbox": [{"x": 0.7150505781173706, "y": 0.19680403172969818}, {"x": 0.7959547638893127, "y": 0.19722455739974976}, {"x": 0.7959547638893127, "y": 0.21825063228607178}, {"x": 0.7150505781173706, "y": 0.2178301066160202}], "text": "64.5 76.4\n65.8\n"}
{"page": 6, "bbox": [{"x": 0.7703747749328613, "y": 0.21026071906089783}, {"x": 0.7953599095344543, "y": 0.21110177040100098}, {"x": 0.7947649955749512, "y": 0.21825063228607178}, {"x": 0.7697799205780029, "y": 0.21740958094596863}], "text": "78.0\n"}
{"page": 6, "bbox": [{"x": 0.8322427272796631, "y": 0.21110177040100098}, {"x": 0.8578227162361145, "y": 0.21110177040100098}, {"x": 0.8578227162361145, "y": 0.2178301066160202}, {"x": 0.8322427272796631, "y": 0.2178301066160202}], "text": "84.9\n"}
{"page": 6, "bbox": [{"x": 0.5151695609092712, "y": 0.23801514506340027}, {"x": 0.8857823014259338, "y": 0.23843565583229065}, {"x": 0.8857823014259338, "y": 0.33473506569862366}, {"x": 0.5151695609092712, "y": 0.3343145549297333}], "text": "Table 3: Comparison of different training schemes,\nmeasured as top-k retrieval accuracy on Natural Ques-\ntions (development set). #N: number of negative\nexamples, IB: in-batch training. G.+BM25 (¹) and\nG.+BM25 (2) denote in-batch training with 1 or 2 ad-\nditional BM25 negatives, which serve as negative pas-\nsages for all questions in the batch.\n"}
{"page": 6, "bbox": [{"x": 0.5145746469497681, "y": 0.36375105381011963}, {"x": 0.8839976191520691, "y": 0.36164844036102295}, {"x": 0.8851873874664307, "y": 0.4533221125602722}, {"x": 0.5157644152641296, "y": 0.4554247260093689}], "text": "Our experiments on Natural Questions show that\nswitching to distantly-supervised passages (using\nthe highest-ranked BM25 passage that contains the\nanswer), has only a small impact: 1 point lower\ntop-k accuracy for retrieval. Appendix A contains\nmore details.\n"}
{"page": 6, "bbox": [{"x": 0.11897680163383484, "y": 0.30109336972236633}, {"x": 0.4913741946220398, "y": 0.30109336972236633}, {"x": 0.4913741946220398, "y": 0.8473507165908813}, {"x": 0.11897680163383484, "y": 0.8473507165908813}], "text": "In-batch negative training We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of-N training set-\nting, where each question in the batch is paired\nwith a positive passage and its own set of n neg-\native passages (Eq. (2)). We find that the choice\nof negatives random, BM25 or gold passages\n(positive passages from other questions) — does\nnot impact the top-k accuracy much in this setting\nwhen k > 20.\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\nor from the whole training set. Effectively, in-batch\nnegative training is an easy and memory-efficient\nway to reuse the negative examples already in the\nbatch rather than creating new ones. It produces\nmore pairs and thus increases the number of train-\ning examples, which might contribute to the good\nmodel performance. As a result, accuracy consis-\ntently improves as the batch size grows.\nFinally, we explore in-batch negative training\nwith additional \"hard\" negative passages that have\nhigh BM25 scores given the question, but do not\ncontain the answer string (the bottom block). These\nadditional passages are used as negative passages\nfor all questions in the same batch. We find that\nadding a single BM25 negative passage improves\nthe result substantially while adding two does not\nhelp further.\n"}
{"page": 6, "bbox": [{"x": 0.5151695609092712, "y": 0.4693019390106201}, {"x": 0.8869720697402954, "y": 0.4697224497795105}, {"x": 0.8857823014259338, "y": 0.9087468385696411}, {"x": 0.5139797925949097, "y": 0.9083263278007507}], "text": "Similarity and loss Besides dot product, cosine\nand Euclidean L2 distance are also commonly used\nas decomposable similarity functions. We test these\nalternatives and find that L2 performs compara-\nble to dot product, and both of them are superior\nto cosine. Similarly, in addition to negative log-\nlikelihood, a popular option for ranking is triplet\nloss, which compares a positive passage and a nega-\ntive one directly with respect to a question (Burges\net al., 2005). Our experiments show that using\ntriplet loss does not affect the results much. More\ndetails can be found in Appendix B.\nCross-dataset generalization One interesting\nquestion regarding DPR's discriminative training\nis how much performance degradation it may suf-\nfer from a non-iid setting. In other words, can\nit still generalize well when directly applied to\na different dataset without additional fine-tuning?\nTo test the cross-dataset generalization, we train\nDPR on Natural Questions only and test it directly\non the smaller WebQuestions and CuratedTREC\ndatasets. We find that DPR generalizes well, with\n3-5 points loss from the best performing fine-tuned\nmodel in top-20 retrieval accuracy (69.9/86.3 vs.\n75.0/89.1 for WebQuestions and TREC, respec-\ntively), while still greatly outperforming the BM25\nbaseline (55.0/70.9).\n"}
{"page": 6, "bbox": [{"x": 0.11957168579101562, "y": 0.8654331564903259}, {"x": 0.48958954215049744, "y": 0.8654331564903259}, {"x": 0.48958954215049744, "y": 0.9091673493385315}, {"x": 0.11957168579101562, "y": 0.9091673493385315}], "text": "Impact of gold passages We use passages that\nmatch the gold contexts in the original datasets\n(when available) as positive examples (Section 4.2).\n"}
{"page": 6, "bbox": [{"x": 0.4830458164215088, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.924726665019989}, {"x": 0.5199286341667175, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6774\n"}
{"page": 7, "bbox": [{"x": 0.11897680163383484, "y": 0.07779646664857864}, {"x": 0.4913741946220398, "y": 0.07821699231863022}, {"x": 0.4913741946220398, "y": 0.20815812051296234}, {"x": 0.11897680163383484, "y": 0.20773759484291077}], "text": "5.3 Qualitative Analysis\nAlthough DPR performs better than BM25 in gen-\neral, passages retrieved by these two methods dif-\nfer qualitatively. Term-matching methods like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\nor semantic relationships better. See Appendix C\nfor examples and more discussion.\n"}
{"page": 7, "bbox": [{"x": 0.5145746469497681, "y": 0.07695542275905609}, {"x": 0.8869720697402954, "y": 0.07695542275905609}, {"x": 0.8869720697402954, "y": 0.3486122786998749}, {"x": 0.5145746469497681, "y": 0.3486122786998749}], "text": "score is chosen as the final answer. The passage\nselection model serves as a reranker through cross-\nattention between the question and the passage. Al-\nthough cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its non-\ndecomposable nature, it has more capacity than the\ndual-encoder model sim(q, p) as in Eq. (1). Apply-\ning it to selecting the passage from a small number\nof retrieved candidates has been shown to work\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let P¿ € R¹×h (1 ≤ i ≤ k) be\na BERT (base, uncased in our experiments) rep-\nresentation for the i-th passage, where L is the\nmaximum length of the passage and h the hidden\ndimension. The probabilities of a token being the\nstarting/ending positions of an answer span and a\npassage being selected are defined as:\n"}
{"page": 7, "bbox": [{"x": 0.12016656994819641, "y": 0.2232968807220459}, {"x": 0.14396192133426666, "y": 0.22371740639209747}, {"x": 0.14396192133426666, "y": 0.2338099181652069}, {"x": 0.12016656994819641, "y": 0.23338940739631653}], "text": "5.4\n"}
{"page": 7, "bbox": [{"x": 0.16240333020687103, "y": 0.2232968807220459}, {"x": 0.3188578188419342, "y": 0.22455845773220062}, {"x": 0.3188578188419342, "y": 0.23507149517536163}, {"x": 0.16240333020687103, "y": 0.2338099181652069}], "text": "Run-time Efficiency\n"}
{"page": 7, "bbox": [{"x": 0.6728137731552124, "y": 0.36417156457901}, {"x": 0.7638310790061951, "y": 0.3658536672592163}, {"x": 0.7632361650466919, "y": 0.37804877758026123}, {"x": 0.672218918800354, "y": 0.3763667047023773}], "text": "softmax (Pi\n"}
{"page": 7, "bbox": [{"x": 0.6418798565864563, "y": 0.37047940492630005}, {"x": 0.6525877714157104, "y": 0.37047940492630005}, {"x": 0.6525877714157104, "y": 0.37426409125328064}, {"x": 0.6418798565864563, "y": 0.37426409125328064}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.782272458076477, "y": 0.3608073890209198}, {"x": 0.825698971748352, "y": 0.3696383535861969}, {"x": 0.8197501301765442, "y": 0.3839360773563385}, {"x": 0.7763236165046692, "y": 0.37552565336227417}], "text": "(start) s'\n"}
{"page": 7, "bbox": [{"x": 0.8613920211791992, "y": 0.36711522936820984}, {"x": 0.8822129964828491, "y": 0.36711522936820984}, {"x": 0.8822129964828491, "y": 0.37804877758026123}, {"x": 0.8613920211791992, "y": 0.37804877758026123}], "text": "(3)\n"}
{"page": 7, "bbox": [{"x": 0.7644259333610535, "y": 0.37047940492630005}, {"x": 0.7775133848190308, "y": 0.37089991569519043}, {"x": 0.7775133848190308, "y": 0.37552565336227417}, {"x": 0.7644259333610535, "y": 0.37552565336227417}], "text": "W\n"}
{"page": 7, "bbox": [{"x": 0.6716240048408508, "y": 0.3843565881252289}, {"x": 0.8143962025642395, "y": 0.3877207636833191}, {"x": 0.8138012886047363, "y": 0.4020185172557831}, {"x": 0.6710291504859924, "y": 0.39865434169769287}], "text": "softmax (P₂Wend) +\n"}
{"page": 7, "bbox": [{"x": 0.8607971668243408, "y": 0.3877207636833191}, {"x": 0.8822129964828491, "y": 0.3877207636833191}, {"x": 0.8822129964828491, "y": 0.39865434169769287}, {"x": 0.8607971668243408, "y": 0.39865434169769287}], "text": "(4)\n"}
{"page": 7, "bbox": [{"x": 0.5466983914375305, "y": 0.3658536672592163}, {"x": 0.6252231001853943, "y": 0.36501261591911316}, {"x": 0.6264128684997559, "y": 0.4213624894618988}, {"x": 0.5478881597518921, "y": 0.42220354080200195}], "text": "Pstart,i\nstart,i \n(s)\nPend,i(t)\nPselected (i)\n"}
{"page": 7, "bbox": [{"x": 0.6412849426269531, "y": 0.39192599058151245}, {"x": 0.6531826257705688, "y": 0.39192599058151245}, {"x": 0.6531826257705688, "y": 0.3965517282485962}, {"x": 0.6412849426269531, "y": 0.3965517282485962}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.672218918800354, "y": 0.40622371435165405}, {"x": 0.8834027647972107, "y": 0.40790581703186035}, {"x": 0.8834027647972107, "y": 0.4234651029109955}, {"x": 0.672218918800354, "y": 0.4217830002307892}], "text": "softmax (PTW selected),, (5)\n"}
{"page": 7, "bbox": [{"x": 0.6418798565864563, "y": 0.41337257623672485}, {"x": 0.6525877714157104, "y": 0.41337257623672485}, {"x": 0.6525877714157104, "y": 0.41715726256370544}, {"x": 0.6418798565864563, "y": 0.41715726256370544}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.11957168579101562, "y": 0.24432295560836792}, {"x": 0.490779310464859, "y": 0.24432295560836792}, {"x": 0.490779310464859, "y": 0.5954583883285522}, {"x": 0.11957168579101562, "y": 0.5954583883285522}], "text": "The main reason that we require a retrieval compo-\nnent for open-domain QA is to reduce the number\nof candidate passages that the reader needs to con-\nsider, which is crucial for answering user's ques-\ntions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors 10,\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 pas-\nsages per question. In contrast, BM25/Lucene (im-\nplemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Com-\nputing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\non a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n"}
{"page": 7, "bbox": [{"x": 0.6395003199577332, "y": 0.4390243887901306}, {"x": 0.674003541469574, "y": 0.43860387802124023}, {"x": 0.6745984554290771, "y": 0.4495374262332916}, {"x": 0.6400951743125916, "y": 0.449957937002182}], "text": "[CLS]\n"}
{"page": 7, "bbox": [{"x": 0.5151695609092712, "y": 0.4444911777973175}, {"x": 0.5812016725540161, "y": 0.44196805357933044}, {"x": 0.5817965269088745, "y": 0.453742653131485}, {"x": 0.5163593292236328, "y": 0.45626577734947205}], "text": "where p\n"}
{"page": 7, "bbox": [{"x": 0.6085663437843323, "y": 0.4478553533554077}, {"x": 0.6085663437843323, "y": 0.4520605504512787}, {"x": 0.5978584289550781, "y": 0.4520605504512787}, {"x": 0.5978584289550781, "y": 0.4478553533554077}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.730517566204071, "y": 0.4516400396823883}, {"x": 0.7382510304450989, "y": 0.4516400396823883}, {"x": 0.7382510304450989, "y": 0.45794785022735596}, {"x": 0.730517566204071, "y": 0.45794785022735596}], "text": "k\n"}
{"page": 7, "bbox": [{"x": 0.5139797925949097, "y": 0.4390243887901306}, {"x": 0.8845925331115723, "y": 0.4377628266811371}, {"x": 0.8851873874664307, "y": 0.535744309425354}, {"x": 0.5145746469497681, "y": 0.5370059013366699}], "text": "[P 1,..., P[CLS)]] = Rhxk and\nW start, Wend, Wselected Є Rh are learnable vectors.\nWe compute a span score of the s-th to t-th words\nfrom the i-th passage as Pstart, i (s) × Pend,i(t), and\na passage selection score of the i-th passage as\nPselected (i).\n"}
{"page": 7, "bbox": [{"x": 0.578822135925293, "y": 0.6097561120986938}, {"x": 0.578822135925293, "y": 0.613540768623352}, {"x": 0.5681142210960388, "y": 0.613540768623352}, {"x": 0.5681142210960388, "y": 0.6097561120986938}], "text": "=\n"}
{"page": 7, "bbox": [{"x": 0.1207614541053772, "y": 0.6122792363166809}, {"x": 0.449137419462204, "y": 0.6122792363166809}, {"x": 0.449137419462204, "y": 0.6248948574066162}, {"x": 0.1207614541053772, "y": 0.6248948574066162}], "text": "6 Experiments: Question Answering\n"}
{"page": 7, "bbox": [{"x": 0.12016656994819641, "y": 0.6396130919456482}, {"x": 0.4878048896789551, "y": 0.6396130919456482}, {"x": 0.4878048896789551, "y": 0.6673675179481506}, {"x": 0.12016656994819641, "y": 0.6673675179481506}], "text": "In this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n"}
{"page": 7, "bbox": [{"x": 0.5133848786354065, "y": 0.5416316390037537}, {"x": 0.8857823014259338, "y": 0.5407905578613281}, {"x": 0.8869720697402954, "y": 0.809924304485321}, {"x": 0.5145746469497681, "y": 0.8107653260231018}], "text": "During training, we sample one positive and\nm 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. m is a hyper-parameter and we\nuse m\n24 in all the experiments. The training ob-\njective is to maximize the marginal log-likelihood\nof all the correct answer spans in the positive pas-\nsage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\nof the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQUAD)\nand 4 for small (TREC, WQ) datasets, and tune k\non the development set. For experiments on small\ndatasets under the Multi setting, in which using\nother datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n"}
{"page": 7, "bbox": [{"x": 0.11838191747665405, "y": 0.6816652417182922}, {"x": 0.4913741946220398, "y": 0.6825063228607178}, {"x": 0.490779310464859, "y": 0.9095878601074219}, {"x": 0.11778703331947327, "y": 0.9087468385696411}], "text": "6.1 End-to-end QA System\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA sys-\ntem consists of a neural reader that outputs the\nanswer to the question. Given the top k retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth =200, search depth = 128.\n"}
{"page": 7, "bbox": [{"x": 0.5163593292236328, "y": 0.8275862336158752}, {"x": 0.6145151853561401, "y": 0.8275862336158752}, {"x": 0.6145151853561401, "y": 0.8359966278076172}, {"x": 0.5163593292236328, "y": 0.8359966278076172}], "text": "6.2 Results\n"}
{"page": 7, "bbox": [{"x": 0.5151695609092712, "y": 0.8486123085021973}, {"x": 0.8857823014259338, "y": 0.8486123085021973}, {"x": 0.8857823014259338, "y": 0.9079058170318604}, {"x": 0.5151695609092712, "y": 0.9079058170318604}], "text": "Table 4 summarizes our final end-to-end QA re-\nsults, measured by exact match with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n"}
{"page": 7, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5199286341667175, "y": 0.9243061542510986}, {"x": 0.5199286341667175, "y": 0.9331371188163757}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6775\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.08158116042613983}, {"x": 0.2986317574977875, "y": 0.07989907264709473}, {"x": 0.2986317574977875, "y": 0.09125315397977829}, {"x": 0.1641879826784134, "y": 0.0929352417588234}], "text": "Training Model\n"}
{"page": 8, "bbox": [{"x": 0.5229030251502991, "y": 0.08116064220666885}, {"x": 0.8393813371658325, "y": 0.08116064220666885}, {"x": 0.8393813371658325, "y": 0.09251471608877182}, {"x": 0.5229030251502991, "y": 0.09251471608877182}], "text": "NQ TriviaQA WQ TREC SQUAD\n"}
{"page": 8, "bbox": [{"x": 0.6627007722854614, "y": 0.10470984131097794}, {"x": 0.7519333958625793, "y": 0.10470984131097794}, {"x": 0.7519333958625793, "y": 0.11269974708557129}, {"x": 0.6627007722854614, "y": 0.11269974708557129}], "text": "17.7 21.3\n"}
{"page": 8, "bbox": [{"x": 0.7935752272605896, "y": 0.10428931564092636}, {"x": 0.8239143490791321, "y": 0.10470984131097794}, {"x": 0.8239143490791321, "y": 0.11312027275562286}, {"x": 0.7935752272605896, "y": 0.11269974708557129}], "text": "33.2\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.10470984131097794}, {"x": 0.55205237865448, "y": 0.10513035953044891}, {"x": 0.55205237865448, "y": 0.11312027275562286}, {"x": 0.5205234885215759, "y": 0.11269974708557129}], "text": "26.5\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.10302775353193283}, {"x": 0.4818560481071472, "y": 0.10428931564092636}, {"x": 0.4818560481071472, "y": 0.11522287875413895}, {"x": 0.24866151809692383, "y": 0.11396130919456482}], "text": "BM25+BERT (Lee et al., 2019)\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.10513035953044891}, {"x": 0.622248649597168, "y": 0.10513035953044891}, {"x": 0.622248649597168, "y": 0.11312027275562286}, {"x": 0.5907198190689087, "y": 0.11312027275562286}], "text": "47.1\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.10428931564092636}, {"x": 0.20999404788017273, "y": 0.10428931564092636}, {"x": 0.20999404788017273, "y": 0.11480235308408737}, {"x": 0.1641879826784134, "y": 0.11480235308408737}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.11984860897064209}, {"x": 0.55205237865448, "y": 0.11942809075117111}, {"x": 0.5526472330093384, "y": 0.12825904786586761}, {"x": 0.5217132568359375, "y": 0.128679558634758}], "text": "33.3\n"}
{"page": 8, "bbox": [{"x": 0.5901249051094055, "y": 0.12068965286016464}, {"x": 0.622248649597168, "y": 0.12068965286016464}, {"x": 0.622248649597168, "y": 0.128679558634758}, {"x": 0.5901249051094055, "y": 0.128679558634758}], "text": "45.0\n"}
{"page": 8, "bbox": [{"x": 0.662105917930603, "y": 0.12068965286016464}, {"x": 0.7501487135887146, "y": 0.12068965286016464}, {"x": 0.7501487135887146, "y": 0.128679558634758}, {"x": 0.662105917930603, "y": 0.128679558634758}], "text": "36.4 30.1\n"}
{"page": 8, "bbox": [{"x": 0.792385458946228, "y": 0.12068965286016464}, {"x": 0.8245092034339905, "y": 0.12068965286016464}, {"x": 0.8245092034339905, "y": 0.128679558634758}, {"x": 0.792385458946228, "y": 0.128679558634758}], "text": "20.2\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.12026913464069366}, {"x": 0.4306960105895996, "y": 0.12026913464069366}, {"x": 0.4306960105895996, "y": 0.1303616464138031}, {"x": 0.24866151809692383, "y": 0.1303616464138031}], "text": "ORQA (Lee et al., 2019)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.11984860897064209}, {"x": 0.21058893203735352, "y": 0.12026913464069366}, {"x": 0.21058893203735352, "y": 0.13120269775390625}, {"x": 0.1641879826784134, "y": 0.13078217208385468}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.13624894618988037}, {"x": 0.55205237865448, "y": 0.13666947185993195}, {"x": 0.55205237865448, "y": 0.14550042152404785}, {"x": 0.5211184024810791, "y": 0.14507989585399628}], "text": "28.1\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.13708999752998352}, {"x": 0.6216537952423096, "y": 0.1375105082988739}, {"x": 0.6216537952423096, "y": 0.14507989585399628}, {"x": 0.5913146734237671, "y": 0.1446593701839447}], "text": "50.9\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.13666947185993195}, {"x": 0.45449137687683105, "y": 0.13666947185993195}, {"x": 0.45449137687683105, "y": 0.14676198363304138}, {"x": 0.24866151809692383, "y": 0.14676198363304138}], "text": "HardEM (Min et al., 2019a)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.13708999752998352}, {"x": 0.21058893203735352, "y": 0.13708999752998352}, {"x": 0.21058893203735352, "y": 0.1480235457420349}, {"x": 0.1641879826784134, "y": 0.1480235457420349}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.15096719563007355}, {"x": 0.55205237865448, "y": 0.15096719563007355}, {"x": 0.55205237865448, "y": 0.1627417951822281}, {"x": 0.5205234885215759, "y": 0.1627417951822281}], "text": "34.5\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.15306980907917023}, {"x": 0.6234384179115295, "y": 0.15306980907917023}, {"x": 0.6234384179115295, "y": 0.16105970740318298}, {"x": 0.5913146734237671, "y": 0.16105970740318298}], "text": "56.0\n"}
{"page": 8, "bbox": [{"x": 0.662105917930603, "y": 0.15306980907917023}, {"x": 0.6930398344993591, "y": 0.15306980907917023}, {"x": 0.6930398344993591, "y": 0.16105970740318298}, {"x": 0.662105917930603, "y": 0.16105970740318298}], "text": "36.4\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.1518082469701767}, {"x": 0.5038667321205139, "y": 0.15096719563007355}, {"x": 0.5038667321205139, "y": 0.16316232085227966}, {"x": 0.24866151809692383, "y": 0.16400335729122162}], "text": "GraphRetriever (Min et al., 2019b)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.15264928340911865}, {"x": 0.20999404788017273, "y": 0.15264928340911865}, {"x": 0.20999404788017273, "y": 0.16316232085227966}, {"x": 0.1641879826784134, "y": 0.16316232085227966}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.16904962062835693}, {"x": 0.5526472330093384, "y": 0.16904962062835693}, {"x": 0.5526472330093384, "y": 0.17703953385353088}, {"x": 0.5211184024810791, "y": 0.17703953385353088}], "text": "32.6\n"}
{"page": 8, "bbox": [{"x": 0.7917906045913696, "y": 0.16904962062835693}, {"x": 0.8251041173934937, "y": 0.16904962062835693}, {"x": 0.8251041173934937, "y": 0.17746004462242126}, {"x": 0.7917906045913696, "y": 0.17746004462242126}], "text": "56.5\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.16736753284931183}, {"x": 0.48423558473587036, "y": 0.16820858418941498}, {"x": 0.48423558473587036, "y": 0.17956265807151794}, {"x": 0.24866151809692383, "y": 0.178721621632576}], "text": "PathRetriever (Asai et al., 2020)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.16820858418941498}, {"x": 0.21058893203735352, "y": 0.16862909495830536}, {"x": 0.21058893203735352, "y": 0.17956265807151794}, {"x": 0.1641879826784134, "y": 0.17914213240146637}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.18460892140865326}, {"x": 0.7507436275482178, "y": 0.18418839573860168}, {"x": 0.7507436275482178, "y": 0.19259881973266602}, {"x": 0.6609161496162415, "y": 0.1930193454027176}], "text": "40.2 46.8\n"}
{"page": 8, "bbox": [{"x": 0.5217132568359375, "y": 0.18418839573860168}, {"x": 0.55205237865448, "y": 0.18460892140865326}, {"x": 0.5514574646949768, "y": 0.19343987107276917}, {"x": 0.5211184024810791, "y": 0.1930193454027176}], "text": "39.2\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.18460892140865326}, {"x": 0.4747174382209778, "y": 0.18460892140865326}, {"x": 0.4747174382209778, "y": 0.19512194395065308}, {"x": 0.24866151809692383, "y": 0.19512194395065308}], "text": "REALMWiki (Guu et al., 2020)\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.18460892140865326}, {"x": 0.20939916372299194, "y": 0.18460892140865326}, {"x": 0.20939916372299194, "y": 0.19596299529075623}, {"x": 0.1641879826784134, "y": 0.19596299529075623}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.6615110039710999, "y": 0.20100925862789154}, {"x": 0.6918500661849976, "y": 0.20058873295783997}, {"x": 0.6918500661849976, "y": 0.2089991569519043}, {"x": 0.6615110039710999, "y": 0.20941968262195587}], "text": "40.7\n"}
{"page": 8, "bbox": [{"x": 0.7192147374153137, "y": 0.20100925862789154}, {"x": 0.7501487135887146, "y": 0.20100925862789154}, {"x": 0.7501487135887146, "y": 0.20941968262195587}, {"x": 0.7192147374153137, "y": 0.20941968262195587}], "text": "42.9\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.20142976939678192}, {"x": 0.5526472330093384, "y": 0.20142976939678192}, {"x": 0.5526472330093384, "y": 0.20941968262195587}, {"x": 0.5205234885215759, "y": 0.20941968262195587}], "text": "40.4\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.20100925862789154}, {"x": 0.20999404788017273, "y": 0.20100925862789154}, {"x": 0.20999404788017273, "y": 0.21152228116989136}, {"x": 0.1641879826784134, "y": 0.21152228116989136}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.24806663393974304, "y": 0.20100925862789154}, {"x": 0.4782867431640625, "y": 0.20100925862789154}, {"x": 0.4782867431640625, "y": 0.21152228116989136}, {"x": 0.24806663393974304, "y": 0.21152228116989136}], "text": "REALMNews (Guu et al., 2020)\n"}
{"page": 8, "bbox": [{"x": 0.6044021248817444, "y": 0.20605550706386566}, {"x": 0.6091611981391907, "y": 0.20605550706386566}, {"x": 0.6091611981391907, "y": 0.20773759484291077}, {"x": 0.6044021248817444, "y": 0.20773759484291077}], "text": "-\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.22371740639209747}, {"x": 0.6228435635566711, "y": 0.2232968807220459}, {"x": 0.6228435635566711, "y": 0.23128679394721985}, {"x": 0.5913146734237671, "y": 0.23170731961727142}], "text": "52.4\n"}
{"page": 8, "bbox": [{"x": 0.24925640225410461, "y": 0.22287636995315552}, {"x": 0.29506245255470276, "y": 0.2232968807220459}, {"x": 0.29506245255470276, "y": 0.232127845287323}, {"x": 0.24925640225410461, "y": 0.23170731961727142}], "text": "BM25\n"}
{"page": 8, "bbox": [{"x": 0.7929803729057312, "y": 0.2232968807220459}, {"x": 0.8227245807647705, "y": 0.22287636995315552}, {"x": 0.8227245807647705, "y": 0.23170731961727142}, {"x": 0.7929803729057312, "y": 0.232127845287323}], "text": "38.1\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.22371740639209747}, {"x": 0.5538370013237, "y": 0.22371740639209747}, {"x": 0.5538370013237, "y": 0.23170731961727142}, {"x": 0.5211184024810791, "y": 0.23170731961727142}], "text": "32.6\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.22371740639209747}, {"x": 0.7513384819030762, "y": 0.22371740639209747}, {"x": 0.7513384819030762, "y": 0.23170731961727142}, {"x": 0.6609161496162415, "y": 0.23170731961727142}], "text": "29.9 24.9\n"}
{"page": 8, "bbox": [{"x": 0.662105917930603, "y": 0.23717409372329712}, {"x": 0.7513384819030762, "y": 0.23969721794128418}, {"x": 0.7507436275482178, "y": 0.2502102553844452}, {"x": 0.6615110039710999, "y": 0.24768713116645813}], "text": "34.6 25.9\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.24011774361133575}, {"x": 0.28316476941108704, "y": 0.24011774361133575}, {"x": 0.28316476941108704, "y": 0.2481076568365097}, {"x": 0.24866151809692383, "y": 0.2481076568365097}], "text": "DPR\n"}
{"page": 8, "bbox": [{"x": 0.5205234885215759, "y": 0.24011774361133575}, {"x": 0.5526472330093384, "y": 0.24011774361133575}, {"x": 0.5526472330093384, "y": 0.2481076568365097}, {"x": 0.5205234885215759, "y": 0.2481076568365097}], "text": "41.5\n"}
{"page": 8, "bbox": [{"x": 0.792385458946228, "y": 0.24011774361133575}, {"x": 0.8233194351196289, "y": 0.24011774361133575}, {"x": 0.8233194351196289, "y": 0.2481076568365097}, {"x": 0.792385458946228, "y": 0.2481076568365097}], "text": "29.8\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.24011774361133575}, {"x": 0.6216537952423096, "y": 0.24011774361133575}, {"x": 0.6216537952423096, "y": 0.24852816760540009}, {"x": 0.5907198190689087, "y": 0.24852816760540009}], "text": "56.8\n"}
{"page": 8, "bbox": [{"x": 0.1641879826784134, "y": 0.2392767071723938}, {"x": 0.21058893203735352, "y": 0.23969721794128418}, {"x": 0.21058893203735352, "y": 0.25105130672454834}, {"x": 0.1641879826784134, "y": 0.25063079595565796}], "text": "Single\n"}
{"page": 8, "bbox": [{"x": 0.7929803729057312, "y": 0.2552565038204193}, {"x": 0.8245092034339905, "y": 0.25483599305152893}, {"x": 0.8245092034339905, "y": 0.26366695761680603}, {"x": 0.7929803729057312, "y": 0.2640874683856964}], "text": "36.7\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.25609755516052246}, {"x": 0.5532421469688416, "y": 0.25609755516052246}, {"x": 0.5532421469688416, "y": 0.2640874683856964}, {"x": 0.5211184024810791, "y": 0.2640874683856964}], "text": "39.0\n"}
{"page": 8, "bbox": [{"x": 0.5913146734237671, "y": 0.25609755516052246}, {"x": 0.6228435635566711, "y": 0.25609755516052246}, {"x": 0.6228435635566711, "y": 0.2640874683856964}, {"x": 0.5913146734237671, "y": 0.2640874683856964}], "text": "57.0\n"}
{"page": 8, "bbox": [{"x": 0.6615110039710999, "y": 0.2556770443916321}, {"x": 0.7513384819030762, "y": 0.2556770443916321}, {"x": 0.7513384819030762, "y": 0.2645079791545868}, {"x": 0.6615110039710999, "y": 0.2645079791545868}], "text": "35.2 28.0\n"}
{"page": 8, "bbox": [{"x": 0.24925640225410461, "y": 0.25609755516052246}, {"x": 0.3408685326576233, "y": 0.25609755516052246}, {"x": 0.3408685326576233, "y": 0.2645079791545868}, {"x": 0.24925640225410461, "y": 0.2645079791545868}], "text": "BM25+DPR\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.278385192155838}, {"x": 0.28197500109672546, "y": 0.27754414081573486}, {"x": 0.28256988525390625, "y": 0.28679561614990234}, {"x": 0.24925640225410461, "y": 0.2876366674900055}], "text": "DPR\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.278385192155838}, {"x": 0.6216537952423096, "y": 0.278385192155838}, {"x": 0.6216537952423096, "y": 0.28679561614990234}, {"x": 0.5907198190689087, "y": 0.28679561614990234}], "text": "56.8\n"}
{"page": 8, "bbox": [{"x": 0.7192147374153137, "y": 0.2788057327270508}, {"x": 0.7513384819030762, "y": 0.278385192155838}, {"x": 0.7513384819030762, "y": 0.28637510538101196}, {"x": 0.7192147374153137, "y": 0.28679561614990234}], "text": "49.4\n"}
{"page": 8, "bbox": [{"x": 0.5199286341667175, "y": 0.278385192155838}, {"x": 0.55205237865448, "y": 0.278385192155838}, {"x": 0.55205237865448, "y": 0.2872161567211151}, {"x": 0.5199286341667175, "y": 0.2872161567211151}], "text": "41.5\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.2788057327270508}, {"x": 0.6936347484588623, "y": 0.2788057327270508}, {"x": 0.6936347484588623, "y": 0.28679561614990234}, {"x": 0.6609161496162415, "y": 0.28679561614990234}], "text": "42.4\n"}
{"page": 8, "bbox": [{"x": 0.792385458946228, "y": 0.2788057327270508}, {"x": 0.8239143490791321, "y": 0.2788057327270508}, {"x": 0.8239143490791321, "y": 0.2872161567211151}, {"x": 0.792385458946228, "y": 0.2872161567211151}], "text": "24.1\n"}
{"page": 8, "bbox": [{"x": 0.1635930985212326, "y": 0.28637510538101196}, {"x": 0.20404520630836487, "y": 0.2855340540409088}, {"x": 0.20464009046554565, "y": 0.2943650186061859}, {"x": 0.1641879826784134, "y": 0.29520606994628906}], "text": "Multi\n"}
{"page": 8, "bbox": [{"x": 0.5211184024810791, "y": 0.2943650186061859}, {"x": 0.5514574646949768, "y": 0.29394447803497314}, {"x": 0.5514574646949768, "y": 0.30235493183135986}, {"x": 0.5211184024810791, "y": 0.30277544260025024}], "text": "38.8\n"}
{"page": 8, "bbox": [{"x": 0.5907198190689087, "y": 0.2947855293750763}, {"x": 0.622248649597168, "y": 0.2947855293750763}, {"x": 0.622248649597168, "y": 0.30277544260025024}, {"x": 0.5907198190689087, "y": 0.30277544260025024}], "text": "57.9\n"}
{"page": 8, "bbox": [{"x": 0.6609161496162415, "y": 0.29394447803497314}, {"x": 0.7531231641769409, "y": 0.2943650186061859}, {"x": 0.7531231641769409, "y": 0.3036164939403534}, {"x": 0.6609161496162415, "y": 0.3031959533691406}], "text": "41.1 50.6\n"}
{"page": 8, "bbox": [{"x": 0.7929803729057312, "y": 0.2943650186061859}, {"x": 0.8233194351196289, "y": 0.2947855293750763}, {"x": 0.8233194351196289, "y": 0.3031959533691406}, {"x": 0.7929803729057312, "y": 0.30277544260025024}], "text": "35.8\n"}
{"page": 8, "bbox": [{"x": 0.24866151809692383, "y": 0.2947855293750763}, {"x": 0.34205830097198486, "y": 0.2947855293750763}, {"x": 0.34205830097198486, "y": 0.3031959533691406}, {"x": 0.24866151809692383, "y": 0.3031959533691406}], "text": "BM25+DPR\n"}
{"page": 8, "bbox": [{"x": 0.12016656994819641, "y": 0.3246425688266754}, {"x": 0.8857823014259338, "y": 0.3246425688266754}, {"x": 0.8857823014259338, "y": 0.37804877758026123}, {"x": 0.12016656994819641, "y": 0.37804877758026123}], "text": "Table 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers.\nREALMwiki and REALM News are the same model but pretrained on Wikipedia and CC-News, respectively. Single\nand Multi denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets\n(all except SQUAD). For WQ and TREC in the Multi setting, we fine-tune the reader trained on NQ.\n"}
{"page": 8, "bbox": [{"x": 0.5151695609092712, "y": 0.40790581703186035}, {"x": 0.8863771557807922, "y": 0.40790581703186035}, {"x": 0.8863771557807922, "y": 0.7737594842910767}, {"x": 0.5151695609092712, "y": 0.7737594842910767}], "text": "trained, following Lee et al. (2019). This approach\nobtains a score of 39.8 EM, which suggests that our\nstrategy of training a strong retriever and reader in\nisolation can leverage effectively available supervi-\nsion, while outperforming a comparable joint train-\ning approach with a simpler design (Appendix D).\nOne thing worth noticing is that our reader does\nconsider more passages compared to ORQA, al-\nthough it is not completely clear how much more\ntime it takes for inference. While DPR processes\nup to 100 passages for each question, the reader\nis able to fit all of them into one batch on a sin-\ngle 32GB GPU, thus the latency remains almost\nidentical to the single passage case (around 20ms).\nThe exact impact on throughput is harder to mea-\nsure: ORQA uses 2-3x longer passages compared\nto DPR (288 word pieces compared to our 100\ntokens) and the computational complexity is super-\nlinear in passage length. We also note that we\nfound k 50 to be optimal for NQ, and k\nleads to only marginal loss in exact match accu-\nracy (40.8 vs. 41.5 EM on NQ), which should be\nroughly comparable to ORQA's 5-passage setup.\n"}
{"page": 8, "bbox": [{"x": 0.11897680163383484, "y": 0.40790581703186035}, {"x": 0.4913741946220398, "y": 0.4074852764606476}, {"x": 0.4919690787792206, "y": 0.9083263278007507}, {"x": 0.11957168579101562, "y": 0.9087468385696411}], "text": "see that higher retriever accuracy typically leads to\nbetter final QA results: in all cases except SQUAD,\nanswers extracted from the passages retrieved by\nDPR are more likely to be correct, compared to\nthose from BM25. For large datasets like NQ and\nTriviaQA, models trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\non smaller datasets like WQ and TREC, the multi-\ndataset setting has a clear advantage. Overall, our\nDPR-based models outperform the previous state-\nof-the-art results on four out of the five datasets,\nwith 1% to 12% absolute differences in exact match\naccuracy. It is interesting to contrast our results to\nthose of ORQA (Lee et al., 2019) and also the\nconcurrently developed approach, REALM (Guu\net al., 2020). While both methods include addi-\ntional pretraining tasks and employ an expensive\nend-to-end training regime, DPR manages to out-\nperform them on both NQ and TriviaQA, simply\nby focusing on learning a strong passage retrieval\nmodel using pairs of questions and answers. The\nadditional pretraining tasks are likely more useful\nonly when the target training sets are small. Al-\nthough the results of DPR on WQ and TREC in the\nsingle-dataset setting are less competitive, adding\nmore question-answer pairs helps boost the perfor-\nmance, achieving the new state of the art.\nTo compare our pipeline training approach with\njoint learning, we run an ablation on Natural Ques-\ntions where the retriever and reader are jointly\n"}
{"page": 8, "bbox": [{"x": 0.8441404104232788, "y": 0.714886486530304}, {"x": 0.8828078508377075, "y": 0.714886486530304}, {"x": 0.8828078508377075, "y": 0.7224558591842651}, {"x": 0.8441404104232788, "y": 0.7224558591842651}], "text": "= 10\n"}
{"page": 8, "bbox": [{"x": 0.5157644152641296, "y": 0.7910008430480957}, {"x": 0.527067244052887, "y": 0.7910008430480957}, {"x": 0.527067244052887, "y": 0.7994112968444824}, {"x": 0.5157644152641296, "y": 0.7994112968444824}], "text": "7\n"}
{"page": 8, "bbox": [{"x": 0.5466983914375305, "y": 0.7905803322792053}, {"x": 0.6662700772285461, "y": 0.7905803322792053}, {"x": 0.6662700772285461, "y": 0.8002523183822632}, {"x": 0.5466983914375305, "y": 0.8002523183822632}], "text": "Related Work\n"}
{"page": 8, "bbox": [{"x": 0.5151695609092712, "y": 0.8145500421524048}, {"x": 0.8863771557807922, "y": 0.8149705529212952}, {"x": 0.8863771557807922, "y": 0.9087468385696411}, {"x": 0.5151695609092712, "y": 0.9083263278007507}], "text": "Passage retrieval has been an important compo-\nnent for open-domain QA (Voorhees, 1999). It\nnot only effectively reduces the search space for\nanswer extraction, but also identifies the support\ncontext for users to verify the answer. Strong sparse\nvector space models like TF-IDF or BM25 have\n"}
{"page": 8, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5211184024810791, "y": 0.9238856434822083}, {"x": 0.5211184024810791, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9331371188163757}], "text": "6776\n"}
{"page": 9, "bbox": [{"x": 0.5151695609092712, "y": 0.07527334243059158}, {"x": 0.8869720697402954, "y": 0.07569386065006256}, {"x": 0.8863771557807922, "y": 0.36164844036102295}, {"x": 0.5145746469497681, "y": 0.36122792959213257}], "text": "effective solution that shows stronger empirical per-\nformance, without relying on additional pretraining\nor complex joint training schemes.\nDPR has also been used as an important mod-\nule in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the pre-\nvious iteration to discover new negatives and con-\nstruct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raf-\nfel et al., 2019), achieving good performance on\nopen-domain QA and other knowledge-intensive\ntasks.\n"}
{"page": 9, "bbox": [{"x": 0.5163593292236328, "y": 0.37973088026046753}, {"x": 0.6418798565864563, "y": 0.37973088026046753}, {"x": 0.6418798565864563, "y": 0.3894028663635254}, {"x": 0.5163593292236328, "y": 0.3894028663635254}], "text": "8 Conclusion\n"}
{"page": 9, "bbox": [{"x": 0.11957168579101562, "y": 0.07695542275905609}, {"x": 0.4913741946220398, "y": 0.07737594842910767}, {"x": 0.4901844263076782, "y": 0.9095878601074219}, {"x": 0.11838191747665405, "y": 0.9091673493385315}], "text": "been used as the standard method applied broadly\nto various QA tasks (e.g., Chen et al., 2017; Yang\net al., 2019a,b; Nie et al., 2019; Min et al., 2019a;\nWolfson et al., 2020). Augmenting text-based re-\ntrieval with external structured information, such\nas knowledge graph and Wikipedia hyperlinks, has\nalso been explored recently (Min et al., 2019b; Asai\net al., 2020).\nThe use of dense vector representations for re-\ntrieval has a long history since Latent Semantic\nAnalysis (Deerwester et al., 1990). Using labeled\npairs of queries and documents, discriminatively\ntrained dense encoders have become popular re-\ncently (Yih et al., 2011; Huang et al., 2013; Gillick\net al., 2019), with applications to cross-lingual\ndocument retrieval, ad relevance prediction, Web\nsearch and entity retrieval. Such approaches com-\nplement the sparse vector methods as they can po-\ntentially give high similarity scores to semantically\nrelevant text pairs, even without exact token match-\ning. The dense representation alone, however, is\ntypically inferior to the sparse one. While not the\nfocus of this work, dense representations from pre-\ntrained models, along with cross-attention mecha-\nnisms, have also been shown effective in passage\nor dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\nof the BERT encoders.\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to re-\ntrieve relevant passages iteratively using reformu-\nlated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) pro-\npose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input ques-\ntions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\noutperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages dur-\ning training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n"}
{"page": 9, "bbox": [{"x": 0.5145746469497681, "y": 0.4053826630115509}, {"x": 0.8857823014259338, "y": 0.4049621522426605}, {"x": 0.8863771557807922, "y": 0.611017644405365}, {"x": 0.5151695609092712, "y": 0.6114381551742554}], "text": "In this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the tradi-\ntional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\nour empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional val-\nues. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\nopen-domain question answering benchmarks.\n"}
{"page": 9, "bbox": [{"x": 0.5151695609092712, "y": 0.6257359385490417}, {"x": 0.674003541469574, "y": 0.6257359385490417}, {"x": 0.674003541469574, "y": 0.6375105381011963}, {"x": 0.5151695609092712, "y": 0.6375105381011963}], "text": "Acknowledgments\n"}
{"page": 9, "bbox": [{"x": 0.5145746469497681, "y": 0.6509671807289124}, {"x": 0.8822129964828491, "y": 0.6509671807289124}, {"x": 0.8822129964828491, "y": 0.6787216067314148}, {"x": 0.5145746469497681, "y": 0.6787216067314148}], "text": "We thank the anonymous reviewers for their helpful\ncomments and suggestions.\n"}
{"page": 9, "bbox": [{"x": 0.5163593292236328, "y": 0.7102607488632202}, {"x": 0.6091611981391907, "y": 0.7102607488632202}, {"x": 0.6091611981391907, "y": 0.7195122241973877}, {"x": 0.5163593292236328, "y": 0.7195122241973877}], "text": "References\n"}
{"page": 9, "bbox": [{"x": 0.5157644152641296, "y": 0.7312868237495422}, {"x": 0.8863771557807922, "y": 0.732127845287323}, {"x": 0.8857823014259338, "y": 0.9095878601074219}, {"x": 0.5151695609092712, "y": 0.9087468385696411}], "text": "Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over Wikipedia graph\nfor question answering. In International Conference\non Learning Representations (ICLR).\nPetr Baudiš and Jan Šedivỳ. 2015. Modeling of the\nquestion answering task in the yodaqa system. In In-\nternational Conference of the Cross-Language Eval-\nuation Forum for European Languages, pages 222-\n228. Springer.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\n"}
{"page": 9, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6777\n"}
{"page": 10, "bbox": [{"x": 0.13860797882080078, "y": 0.0786375105381012}, {"x": 0.48958954215049744, "y": 0.07821699231863022}, {"x": 0.48958954215049744, "y": 0.10260723531246185}, {"x": 0.13860797882080078, "y": 0.10302775353193283}], "text": "question-answer pairs. In Empirical Methods in Nat-\nural Language Processing (EMNLP).\n"}
{"page": 10, "bbox": [{"x": 0.5324211716651917, "y": 0.07947855442762375}, {"x": 0.8857823014259338, "y": 0.07821699231863022}, {"x": 0.8857823014259338, "y": 0.11522287875413895}, {"x": 0.5324211716651917, "y": 0.11648444086313248}], "text": "clickthrough data. In ACM International Confer-\nence on Information and Knowledge Management\n(CIKM), pages 2333-2338.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.1160639226436615}, {"x": 0.4901844263076782, "y": 0.11564339697360992}, {"x": 0.4901844263076782, "y": 0.16568544507026672}, {"x": 0.12016656994819641, "y": 0.1661059707403183}], "text": "Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nSäckinger, and Roopak Shah. 1994. Signature verifi-\ncation using a “Siamese” time delay neural network.\nIn NIPS, pages 737–744.\n"}
{"page": 10, "bbox": [{"x": 0.5163593292236328, "y": 0.13120269775390625}, {"x": 0.8851873874664307, "y": 0.13078217208385468}, {"x": 0.8851873874664307, "y": 0.24558451771736145}, {"x": 0.5163593292236328, "y": 0.24600504338741302}], "text": "Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In International Conference\non Learning Representations (ICLR).\nGautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. ArXiv, abs/2007.01282.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.26156434416770935}, {"x": 0.8851873874664307, "y": 0.26156434416770935}, {"x": 0.8851873874664307, "y": 0.29562658071517944}, {"x": 0.5157644152641296, "y": 0.29562658071517944}], "text": "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with GPUs. Arxiv,\nabs/1702.08734.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.17830109596252441}, {"x": 0.4913741946220398, "y": 0.17914213240146637}, {"x": 0.490779310464859, "y": 0.38099244236946106}, {"x": 0.11957168579101562, "y": 0.3801513910293579}], "text": "Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\nProceedings of the 22nd international conference on\nMachine learning, pages 89–96.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Association for Computa-\ntional Linguistics (ACL), pages 1870-1879.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nand Andrew McCallum. 2019. Multi-step retriever-\nreader interaction for scalable open-domain question\nanswering. In International Conference on Learn-\ning Representations (ICLR).\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.3128679692745209}, {"x": 0.8857823014259338, "y": 0.3124474287033081}, {"x": 0.8857823014259338, "y": 0.37594616413116455}, {"x": 0.5157644152641296, "y": 0.3763667047023773}], "text": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Association for Computational Lin-\nguistics (ACL), pages 1601–1611.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.3910849392414093}, {"x": 0.8857823014259338, "y": 0.39150547981262207}, {"x": 0.8857823014259338, "y": 0.453742653131485}, {"x": 0.5157644152641296, "y": 0.4533221125602722}], "text": "Omar Khattab and Matei Zaharia. 2020. ColBERT:\nEfficient and effective passage search via contextu-\nalized late interaction over BERT. In ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval (SIGIR), pages 39-48.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.39360806345939636}, {"x": 0.490779310464859, "y": 0.393187552690506}, {"x": 0.490779310464859, "y": 0.4550042152404785}, {"x": 0.12016656994819641, "y": 0.4554247260093689}], "text": "Scott Deerwester, Susan T Dumais, George W Fur-\nnas, Thomas K Landauer, and Richard Harshman.\n1990. Indexing by latent semantic analysis. Jour-\nnal of the American society for information science,\n41(6):391-407.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.4676198363304138}, {"x": 0.8851873874664307, "y": 0.4680403769016266}, {"x": 0.8851873874664307, "y": 0.5033641457557678}, {"x": 0.5157644152641296, "y": 0.5029436349868774}], "text": "Brian Kulis. 2013. Metric learning: A survey. Foun-\ndations and Trends in Machine Learning, 5(4):287–\n364.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.46888139843940735}, {"x": 0.4901844263076782, "y": 0.46888139843940735}, {"x": 0.4901844263076782, "y": 0.5319596529006958}, {"x": 0.12016656994819641, "y": 0.5319596529006958}], "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL).\n"}
{"page": 10, "bbox": [{"x": 0.1207614541053772, "y": 0.5437342524528503}, {"x": 0.4901844263076782, "y": 0.5449957847595215}, {"x": 0.4901844263076782, "y": 0.581581175327301}, {"x": 0.1207614541053772, "y": 0.5803195834159851}], "text": "David A Ferrucci. 2012. Introduction to \"This is Wat-\nson\". IBM Journal of Research and Development,\n56(3.4):1-1.\n"}
{"page": 10, "bbox": [{"x": 0.5163593292236328, "y": 0.5201850533485413}, {"x": 0.8851873874664307, "y": 0.5206055641174316}, {"x": 0.8851873874664307, "y": 0.6362489461898804}, {"x": 0.5163593292236328, "y": 0.63582843542099}], "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics (TACL).\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.6501261591911316}, {"x": 0.8857823014259338, "y": 0.6509671807289124}, {"x": 0.8857823014259338, "y": 0.7014297842979431}, {"x": 0.5157644152641296, "y": 0.7005887031555176}], "text": "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Association for Com-\nputational Linguistics (ACL), pages 6086-6096.\n"}
{"page": 10, "bbox": [{"x": 0.11957168579101562, "y": 0.593776285648346}, {"x": 0.4901844263076782, "y": 0.5933557748794556}, {"x": 0.4901844263076782, "y": 0.7813288569450378}, {"x": 0.11957168579101562, "y": 0.7817493677139282}], "text": "Daniel Gillick, Sayali Kulkarni, Larry Lansing,\nAlessandro Presta, Jason Baldridge, Eugene Ie, and\nDiego Garcia-Olano. 2019. Learning dense repre-\nsentations for entity retrieval. In Computational Nat-\nural Language Learning (CoNLL).\nRuiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and\nDavid Simcha. 2016. Quantization based fast inner\nproduct search. In Artificial Intelligence and Statis-\ntics, pages 482-490.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training.\nArXiv, abs/2002.08909.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.7157275080680847}, {"x": 0.8857823014259338, "y": 0.7161480188369751}, {"x": 0.8857823014259338, "y": 0.8057190775871277}, {"x": 0.5157644152641296, "y": 0.8052985668182373}], "text": "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Association for Computa-\ntional Linguistics (ACL), pages 7871-7880.\n"}
{"page": 10, "bbox": [{"x": 0.12016656994819641, "y": 0.7964676022529602}, {"x": 0.4901844263076782, "y": 0.7960470914840698}, {"x": 0.4901844263076782, "y": 0.8582842946052551}, {"x": 0.12016656994819641, "y": 0.8587048053741455}], "text": "Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nhsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Effi-\ncient natural language response suggestion for smart\nreply. ArXiv, abs/1705.00652.\n"}
{"page": 10, "bbox": [{"x": 0.5157644152641296, "y": 0.8195962905883789}, {"x": 0.8845925331115723, "y": 0.8195962905883789}, {"x": 0.8845925331115723, "y": 0.9087468385696411}, {"x": 0.5157644152641296, "y": 0.9087468385696411}], "text": "Patrick Lewis, Ethan Perez, Aleksandara Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in\nNeural Information Processing Systems (NeurIPS).\n"}
{"page": 10, "bbox": [{"x": 0.1207614541053772, "y": 0.8721615076065063}, {"x": 0.48958954215049744, "y": 0.8721615076065063}, {"x": 0.48958954215049744, "y": 0.9083263278007507}, {"x": 0.1207614541053772, "y": 0.9083263278007507}], "text": "Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for Web search using\n"}
{"page": 10, "bbox": [{"x": 0.4830458164215088, "y": 0.924726665019989}, {"x": 0.5193337202072144, "y": 0.924726665019989}, {"x": 0.5193337202072144, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6778\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.07821699231863022}, {"x": 0.8857823014259338, "y": 0.07947855442762375}, {"x": 0.8851873874664307, "y": 0.1408746838569641}, {"x": 0.5151695609092712, "y": 0.13961312174797058}], "text": "Anshumali Shrivastava and Ping Li. 2014. Asymmet-\nric LSH (ALSH) for sublinear time maximum inner\nproduct search (MIPS). In Advances in Neural In-\nformation Processing Systems (NIPS), pages 2321-\n2329.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.07905802875757217}, {"x": 0.4913741946220398, "y": 0.0786375105381012}, {"x": 0.4913741946220398, "y": 0.20311185717582703}, {"x": 0.12016656994819641, "y": 0.2035323828458786}], "text": "Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun.\n2018. Denoising distantly supervised open-domain\nquestion answering. In Association for Computa-\ntional Linguistics (ACL), pages 1736-1745.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\nproach for weakly supervised question answering.\nIn Empirical Methods in Natural Language Process-\ning (EMNLP).\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.15391084551811218}, {"x": 0.8857823014259338, "y": 0.15475189685821533}, {"x": 0.8857823014259338, "y": 0.18965516984462738}, {"x": 0.5157644152641296, "y": 0.18881413340568542}], "text": "Ellen M Voorhees. 1999. The TREC-8 question an-\nswering track report. In TREC, volume 99, pages\n77-82.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.2161480188369751}, {"x": 0.4901844263076782, "y": 0.2161480188369751}, {"x": 0.4901844263076782, "y": 0.3393608033657074}, {"x": 0.12016656994819641, "y": 0.3393608033657074}], "text": "Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\ntrieval and reading for open domain question answer-\ning. ArXiv, abs/1911.03868.\nDan Moldovan, Marius Paşca, Sanda Harabagiu, and\nMihai Surdeanu. 2003. Performance issues and er-\nror analysis in an open-domain question answering\nsystem. ACM Transactions on Information Systems\n(TOIS), 21(2):133–154.\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.20479394495487213}, {"x": 0.8857823014259338, "y": 0.20437341928482056}, {"x": 0.8863771557807922, "y": 0.35618165135383606}, {"x": 0.5163593292236328, "y": 0.35660219192504883}], "text": "Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R^3:\nReinforced ranker-reader for open-domain question\nanswering. In Conference on Artificial Intelligence\n(AAAI).\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-\npati, and Bing Xiang. 2019. Multi-passage BERT:\nA globally normalized bert model for open-domain\nquestion answering. In Empirical Methods in Natu-\nral Language Processing (EMNLP).\n"}
{"page": 11, "bbox": [{"x": 0.5163593292236328, "y": 0.3692178428173065}, {"x": 0.8857823014259338, "y": 0.36879730224609375}, {"x": 0.8857823014259338, "y": 0.4318755269050598}, {"x": 0.5163593292236328, "y": 0.4322960376739502}], "text": "Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question under-\nstanding benchmark. Transactions of the Associa-\ntion of Computational Linguistics (TACL).\n"}
{"page": 11, "bbox": [{"x": 0.1207614541053772, "y": 0.3519764542579651}, {"x": 0.4901844263076782, "y": 0.3515559434890747}, {"x": 0.4901844263076782, "y": 0.4629940986633301}, {"x": 0.1207614541053772, "y": 0.46341463923454285}], "text": "Stephen Mussmann and Stefano Ermon. 2016. Learn-\ning and inference via maximum inner product search.\nIn International Conference on Machine Learning\n(ICML), pages 2587-2596.\nYixin Nie, Songhe Wang, and Mohit Bansal. 2019. Re-\nvealing the importance of semantic retrieval for ma-\nchine reading at scale. In Empirical Methods in Nat-\nural Language Processing (EMNLP).\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.44533219933509827}, {"x": 0.8851873874664307, "y": 0.44533219933509827}, {"x": 0.8851873874664307, "y": 0.5058873295783997}, {"x": 0.5157644152641296, "y": 0.5058873295783997}], "text": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020a. Approximate nearest neighbor\nnegative contrastive learning for dense text retrieval.\nArXiv, abs/2007.00808.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.47603029012680054}, {"x": 0.4878048896789551, "y": 0.4751892387866974}, {"x": 0.4878048896789551, "y": 0.4983179271221161}, {"x": 0.12016656994819641, "y": 0.49915894865989685}], "text": "Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with BERT. ArXiv, abs/1901.04085.\n"}
{"page": 11, "bbox": [{"x": 0.1207614541053772, "y": 0.5117745995521545}, {"x": 0.4901844263076782, "y": 0.5117745995521545}, {"x": 0.4901844263076782, "y": 0.5723296999931335}, {"x": 0.1207614541053772, "y": 0.5723296999931335}], "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. ArXiv, abs/1910.10683.\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.5197644829750061}, {"x": 0.8845925331115723, "y": 0.5206055641174316}, {"x": 0.8845925331115723, "y": 0.5693860650062561}, {"x": 0.5157644152641296, "y": 0.5685449838638306}], "text": "Wenhan Xiong, Hankang Wang, and William Yang\nWang. 2020b. Progressively pretrained dense corpus\nindex for open-domain question answering. ArXiv,\nabs/2005.00038.\n"}
{"page": 11, "bbox": [{"x": 0.5151695609092712, "y": 0.5832632184028625}, {"x": 0.8857823014259338, "y": 0.5836837887763977}, {"x": 0.8857823014259338, "y": 0.6463414430618286}, {"x": 0.5151695609092712, "y": 0.6459209322929382}], "text": "Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.\nEnd-to-end open-domain question answering with\nbertserini. In North American Association for Com-\nputational Linguistics (NAACL), pages 72–77.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.5862069129943848}, {"x": 0.48899465799331665, "y": 0.5841042995452881}, {"x": 0.490779310464859, "y": 0.7216147780418396}, {"x": 0.12195122241973877, "y": 0.7237173914909363}], "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQUAD: 100,000+ questions\nfor machine comprehension of text. In Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2383-2392.\nParikshit Ram and Alexander G Gray. 2012. Maximum\ninner-product search using cone trees. In Proceed-\nings of the 18th ACM SIGKDD international con-\nference on Knowledge discovery and data mining,\npages 931-939.\n"}
{"page": 11, "bbox": [{"x": 0.5157644152641296, "y": 0.6593776345252991}, {"x": 0.8851873874664307, "y": 0.6589571237564087}, {"x": 0.8851873874664307, "y": 0.707317054271698}, {"x": 0.5157644152641296, "y": 0.7077375650405884}], "text": "Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019b. Data augmentation for\nbert fine-tuning in open-domain question answering.\nArXiv, abs/1904.06652.\n"}
{"page": 11, "bbox": [{"x": 0.5151695609092712, "y": 0.7220353484153748}, {"x": 0.8851873874664307, "y": 0.7190916538238525}, {"x": 0.8863771557807922, "y": 0.7821698784828186}, {"x": 0.5163593292236328, "y": 0.785113513469696}], "text": "Wen-tau Yih, Kristina Toutanova, John C Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In Com-\nputational Natural Language Learning (CoNLL),\npages 247-256.\n"}
{"page": 11, "bbox": [{"x": 0.1207614541053772, "y": 0.7354919910430908}, {"x": 0.490779310464859, "y": 0.7350714802742004}, {"x": 0.490779310464859, "y": 0.7720773816108704}, {"x": 0.1207614541053772, "y": 0.7724978923797607}], "text": "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? ArXiv, abs/2002.08910.\n"}
{"page": 11, "bbox": [{"x": 0.12016656994819641, "y": 0.7846930027008057}, {"x": 0.490779310464859, "y": 0.7842724919319153}, {"x": 0.490779310464859, "y": 0.9079058170318604}, {"x": 0.12016656994819641, "y": 0.9083263278007507}], "text": "Stephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333-389.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.\nReal-time open-domain question answering with\ndense-sparse phrase index. In Association for Com-\nputational Linguistics (ACL).\n"}
{"page": 11, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9243061542510986}, {"x": 0.518738865852356, "y": 0.9327165484428406}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6779\n"}
{"page": 12, "bbox": [{"x": 0.11957168579101562, "y": 0.07569386065006256}, {"x": 0.3242117762565613, "y": 0.07737594842910767}, {"x": 0.3242117762565613, "y": 0.09083263576030731}, {"x": 0.11957168579101562, "y": 0.08915054798126221}], "text": "A Distant Supervision\n"}
{"page": 12, "bbox": [{"x": 0.6157049536705017, "y": 0.08158116042613983}, {"x": 0.8744794726371765, "y": 0.08158116042613983}, {"x": 0.8744794726371765, "y": 0.0929352417588234}, {"x": 0.6157049536705017, "y": 0.0929352417588234}], "text": "Top-1 Top-5 Top-20 Top-100\n"}
{"page": 12, "bbox": [{"x": 0.6210588812828064, "y": 0.10428931564092636}, {"x": 0.7810826897621155, "y": 0.10428931564092636}, {"x": 0.7810826897621155, "y": 0.12910008430480957}, {"x": 0.6210588812828064, "y": 0.12910008430480957}], "text": "44.9 66.8 78.1\n43.9 65.3 77.1\n"}
{"page": 12, "bbox": [{"x": 0.8262938857078552, "y": 0.10470984131097794}, {"x": 0.8578227162361145, "y": 0.10470984131097794}, {"x": 0.8578227162361145, "y": 0.128679558634758}, {"x": 0.8262938857078552, "y": 0.128679558634758}], "text": "85.0\n84.4\n"}
{"page": 12, "bbox": [{"x": 0.5246877074241638, "y": 0.10428931564092636}, {"x": 0.5984532833099365, "y": 0.10513035953044891}, {"x": 0.5978584289550781, "y": 0.13162320852279663}, {"x": 0.5240927934646606, "y": 0.13078217208385468}], "text": "Gold\nDist. Sup.\n"}
{"page": 12, "bbox": [{"x": 0.5157644152641296, "y": 0.14928510785102844}, {"x": 0.8851873874664307, "y": 0.1501261591911316}, {"x": 0.8851873874664307, "y": 0.20479394495487213}, {"x": 0.5157644152641296, "y": 0.20395290851593018}], "text": "Table 5: Retrieval accuracy on the development set of\nNatural Questions, trained on passages that match the\ngold context (Gold) or the top BM25 passage that con-\ntains the answer (Dist. Sup.).\n"}
{"page": 12, "bbox": [{"x": 0.11957168579101562, "y": 0.10555088520050049}, {"x": 0.4913741946220398, "y": 0.10555088520050049}, {"x": 0.4913741946220398, "y": 0.3263246417045593}, {"x": 0.11957168579101562, "y": 0.3263246417045593}], "text": "When training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion and answer pairs, it is thus interesting\nto see when using the passages that contain the\nanswers as positives (i.e., the distant supervision\nsetting), whether there is a significant performance\ndegradation. Using the question and answer to-\ngether as the query, we run Lucene-BM25 and pick\nthe top passage that contains the answer as the pos-\nitive passage. Table 5 shows the performance of\nDPR when trained using the original setting and\nthe distant supervision setting.\n"}
{"page": 12, "bbox": [{"x": 0.5252825617790222, "y": 0.2283431440591812}, {"x": 0.6044021248817444, "y": 0.2287636697292328}, {"x": 0.6044021248817444, "y": 0.23717409372329712}, {"x": 0.5252825617790222, "y": 0.23675356805324554}], "text": "Sim Loss\n"}
{"page": 12, "bbox": [{"x": 0.6781677603721619, "y": 0.22708158195018768}, {"x": 0.8292682766914368, "y": 0.2287636697292328}, {"x": 0.8292682766914368, "y": 0.23885618150234222}, {"x": 0.6781677603721619, "y": 0.23717409372329712}], "text": "Retrieval Accuracy\n"}
{"page": 12, "bbox": [{"x": 0.5687090754508972, "y": 0.24432295560836792}, {"x": 0.8744794726371765, "y": 0.24516400694847107}, {"x": 0.8744794726371765, "y": 0.2771236300468445}, {"x": 0.5687090754508972, "y": 0.27628257870674133}], "text": "Top-1 Top-5 Top-20 Top-100\nNLL 44.9 66.8 78.1\n"}
{"page": 12, "bbox": [{"x": 0.8262938857078552, "y": 0.26703113317489624}, {"x": 0.8584176301956177, "y": 0.26661059260368347}, {"x": 0.8584176301956177, "y": 0.27544155716896057}, {"x": 0.8262938857078552, "y": 0.27586206793785095}], "text": "85.0\n"}
{"page": 12, "bbox": [{"x": 0.5252825617790222, "y": 0.27544155716896057}, {"x": 0.5490779280662537, "y": 0.27544155716896057}, {"x": 0.5490779280662537, "y": 0.28343144059181213}, {"x": 0.5252825617790222, "y": 0.28343144059181213}], "text": "DP\n"}
{"page": 12, "bbox": [{"x": 0.8262938857078552, "y": 0.28301092982292175}, {"x": 0.8572278618812561, "y": 0.2821698784828186}, {"x": 0.8578227162361145, "y": 0.2910008430480957}, {"x": 0.8268887400627136, "y": 0.29184189438819885}], "text": "84.5\n"}
{"page": 12, "bbox": [{"x": 0.6942296028137207, "y": 0.28343144059181213}, {"x": 0.7876263856887817, "y": 0.28343144059181213}, {"x": 0.7876263856887817, "y": 0.29184189438819885}, {"x": 0.6942296028137207, "y": 0.29184189438819885}], "text": "65.0 77.2\n"}
{"page": 12, "bbox": [{"x": 0.5681142210960388, "y": 0.28259041905403137}, {"x": 0.7864366173744202, "y": 0.2809083163738251}, {"x": 0.7870315313339233, "y": 0.3317914307117462}, {"x": 0.5687090754508972, "y": 0.3334735035896301}], "text": "Triplet 41.6\nNLL 43.5 64.7\nTriplet 42.2 66.0 78.1\n"}
{"page": 12, "bbox": [{"x": 0.8274836540222168, "y": 0.3052985668182373}, {"x": 0.8572278618812561, "y": 0.3057190775871277}, {"x": 0.8572278618812561, "y": 0.31497055292129517}, {"x": 0.8274836540222168, "y": 0.3145500421524048}], "text": "83.1\n"}
{"page": 12, "bbox": [{"x": 0.7566924691200256, "y": 0.30613961815834045}, {"x": 0.7858417630195618, "y": 0.30613961815834045}, {"x": 0.7858417630195618, "y": 0.3145500421524048}, {"x": 0.7566924691200256, "y": 0.3145500421524048}], "text": "76.1\n"}
{"page": 12, "bbox": [{"x": 0.5246877074241638, "y": 0.31370899081230164}, {"x": 0.545508623123169, "y": 0.31370899081230164}, {"x": 0.545508623123169, "y": 0.32253995537757874}, {"x": 0.5246877074241638, "y": 0.32253995537757874}], "text": "L2\n"}
{"page": 12, "bbox": [{"x": 0.8262938857078552, "y": 0.32211941480636597}, {"x": 0.8572278618812561, "y": 0.3216989040374756}, {"x": 0.8572278618812561, "y": 0.32968881726264954}, {"x": 0.8262938857078552, "y": 0.3301093280315399}], "text": "84.9\n"}
{"page": 12, "bbox": [{"x": 0.12016656994819641, "y": 0.34398654103279114}, {"x": 0.4521118402481079, "y": 0.3406223654747009}, {"x": 0.4527067244052887, "y": 0.37047940492630005}, {"x": 0.1207614541053772, "y": 0.37384358048439026}], "text": "B Alternative Similarity Functions &\nTriplet Loss\n"}
{"page": 12, "bbox": [{"x": 0.5157644152641296, "y": 0.35071489214897156}, {"x": 0.8834027647972107, "y": 0.35113540291786194}, {"x": 0.8834027647972107, "y": 0.3894028663635254}, {"x": 0.5157644152641296, "y": 0.3889823257923126}], "text": "Table 6: Retrieval Top-k accuracy on the development\nset of Natural Questions using different similarity and\nloss functions.\n"}
{"page": 12, "bbox": [{"x": 0.5151695609092712, "y": 0.4213624894618988}, {"x": 0.8851873874664307, "y": 0.4217830002307892}, {"x": 0.8851873874664307, "y": 0.5142977237701416}, {"x": 0.5151695609092712, "y": 0.5138772130012512}], "text": "the correct answer, presumably by matching \"body\nof water\" with semantic neighbors such as sea and\nchannel, even though no lexical overlap exists. The\nsecond example is one where BM25 does better.\nThe salient phrase \"Thoros of Myr” is critical, and\nDPR is unable to capture it.\n"}
{"page": 12, "bbox": [{"x": 0.12016656994819641, "y": 0.38645920157432556}, {"x": 0.49256396293640137, "y": 0.38687974214553833}, {"x": 0.4919690787792206, "y": 0.656854510307312}, {"x": 0.11957168579101562, "y": 0.6564339995384216}], "text": "In addition to dot product (DP) and negative log-\nlikelihood based on softmax (NLL), we also exper-\niment with Euclidean distance (L2) and the triplet\nloss. We negate L2 similarity scores before ap-\nplying softmax and change signs of question-to-\npositive and question-to-negative similarities when\napplying the triplet loss on dot product scores. The\nmargin value of the triplet loss is set to 1. Ta-\nble 6 summarizes the results. All these additional\nexperiments are conducted using the same hyper-\nparameters tuned for the baseline (DP, NLL).\nNote that the retrieval accuracy for our \"baseline\"\nsettings reported in Table 5 (Gold) and Table 6\n(DP, NLL) is slightly better than those reported in\nTable 3. This is due to a better hyper-parameter\nsetting used in these analysis experiments, which\nis documented in our code release.\n"}
{"page": 12, "bbox": [{"x": 0.5157644152641296, "y": 0.5306980609893799}, {"x": 0.817370593547821, "y": 0.5298570394515991}, {"x": 0.817370593547821, "y": 0.556770384311676}, {"x": 0.5157644152641296, "y": 0.5576114654541016}], "text": "D Joint Training of Retriever and\nReader\n"}
{"page": 12, "bbox": [{"x": 0.5139797925949097, "y": 0.5756938457489014}, {"x": 0.8857823014259338, "y": 0.5756938457489014}, {"x": 0.8857823014259338, "y": 0.910008430480957}, {"x": 0.5139797925949097, "y": 0.910008430480957}], "text": "We fix the passage encoder in our joint-training\nscheme while allowing only the question encoder\nto receive backpropagation signal from the com-\nbined (retriever + reader) loss function. This allows\nus to leverage the HNSW-based FAISS index for\nefficient low-latency retrieving, without reindexing\nthe passages during model updates. Our loss func-\ntion largely follows ORQA's approach, which uses\nlog probabilities of positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages'\nvectors participate in the loss calculation for all\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected\n"}
{"page": 12, "bbox": [{"x": 0.11957168579101562, "y": 0.6761984825134277}, {"x": 0.490779310464859, "y": 0.6761984825134277}, {"x": 0.490779310464859, "y": 0.9087468385696411}, {"x": 0.11957168579101562, "y": 0.9087468385696411}], "text": "C Qualitative Analysis\nAlthough DPR performs better than BM25 in gen-\neral, the retrieved passages of these two retrievers\nactually differ qualitatively. Methods like BM25\nare sensitive to highly selective keywords and\nphrases, but cannot capture lexical variations or se-\nmantic relationships well. In contrast, DPR excels\nat semantic representation, but might lack sufficient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples. In the first example, the top scor-\ning passage from BM25 is irrelevant, even though\nkeywords such as England and Ireland appear mul-\ntiple times. In comparison, DPR is able to return\n"}
{"page": 12, "bbox": [{"x": 0.4830458164215088, "y": 0.9243061542510986}, {"x": 0.5193337202072144, "y": 0.9238856434822083}, {"x": 0.5193337202072144, "y": 0.9322960376739502}, {"x": 0.4830458164215088, "y": 0.9327165484428406}], "text": "6780\n"}
{"page": 13, "bbox": [{"x": 0.13087448477745056, "y": 0.07989907264709473}, {"x": 0.17430101335048676, "y": 0.07947855442762375}, {"x": 0.17430101335048676, "y": 0.085786372423172}, {"x": 0.13087448477745056, "y": 0.08620689809322357}], "text": "Question\n"}
{"page": 13, "bbox": [{"x": 0.2938726842403412, "y": 0.07947855442762375}, {"x": 0.4259369373321533, "y": 0.0786375105381012}, {"x": 0.4259369373321533, "y": 0.08662741631269455}, {"x": 0.2938726842403412, "y": 0.0874684602022171}], "text": "Passage received by BM25\n"}
{"page": 13, "bbox": [{"x": 0.6008328199386597, "y": 0.07989907264709473}, {"x": 0.7293277978897095, "y": 0.07905802875757217}, {"x": 0.7293277978897095, "y": 0.08662741631269455}, {"x": 0.6008328199386597, "y": 0.0874684602022171}], "text": "Passage retrieved by DPR\n"}
{"page": 13, "bbox": [{"x": 0.29506245255470276, "y": 0.09419680386781693}, {"x": 0.3920285403728485, "y": 0.09545836597681046}, {"x": 0.3920285403728485, "y": 0.1034482792019844}, {"x": 0.29506245255470276, "y": 0.10218670964241028}], "text": "Title: British Cycling\n"}
{"page": 13, "bbox": [{"x": 0.6008328199386597, "y": 0.09587889164686203}, {"x": 0.6710291504859924, "y": 0.09629940986633301}, {"x": 0.6710291504859924, "y": 0.10218670964241028}, {"x": 0.6008328199386597, "y": 0.1017661914229393}], "text": "Title: Irish Sea\n"}
{"page": 13, "bbox": [{"x": 0.13027960062026978, "y": 0.09545836597681046}, {"x": 0.27305176854133606, "y": 0.09545836597681046}, {"x": 0.27305176854133606, "y": 0.11269974708557129}, {"x": 0.13027960062026978, "y": 0.11269974708557129}], "text": "What is the body of water\nbetween England and Ireland?\n"}
{"page": 13, "bbox": [{"x": 0.6002379655838013, "y": 0.10555088520050049}, {"x": 0.8875669240951538, "y": 0.10555088520050049}, {"x": 0.8875669240951538, "y": 0.17830109596252441}, {"x": 0.6002379655838013, "y": 0.17830109596252441}], "text": "...Annual traffic between Great Britain and Ireland amounts\nto over 12 million passengers and of traded goods. The Irish\nSea is connected to the North Atlantic at both its northern\nand southern ends. To the north, the connection is through\nthe North Channel between Scotland and Northern Ireland\nand the Malin Sea. The southern end is linked to the Atlantic\nthrough the St George's Channel between Ireland and Pem-\nbrokeshire, and the Celtic Sea....\n"}
{"page": 13, "bbox": [{"x": 0.2938726842403412, "y": 0.10513035953044891}, {"x": 0.5823914408683777, "y": 0.10555088520050049}, {"x": 0.5817965269088745, "y": 0.26198485493659973}, {"x": 0.2932778000831604, "y": 0.26156434416770935}], "text": "... England is not recognised as a region by the UCI, and\nthere is no English cycling team outside the Commonwealth\nGames. For those occasions, British Cycling selects and sup-\nports the England team. Cycling is represented on the Isle\nof Man by the Isle of Man Cycling Association. Cycling in\nNorthern Ireland is organised under Cycling Ulster, part of\nthe all-Ireland governing body Cycling Ireland. Until 2006,\na rival governing body existed, ...\nTitle: No One (Game of Thrones)\n... He may be \"no one,\" but there's still enough of a person\nleft in him to respect, and admire who this girl is and what\nshe's become. Arya finally tells us something that we've kind\nof known all along, that she's not no one, she's Arya Stark\nof Winterfell.\" \"No One\" saw the reintroduction of Richard\nDormer and Paul Kaye, who portrayed Beric Dondarrion and\nThoros of Myr, respectively, in the third season,...\n"}
{"page": 13, "bbox": [{"x": 0.6014277338981628, "y": 0.18671151995658875}, {"x": 0.7114812731742859, "y": 0.1875525712966919}, {"x": 0.7114812731742859, "y": 0.1947014331817627}, {"x": 0.6014277338981628, "y": 0.19386038184165955}], "text": "Title: Pål Sverre Hagen\n"}
{"page": 13, "bbox": [{"x": 0.13027960062026978, "y": 0.18797308206558228}, {"x": 0.26472339034080505, "y": 0.1875525712966919}, {"x": 0.26472339034080505, "y": 0.20311185717582703}, {"x": 0.13027960062026978, "y": 0.2035323828458786}], "text": "Who plays Thoros of Myr in\nGame of Thrones?\n"}
{"page": 13, "bbox": [{"x": 0.6002379655838013, "y": 0.19722455739974976}, {"x": 0.888161838054657, "y": 0.19722455739974976}, {"x": 0.888161838054657, "y": 0.27081581950187683}, {"x": 0.6002379655838013, "y": 0.27081581950187683}], "text": "Pål Sverre Valheim Hagen (born 6 November 1980) is a Nor-\nwegian stage and screen actor. He appeared in the Norwe-\ngian film \"Max Manus\" and played Thor Heyerdahl in the\nOscar-nominated 2012 film \"Kon-Tiki”. Pl Hagen was born\nin Stavanger, Norway, the son of Roar Hagen, a Norwegian\ncartoonist who has long been associated with Norwayś largest\ndaily, \"VG\". He lived in Jtten, a neighborhood in the city of\nStavanger in south-western Norway....\n"}
{"page": 13, "bbox": [{"x": 0.11957168579101562, "y": 0.288898229598999}, {"x": 0.8839976191520691, "y": 0.288898229598999}, {"x": 0.8839976191520691, "y": 0.3128679692745209}, {"x": 0.11957168579101562, "y": 0.3128679692745209}], "text": "Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content\nwords in the question are written in bold.\n"}
{"page": 13, "bbox": [{"x": 0.11957168579101562, "y": 0.34188392758369446}, {"x": 0.4901844263076782, "y": 0.3423044681549072}, {"x": 0.48958954215049744, "y": 0.5151387453079224}, {"x": 0.11897680163383484, "y": 0.514718234539032}], "text": "from the top 5 positive and top 30 negative passages\n(from the set of top 100 passages retrieved from\nthe same question). The question encoder's initial\nstate is taken from a DPR model previously trained\non the NQ dataset. The reader's initial state is a\nBERT-base model. In terms of the end-to-end QA\nresults, our joint-training scheme does not provide\nbetter results compared to the usual retriever/reader\ntraining pipeline, resulting in the same 39.8 exact\nmatch score on NQ dev as in our regular reader\nmodel training.\n"}
{"page": 13, "bbox": [{"x": 0.4836407005786896, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.924726665019989}, {"x": 0.518738865852356, "y": 0.9327165484428406}, {"x": 0.4836407005786896, "y": 0.9327165484428406}], "text": "6781\n"}
